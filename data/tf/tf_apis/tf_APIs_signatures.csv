API,Description,Example
"tf.CriticalSection(
    name=None, shared_name=None, critical_section_def=None, import_scope=None
)
",[],"v = resource_variable_ops.ResourceVariable(0.0, name=""v"")

def count():
  value = v.read_value()
  with tf.control_dependencies([value]):
    with tf.control_dependencies([v.assign_add(1)]):
      return tf.identity(value)
"
"tf.DeviceSpec(
    job=None, replica=None, task=None, device_type=None, device_index=None
)
",[],"# Place the operations on device ""GPU:0"" in the ""ps"" job.
device_spec = DeviceSpec(job=""ps"", device_type=""GPU"", device_index=0)
with tf.device(device_spec.to_string()):
  # Both my_var and squared_var will be placed on /job:ps/device:GPU:0.
  my_var = tf.Variable(..., name=""my_variable"")
  squared_var = tf.square(my_var)
"
"tf.GradientTape(
    persistent=False, watch_accessed_variables=True
)
",[],"x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
dy_dx = g.gradient(y, x)
print(dy_dx)
tf.Tensor(6.0, shape=(), dtype=float32)"
"tf.IndexedSlices(
    values, indices, dense_shape=None
)
",[],"dense[slices.indices[i], :, :, :, ...] = slices.values[i, :, :, :, ...]
"
"tf.IndexedSlicesSpec(
    shape=None,
    dtype=tf.dtypes.float32,
    indices_dtype=tf.dtypes.int64,
    dense_shape_dtype=None,
    indices_shape=None
)
","[['Type specification for a ', 'tf.IndexedSlices', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.Operation(
    node_def,
    g,
    inputs=None,
    output_types=None,
    control_inputs=None,
    input_types=None,
    original_op=None,
    op_def=None
)
",[],"colocation_groups()
"
"tf.OptionalSpec(
    element_spec
)
","[['Type specification for ', 'tf.experimental.Optional', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","@tf.function(input_signature=[tf.OptionalSpec(
  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))])
def maybe_square(optional):
  if optional.has_value():
    x = optional.get_value()
    return x * x
  return -1
optional = tf.experimental.Optional.from_value(5)
print(maybe_square(optional))
tf.Tensor(25, shape=(), dtype=int32)"
"tf.ragged.constant([[0], [1, 2]]).shape","[[None, '\n'], ['A ', 'RaggedTensor', ' is a tensor with one or more ', 'ragged dimensions', ', which are\ndimensions whose slices may have different lengths.  For example, the inner\n(column) dimension of ', 'rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]', ' is ragged,\nsince the column slices (', 'rt[0, :]', ', ..., ', 'rt[4, :]', ') have different lengths.\nDimensions whose slices all have the same length are called ', 'uniform\ndimensions', '.  The outermost dimension of a ', 'RaggedTensor', ' is always uniform,\nsince it consists of a single slice (and so there is no possibility for\ndiffering slice lengths).'], ['The total number of dimensions in a ', 'RaggedTensor', ' is called its ', 'rank', ',\nand the number of ragged dimensions in a ', 'RaggedTensor', ' is called its\n', 'ragged-rank', '.  A ', 'RaggedTensor', ""'s ragged-rank is fixed at graph creation\ntime: it can't depend on the runtime values of "", 'Tensor', ""s, and can't vary\ndynamically for different session runs.""], ['Note that the ', '__init__', ' constructor is private. Please use one of the\nfollowing methods to construct a ', 'RaggedTensor', ':'], ['\n', 'tf.RaggedTensor.from_row_lengths', '\n', 'tf.RaggedTensor.from_value_rowids', '\n', 'tf.RaggedTensor.from_row_splits', '\n', 'tf.RaggedTensor.from_row_starts', '\n', 'tf.RaggedTensor.from_row_limits', '\n', 'tf.RaggedTensor.from_nested_row_splits', '\n', 'tf.RaggedTensor.from_nested_row_lengths', '\n', 'tf.RaggedTensor.from_nested_value_rowids', '\n'], ['Many ops support both ', 'Tensor', 's and ', 'RaggedTensor', 's\n(see ', 'tf.ragged', ' for a\nfull listing). The term ""potentially ragged tensor"" may be used to refer to a\ntensor that might be either a ', 'Tensor', ' or a ', 'RaggedTensor', '.  The ragged-rank\nof a ', 'Tensor', ' is zero.'], ['When documenting the shape of a RaggedTensor, ragged dimensions can be\nindicated by enclosing them in parentheses.  For example, the shape of\na 3-D ', 'RaggedTensor', ' that stores the fixed-size word embedding for each\nword in a sentence, for each sentence in a batch, could be written as\n', '[num_sentences, (num_words), embedding_size]', '.  The parentheses around\n', '(num_words)', ' indicate that dimension is ragged, and that the length\nof each element list in that dimension may vary for each item.'], ['Internally, a ', 'RaggedTensor', ' consists of a concatenated list of values that\nare partitioned into variable-length rows.  In particular, each ', 'RaggedTensor', '\nconsists of:'], ['\n', 'A ', '\n', 'A ', '\n'], ['\n', 'print(tf.RaggedTensor.from_row_splits(', '\n', '      values=[3, 1, 4, 1, 5, 9, 2, 6],', '\n', '      row_splits=[0, 4, 4, 7, 8, 8]))', '\n', '<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>', '\n'], ['In addition to ', 'row_splits', ', ragged tensors provide support for five other\nrow-partitioning schemes:'], ['\n', 'row_lengths', '\n', 'value_rowids', '\n', 'row_starts', '\n', 'row_limits', '\n', 'uniform_row_length', '\n'], ['Example: The following ragged tensors are equivalent, and all represent the\nnested list ', '[[3, 1, 4, 1], [], [5, 9, 2], [6], []]', '.'], ['\n', 'values = [3, 1, 4, 1, 5, 9, 2, 6]', '\n', 'RaggedTensor.from_row_splits(values, row_splits=[0, 4, 4, 7, 8, 8])', '\n', '<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>', '\n', 'RaggedTensor.from_row_lengths(values, row_lengths=[4, 0, 3, 1, 0])', '\n', '<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>', '\n', 'RaggedTensor.from_value_rowids(', '\n', '    values, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)', '\n', '<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>', '\n', 'RaggedTensor.from_row_starts(values, row_starts=[0, 4, 4, 7, 8])', '\n', '<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>', '\n', 'RaggedTensor.from_row_limits(values, row_limits=[4, 4, 7, 8, 8])', '\n', '<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>', '\n', 'RaggedTensor.from_uniform_row_length(values, uniform_row_length=2)', '\n', '<tf.RaggedTensor [[3, 1], [4, 1], [5, 9], [2, 6]]>', '\n'], ['RaggedTensor', 's with multiple ragged dimensions can be defined by using\na nested ', 'RaggedTensor', ' for the ', 'values', ' tensor.  Each nested ', 'RaggedTensor', '\nadds a single ragged dimension.'], ['\n', 'inner_rt = RaggedTensor.from_row_splits(  # =rt1 from above', '\n', '    values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])', '\n', 'outer_rt = RaggedTensor.from_row_splits(', '\n', '    values=inner_rt, row_splits=[0, 3, 3, 5])', '\n', 'print(outer_rt.to_list())', '\n', '[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]', '\n', 'print(outer_rt.ragged_rank)', '\n', '2', '\n'], ['The factory function ', 'RaggedTensor.from_nested_row_splits', ' may be used to\nconstruct a ', 'RaggedTensor', ' with multiple ragged dimensions directly, by\nproviding a list of ', 'row_splits', ' tensors:'], ['\n', 'RaggedTensor.from_nested_row_splits(', '\n', '    flat_values=[3, 1, 4, 1, 5, 9, 2, 6],', '\n', '    nested_row_splits=([0, 3, 3, 5], [0, 4, 4, 7, 8, 8])).to_list()', '\n', '[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]', '\n'], ['RaggedTensor', 's with uniform inner dimensions can be defined\nby using a multidimensional ', 'Tensor', ' for ', 'values', '.'], ['\n', 'rt = RaggedTensor.from_row_splits(values=tf.ones([5, 3], tf.int32),', '\n', '                                  row_splits=[0, 2, 5])', '\n', 'print(rt.to_list())', '\n', '[[[1, 1, 1], [1, 1, 1]],', '\n', ' [[1, 1, 1], [1, 1, 1], [1, 1, 1]]]', '\n', 'print(rt.shape)', '\n', '(2, None, 3)', '\n'], ['RaggedTensor', 's with uniform outer dimensions can be defined by using\none or more ', 'RaggedTensor', ' with a ', 'uniform_row_length', ' row-partitioning\ntensor.  For example, a ', 'RaggedTensor', ' with shape ', '[2, 2, None]', ' can be\nconstructed with this method from a ', 'RaggedTensor', ' values with shape\n', '[4, None]', ':'], ['\n', 'values = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])', '\n', 'print(values.shape)', '\n', '(4, None)', '\n', 'rt6 = tf.RaggedTensor.from_uniform_row_length(values, 2)', '\n', 'print(rt6)', '\n', '<tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]>', '\n', 'print(rt6.shape)', '\n', '(2, 2, None)', '\n'], ['Note that ', 'rt6', ' only contains one ragged dimension (the innermost\ndimension). In contrast, if ', 'from_row_splits', ' is used to construct a similar\n', 'RaggedTensor', ', then that ', 'RaggedTensor', ' will have two ragged dimensions:'], ['\n', 'rt7 = tf.RaggedTensor.from_row_splits(values, [0, 2, 4])', '\n', 'print(rt7.shape)', '\n', '(2, None, None)', '\n'], ['Uniform and ragged outer dimensions may be interleaved, meaning that a\ntensor with any combination of ragged and uniform dimensions may be created.\nFor example, a RaggedTensor ', 't4', ' with shape ', '[3, None, 4, 8, None, 2]', ' could\nbe constructed as follows:'], ['Concretely, if ', 'rt.values', ' is a ', 'Tensor', ', then ', 'rt.flat_values', ' is\n', 'rt.values', '; otherwise, ', 'rt.flat_values', ' is ', 'rt.values.flat_values', '.'], ['Conceptually, ', 'flat_values', ' is the tensor formed by flattening the\noutermost dimension and all of the ragged dimensions into a single\ndimension.'], ['rt.flat_values.shape = [nvals] + rt.shape[rt.ragged_rank + 1:]', '\n(where ', 'nvals', ' is the number of items in the flattened dimensions).'], ['\n', 'rt = tf.ragged.constant([[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]])', '\n', 'print(rt.flat_values)', '\n', 'tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)', '\n'], ['rt.nested_row_splits', ' is a tuple containing the ', 'row_splits', ' tensors for\nall ragged dimensions in ', 'rt', ', ordered from outermost to innermost.  In\nparticular, ', 'rt.nested_row_splits = (rt.row_splits,) + value_splits', ' where:'], ['\n', 'rt = tf.ragged.constant(', '\n', '    [[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]])', '\n', 'for i, splits in enumerate(rt.nested_row_splits):', '\n', ""  print('Splits for dimension %d: %s' % (i+1, splits.numpy()))"", '\n', 'Splits for dimension 1: [0 3]', '\n', 'Splits for dimension 2: [0 3 3 5]', '\n', 'Splits for dimension 3: [0 4 4 7 8 8]', '\n'], ['\n', 'values = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])', '\n', 'values.ragged_rank', '\n', '1', '\n'], ['\n', 'rt = tf.RaggedTensor.from_uniform_row_length(values, 2)', '\n', 'rt.ragged_rank', '\n', '2', '\n'], ['rt.row_splits', ' specifies where the values for each row begin and end in\n', 'rt.values', '.  In particular, the values for row ', 'rt[i]', ' are stored in\nthe slice ', 'rt.values[rt.row_splits[i]:rt.row_splits[i+1]]', '.'], ['\n', 'rt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])', '\n', 'print(rt.row_splits)  # indices of row splits in rt.values', '\n', 'tf.Tensor([0 4 4 7 8 8], shape=(6,), dtype=int64)', '\n']]","values = [3, 1, 4, 1, 5, 9, 2, 6]
RaggedTensor.from_row_splits(values, row_splits=[0, 4, 4, 7, 8, 8])
<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>
RaggedTensor.from_row_lengths(values, row_lengths=[4, 0, 3, 1, 0])
<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>
RaggedTensor.from_value_rowids(
    values, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)
<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>
RaggedTensor.from_row_starts(values, row_starts=[0, 4, 4, 7, 8])
<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>
RaggedTensor.from_row_limits(values, row_limits=[4, 4, 7, 8, 8])
<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>
RaggedTensor.from_uniform_row_length(values, uniform_row_length=2)
<tf.RaggedTensor [[3, 1], [4, 1], [5, 9], [2, 6]]>"
"tf.RaggedTensorSpec(
    shape=None,
    dtype=tf.dtypes.float32,
    ragged_rank=None,
    row_splits_dtype=tf.dtypes.int64,
    flat_values_spec=None
)
","[['Type specification for a ', 'tf.RaggedTensor', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","rt = tf.ragged.constant([[""a""], [""b"", ""c""]], dtype=tf.string)
tf.type_spec_from_value(rt).dtype
tf.string"
"tf.RegisterGradient(
    op_type
)
",[],"@tf.RegisterGradient(""Sub"")
def _sub_grad(unused_op, grad):
  return grad, tf.negative(grad)
"
"tf.sparse.SparseTensor(
    indices, values, dense_shape
)
",[],"dense.shape = dense_shape
dense[tuple(indices[i])] = values[i]
"
"tf.SparseTensorSpec(
    shape=None,
    dtype=tf.dtypes.float32
)
","[['Type specification for a ', 'tf.sparse.SparseTensor', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.Tensor(
    op, value_index, dtype
)
","[[None, '\n'], ['A ', 'tf.Tensor', ' represents a multidimensional array of elements.']]","# Compute some values using a Tensor
c = tf.constant([[1.0, 2.0], [3.0, 4.0]])
d = tf.constant([[1.0, 1.0], [0.0, 1.0]])
e = tf.matmul(c, d)
print(e)
tf.Tensor(
[[1. 3.]
 [3. 7.]], shape=(2, 2), dtype=float32)"
"tf.TensorArray(
    dtype,
    size=None,
    dynamic_size=None,
    clear_after_read=None,
    tensor_array_name=None,
    handle=None,
    flow=None,
    infer_shape=True,
    element_shape=None,
    colocate_with_first_write_call=True,
    name=None
)
",[],"ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)
ta = ta.write(0, 10)
ta = ta.write(1, 20)
ta = ta.write(2, 30)
ta.read(0)
<tf.Tensor: shape=(), dtype=float32, numpy=10.0>
ta.read(1)
<tf.Tensor: shape=(), dtype=float32, numpy=20.0>
ta.read(2)
<tf.Tensor: shape=(), dtype=float32, numpy=30.0>
ta.stack()
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 20., 30.],
dtype=float32)>"
"tf.TensorArraySpec(
    element_shape=None,
    dtype=tf.dtypes.float32,
    dynamic_size=False,
    infer_shape=True
)
","[['Type specification for a ', 'tf.TensorArray', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.TensorShape(
    dims
)
","[['Represents the shape of a ', 'Tensor', '.'], ['Inherits From: ', 'TraceType']]","as_list()
"
"tf.TensorSpec(
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
","[['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","experimental_as_proto() -> struct_pb2.TensorSpecProto
"
"tf.Variable(
    initial_value=None,
    trainable=None,
    validate_shape=True,
    caching_device=None,
    name=None,
    variable_def=None,
    dtype=None,
    import_scope=None,
    constraint=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE,
    shape=None,
    experimental_enable_variable_lifting=True
)
","[[None, '\n'], ['See the ', 'variable guide', '.']]","v = tf.Variable(1.)
v.assign(2.)
<tf.Variable ... shape=() dtype=float32, numpy=2.0>
v.assign_add(0.5)
<tf.Variable ... shape=() dtype=float32, numpy=2.5>"
"tf.Variable.SaveSliceInfo(
    full_name=None,
    full_shape=None,
    var_offset=None,
    var_shape=None,
    save_slice_info_def=None,
    import_scope=None
)
",[],"to_proto(
    export_scope=None
)
"
"tf.math.abs(
    x, name=None
)
","[[None, '\n']]","# real number
x = tf.constant([-2.25, 3.25])
tf.abs(x)
<tf.Tensor: shape=(2,), dtype=float32,
numpy=array([2.25, 3.25], dtype=float32)>"
"tf.math.acos(
    x, name=None
)
",[],"x = tf.constant([1.0, -0.5, 3.4, 0.2, 0.0, -2], dtype = tf.float32)
tf.math.acos(x)
<tf.Tensor: shape=(6,), dtype=float32,
numpy= array([0. , 2.0943952, nan, 1.3694383, 1.5707964, nan],
dtype=float32)>"
"tf.math.acosh(
    x, name=None
)
",[],"x = tf.constant([-2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
tf.math.acosh(x) ==> [nan nan 0. 0.62236255 5.9914584 9.903487 inf]
"
"tf.math.add(
    x, y, name=None
)
",[],"x = [1, 2, 3, 4, 5]
y = 1
tf.add(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],
dtype=int32)>"
"tf.math.add_n(
    inputs, name=None
)
",[],"a = tf.constant([[3, 5], [4, 8]])
b = tf.constant([[1, 6], [2, 9]])
tf.math.add_n([a, b, a]).numpy()
array([[ 7, 16],
       [10, 25]], dtype=int32)"
"tf.approx_top_k(
    input,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    is_max_k=True,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
",[],[]
"tf.math.argmax(
    input,
    axis=None,
    output_type=tf.dtypes.int64,
    name=None
)
",[],"A = tf.constant([2, 20, 30, 3, 6])
tf.math.argmax(A)  # A[2] is maximum in tensor A
<tf.Tensor: shape=(), dtype=int64, numpy=2>
B = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],
                 [14, 45, 23, 5, 27]])
tf.math.argmax(B, 0)
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])>
tf.math.argmax(B, 1)
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])>
C = tf.constant([0, 0, 0, 0])
tf.math.argmax(C) # Returns smallest index in case of ties
<tf.Tensor: shape=(), dtype=int64, numpy=0>"
"tf.math.argmin(
    input,
    axis=None,
    output_type=tf.dtypes.int64,
    name=None
)
",[],"import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmin(input = a)
c = tf.keras.backend.eval(b)
# c = 0
# here a[0] = 1 which is the smallest element of a across axis 0
"
"tf.argsort(
    values, axis=-1, direction='ASCENDING', stable=False, name=None
)
",[],"values = [1, 10, 26.9, 2.8, 166.32, 62.3]
sort_order = tf.argsort(values)
sort_order.numpy()
array([0, 3, 1, 2, 5, 4], dtype=int32)"
"tf.dtypes.as_dtype(
    type_value
)
","[['Converts the given ', 'type_value', ' to a ', 'DType', '.']]",[]
"tf.strings.as_string(
    input,
    precision=-1,
    scientific=False,
    shortest=False,
    width=-1,
    fill='',
    name=None
)
",[],"tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)"
"tf.math.asin(
    x, name=None
)
",[],"# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]
x = tf.constant([1.047, 0.785])
y = tf.math.sin(x) # [0.8659266, 0.7068252]

tf.math.asin(y) # [1.047, 0.785] = x
"
"tf.math.asinh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.asinh(x) ==> [-inf -1.4436355 -0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf]
"
"tf.debugging.assert_equal(
    x, y, message=None, summarize=None, name=None
)
","[['Assert the condition ', 'x == y', ' holds element-wise.']]",[]
"tf.debugging.assert_greater(
    x, y, message=None, summarize=None, name=None
)
","[['Assert the condition ', 'x > y', ' holds element-wise.']]",[]
"tf.debugging.assert_less(
    x, y, message=None, summarize=None, name=None
)
","[['Assert the condition ', 'x < y', ' holds element-wise.']]",[]
"tf.debugging.assert_rank(
    x, rank, message=None, name=None
)
","[['Assert that ', 'x', ' has rank equal to ', 'rank', '.']]",[]
"tf.math.atan(
    x, name=None
)
",[],"# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]
x = tf.constant([1.047, 0.785])
y = tf.math.tan(x) # [1.731261, 0.99920404]

tf.math.atan(y) # [1.047, 0.785] = x
"
"tf.math.atan2(
    y, x, name=None
)
","[[None, '\n'], ['Computes arctangent of ', 'y/x', ' element-wise, respecting signs of the arguments.']]","x = [1., 1.]
y = [1., -1.]
print((tf.math.atan2(y,x) * (180 / np.pi)).numpy())
[ 45. -45.]"
"tf.math.atanh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -1, -0.5, 1, 0, 0.5, 10, float(""inf"")])
  tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]
"
"tf.batch_to_space(
    input, block_shape, crops, name=None
)
",[],[]
"tf.bitcast(
    input, type, name=None
)
",[],"a = [1., 2., 3.]
equality_bitcast = tf.bitcast(a, tf.complex128)
Traceback (most recent call last):
InvalidArgumentError: Cannot bitcast from 1 to 18 [Op:Bitcast]
equality_cast = tf.cast(a, tf.complex128)
print(equality_cast)
tf.Tensor([1.+0.j 2.+0.j 3.+0.j], shape=(3,), dtype=complex128)"
"tf.bitwise.bitwise_and(
    x, y, name=None
)
","[['Elementwise computes the bitwise AND of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([0, 0, 3, 10], dtype=tf.float32)

  res = bitwise_ops.bitwise_and(lhs, rhs)
  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE
"
"tf.bitwise.bitwise_or(
    x, y, name=None
)
","[['Elementwise computes the bitwise OR of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([5, 5, 7, 15], dtype=tf.float32)

  res = bitwise_ops.bitwise_or(lhs, rhs)
  tf.assert_equal(tf.cast(res,  tf.float32), exp)  # TRUE
"
"tf.bitwise.bitwise_xor(
    x, y, name=None
)
","[['Elementwise computes the bitwise XOR of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([5, 5, 4, 5],  dtype=tf.float32)

  res = bitwise_ops.bitwise_xor(lhs, rhs)
  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE
"
"tf.bitwise.invert(
    x, name=None
)
","[['Invert (flip) each bit of supported types; for example, type ', 'uint8', ' value 01010101 becomes 10101010.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops

# flip 2 (00000010) to -3 (11111101)
tf.assert_equal(-3, bitwise_ops.invert(2))

dtype_list = [dtypes.int8, dtypes.int16, dtypes.int32, dtypes.int64,
              dtypes.uint8, dtypes.uint16, dtypes.uint32, dtypes.uint64]

inputs = [0, 5, 3, 14]
for dtype in dtype_list:
  # Because of issues with negative numbers, let's test this indirectly.
  # 1. invert(a) and a = 0
  # 2. invert(a) or a = invert(0)
  input_tensor = tf.constant([0, 5, 3, 14], dtype=dtype)
  not_a_and_a, not_a_or_a, not_0 = [bitwise_ops.bitwise_and(
                                      input_tensor, bitwise_ops.invert(input_tensor)),
                                    bitwise_ops.bitwise_or(
                                      input_tensor, bitwise_ops.invert(input_tensor)),
                                    bitwise_ops.invert(
                                      tf.constant(0, dtype=dtype))]

  expected = tf.constant([0, 0, 0, 0], dtype=tf.float32)
  tf.assert_equal(tf.cast(not_a_and_a, tf.float32), expected)

  expected = tf.cast([not_0] * 4, tf.float32)
  tf.assert_equal(tf.cast(not_a_or_a, tf.float32), expected)

  # For unsigned dtypes let's also check the result directly.
  if dtype.is_unsigned:
    inverted = bitwise_ops.invert(input_tensor)
    expected = tf.constant([dtype.max - x for x in inputs], dtype=tf.float32)
    tf.assert_equal(tf.cast(inverted, tf.float32), tf.cast(expected, tf.float32))
"
"tf.bitwise.left_shift(
    x, y, name=None
)
","[['Elementwise computes the bitwise left-shift of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
import numpy as np
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]

for dtype in dtype_list:
  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)

  left_shift_result = bitwise_ops.left_shift(lhs, rhs)

  print(left_shift_result)

# This will print:
# tf.Tensor([ -32   -5 -128    0], shape=(4,), dtype=int8)
# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int16)
# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int32)
# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int64)

lhs = np.array([-2, 64, 101, 32], dtype=np.int8)
rhs = np.array([-1, -5, -3, -14], dtype=np.int8)
bitwise_ops.left_shift(lhs, rhs)
# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>
"
"tf.bitwise.right_shift(
    x, y, name=None
)
","[['Elementwise computes the bitwise right-shift of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
import numpy as np
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]

for dtype in dtype_list:
  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)

  right_shift_result = bitwise_ops.right_shift(lhs, rhs)

  print(right_shift_result)

# This will print:
# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int8)
# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int16)
# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int32)
# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int64)

lhs = np.array([-2, 64, 101, 32], dtype=np.int8)
rhs = np.array([-1, -5, -3, -14], dtype=np.int8)
bitwise_ops.right_shift(lhs, rhs)
# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>
"
"tf.boolean_mask(
    tensor, mask, axis=None, name='boolean_mask'
)
",[],"tensor = [0, 1, 2, 3]  # 1-D example
mask = np.array([True, False, True, False])
tf.boolean_mask(tensor, mask)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 2], dtype=int32)>"
"tf.broadcast_dynamic_shape(
    shape_x, shape_y
)
",[],"shape_x = (1, 2, 3)
shape_y = (5, 1, 3)
tf.broadcast_dynamic_shape(shape_x, shape_y)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 2, 3], ...>"
"tf.broadcast_static_shape(
    shape_x, shape_y
)
",[],"shape_x = tf.TensorShape([1, 2, 3])
shape_y = tf.TensorShape([5, 1 ,3])
tf.broadcast_static_shape(shape_x, shape_y)
TensorShape([5, 2, 3])"
"tf.broadcast_to(
    input, shape, name=None
)
",[],"x = tf.constant([[1, 2, 3]])   # Shape (1, 3,)
y = tf.broadcast_to(x, [2, 3])
print(y)
tf.Tensor(
    [[1 2 3]
     [1 2 3]], shape=(2, 3), dtype=int32)"
"tf.case(
    pred_fn_pairs,
    default=None,
    exclusive=False,
    strict=False,
    name='case'
)
",[],"if (x < y) return 17;
else return 23;
"
"tf.cast(
    x, dtype, name=None
)
",[],"x = tf.constant([1.8, 2.2], dtype=tf.float32)
tf.cast(x, tf.int32)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>"
"tf.clip_by_global_norm(
    t_list, clip_norm, use_norm=None, name=None
)
",[],"t_list[i] * clip_norm / max(global_norm, clip_norm)
"
"tf.clip_by_norm(
    t, clip_norm, axes=None, name=None
)
",[],"some_nums = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.float32)
tf.clip_by_norm(some_nums, 2.0).numpy()
array([[0.26967996, 0.5393599 , 0.80903983, 1.0787199 , 1.3483998 ]],
      dtype=float32)"
"tf.clip_by_value(
    t, clip_value_min, clip_value_max, name=None
)
",[],"t = tf.constant([[-10., -1., 0.], [0., 2., 10.]])
t2 = tf.clip_by_value(t, clip_value_min=-1, clip_value_max=1)
t2.numpy()
array([[-1., -1.,  0.],
       [ 0.,  1.,  1.]], dtype=float32)"
"tf.compat.as_bytes(
    bytes_or_text, encoding='utf-8'
)
","[['Converts ', 'bytearray', ', ', 'bytes', ', or unicode python input types to ', 'bytes', '.']]",[]
"tf.compat.as_str(
    bytes_or_text, encoding='utf-8'
)
",[],[]
"tf.compat.as_str_any(
    value, encoding='utf-8'
)
","[['Converts input to ', 'str', ' type.']]",[]
"tf.compat.as_text(
    bytes_or_text, encoding='utf-8'
)
",[],[]
"tf.compat.dimension_at_index(
    shape, index
)
",[],"# If you had this in your V1 code:
dim = tensor_shape[i]

# Use `dimension_at_index` as direct replacement compatible with both V1 & V2:
dim = dimension_at_index(tensor_shape, i)

# Another possibility would be this, but WARNING: it only works if the
# tensor_shape instance has a defined rank.
dim = tensor_shape.dims[i]  # `dims` may be None if the rank is undefined!

# In native V2 code, we recommend instead being more explicit:
if tensor_shape.rank is None:
  dim = Dimension(None)
else:
  dim = tensor_shape.dims[i]

# Being more explicit will save you from the following trap (present in V1):
# you might do in-place modifications to `dim` and expect them to be reflected
# in `tensor_shape[i]`, but they would not be (as the Dimension object was
# instantiated on the fly.
"
"tf.compat.dimension_value(
    dimension
)
",[],"# If you had this in your V1 code:
value = tensor_shape[i].value

# Use `dimension_value` as direct replacement compatible with both V1 & V2:
value = dimension_value(tensor_shape[i])

# This would be the V2 equivalent:
value = tensor_shape[i]  # Warning: this will return the dim value in V2!
"
"tf.compat.forward_compatible(
    year, month, day
)
",[],"def add(inputs, name=None):
  return gen_math_ops.add(inputs, name)
"
"tf.compat.path_to_str(
    path
)
","[[None, '\n'], ['Converts input which is a ', 'PathLike', ' object to ', 'str', ' type.']]","$ tf.compat.path_to_str('C:\XYZ\tensorflow\./.././tensorflow')
'C:\XYZ\tensorflow\./.././tensorflow' # Windows OS
$ tf.compat.path_to_str(Path('C:\XYZ\tensorflow\./.././tensorflow'))
'C:\XYZ\tensorflow\..\tensorflow' # Windows OS
$ tf.compat.path_to_str(Path('./corpus'))
'corpus' # Linux OS
$ tf.compat.path_to_str('./.././Corpus')
'./.././Corpus' # Linux OS
$ tf.compat.path_to_str(Path('./.././Corpus'))
'../Corpus' # Linux OS
$ tf.compat.path_to_str(Path('./..////../'))
'../..' # Linux OS

"
"tf.dtypes.complex(
    real, imag, name=None
)
","[[None, '\n']]","real = tf.constant([2.25, 3.25])
imag = tf.constant([4.75, 5.75])
tf.complex(real, imag)  # [[2.25 + 4.75j], [3.25 + 5.75j]]
"
"tf.concat(
    values, axis, name='concat'
)
",[],"[D0, D1, ... Raxis, ...Dn]
"
"tf.cond(
    pred, true_fn=None, false_fn=None, name=None
)
","[['Return ', 'true_fn()', ' if the predicate ', 'pred', ' is true else ', 'false_fn()', '.']]","z = tf.multiply(a, b)
result = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))
"
"tf.config.LogicalDevice(
    name, device_type
)
",[],[]
"tf.config.LogicalDeviceConfiguration(
    memory_limit=None,
    experimental_priority=None,
    experimental_device_ordinal=None
)
",[],[]
"tf.config.PhysicalDevice(
    name, device_type
)
",[],[]
"tf.config.LogicalDeviceConfiguration(
    memory_limit=None,
    experimental_priority=None,
    experimental_device_ordinal=None
)
",[],[]
"tf.config.experimental.enable_tensor_float_32_execution(
    enabled
)
",[],"x = tf.fill((2, 2), 1.0001)
y = tf.fill((2, 2), 1.)
# TensorFloat-32 is enabled, so matmul is run with reduced precision
print(tf.linalg.matmul(x, y))  # [[2., 2.], [2., 2.]]
tf.config.experimental.enable_tensor_float_32_execution(False)
# Matmul is run with full precision
print(tf.linalg.matmul(x, y))  # [[2.0002, 2.0002], [2.0002, 2.0002]]
"
"tf.config.experimental.get_device_details(
    device
)
",[],"gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
  details = tf.config.experimental.get_device_details(gpu_devices[0])
  details.get('device_name', 'Unknown GPU')"
"tf.config.experimental.get_memory_growth(
    device
)
","[['Get if memory growth is enabled for a ', 'PhysicalDevice', '.']]","physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.experimental.set_memory_growth(physical_devices[0], True)
  assert tf.config.experimental.get_memory_growth(physical_devices[0])
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass"
"tf.config.experimental.get_memory_info(
    device
)
",[],"if tf.config.list_physical_devices('GPU'):
  # Returns a dict in the form {'current': <current mem usage>,
  #                             'peak': <peak mem usage>}
  tf.config.experimental.get_memory_info('GPU:0')"
"tf.config.experimental.get_memory_usage(
    device
)
",[],"gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
  tf.config.experimental.get_memory_usage('GPU:0')"
"tf.config.get_logical_device_configuration(
    device
)
","[['Get the virtual device configuration for a ', 'tf.config.PhysicalDevice', '.']]","physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
configs = tf.config.get_logical_device_configuration(
  physical_devices[0])
try:
  assert configs is None
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  configs = tf.config.get_logical_device_configuration(
    physical_devices[0])
  assert len(configs) == 2
except:
  # Cannot modify virtual devices once initialized.
  pass"
"tf.config.get_visible_devices(
    device_type=None
)
",[],"physical_devices = tf.config.list_physical_devices('GPU')
try:
  # Disable all GPUS
  tf.config.set_visible_devices([], 'GPU')
  visible_devices = tf.config.get_visible_devices()
  for device in visible_devices:
    assert device.device_type != 'GPU'
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass"
"tf.config.list_logical_devices(
    device_type=None
)
",[],"logical_devices = tf.config.list_logical_devices('GPU')
if len(logical_devices) > 0:
  # Allocate on GPU:0
  with tf.device(logical_devices[0].name):
    one = tf.constant(1)
  # Allocate on GPU:1
  with tf.device(logical_devices[1].name):
    two = tf.constant(2)"
"tf.config.list_physical_devices(
    device_type=None
)
",[],"physical_devices = tf.config.list_physical_devices('GPU')
print(""Num GPUs:"", len(physical_devices))
Num GPUs: ..."
"tf.config.experimental.reset_memory_stats(
    device
)
",[],"if tf.config.list_physical_devices('GPU'):
  # Sets the peak memory to the current memory.
  tf.config.experimental.reset_memory_stats('GPU:0')
  # Creates the first peak memory usage.
  x1 = tf.ones(1000 * 1000, dtype=tf.float64)
  del x1 # Frees the memory referenced by `x1`.
  peak1 = tf.config.experimental.get_memory_info('GPU:0')['peak']
  # Sets the peak memory to the current memory again.
  tf.config.experimental.reset_memory_stats('GPU:0')
  # Creates the second peak memory usage.
  x2 = tf.ones(1000 * 1000, dtype=tf.float32)
  del x2
  peak2 = tf.config.experimental.get_memory_info('GPU:0')['peak']
  assert peak2 < peak1  # tf.float32 consumes less memory than tf.float64."
"tf.config.experimental.set_device_policy(
    device_policy
)
",[],[]
"tf.config.experimental.set_memory_growth(
    device, enable
)
","[['Set if memory growth should be enabled for a ', 'PhysicalDevice', '.']]","physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.experimental.set_memory_growth(physical_devices[0], True)
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass"
"tf.config.experimental.set_synchronous_execution(
    enable
)
",[],[]
"tf.config.set_logical_device_configuration(
    device, logical_devices
)
","[['Set the logical device configuration for a ', 'tf.config.PhysicalDevice', '.']]","physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
# Specify 2 virtual CPUs. Note currently memory limit is not supported.
try:
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  logical_devices = tf.config.list_logical_devices('CPU')
  assert len(logical_devices) == 2
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
except:
  # Cannot modify logical devices once initialized.
  pass"
"tf.config.set_visible_devices(
    devices, device_type=None
)
",[],"physical_devices = tf.config.list_physical_devices('GPU')
try:
  # Disable first GPU
  tf.config.set_visible_devices(physical_devices[1:], 'GPU')
  logical_devices = tf.config.list_logical_devices('GPU')
  # Logical device was not created for first GPU
  assert len(logical_devices) == len(physical_devices) - 1
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass"
"tf.config.experimental_connect_to_cluster(
    cluster_spec_or_resolver,
    job_name='localhost',
    task_index=0,
    protocol=None,
    make_master_device_default=True,
    cluster_device_filters=None
)
",[],"cdf = tf.config.experimental.ClusterDeviceFilters()
# For any worker, only the devices on PS nodes and itself are visible
for i in range(num_workers):
  cdf.set_device_filters('worker', i, ['/job:ps'])
# Similarly for any ps, only the devices on workers and itself are visible
for i in range(num_ps):
  cdf.set_device_filters('ps', i, ['/job:worker'])

tf.config.experimental_connect_to_cluster(cluster_def,
                                          cluster_device_filters=cdf)
"
"tf.config.experimental_connect_to_host(
    remote_host=None, job_name='worker'
)
",[],"# When eager execution is enabled, connect to the remote host.
tf.config.experimental_connect_to_host(""exampleaddr.com:9876"")

with ops.device(""job:worker/replica:0/task:1/device:CPU:0""):
  # The following tensors should be resident on the remote device, and the op
  # will also execute remotely.
  x1 = array_ops.ones([2, 2])
  x2 = array_ops.ones([2, 2])
  y = math_ops.matmul(x1, x2)
"
"tf.config.experimental_run_functions_eagerly(
    run_eagerly
)
","[['Enables / disables eager execution of ', 'tf.function', 's. (deprecated)']]",[]
"tf.config.get_logical_device_configuration(
    device
)
","[['Get the virtual device configuration for a ', 'tf.config.PhysicalDevice', '.']]","physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
configs = tf.config.get_logical_device_configuration(
  physical_devices[0])
try:
  assert configs is None
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  configs = tf.config.get_logical_device_configuration(
    physical_devices[0])
  assert len(configs) == 2
except:
  # Cannot modify virtual devices once initialized.
  pass"
"tf.config.get_visible_devices(
    device_type=None
)
",[],"physical_devices = tf.config.list_physical_devices('GPU')
try:
  # Disable all GPUS
  tf.config.set_visible_devices([], 'GPU')
  visible_devices = tf.config.get_visible_devices()
  for device in visible_devices:
    assert device.device_type != 'GPU'
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass"
"tf.config.list_logical_devices(
    device_type=None
)
",[],"logical_devices = tf.config.list_logical_devices('GPU')
if len(logical_devices) > 0:
  # Allocate on GPU:0
  with tf.device(logical_devices[0].name):
    one = tf.constant(1)
  # Allocate on GPU:1
  with tf.device(logical_devices[1].name):
    two = tf.constant(2)"
"tf.config.list_physical_devices(
    device_type=None
)
",[],"physical_devices = tf.config.list_physical_devices('GPU')
print(""Num GPUs:"", len(physical_devices))
Num GPUs: ..."
"tf.config.optimizer.get_jit() -> str
","[['Returns JIT compilation configuration for code inside ', 'tf.function', '.']]",[]
"tf.config.optimizer.set_experimental_options(
    options
)
",[],[]
"tf.config.optimizer.set_jit(
    enabled: Union[bool, str]
)
",[],[]
"tf.config.run_functions_eagerly(
    run_eagerly
)
","[['Enables / disables eager execution of ', 'tf.function', 's.']]","def my_func(a):
 print(""Python side effect"")
 return a + a
a_fn = tf.function(my_func)"
"tf.config.set_logical_device_configuration(
    device, logical_devices
)
","[['Set the logical device configuration for a ', 'tf.config.PhysicalDevice', '.']]","physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
# Specify 2 virtual CPUs. Note currently memory limit is not supported.
try:
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  logical_devices = tf.config.list_logical_devices('CPU')
  assert len(logical_devices) == 2
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
except:
  # Cannot modify logical devices once initialized.
  pass"
"tf.config.set_soft_device_placement(
    enabled
)
",[],[]
"tf.config.set_visible_devices(
    devices, device_type=None
)
",[],"physical_devices = tf.config.list_physical_devices('GPU')
try:
  # Disable first GPU
  tf.config.set_visible_devices(physical_devices[1:], 'GPU')
  logical_devices = tf.config.list_logical_devices('GPU')
  # Logical device was not created for first GPU
  assert len(logical_devices) == len(physical_devices) - 1
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass"
"tf.config.threading.set_inter_op_parallelism_threads(
    num_threads
)
",[],[]
"tf.config.threading.set_intra_op_parallelism_threads(
    num_threads
)
",[],[]
"tf.constant(
    value, dtype=None, shape=None, name='Const'
)
",[],"# Constant 1-D Tensor from a python list.
tf.constant([1, 2, 3, 4, 5, 6])
<tf.Tensor: shape=(6,), dtype=int32,
    numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>
# Or a numpy array
a = np.array([[1, 2, 3], [4, 5, 6]])
tf.constant(a)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
  array([[1, 2, 3],
         [4, 5, 6]])>"
"tf.constant_initializer(
    value=0
)
",[],"def make_variables(k, initializer):
  return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),
          tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))
v1, v2 = make_variables(3, tf.constant_initializer(2.))
v1
<tf.Variable ... shape=(3,) ... numpy=array([2., 2., 2.], dtype=float32)>
v2
<tf.Variable ... shape=(3, 3) ... numpy=
array([[2., 2., 2.],
       [2., 2., 2.],
       [2., 2., 2.]], dtype=float32)>
make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ..."
"tf.control_dependencies(
    control_inputs
)
","[['Wrapper for ', 'Graph.control_dependencies()', ' using the default graph.']]",[]
"tf.convert_to_tensor(
    value, dtype=None, dtype_hint=None, name=None
)
","[['Converts the given ', 'value', ' to a ', 'Tensor', '.']]","import numpy as np
def my_func(arg):
  arg = tf.convert_to_tensor(arg, dtype=tf.float32)
  return arg"
"tf.math.cos(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]
"
"tf.math.cosh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.cosh(x) ==> [inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf]
"
"tf.math.cumsum(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","[['Compute the cumulative sum of the tensor ', 'x', ' along ', 'axis', '.']]","# tf.cumsum([a, b, c])   # [a, a + b, a + b + c]
x = tf.constant([2, 4, 6, 8])
tf.cumsum(x)
<tf.Tensor: shape=(4,), dtype=int32,
numpy=array([ 2,  6, 12, 20], dtype=int32)>"
"tf.custom_gradient(
    f=None
)
",[],"def log1pexp(x):
  return tf.math.log(1 + tf.exp(x))
"
"tf.data.Dataset(
    variant_tensor
)
",[],"dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
for element in dataset:
  print(element)
tf.Tensor(1, shape=(), dtype=int32)
tf.Tensor(2, shape=(), dtype=int32)
tf.Tensor(3, shape=(), dtype=int32)"
"tf.data.DatasetSpec(
    element_spec, dataset_shape=()
)
","[['Type specification for ', 'tf.data.Dataset', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","dataset = tf.data.Dataset.range(3)
tf.data.DatasetSpec.from_value(dataset)
DatasetSpec(TensorSpec(shape=(), dtype=tf.int64, name=None), TensorShape([]))"
"tf.data.FixedLengthRecordDataset(
    filenames,
    record_bytes,
    header_bytes=None,
    footer_bytes=None,
    buffer_size=None,
    compression_type=None,
    num_parallel_reads=None,
    name=None
)
","[['A ', 'Dataset', ' of fixed-length records from one or more binary files.'], ['Inherits From: ', 'Dataset']]","with open('/tmp/fixed_length0.bin', 'wb') as f:
  f.write(b'HEADER012345FOOTER')
with open('/tmp/fixed_length1.bin', 'wb') as f:
  f.write(b'HEADER6789abFOOTER')"
"tf.data.IteratorSpec(
    element_spec
)
","[['Type specification for ', 'tf.data.Iterator', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","@tf.function(input_signature=[tf.data.IteratorSpec(
  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))])
def square(iterator):
  x = iterator.get_next()
  return x * x
dataset = tf.data.Dataset.from_tensors(5)
iterator = iter(dataset)
print(square(iterator))
tf.Tensor(25, shape=(), dtype=int32)"
"tf.data.TFRecordDataset(
    filenames,
    compression_type=None,
    buffer_size=None,
    num_parallel_reads=None,
    name=None
)
","[['A ', 'Dataset', ' comprising records from one or more TFRecord files.'], ['Inherits From: ', 'Dataset']]","import tempfile
example_path = os.path.join(tempfile.gettempdir(), ""example.tfrecords"")
np.random.seed(0)"
"tf.data.TextLineDataset(
    filenames,
    compression_type=None,
    buffer_size=None,
    num_parallel_reads=None,
    name=None
)
","[['Creates a ', 'Dataset', ' comprising lines from one or more text files.'], ['Inherits From: ', 'Dataset']]","with open('/tmp/text_lines0.txt', 'w') as f:
  f.write('the cow\n')
  f.write('jumped over\n')
  f.write('the moon\n')
with open('/tmp/text_lines1.txt', 'w') as f:
  f.write('jack and jill\n')
  f.write('went up\n')
  f.write('the hill\n')"
"tf.data.experimental.CheckpointInputPipelineHook(
    estimator, external_state_policy=None
)
","[['Inherits From: ', 'SessionRunHook']]","est = tf.estimator.Estimator(model_fn)
while True:
  est.train(
      train_input_fn,
      hooks=[tf.data.experimental.CheckpointInputPipelineHook(est)],
      steps=train_steps_per_eval)
  # Note: We do not pass the hook here.
  metrics = est.evaluate(eval_input_fn)
  if should_stop_the_training(metrics):
    break
"
"tf.data.experimental.Counter(
    start=0,
    step=1,
    dtype=tf.dtypes.int64
)
","[['Creates a ', 'Dataset', ' that counts from ', 'start', ' in steps of size ', 'step', '. (deprecated)']]","dataset = tf.data.experimental.Counter().take(5)
list(dataset.as_numpy_iterator())
[0, 1, 2, 3, 4]
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int64, name=None)
dataset = tf.data.experimental.Counter(dtype=tf.int32)
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int32, name=None)
dataset = tf.data.experimental.Counter(start=2).take(5)
list(dataset.as_numpy_iterator())
[2, 3, 4, 5, 6]
dataset = tf.data.experimental.Counter(start=2, step=5).take(5)
list(dataset.as_numpy_iterator())
[2, 7, 12, 17, 22]
dataset = tf.data.experimental.Counter(start=10, step=-1).take(5)
list(dataset.as_numpy_iterator())
[10, 9, 8, 7, 6]"
"tf.data.experimental.CsvDataset(
    filenames,
    record_defaults,
    compression_type=None,
    buffer_size=None,
    header=False,
    field_delim=',',
    use_quote_delim=True,
    na_value='',
    select_cols=None,
    exclude_cols=None
)
","[['Inherits From: ', 'Dataset']]","with open('/tmp/my_file0.csv', 'w') as f:
  f.write('abcdefg,4.28E10,5.55E6,12\n')
  f.write('hijklmn,-5.3E14,,2\n')"
"tf.data.experimental.DatasetInitializer(
    dataset
)
","[['Creates a table initializer from a ', 'tf.data.Dataset', '.']]","keys = tf.data.Dataset.range(100)
values = tf.data.Dataset.range(100).map(
    lambda x: tf.strings.as_string(x * 2))
ds = tf.data.Dataset.zip((keys, values))
init = tf.data.experimental.DatasetInitializer(ds)
table = tf.lookup.StaticHashTable(init, """")
table.lookup(tf.constant([0, 1, 2], dtype=tf.int64)).numpy()
array([b'0', b'2', b'4'], dtype=object)"
"tf.data.experimental.RandomDataset(
    seed=None, name=None
)
","[['A ', 'Dataset', ' of pseudorandom values. (deprecated)'], ['Inherits From: ', 'Dataset']]","dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int32, name=None)"
"tf.data.experimental.Reducer(
    init_func, reduce_func, finalize_func
)
",[],"def init_func(_):
  return (0.0, 0.0)

def reduce_func(state, value):
  return (state[0] + value['features'], state[1] + 1)

def finalize_func(s, n):
  return s / n

reducer = tf.data.experimental.Reducer(init_func, reduce_func, finalize_func)
"
"tf.data.experimental.SqlDataset(
    driver_name, data_source_name, query, output_types
)
","[['A ', 'Dataset', ' consisting of the results from a SQL query.'], ['Inherits From: ', 'Dataset']]","dataset = tf.data.experimental.SqlDataset(""sqlite"", ""/foo/bar.sqlite3"",
                                          ""SELECT name, age FROM people"",
                                          (tf.string, tf.int32))
# Prints the rows of the result set of the above query.
for element in dataset:
  print(element)
"
"tf.data.experimental.TFRecordWriter(
    filename, compression_type=None
)
",[],"dataset = tf.data.Dataset.range(3)
dataset = dataset.map(tf.io.serialize_tensor)
writer = tf.data.experimental.TFRecordWriter(""/path/to/file.tfrecord"")
writer.write(dataset)
"
"tf.data.experimental.assert_cardinality(
    expected_cardinality
)
",[],"dataset = tf.data.TFRecordDataset(""examples.tfrecord"")
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.UNKNOWN_CARDINALITY).numpy())
True
dataset = dataset.apply(tf.data.experimental.assert_cardinality(42))
print(tf.data.experimental.cardinality(dataset).numpy())
42"
"tf.data.experimental.bucket_by_sequence_length(
    element_length_func,
    bucket_boundaries,
    bucket_batch_sizes,
    padded_shapes=None,
    padding_values=None,
    pad_to_bucket_boundary=False,
    no_padding=False,
    drop_remainder=False
)
","[['A transformation that buckets elements in a ', 'Dataset', ' by length. (deprecated)']]","elements = [
  [0], [1, 2, 3, 4], [5, 6, 7],
  [7, 8, 9, 10, 11], [13, 14, 15, 16, 19, 20], [21, 22]]"
"tf.data.experimental.cardinality(
    dataset
)
","[['Returns the cardinality of ', 'dataset', ', if known.']]","dataset = tf.data.Dataset.range(42)
print(tf.data.experimental.cardinality(dataset).numpy())
42
dataset = dataset.repeat()
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.INFINITE_CARDINALITY).numpy())
True
dataset = dataset.filter(lambda x: True)
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.UNKNOWN_CARDINALITY).numpy())
True"
"tf.data.experimental.choose_from_datasets(
    datasets, choice_dataset, stop_on_empty_dataset=False
)
","[['Creates a dataset that deterministically chooses elements from ', 'datasets', '. (deprecated)']]","datasets = [tf.data.Dataset.from_tensors(""foo"").repeat(),
            tf.data.Dataset.from_tensors(""bar"").repeat(),
            tf.data.Dataset.from_tensors(""baz"").repeat()]

# Define a dataset containing `[0, 1, 2, 0, 1, 2, 0, 1, 2]`.
choice_dataset = tf.data.Dataset.range(3).repeat(3)

result = tf.data.experimental.choose_from_datasets(datasets, choice_dataset)
"
"tf.data.experimental.copy_to_device(
    target_device, source_device='/cpu:0'
)
","[['A transformation that copies dataset elements to the given ', 'target_device', '.']]",[]
"tf.data.experimental.dense_to_ragged_batch(
    batch_size,
    drop_remainder=False,
    row_splits_dtype=tf.dtypes.int64
)
","[['A transformation that batches ragged elements into ', 'tf.RaggedTensor', 's.']]","dataset = tf.data.Dataset.from_tensor_slices(np.arange(6))
dataset = dataset.map(lambda x: tf.range(x))
dataset.element_spec.shape
TensorShape([None])
dataset = dataset.apply(
    tf.data.experimental.dense_to_ragged_batch(batch_size=2))
for batch in dataset:
  print(batch)
<tf.RaggedTensor [[], [0]]>
<tf.RaggedTensor [[0, 1], [0, 1, 2]]>
<tf.RaggedTensor [[0, 1, 2, 3], [0, 1, 2, 3, 4]]>"
"tf.data.experimental.dense_to_sparse_batch(
    batch_size, row_shape
)
","[['A transformation that batches ragged elements into ', 'tf.sparse.SparseTensor', 's.']]","# NOTE: The following examples use `{ ... }` to represent the
# contents of a dataset.
a = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] }

a.apply(tf.data.experimental.dense_to_sparse_batch(
    batch_size=2, row_shape=[6])) ==
{
    ([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # indices
     ['a', 'b', 'c', 'a', 'b'],                 # values
     [2, 6]),                                   # dense_shape
    ([[0, 0], [0, 1], [0, 2], [0, 3]],
     ['a', 'b', 'c', 'd'],
     [1, 6])
}
"
"tf.data.experimental.enumerate_dataset(
    start=0
)
",[],"# NOTE: The following examples use `{ ... }` to represent the
# contents of a dataset.
a = { 1, 2, 3 }
b = { (7, 8), (9, 10) }

# The nested structure of the `datasets` argument determines the
# structure of elements in the resulting dataset.
a.apply(tf.data.experimental.enumerate_dataset(start=5))
=> { (5, 1), (6, 2), (7, 3) }
b.apply(tf.data.experimental.enumerate_dataset())
=> { (0, (7, 8)), (1, (9, 10)) }
"
"tf.data.experimental.from_list(
    elements, name=None
)
","[['Creates a ', 'Dataset', ' comprising the given list of elements.']]","dataset = tf.data.experimental.from_list([(1, 'a'), (2, 'b'), (3, 'c')])
list(dataset.as_numpy_iterator())
[(1, b'a'), (2, b'b'), (3, b'c')]"
"tf.data.experimental.from_variant(
    variant, structure
)
",[],[]
"tf.data.experimental.get_next_as_optional(
    iterator
)
","[['Returns a ', 'tf.experimental.Optional', ' with the next element of the iterator. (deprecated)']]",[]
"tf.data.experimental.get_single_element(
    dataset
)
","[['Returns the single element of the ', 'dataset', ' as a nested structure of tensors. (deprecated)']]","def preprocessing_fn(raw_feature):
  # ... the raw_feature is preprocessed as per the use-case
  return feature

raw_features = ...  # input batch of BATCH_SIZE elements.
dataset = (tf.data.Dataset.from_tensor_slices(raw_features)
           .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)
           .batch(BATCH_SIZE))

processed_features = tf.data.experimental.get_single_element(dataset)
"
"tf.data.experimental.get_structure(
    dataset_or_iterator
)
",[],"dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
tf.data.experimental.get_structure(dataset)
TensorSpec(shape=(), dtype=tf.int32, name=None)"
"tf.data.experimental.group_by_reducer(
    key_func, reducer
)
",[],[]
"tf.data.experimental.group_by_window(
    key_func, reduce_func, window_size=None, window_size_func=None
)
",[],[]
"tf.data.experimental.ignore_errors(
    log_warning=False
)
","[['Creates a ', 'Dataset', ' from another ', 'Dataset', ' and silently ignores any errors. (deprecated)']]","dataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])

# Computing `tf.debugging.check_numerics(1. / 0.)` will raise an
InvalidArgumentError.
dataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, ""error""))

# Using `ignore_errors()` will drop the element that causes an error.
dataset =
    dataset.apply(tf.data.experimental.ignore_errors())  # ==> {1., 0.5, 0.2}
"
"tf.data.experimental.index_table_from_dataset(
    dataset=None,
    num_oov_buckets=0,
    vocab_size=None,
    default_value=-1,
    hasher_spec=lookup_ops.FastHashSpec,
    key_dtype=tf.dtypes.string,
    name=None
)
",[],"ds = tf.data.Dataset.range(100).map(lambda x: tf.strings.as_string(x * 2))
table = tf.data.experimental.index_table_from_dataset(
                                    ds, key_dtype=dtypes.int64)
table.lookup(tf.constant(['0', '2', '4'], dtype=tf.string)).numpy()
array([0, 1, 2])"
"tf.data.experimental.load(
    path, element_spec=None, compression=None, reader_func=None
)
",[],"import tempfile
path = os.path.join(tempfile.gettempdir(), ""saved_data"")
# Save a dataset
dataset = tf.data.Dataset.range(2)
tf.data.experimental.save(dataset, path)
new_dataset = tf.data.experimental.load(path)
for elem in new_dataset:
  print(elem)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)"
"tf.data.experimental.make_batched_features_dataset(
    file_pattern,
    batch_size,
    features,
    reader=None,
    label_key=None,
    reader_args=None,
    num_epochs=None,
    shuffle=True,
    shuffle_buffer_size=10000,
    shuffle_seed=None,
    prefetch_buffer_size=None,
    reader_num_threads=None,
    parser_num_threads=None,
    sloppy_ordering=False,
    drop_final_batch=False
)
","[['Returns a ', 'Dataset', ' of feature dictionaries from ', 'Example', ' protos.']]","serialized_examples = [
  features {
    feature { key: ""age"" value { int64_list { value: [ 0 ] } } }
    feature { key: ""gender"" value { bytes_list { value: [ ""f"" ] } } }
    feature { key: ""kws"" value { bytes_list { value: [ ""code"", ""art"" ] } } }
  },
  features {
    feature { key: ""age"" value { int64_list { value: [] } } }
    feature { key: ""gender"" value { bytes_list { value: [ ""f"" ] } } }
    feature { key: ""kws"" value { bytes_list { value: [ ""sports"" ] } } }
  }
]
"
"tf.data.experimental.make_csv_dataset(
    file_pattern,
    batch_size,
    column_names=None,
    column_defaults=None,
    label_name=None,
    select_columns=None,
    field_delim=',',
    use_quote_delim=True,
    na_value='',
    header=True,
    num_epochs=None,
    shuffle=True,
    shuffle_buffer_size=10000,
    shuffle_seed=None,
    prefetch_buffer_size=None,
    num_parallel_reads=None,
    sloppy=False,
    num_rows_for_inference=100,
    compression_type=None,
    ignore_errors=False,
    encoding='utf-8'
)
",[],"# No label column specified
dataset = tf.data.experimental.make_csv_dataset(filename, batch_size=2)
iterator = dataset.as_numpy_iterator()
print(dict(next(iterator)))
# prints a dictionary of batched features:
# OrderedDict([('Feature_A', array([1, 4], dtype=int32)),
#              ('Feature_B', array([b'a', b'd'], dtype=object))])
"
"tf.data.experimental.make_saveable_from_iterator(
    iterator, external_state_policy=None
)
",[],"with tf.Graph().as_default():
  ds = tf.data.Dataset.range(10)
  iterator = ds.make_initializable_iterator()
  # Build the iterator SaveableObject.
  saveable_obj = tf.data.experimental.make_saveable_from_iterator(iterator)
  # Add the SaveableObject to the SAVEABLE_OBJECTS collection so
  # it can be automatically saved using Saver.
  tf.compat.v1.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable_obj)
  saver = tf.compat.v1.train.Saver()

  while continue_training:
    ... Perform training ...
    if should_save_checkpoint:
      saver.save()
"
"tf.data.experimental.map_and_batch(
    map_func,
    batch_size,
    num_parallel_batches=None,
    drop_remainder=False,
    num_parallel_calls=None
)
","[['Fused implementation of ', 'map', ' and ', 'batch', '. (deprecated)']]",[]
"tf.data.experimental.parallel_interleave(
    map_func,
    cycle_length,
    block_length=1,
    sloppy=False,
    buffer_output_elements=None,
    prefetch_input_elements=None
)
","[['A parallel version of the ', 'Dataset.interleave()', ' transformation. (deprecated)']]","# Preprocess 4 files concurrently.
filenames = tf.data.Dataset.list_files(""/path/to/data/train*.tfrecords"")
dataset = filenames.apply(
    tf.data.experimental.parallel_interleave(
        lambda filename: tf.data.TFRecordDataset(filename),
        cycle_length=4))
"
"tf.data.experimental.parse_example_dataset(
    features, num_parallel_calls=1, deterministic=None
)
","[['A transformation that parses ', 'Example', ' protos into a ', 'dict', ' of tensors.']]",[]
"tf.data.experimental.prefetch_to_device(
    device, buffer_size=None
)
","[['A transformation that prefetches dataset values to the given ', 'device', '.']]",">>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
>>> dataset = dataset.apply(tf.data.experimental.prefetch_to_device(""/cpu:0""))
>>> for element in dataset:
...   print(f'Tensor {element} is on device {element.device}')
Tensor 1 is on device /job:localhost/replica:0/task:0/device:CPU:0
Tensor 2 is on device /job:localhost/replica:0/task:0/device:CPU:0
Tensor 3 is on device /job:localhost/replica:0/task:0/device:CPU:0
"
"tf.data.experimental.rejection_resample(
    class_func, target_dist, initial_dist=None, seed=None
)
",[],[]
"tf.data.experimental.sample_from_datasets(
    datasets, weights=None, seed=None, stop_on_empty_dataset=False
)
","[['Samples elements at random from the datasets in ', 'datasets', '. (deprecated)']]","dataset1 = tf.data.Dataset.range(0, 3)
dataset2 = tf.data.Dataset.range(100, 103)
"
"tf.data.experimental.save(
    dataset, path, compression=None, shard_func=None, checkpoint_args=None
)
",[],"import tempfile
path = os.path.join(tempfile.gettempdir(), ""saved_data"")
# Save a dataset
dataset = tf.data.Dataset.range(2)
tf.data.experimental.save(dataset, path)
new_dataset = tf.data.experimental.load(path)
for elem in new_dataset:
  print(elem)
tf.Tensor(0, shape=(), dtype=int64)
tf.Tensor(1, shape=(), dtype=int64)"
"tf.data.experimental.scan(
    initial_state, scan_func
)
",[],[]
"tf.data.experimental.service.CrossTrainerCache(
    trainer_id
)
",[],"dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=tf.data.experimental.service.ShardingPolicy.OFF,
    service=FLAGS.tf_data_service_address,
    job_name=""job"",
    cross_trainer_cache=data_service_ops.CrossTrainerCache(
        trainer_id=trainer_id())))
"
"tf.data.experimental.service.DispatchServer(
    config=None, start=True
)
",[],"dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
    dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=""parallel_epochs"", service=dispatcher.target))
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.service.DispatcherConfig(
    port=0,
    protocol=None,
    work_dir=None,
    fault_tolerant_mode=False,
    worker_addresses=None,
    job_gc_check_interval_ms=None,
    job_gc_timeout_ms=None
)
",[],[]
"tf.data.experimental.service.WorkerConfig(
    dispatcher_address,
    worker_address=None,
    port=0,
    protocol=None,
    heartbeat_interval_ms=None,
    dispatcher_timeout_ms=None,
    data_transfer_protocol=None,
    data_transfer_address=None
)
",[],[]
"tf.data.experimental.service.WorkerServer(
    config, start=True
)
",[],"dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=""parallel_epochs"", service=dispatcher.target))
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.service.distribute(
    processing_mode,
    service,
    job_name=None,
    consumer_index=None,
    num_consumers=None,
    max_outstanding_requests=None,
    data_transfer_protocol=None,
    compression='AUTO',
    cross_trainer_cache=None,
    target_workers='AUTO'
)
",[],"dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
# Start two workers
workers = [
    tf.data.experimental.service.WorkerServer(
        tf.data.experimental.service.WorkerConfig(
            dispatcher_address=dispatcher_address)) for _ in range(2)
]
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=""parallel_epochs"", service=dispatcher.target))
print(sorted(list(dataset.as_numpy_iterator())))
[0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9]"
"tf.data.experimental.service.from_dataset_id(
    processing_mode,
    service,
    dataset_id,
    element_spec=None,
    job_name=None,
    consumer_index=None,
    num_consumers=None,
    max_outstanding_requests=None,
    data_transfer_protocol=None,
    cross_trainer_cache=None,
    target_workers='AUTO'
)
",[],"dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset_id = tf.data.experimental.service.register_dataset(
    dispatcher.target, dataset)
dataset = tf.data.experimental.service.from_dataset_id(
    processing_mode=""parallel_epochs"",
    service=dispatcher.target,
    dataset_id=dataset_id,
    element_spec=dataset.element_spec)
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.service.register_dataset(
    service, dataset, compression='AUTO', dataset_id=None
)
",[],"dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset_id = tf.data.experimental.service.register_dataset(
    dispatcher.target, dataset)
dataset = tf.data.experimental.service.from_dataset_id(
    processing_mode=""parallel_epochs"",
    service=dispatcher.target,
    dataset_id=dataset_id,
    element_spec=dataset.element_spec)
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.shuffle_and_repeat(
    buffer_size, count=None, seed=None
)
",[],"d = tf.data.Dataset.from_tensor_slices([1, 2, 3])
d = d.apply(tf.data.experimental.shuffle_and_repeat(2, count=2))
[elem.numpy() for elem in d] # doctest: +SKIP
[2, 3, 1, 1, 3, 2]"
"tf.data.experimental.snapshot(
    path, compression='AUTO', reader_func=None, shard_func=None
)
",[],"dataset = ...
dataset = dataset.enumerate()
dataset = dataset.apply(tf.data.experimental.snapshot(""/path/to/snapshot/dir"",
    shard_func=lambda x, y: x % NUM_SHARDS, ...))
dataset = dataset.map(lambda x, y: y)
"
"tf.data.experimental.table_from_dataset(
    dataset=None,
    num_oov_buckets=0,
    vocab_size=None,
    default_value=None,
    hasher_spec=lookup_ops.FastHashSpec,
    key_dtype=tf.dtypes.string,
    name=None
)
",[],"keys = tf.data.Dataset.range(100)
values = tf.data.Dataset.range(100).map(
    lambda x: tf.strings.as_string(x * 2))
ds = tf.data.Dataset.zip((keys, values))
table = tf.data.experimental.table_from_dataset(
                              ds, default_value='n/a', key_dtype=tf.int64)
table.lookup(tf.constant([0, 1, 2], dtype=tf.int64)).numpy()
array([b'0', b'2', b'4'], dtype=object)"
"tf.data.experimental.take_while(
    predicate
)
","[['A transformation that stops dataset iteration based on a ', 'predicate', '. (deprecated)']]",[]
"tf.data.experimental.to_variant(
    dataset
)
",[],[]
"tf.device(
    device_name
)
",[],"with tf.device('/job:foo'):
  # ops created here have devices with /job:foo
  with tf.device('/job:bar/task:0/device:gpu:2'):
    # ops created here have the fully specified device above
  with tf.device('/device:gpu:1'):
    # ops created here have the device '/job:foo/device:gpu:1'
"
"tf.distribute.HierarchicalCopyAllReduce(
    num_packs=1
)
","[['Inherits From: ', 'CrossDeviceOps']]","  strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())
"
"tf.distribute.InputContext(
    num_input_pipelines=1, input_pipeline_id=0, num_replicas_in_sync=1
)
",[],"get_per_replica_batch_size(
    global_batch_size
)
"
"tf.distribute.InputOptions(
    experimental_fetch_to_device=None,
    experimental_replication_mode=tf.distribute.InputReplicationMode.PER_WORKER,
    experimental_place_dataset_on_device=False,
    experimental_per_replica_buffer_size=1
)
","[['Run options for ', 'experimental_distribute_dataset(s_from_function)', '.']]","# Setup TPUStrategy
resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)

dataset = tf.data.Dataset.range(16)
distributed_dataset_on_host = (
    strategy.experimental_distribute_dataset(
        dataset,
        tf.distribute.InputOptions(
            experimental_replication_mode=
            experimental_replication_mode.PER_WORKER,
            experimental_place_dataset_on_device=False,
            experimental_per_replica_buffer_size=1)))
"
"tf.distribute.MirroredStrategy(
    devices=None, cross_device_ops=None
)
","[['Inherits From: ', 'Strategy']]","strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1""])
with strategy.scope():
  x = tf.Variable(1.)
x
MirroredVariable:{
  0: <tf.Variable ... shape=() dtype=float32, numpy=1.0>,
  1: <tf.Variable ... shape=() dtype=float32, numpy=1.0>
}"
"tf.distribute.MultiWorkerMirroredStrategy(
    cluster_resolver=None, communication_options=None
)
","[['Inherits From: ', 'Strategy']]","TF_CONFIG = '{""cluster"": {""worker"": [""localhost:12345"", ""localhost:23456""]}, ""task"": {""type"": ""worker"", ""index"": 0} }'
"
"tf.distribute.NcclAllReduce(
    num_packs=1
)
","[['Inherits From: ', 'CrossDeviceOps']]","  strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.NcclAllReduce())
"
"tf.distribute.OneDeviceStrategy(
    device
)
","[['Inherits From: ', 'Strategy']]","strategy = tf.distribute.OneDeviceStrategy(device=""/gpu:0"")

with strategy.scope():
  v = tf.Variable(1.0)
  print(v.device)  # /job:localhost/replica:0/task:0/device:GPU:0

def step_fn(x):
  return x * 2

result = 0
for i in range(10):
  result += strategy.run(step_fn, args=(i,))
print(result)  # 90
"
"tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver, variable_partitioner=None
)
","[['Inherits From: ', 'Strategy']]","# Prepare a strategy to use with the cluster and variable partitioning info.
strategy = tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver=...,
    variable_partitioner=...)
coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(
    strategy=strategy)

# Prepare a distribute dataset that will place datasets on the workers.
distributed_dataset = coordinator.create_per_worker_dataset(dataset_fn=...)

with strategy.scope():
  model = ...
  optimizer, metrics = ...  # Keras optimizer/metrics are great choices
  checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
  checkpoint_manager = tf.train.CheckpointManager(
      checkpoint, checkpoint_dir, max_to_keep=2)
  # `load_checkpoint` infers initial epoch from `optimizer.iterations`.
  initial_epoch = load_checkpoint(checkpoint_manager) or 0

@tf.function
def worker_fn(iterator):

  def replica_fn(inputs):
    batch_data, labels = inputs
    # calculate gradient, applying gradient, metrics update etc.

  strategy.run(replica_fn, args=(next(iterator),))

for epoch in range(initial_epoch, num_epoch):
  distributed_iterator = iter(distributed_dataset)  # Reset iterator state.
  for step in range(steps_per_epoch):

    # Asynchronously schedule the `worker_fn` to be executed on an arbitrary
    # worker. This call returns immediately.
    coordinator.schedule(worker_fn, args=(distributed_iterator,))

  # `join` blocks until all scheduled `worker_fn`s finish execution. Once it
  # returns, we can read the metrics and save checkpoints as needed.
  coordinator.join()
  logging.info('Metric result: %r', metrics.result())
  train_accuracy.reset_states()
  checkpoint_manager.save()
"
"tf.distribute.ReductionToOneDevice(
    reduce_to_device=None, accumulation_fn=None
)
","[['Inherits From: ', 'CrossDeviceOps']]","  strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.ReductionToOneDevice())
"
"tf.distribute.ReplicaContext(
    strategy, replica_id_in_sync_group
)
",[],"strategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1'])
def func():
  replica_context = tf.distribute.get_replica_context()
  return replica_context.replica_id_in_sync_group
strategy.run(func)
PerReplica:{
  0: <tf.Tensor: shape=(), dtype=int32, numpy=0>,
  1: <tf.Tensor: shape=(), dtype=int32, numpy=1>
}"
"tf.distribute.RunOptions(
    experimental_enable_dynamic_batch_size=True,
    experimental_bucketizing_dynamic_shape=False,
    experimental_xla_options=None
)
","[['Run options for ', 'strategy.run', '.']]",[]
"tf.distribute.Server(
    server_or_cluster_def,
    job_name=None,
    task_index=None,
    protocol=None,
    config=None,
    start=True
)
",[],"server = tf.distribute.Server(...)
with tf.compat.v1.Session(server.target):
  # ...
"
"tf.distribute.Strategy(
    extended
)
",[],"with my_strategy.scope():
  @tf.function
  def distribute_train_epoch(dataset):
    def replica_fn(input):
      # process input and return result
      return result

    total_result = 0
    for x in dataset:
      per_replica_result = my_strategy.run(replica_fn, args=(x,))
      total_result += my_strategy.reduce(tf.distribute.ReduceOp.SUM,
                                         per_replica_result, axis=None)
    return total_result

  dist_dataset = my_strategy.experimental_distribute_dataset(dataset)
  for _ in range(EPOCHS):
    train_result = distribute_train_epoch(dist_dataset)
"
"tf.distribute.StrategyExtended(
    container_strategy
)
",[],"batch_reduce_to(
    reduce_op, value_destination_pairs, options=None
)
"
"tf.distribute.TPUStrategy(
    tpu_cluster_resolver=None,
    experimental_device_assignment=None,
    experimental_spmd_xla_partitioning=False
)
","[['Inherits From: ', 'Strategy']]","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.TPUStrategy(resolver)"
"tf.distribute.cluster_resolver.GCEClusterResolver(
    project,
    zone,
    instance_group,
    port,
    task_type='worker',
    task_id=0,
    rpc_layer='grpc',
    credentials='default',
    service=None
)
","[['Inherits From: ', 'ClusterResolver']]","  # On worker 0
  cluster_resolver = GCEClusterResolver(""my-project"", ""us-west1"",
                                        ""my-instance-group"",
                                        task_type=""worker"", task_id=0)
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)

  # On worker 1
  cluster_resolver = GCEClusterResolver(""my-project"", ""us-west1"",
                                        ""my-instance-group"",
                                        task_type=""worker"", task_id=1)
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)
"
"tf.distribute.cluster_resolver.KubernetesClusterResolver(
    job_to_label_mapping=None,
    tf_server_port=8470,
    rpc_layer='grpc',
    override_client=None
)
","[['Inherits From: ', 'ClusterResolver']]","  # On worker 0
  cluster_resolver = KubernetesClusterResolver(
      {""worker"": [""job-name=worker-cluster-a"", ""job-name=worker-cluster-b""]})
  cluster_resolver.task_type = ""worker""
  cluster_resolver.task_id = 0
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)

  # On worker 1
  cluster_resolver = KubernetesClusterResolver(
      {""worker"": [""job-name=worker-cluster-a"", ""job-name=worker-cluster-b""]})
  cluster_resolver.task_type = ""worker""
  cluster_resolver.task_id = 1
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)
"
"tf.distribute.cluster_resolver.SimpleClusterResolver(
    cluster_spec,
    master='',
    task_type=None,
    task_id=None,
    environment='',
    num_accelerators=None,
    rpc_layer=None
)
","[['Inherits From: ', 'ClusterResolver']]","  cluster = tf.train.ClusterSpec({""worker"": [""worker0.example.com:2222"",
                                             ""worker1.example.com:2222""]})

  # On worker 0
  cluster_resolver = SimpleClusterResolver(cluster, task_type=""worker"",
                                           task_id=0,
                                           num_accelerators={""GPU"": 8},
                                           rpc_layer=""grpc"")
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)

  # On worker 1
  cluster_resolver = SimpleClusterResolver(cluster, task_type=""worker"",
                                           task_id=1,
                                           num_accelerators={""GPU"": 8},
                                           rpc_layer=""grpc"")
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)
"
"tf.distribute.cluster_resolver.SlurmClusterResolver(
    jobs=None,
    port_base=8888,
    gpus_per_node=None,
    gpus_per_task=None,
    tasks_per_node=None,
    auto_set_gpu=True,
    rpc_layer='grpc'
)
","[['Inherits From: ', 'ClusterResolver']]","cluster_spec = tf.train.ClusterSpec({
    ""ps"": [""localhost:2222"", ""localhost:2223""],
    ""worker"": [""localhost:2224"", ""localhost:2225"", ""localhost:2226""]
})

# SimpleClusterResolver is used here for illustration; other cluster
# resolvers may be used for other source of task type/id.
simple_resolver = SimpleClusterResolver(cluster_spec, task_type=""worker"",
                                        task_id=0)

...

if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:
  # Perform something that's only applicable on 'worker' type, id 0. This
  # block will run on this particular instance since we've specified this
  # task to be a 'worker', id 0 in above cluster resolver.
else:
  # Perform something that's only applicable on other ids. This block will
  # not run on this particular instance.
"
"tf.distribute.cluster_resolver.TFConfigClusterResolver(
    task_type=None, task_id=None, rpc_layer=None, environment=None
)
","[['Inherits From: ', 'ClusterResolver']]","  os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': [""localhost:12345"", ""localhost:23456""]
    },
    'task': {'type': 'worker', 'index': 0}
  })
"
"tf.distribute.cluster_resolver.TPUClusterResolver(
    tpu=None,
    zone=None,
    project=None,
    job_name='worker',
    coordinator_name=None,
    coordinator_address=None,
    credentials='default',
    service=None,
    discovery_url=None
)
","[['Inherits From: ', 'ClusterResolver']]","cluster_spec = tf.train.ClusterSpec({
    ""ps"": [""localhost:2222"", ""localhost:2223""],
    ""worker"": [""localhost:2224"", ""localhost:2225"", ""localhost:2226""]
})

# SimpleClusterResolver is used here for illustration; other cluster
# resolvers may be used for other source of task type/id.
simple_resolver = SimpleClusterResolver(cluster_spec, task_type=""worker"",
                                        task_id=0)

...

if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:
  # Perform something that's only applicable on 'worker' type, id 0. This
  # block will run on this particular instance since we've specified this
  # task to be a 'worker', id 0 in above cluster resolver.
else:
  # Perform something that's only applicable on other ids. This block will
  # not run on this particular instance.
"
"tf.distribute.cluster_resolver.UnionResolver(
    *args, **kwargs
)
","[['Inherits From: ', 'ClusterResolver']]","  cluster_0 = tf.train.ClusterSpec({""worker"": [""worker0.example.com:2222"",
                                               ""worker1.example.com:2222""]})
  cluster_resolver_0 = SimpleClusterResolver(cluster, task_type=""worker"",
                                             task_id=0,
                                             rpc_layer=""grpc"")

  cluster_1 = tf.train.ClusterSpec({""ps"": [""ps0.example.com:2222"",
                                           ""ps1.example.com:2222""]})
  cluster_resolver_1 = SimpleClusterResolver(cluster, task_type=""ps"",
                                             task_id=0,
                                             rpc_layer=""grpc"")

  # Its task type would be ""worker"".
  cluster_resolver = UnionClusterResolver(cluster_resolver_0,
                                          cluster_resolver_1)
"
"tf.distribute.experimental.coordinator.ClusterCoordinator(
    strategy
)
",[],"create_per_worker_dataset(
    dataset_fn
)
"
"tf.distribute.experimental.coordinator.PerWorkerValues(
    values
)
",[],[]
"tf.distribute.experimental.CentralStorageStrategy(
    compute_devices=None, parameter_device=None
)
","[['Inherits From: ', 'Strategy']]","strategy = tf.distribute.experimental.CentralStorageStrategy()
# Create a dataset
ds = tf.data.Dataset.range(5).batch(2)
# Distribute that dataset
dist_dataset = strategy.experimental_distribute_dataset(ds)

with strategy.scope():
  @tf.function
  def train_step(val):
    return val + 1

  # Iterate over the distributed dataset
  for x in dist_dataset:
    # process dataset elements
    strategy.run(train_step, args=(x,))
"
"tf.distribute.experimental.CollectiveHints(
    bytes_per_pack=0, timeout_seconds=None
)
",[],"hints = tf.distribute.experimental.CollectiveHints(
    bytes_per_pack=50 * 1024 * 1024)
grads = tf.distribute.get_replica_context().all_reduce(
    'sum', grads, experimental_hints=hints)
optimizer.apply_gradients(zip(grads, vars),
    experimental_aggregate_gradients=False)
"
"tf.distribute.experimental.CommunicationOptions(
    bytes_per_pack=0,
    timeout_seconds=None,
    implementation=tf.distribute.experimental.CollectiveCommunication.AUTO
)
",[],"options = tf.distribute.experimental.CommunicationOptions(
    bytes_per_pack=50 * 1024 * 1024,
    timeout_seconds=120.0,
    implementation=tf.distribute.experimental.CommunicationImplementation.NCCL
)
grads = tf.distribute.get_replica_context().all_reduce(
    'sum', grads, options=options)
optimizer.apply_gradients(zip(grads, vars),
    experimental_aggregate_gradients=False)
"
"tf.distribute.experimental.MultiWorkerMirroredStrategy(
    communication=tf.distribute.experimental.CollectiveCommunication.AUTO,
    cluster_resolver=None
)
","[['Inherits From: ', 'MultiWorkerMirroredStrategy', ', ', 'Strategy']]","TF_CONFIG = '{""cluster"": {""worker"": [""localhost:12345"", ""localhost:23456""]}, ""task"": {""type"": ""worker"", ""index"": 0} }'
"
"tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver, variable_partitioner=None
)
","[['Inherits From: ', 'Strategy']]","# Prepare a strategy to use with the cluster and variable partitioning info.
strategy = tf.distribute.experimental.ParameterServerStrategy(
    cluster_resolver=...,
    variable_partitioner=...)
coordinator = tf.distribute.experimental.coordinator.ClusterCoordinator(
    strategy=strategy)

# Prepare a distribute dataset that will place datasets on the workers.
distributed_dataset = coordinator.create_per_worker_dataset(dataset_fn=...)

with strategy.scope():
  model = ...
  optimizer, metrics = ...  # Keras optimizer/metrics are great choices
  checkpoint = tf.train.Checkpoint(model=model, optimizer=optimizer)
  checkpoint_manager = tf.train.CheckpointManager(
      checkpoint, checkpoint_dir, max_to_keep=2)
  # `load_checkpoint` infers initial epoch from `optimizer.iterations`.
  initial_epoch = load_checkpoint(checkpoint_manager) or 0

@tf.function
def worker_fn(iterator):

  def replica_fn(inputs):
    batch_data, labels = inputs
    # calculate gradient, applying gradient, metrics update etc.

  strategy.run(replica_fn, args=(next(iterator),))

for epoch in range(initial_epoch, num_epoch):
  distributed_iterator = iter(distributed_dataset)  # Reset iterator state.
  for step in range(steps_per_epoch):

    # Asynchronously schedule the `worker_fn` to be executed on an arbitrary
    # worker. This call returns immediately.
    coordinator.schedule(worker_fn, args=(distributed_iterator,))

  # `join` blocks until all scheduled `worker_fn`s finish execution. Once it
  # returns, we can read the metrics and save checkpoints as needed.
  coordinator.join()
  logging.info('Metric result: %r', metrics.result())
  train_accuracy.reset_states()
  checkpoint_manager.save()
"
"tf.distribute.experimental.PreemptionCheckpointHandler(
    cluster_resolver,
    checkpoint_or_checkpoint_manager,
    checkpoint_dir=None,
    termination_config=None
)
",[],"strategy = tf.distribute.MultiWorkerMirroredStrategy()

with strategy.scope():
  dataset, model, optimizer = ...

  checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)

  preemption_handler = tf.distribute.experimental.PreemptionCheckpointHandler(cluster_resolver, checkpoint, checkpoint_directory)

  # preemption_handler.total_run_calls will be restored to its saved value if
  # training is restored after interruption.
  for epoch in range(preemption_handler.total_run_calls // STEPS_PER_EPOCH, num_epochs):
    for step in range(preemption_handler.total_run_calls % STEPS_PER_EPOCH, STEPS_PER_EPOCH):
      # distributed_train_step is a single-step training function wrapped by tf.distribute.Strategy.run.
      loss += preemption_handler.run(distributed_train_step, args=(next(dataset),))
"
"tf.distribute.experimental.TPUStrategy(
    tpu_cluster_resolver=None, device_assignment=None
)
","[['Inherits From: ', 'Strategy']]","resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
tf.config.experimental_connect_to_cluster(resolver)
tf.tpu.experimental.initialize_tpu_system(resolver)
strategy = tf.distribute.experimental.TPUStrategy(resolver)"
"tf.distribute.experimental.TerminationConfig(
    termination_watcher_fn=None, exit_fn=None, grace_period=None
)
","[['Customization of ', 'PreemptionCheckpointHandler', ' for various platforms.']]",[]
"tf.distribute.experimental.ValueContext(
    replica_id_in_sync_group=0, num_replicas_in_sync=1
)
",[],[]
"tf.distribute.experimental.coordinator.ClusterCoordinator(
    strategy
)
",[],"create_per_worker_dataset(
    dataset_fn
)
"
"tf.distribute.experimental.coordinator.PerWorkerValues(
    values
)
",[],[]
"tf.distribute.experimental.partitioners.FixedShardsPartitioner(
    num_shards
)
","[['Inherits From: ', 'Partitioner']]","# standalone usage:
partitioner = FixedShardsPartitioner(num_shards=2)
partitions = partitioner(tf.TensorShape([10, 3]), tf.float32)
[2, 1]
# use in ParameterServerStrategy
# strategy = tf.distribute.experimental.ParameterServerStrategy(
#   cluster_resolver=cluster_resolver, variable_partitioner=partitioner)"
"tf.distribute.experimental.partitioners.MaxSizePartitioner(
    max_shard_bytes, max_shards=None, bytes_per_string=16
)
","[['Partitioner that keeps shards below ', 'max_shard_bytes', '.'], ['Inherits From: ', 'Partitioner']]","partitioner = MaxSizePartitioner(max_shard_bytes=4)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[6, 1]
partitioner = MaxSizePartitioner(max_shard_bytes=4, max_shards=2)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[2, 1]
partitioner = MaxSizePartitioner(max_shard_bytes=1024)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[1, 1]
# use in ParameterServerStrategy
# strategy = tf.distribute.experimental.ParameterServerStrategy(
#   cluster_resolver=cluster_resolver, variable_partitioner=partitioner)"
"tf.distribute.experimental.partitioners.MinSizePartitioner(
    min_shard_bytes=(256 << 10), max_shards=1, bytes_per_string=16
)
","[['Inherits From: ', 'Partitioner']]","partitioner = MinSizePartitioner(min_shard_bytes=4, max_shards=2)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[2, 1]
partitioner = MinSizePartitioner(min_shard_bytes=4, max_shards=10)
partitions = partitioner(tf.TensorShape([6, 1]), tf.float32)
[6, 1]
# use in ParameterServerStrategy
# strategy = tf.distribute.experimental.ParameterServerStrategy(
#   cluster_resolver=cluster_resolver, variable_partitioner=partitioner)"
"tf.distribute.experimental_set_strategy(
    strategy
)
","[['Set a ', 'tf.distribute.Strategy', ' as current without ', 'with strategy.scope()', '.']]","tf.distribute.experimental_set_strategy(strategy1)
f()
tf.distribute.experimental_set_strategy(strategy2)
g()
tf.distribute.experimental_set_strategy(None)
h()
"
"tf.math.divide(
    x, y, name=None
)
","[['Computes Python style division of ', 'x', ' by ', 'y', '.']]","x = tf.constant([16, 12, 11])
y = tf.constant([4, 6, 2])
tf.divide(x,y)
<tf.Tensor: shape=(3,), dtype=float64,
numpy=array([4. , 2. , 5.5])>"
"tf.dtypes.as_dtype(
    type_value
)
","[['Converts the given ', 'type_value', ' to a ', 'DType', '.']]",[]
"tf.cast(
    x, dtype, name=None
)
",[],"x = tf.constant([1.8, 2.2], dtype=tf.float32)
tf.cast(x, tf.int32)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>"
"tf.dtypes.complex(
    real, imag, name=None
)
","[[None, '\n']]","real = tf.constant([2.25, 3.25])
imag = tf.constant([4.75, 5.75])
tf.complex(real, imag)  # [[2.25 + 4.75j], [3.25 + 5.75j]]
"
"tf.dtypes.saturate_cast(
    value, dtype, name=None
)
","[['Performs a safe saturating cast of ', 'value', ' to ', 'dtype', '.']]",[]
"tf.dynamic_partition(
    data, partitions, num_partitions, name=None
)
","[['Partitions ', 'data', ' into ', 'num_partitions', ' tensors using indices from ', 'partitions', '.']]","    outputs[i].shape = [sum(partitions == i)] + data.shape[partitions.ndim:]

    outputs[i] = pack([data[js, ...] for js if partitions[js] == i])
"
"tf.dynamic_stitch(
    indices, data, name=None
)
","[['Interleave the values from the ', 'data', ' tensors into a single tensor.']]","    merged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]
"
"tf.edit_distance(
    hypothesis, truth, normalize=True, name='edit_distance'
)
",[],"hypothesis = tf.SparseTensor(
  [[0, 0, 0],
   [1, 0, 0]],
  [""a"", ""b""],
  (2, 1, 1))
truth = tf.SparseTensor(
  [[0, 1, 0],
   [1, 0, 0],
   [1, 0, 1],
   [1, 1, 0]],
   [""a"", ""b"", ""c"", ""a""],
   (2, 2, 2))
tf.edit_distance(hypothesis, truth, normalize=True)
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[inf, 1. ],
       [0.5, 1. ]], dtype=float32)>"
"tf.linalg.eig(
    tensor, name=None
)
",[],[]
"tf.linalg.eigvals(
    tensor, name=None
)
",[],[]
"tf.einsum(
    equation, *inputs, **kwargs
)
","[[None, '\n']]","C[i,k] = sum_j A[i,j] * B[j,k]
"
"tf.ensure_shape(
    x, shape, name=None
)
",[],"x = tf.constant([[1, 2, 3],
                 [4, 5, 6]])
x = tf.ensure_shape(x, [2, 3])"
"tf.math.equal(
    x, y, name=None
)
",[],"x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>"
"tf.math.exp(
    x, name=None
)
","[[None, '\n']]","x = tf.constant(2.0)
tf.math.exp(x)
<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>"
"tf.expand_dims(
    input, axis, name=None
)
","[['Returns a tensor with a length 1 axis inserted at index ', 'axis', '.']]","image = tf.zeros([10,10,3])"
"tf.experimental.BatchableExtensionType(
    *args, **kwargs
)
","[['Inherits From: ', 'ExtensionType']]","class Vehicle(tf.experimental.BatchableExtensionType):
  top_speed: tf.Tensor
  mpg: tf.Tensor
batch = Vehicle([120, 150, 80], [30, 40, 12])
tf.map_fn(lambda vehicle: vehicle.top_speed * vehicle.mpg, batch,
          fn_output_signature=tf.int32).numpy()
array([3600, 6000,  960], dtype=int32)"
"tf.experimental.DynamicRaggedShape(
    row_partitions: Sequence[tf.experimental.RowPartition],
    inner_shape: tf.types.experimental.TensorLike,
    dtype: Optional[tf.dtypes.DType] = None,
    validate: bool = False,
    static_inner_shape: ... = None
)
","[['Inherits From: ', 'BatchableExtensionType', ', ', 'ExtensionType']]","@classmethod
from_lengths(
    lengths: Sequence[Union[Sequence[int], int]],
    num_row_partitions=None,
    dtype="
"tf.experimental.DynamicRaggedShape.Spec(
    row_partitions: Tuple[RowPartitionSpec, ...],
    static_inner_shape: tf.TensorShape,
    dtype: tf.dtypes.DType
)
","[['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.experimental.ExtensionType(
    *args, **kwargs
)
","[['Base class for TensorFlow ', 'ExtensionType', ' classes.']]","class MaskedTensor(ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor"
"tf.experimental.RowPartition(
    row_splits,
    row_lengths=None,
    value_rowids=None,
    nrows=None,
    uniform_row_length=None,
    nvals=None,
    internal=False
)
",[],"p1 = RowPartition.from_row_lengths([4, 0, 3, 1, 0])
p2 = RowPartition.from_row_splits([0, 4, 4, 7, 8, 8])
p3 = RowPartition.from_row_starts([0, 4, 4, 7, 8], nvals=8)
p4 = RowPartition.from_row_limits([4, 4, 7, 8, 8])
p5 = RowPartition.from_value_rowids([0, 0, 0, 0, 2, 2, 2, 3], nrows=5)"
"tf.experimental.StructuredTensor(
    fields: Mapping[str, _FieldValue],
    ragged_shape: tf.experimental.DynamicRaggedShape
)
","[['Inherits From: ', 'BatchableExtensionType', ', ', 'ExtensionType']]","# A scalar StructuredTensor describing a single person.
s1 = tf.experimental.StructuredTensor.from_pyval(
    {""age"": 82, ""nicknames"": [""Bob"", ""Bobby""]})
s1.shape
TensorShape([])
s1[""age""]
<tf.Tensor: shape=(), dtype=int32, numpy=82>"
"tf.experimental.StructuredTensor.Spec(
    _fields, _ragged_shape
)
","[['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.experimental.dispatch_for_api(
    api, *signatures
)
",[],"class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor"
"tf.experimental.dispatch_for_binary_elementwise_apis(
    x_type, y_type
)
",[],"class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor
@dispatch_for_binary_elementwise_apis(MaskedTensor, MaskedTensor)
def binary_elementwise_api_handler(api_func, x, y):
  return MaskedTensor(api_func(x.values, y.values), x.mask & y.mask)
a = MaskedTensor([1, 2, 3, 4, 5], [True, True, True, True, False])
b = MaskedTensor([2, 4, 6, 8, 0], [True, True, True, False, True])
c = tf.add(a, b)
print(f""values={c.values.numpy()}, mask={c.mask.numpy()}"")
values=[ 3 6 9 12 5], mask=[ True True True False False]"
"tf.experimental.dispatch_for_binary_elementwise_assert_apis(
    x_type, y_type
)
",[],"class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor
@dispatch_for_binary_elementwise_assert_apis(MaskedTensor, MaskedTensor)
def binary_elementwise_assert_api_handler(assert_func, x, y):
  merged_mask = tf.logical_and(x.mask, y.mask)
  selected_x_values = tf.boolean_mask(x.values, merged_mask)
  selected_y_values = tf.boolean_mask(y.values, merged_mask)
  assert_func(selected_x_values, selected_y_values)
a = MaskedTensor([1, 1, 0, 1, 1], [False, False, True, True, True])
b = MaskedTensor([2, 2, 0, 2, 2], [True, True, True, False, False])
tf.debugging.assert_equal(a, b) # assert passed; no exception was thrown"
"tf.experimental.dispatch_for_unary_elementwise_apis(
    x_type
)
",[],"class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor
@dispatch_for_unary_elementwise_apis(MaskedTensor)
def unary_elementwise_api_handler(api_func, x):
  return MaskedTensor(api_func(x.values), x.mask)
mt = MaskedTensor([1, -2, -3], [True, False, True])
abs_mt = tf.abs(mt)
print(f""values={abs_mt.values.numpy()}, mask={abs_mt.mask.numpy()}"")
values=[1 2 3], mask=[ True False True]"
"tf.experimental.dlpack.from_dlpack(
    dlcapsule
)
",[],"  a = tf.experimental.dlpack.from_dlpack(dlcapsule)
  # `a` uses the memory shared by dlpack
"
"tf.experimental.dlpack.to_dlpack(
    tf_tensor
)
",[],"  a = tf.tensor([1, 10])
  dlcapsule = tf.experimental.dlpack.to_dlpack(a)
  # dlcapsule represents the dlpack data structure
"
"tf.experimental.dtensor.DTensorCheckpoint(
    mesh: tf.experimental.dtensor.Mesh,
    root=None,
    **kwargs
)
","[['Inherits From: ', 'Checkpoint']]","read(
    save_path, options=None
)
"
"tf.experimental.dtensor.DTensorDataset(
    dataset: tf.data.Dataset,
    *,
    mesh: tf.experimental.dtensor.Mesh,
    layouts: Any,
    global_batch_size: int,
    dataset_already_batched: bool = False,
    batch_dim: Optional[str] = None,
    prefetch: Optional[int] = None,
    tf_data_service_config: Optional[TFDataServiceConfig] = None
)
","[['Inherits From: ', 'Dataset']]","dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int32, name=None)"
"tf.experimental.dtensor.DVariable(
    initial_value, *args, dtype=None, **kwargs
)
","[[None, '\n'], ['Inherits From: ', 'Variable', ', ', 'Variable']]","assign(
    value, use_locking=None, name=None, read_value=True
)
"
"tf.Variable.SaveSliceInfo(
    full_name=None,
    full_shape=None,
    var_offset=None,
    var_shape=None,
    save_slice_info_def=None,
    import_scope=None
)
",[],"to_proto(
    export_scope=None
)
"
"tf.experimental.dtensor.Layout(
    sharding_specs: List[str],
    mesh: tf.experimental.dtensor.Mesh
)
",[],"Mesh([""TPU:0"", ""TPU:1"", ""TPU:2"", ""TPU:3"", ""TPU:4"", ""TPU:5""], [(""x"", 6)])
"
"tf.experimental.dtensor.Mesh(
    dim_names: List[str],
    global_device_ids: np.ndarray,
    local_device_ids: List[int],
    local_devices: List[tf.compat.v1.DeviceSpec],
    mesh_name: str = '',
    global_devices: Optional[List[tf_device.DeviceSpec]] = None
)
",[],"[(device_id / (b*c*d)) % a,
 (device_id / (c*d))   % b,
 (device_id / (d))     % c,
 (device_id)           % d]
"
"tf.experimental.dtensor.barrier(
    mesh: tf.experimental.dtensor.Mesh,
    barrier_name: Optional[str] = None
)
",[],"
x = [1, 2, 3]
x = dtensor.relayout(x, dtensor.Layout.batch_sharded(mesh, 'batch', 1))
dtensor.barrier(mesh)

# At this point all devices on all clients in the mesh have completed
# operations before the barrier. Therefore it is OK to tear down the clients.
sys.exit()
"
"tf.experimental.dtensor.call_with_layout(
    fn: Callable[..., Any],
    layout: Optional[tf.experimental.dtensor.Layout],
    *args,
    **kwargs
) -> Any
","[['Calls a function in the DTensor device scope if ', 'layout', ' is not None.']]",[]
"tf.experimental.dtensor.check_layout(
    tensor: tf.Tensor,
    layout: tf.experimental.dtensor.Layout
) -> None
","[['Asserts that the layout of the DTensor is ', 'layout', '.']]",[]
"tf.experimental.dtensor.client_id() -> int
",[],[]
"tf.experimental.dtensor.copy_to_mesh(
    tensor: Any,
    layout: tf.experimental.dtensor.Layout,
    source_layout: Optional[tf.experimental.dtensor.Layout] = None
) -> tf.Tensor
",[],[]
"tf.experimental.dtensor.create_distributed_mesh(
    mesh_dims: List[Tuple[str, int]],
    mesh_name: str = '',
    local_devices: Optional[List[str]] = None,
    device_type: Optional[str] = None
) -> tf.experimental.dtensor.Mesh
",[],[]
"tf.experimental.dtensor.create_mesh(
    mesh_dims: Optional[List[Tuple[str, int]]] = None,
    mesh_name: str = '',
    devices: Optional[List[str]] = None,
    device_type: Optional[str] = None
) -> tf.experimental.dtensor.Mesh
",[],[]
"tf.experimental.dtensor.create_tpu_mesh(
    mesh_dim_names: List[str],
    mesh_shape: List[int],
    mesh_name: str,
    ring_dims: Optional[int] = None,
    ring_axes: Optional[List[str]] = None,
    ring_bounds: Optional[List[int]] = None,
    can_split_host_across_rings: bool = True,
    build_ring_across_rings: bool = False,
    rotate_ring_across_rings: bool = False
) -> tf.experimental.dtensor.Mesh
",[],[]
"tf.experimental.dtensor.device_name() -> str
",[],"import tensorflow as tf

with tf.device(dtensor.device_name()):
  # ...
"
"tf.experimental.dtensor.enable_save_as_bf16(
    variables: List[tf.Variable]
)
",[],[]
"tf.experimental.dtensor.fetch_layout(
    tensor: tf.Tensor
) -> tf.experimental.dtensor.Layout
",[],[]
"tf.experimental.dtensor.full_job_name(
    task_id: Optional[int] = None
) -> str
",[],[]
"tf.experimental.dtensor.heartbeat_enabled() -> bool
",[],[]
"tf.experimental.dtensor.initialize_accelerator_system(
    device_type: Optional[str] = None,
    enable_coordination_service: Optional[bool] = False
) -> str
",[],[]
"tf.experimental.dtensor.initialize_accelerator_system(
    device_type: Optional[str] = None,
    enable_coordination_service: Optional[bool] = False
) -> str
",[],[]
"tf.experimental.dtensor.initialize_accelerator_system(
    device_type: Optional[str] = None,
    enable_coordination_service: Optional[bool] = False
) -> str
",[],[]
"tf.experimental.dtensor.job_name() -> str
",[],[]
"tf.experimental.dtensor.jobs() -> List[str]
",[],[]
"tf.experimental.dtensor.local_devices(
    device_type: str, for_client_id: Optional[int] = None
) -> List[tf.compat.v1.DeviceSpec]
",[],[]
"tf.experimental.dtensor.name_based_restore(
    mesh: tf.experimental.dtensor.Mesh,
    checkpoint_prefix: str,
    name_tensor_dict: Dict[str, Union[ops.Tensor, tf_variables.Variable]]
)
",[],[]
"tf.experimental.dtensor.name_based_save(
    mesh: tf.experimental.dtensor.Mesh,
    checkpoint_prefix: Union[str, tf.Tensor],
    name_tensor_dict: Dict[str, Union[ops.Tensor, tf_variables.Variable]]
)
",[],[]
"tf.experimental.dtensor.num_clients() -> int
",[],[]
"tf.experimental.dtensor.num_global_devices(
    device_type: str
) -> int
",[],[]
"tf.experimental.dtensor.num_local_devices(
    device_type: str
) -> int
",[],[]
"tf.experimental.dtensor.pack(
    tensors: Sequence[Any],
    layout: tf.experimental.dtensor.Layout
) -> Any
","[['Packs ', 'tf.Tensor', ' components into a DTensor.']]","* unpack(pack(tensors)) == tensors
* pack(unpack(dtensor)) == dtensor
"
"tf.experimental.dtensor.preferred_device_type() -> str
",[],[]
"tf.experimental.dtensor.relayout(
    tensor: tf.Tensor,
    layout: tf.experimental.dtensor.Layout
) -> tf.Tensor
","[['Changes the layout of ', 'tensor', '.']]",[]
"tf.experimental.dtensor.sharded_save(
    mesh: tf.experimental.dtensor.Mesh,
    file_prefix: Union[str, tf.Tensor],
    tensor_names: Union[List[str], tf.Tensor],
    shape_and_slices: Union[List[str], tf.Tensor],
    tensors: List[Union[ops.Tensor, tf_variables.Variable]]
)
",[],[]
"tf.experimental.dtensor.shutdown_accelerator_system() -> None
",[],[]
"tf.experimental.dtensor.shutdown_accelerator_system() -> None
",[],[]
"tf.experimental.dtensor.unpack(
    tensor: Any
) -> Sequence[Any]
","[['Unpacks a DTensor into ', 'tf.Tensor', ' components.']]","* unpack(pack(tensors)) == tensors
* pack(unpack(dtensor)) == dtensor
"
"tf.experimental.numpy.abs(
    x
)
","[[""TensorFlow variant of NumPy's "", 'abs', '.']]",[]
"tf.experimental.numpy.absolute(
    x
)
","[[""TensorFlow variant of NumPy's "", 'absolute', '.']]",[]
"tf.experimental.numpy.add(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'add', '.']]",[]
"tf.experimental.numpy.all(
    a, axis=None, keepdims=None
)
","[[""TensorFlow variant of NumPy's "", 'all', '.']]",[]
"tf.experimental.numpy.allclose(
    a, b, rtol=1e-05, atol=1e-08, equal_nan=False
)
","[[""TensorFlow variant of NumPy's "", 'allclose', '.']]",[]
"tf.experimental.numpy.amax(
    a, axis=None, out=None, keepdims=None
)
","[[""TensorFlow variant of NumPy's "", 'amax', '.']]",[]
"tf.experimental.numpy.amin(
    a, axis=None, out=None, keepdims=None
)
","[[""TensorFlow variant of NumPy's "", 'amin', '.']]",[]
"tf.experimental.numpy.angle(
    z, deg=False
)
","[[""TensorFlow variant of NumPy's "", 'angle', '.']]",[]
"tf.experimental.numpy.any(
    a, axis=None, keepdims=None
)
","[[""TensorFlow variant of NumPy's "", 'any', '.']]",[]
"tf.experimental.numpy.append(
    arr, values, axis=None
)
","[[""TensorFlow variant of NumPy's "", 'append', '.']]",[]
"tf.experimental.numpy.arange(
    start, stop=None, step=1, dtype=None
)
","[[""TensorFlow variant of NumPy's "", 'arange', '.']]",[]
"tf.experimental.numpy.arccos(
    x
)
","[[""TensorFlow variant of NumPy's "", 'arccos', '.']]",[]
"tf.experimental.numpy.arccosh(
    x
)
","[[""TensorFlow variant of NumPy's "", 'arccosh', '.']]",[]
"tf.experimental.numpy.arcsin(
    x
)
","[[""TensorFlow variant of NumPy's "", 'arcsin', '.']]",[]
"tf.experimental.numpy.arcsinh(
    x
)
","[[""TensorFlow variant of NumPy's "", 'arcsinh', '.']]",[]
"tf.experimental.numpy.arctan(
    x
)
","[[""TensorFlow variant of NumPy's "", 'arctan', '.']]",[]
"tf.experimental.numpy.arctan2(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'arctan2', '.']]",[]
"tf.experimental.numpy.arctanh(
    x
)
","[[""TensorFlow variant of NumPy's "", 'arctanh', '.']]",[]
"tf.experimental.numpy.argmax(
    a, axis=None
)
","[[""TensorFlow variant of NumPy's "", 'argmax', '.']]",[]
"tf.experimental.numpy.argmin(
    a, axis=None
)
","[[""TensorFlow variant of NumPy's "", 'argmin', '.']]",[]
"tf.experimental.numpy.argsort(
    a, axis=-1, kind='quicksort', order=None
)
","[[""TensorFlow variant of NumPy's "", 'argsort', '.']]",[]
"tf.experimental.numpy.around(
    a, decimals=0
)
","[[""TensorFlow variant of NumPy's "", 'around', '.']]",[]
"tf.experimental.numpy.array(
    val, dtype=None, copy=True, ndmin=0
)
","[[""TensorFlow variant of NumPy's "", 'array', '.']]",[]
"tf.experimental.numpy.array_equal(
    a1, a2
)
","[[""TensorFlow variant of NumPy's "", 'array_equal', '.']]",[]
"tf.experimental.numpy.asanyarray(
    a, dtype=None
)
","[[""TensorFlow variant of NumPy's "", 'asanyarray', '.']]",[]
"tf.experimental.numpy.asarray(
    a, dtype=None
)
","[[""TensorFlow variant of NumPy's "", 'asarray', '.']]",[]
"tf.experimental.numpy.ascontiguousarray(
    a, dtype=None
)
","[[""TensorFlow variant of NumPy's "", 'ascontiguousarray', '.']]",[]
"tf.experimental.numpy.atleast_1d(
    *arys
)
","[[""TensorFlow variant of NumPy's "", 'atleast_1d', '.']]",[]
"tf.experimental.numpy.atleast_2d(
    *arys
)
","[[""TensorFlow variant of NumPy's "", 'atleast_2d', '.']]",[]
"tf.experimental.numpy.atleast_3d(
    *arys
)
","[[""TensorFlow variant of NumPy's "", 'atleast_3d', '.']]",[]
"tf.experimental.numpy.average(
    a, axis=None, weights=None, returned=False
)
","[[""TensorFlow variant of NumPy's "", 'average', '.']]",[]
"tf.experimental.numpy.bitwise_and(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'bitwise_and', '.']]",[]
"tf.experimental.numpy.bitwise_not(
    x
)
","[[""TensorFlow variant of NumPy's "", 'bitwise_not', '.']]",[]
"tf.experimental.numpy.bitwise_or(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'bitwise_or', '.']]",[]
"tf.experimental.numpy.bitwise_xor(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'bitwise_xor', '.']]",[]
"tf.experimental.numpy.bool_(
    *args, **kwargs
)
",[],"all()
"
"tf.experimental.numpy.broadcast_arrays(
    *args, **kwargs
)
","[[""TensorFlow variant of NumPy's "", 'broadcast_arrays', '.']]",[]
"tf.experimental.numpy.broadcast_to(
    array, shape
)
","[[""TensorFlow variant of NumPy's "", 'broadcast_to', '.']]",[]
"tf.experimental.numpy.cbrt(
    x
)
","[[""TensorFlow variant of NumPy's "", 'cbrt', '.']]",[]
"tf.experimental.numpy.ceil(
    x
)
","[[""TensorFlow variant of NumPy's "", 'ceil', '.']]",[]
"tf.experimental.numpy.clip(
    a, a_min, a_max
)
","[[""TensorFlow variant of NumPy's "", 'clip', '.']]",[]
"tf.experimental.numpy.complex128(
    *args, **kwargs
)
","[['Inherits From: ', 'inexact']]","all()
"
"tf.experimental.numpy.complex64(
    *args, **kwargs
)
","[['Inherits From: ', 'inexact']]","all()
"
"tf.experimental.numpy.complex128(
    *args, **kwargs
)
","[['Inherits From: ', 'inexact']]","all()
"
"tf.experimental.numpy.compress(
    condition, a, axis=None
)
","[[""TensorFlow variant of NumPy's "", 'compress', '.']]",[]
"tf.experimental.numpy.concatenate(
    arys, axis=0
)
","[[""TensorFlow variant of NumPy's "", 'concatenate', '.']]",[]
"tf.experimental.numpy.conj(
    x
)
","[[""TensorFlow variant of NumPy's "", 'conj', '.']]",[]
"tf.experimental.numpy.conjugate(
    x
)
","[[""TensorFlow variant of NumPy's "", 'conjugate', '.']]",[]
"tf.experimental.numpy.copy(
    a
)
","[[""TensorFlow variant of NumPy's "", 'copy', '.']]",[]
"tf.experimental.numpy.cos(
    x
)
","[[""TensorFlow variant of NumPy's "", 'cos', '.']]",[]
"tf.experimental.numpy.cosh(
    x
)
","[[""TensorFlow variant of NumPy's "", 'cosh', '.']]",[]
"tf.experimental.numpy.count_nonzero(
    a, axis=None
)
","[[""TensorFlow variant of NumPy's "", 'count_nonzero', '.']]",[]
"tf.experimental.numpy.cross(
    a, b, axisa=-1, axisb=-1, axisc=-1, axis=None
)
","[[""TensorFlow variant of NumPy's "", 'cross', '.']]",[]
"tf.experimental.numpy.cumprod(
    a, axis=None, dtype=None
)
","[[""TensorFlow variant of NumPy's "", 'cumprod', '.']]",[]
"tf.experimental.numpy.cumsum(
    a, axis=None, dtype=None
)
","[[""TensorFlow variant of NumPy's "", 'cumsum', '.']]",[]
"tf.experimental.numpy.deg2rad(
    x
)
","[[""TensorFlow variant of NumPy's "", 'deg2rad', '.']]",[]
"tf.experimental.numpy.diag(
    v, k=0
)
","[[""TensorFlow variant of NumPy's "", 'diag', '.']]",[]
"tf.experimental.numpy.diag_indices(
    n, ndim=2
)
","[[""TensorFlow variant of NumPy's "", 'diag_indices', '.']]",[]
"tf.experimental.numpy.diagflat(
    v, k=0
)
","[[""TensorFlow variant of NumPy's "", 'diagflat', '.']]",[]
"tf.experimental.numpy.diagonal(
    a, offset=0, axis1=0, axis2=1
)
","[[""TensorFlow variant of NumPy's "", 'diagonal', '.']]",[]
"tf.experimental.numpy.diff(
    a, n=1, axis=-1
)
","[[""TensorFlow variant of NumPy's "", 'diff', '.']]",[]
"tf.experimental.numpy.divide(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'divide', '.']]",[]
"tf.experimental.numpy.divmod(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'divmod', '.']]",[]
"tf.experimental.numpy.dot(
    a, b
)
","[[""TensorFlow variant of NumPy's "", 'dot', '.']]",[]
"tf.experimental.numpy.dsplit(
    ary, indices_or_sections
)
","[[""TensorFlow variant of NumPy's "", 'dsplit', '.']]",[]
"tf.experimental.numpy.dstack(
    tup
)
","[[""TensorFlow variant of NumPy's "", 'dstack', '.']]",[]
"tf.experimental.numpy.einsum(
    subscripts, *operands, **kwargs
)
","[[""TensorFlow variant of NumPy's "", 'einsum', '.']]",[]
"tf.experimental.numpy.empty(
    shape, dtype=float
)
","[[""TensorFlow variant of NumPy's "", 'empty', '.']]",[]
"tf.experimental.numpy.empty_like(
    a, dtype=None
)
","[[""TensorFlow variant of NumPy's "", 'empty_like', '.']]",[]
"tf.experimental.numpy.equal(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'equal', '.']]",[]
"tf.experimental.numpy.exp(
    x
)
","[[""TensorFlow variant of NumPy's "", 'exp', '.']]",[]
"tf.experimental.numpy.exp2(
    x
)
","[[""TensorFlow variant of NumPy's "", 'exp2', '.']]",[]
"tf.experimental.numpy.expand_dims(
    a, axis
)
","[[""TensorFlow variant of NumPy's "", 'expand_dims', '.']]",[]
"tf.experimental.numpy.experimental_enable_numpy_behavior(
    prefer_float32=False
)
",[],[]
"tf.experimental.numpy.expm1(
    x
)
","[[""TensorFlow variant of NumPy's "", 'expm1', '.']]",[]
"tf.experimental.numpy.eye(
    N, M=None, k=0, dtype=float
)
","[[""TensorFlow variant of NumPy's "", 'eye', '.']]",[]
"tf.experimental.numpy.fabs(
    x
)
","[[""TensorFlow variant of NumPy's "", 'fabs', '.']]",[]
"tf.experimental.numpy.finfo(
    dtype
)
","[[""TensorFlow variant of NumPy's "", 'finfo', '.']]",[]
"tf.experimental.numpy.fix(
    x
)
","[[""TensorFlow variant of NumPy's "", 'fix', '.']]",[]
"tf.experimental.numpy.flip(
    m, axis=None
)
","[[""TensorFlow variant of NumPy's "", 'flip', '.']]",[]
"tf.experimental.numpy.fliplr(
    m
)
","[[""TensorFlow variant of NumPy's "", 'fliplr', '.']]",[]
"tf.experimental.numpy.flipud(
    m
)
","[[""TensorFlow variant of NumPy's "", 'flipud', '.']]",[]
"tf.experimental.numpy.float16(
    *args, **kwargs
)
","[['Inherits From: ', 'inexact']]","all()
"
"tf.experimental.numpy.float32(
    *args, **kwargs
)
","[['Single-precision floating-point number type, compatible with C ', 'float', '.'], ['Inherits From: ', 'inexact']]","all()
"
"tf.experimental.numpy.float64(
    *args, **kwargs
)
","[['Double-precision floating-point number type, compatible with Python ', 'float'], ['Inherits From: ', 'inexact']]","all()
"
"tf.experimental.numpy.float64(
    *args, **kwargs
)
","[['Double-precision floating-point number type, compatible with Python ', 'float'], ['Inherits From: ', 'inexact']]","all()
"
"tf.experimental.numpy.float_power(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'float_power', '.']]",[]
"tf.experimental.numpy.floor(
    x
)
","[[""TensorFlow variant of NumPy's "", 'floor', '.']]",[]
"tf.experimental.numpy.floor_divide(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'floor_divide', '.']]",[]
"tf.experimental.numpy.full(
    shape, fill_value, dtype=None
)
","[[""TensorFlow variant of NumPy's "", 'full', '.']]",[]
"tf.experimental.numpy.full_like(
    a, fill_value, dtype=None, order='K', subok=True, shape=None
)
","[[""TensorFlow variant of NumPy's "", 'full_like', '.']]",[]
"tf.experimental.numpy.gcd(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'gcd', '.']]",[]
"tf.experimental.numpy.geomspace(
    start, stop, num=50, endpoint=True, dtype=None, axis=0
)
","[[""TensorFlow variant of NumPy's "", 'geomspace', '.']]",[]
"tf.experimental.numpy.greater(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'greater', '.']]",[]
"tf.experimental.numpy.greater_equal(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'greater_equal', '.']]",[]
"tf.experimental.numpy.heaviside(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'heaviside', '.']]",[]
"tf.experimental.numpy.hsplit(
    ary, indices_or_sections
)
","[[""TensorFlow variant of NumPy's "", 'hsplit', '.']]",[]
"tf.experimental.numpy.hstack(
    tup
)
","[[""TensorFlow variant of NumPy's "", 'hstack', '.']]",[]
"tf.experimental.numpy.hypot(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'hypot', '.']]",[]
"tf.experimental.numpy.identity(
    n, dtype=float
)
","[[""TensorFlow variant of NumPy's "", 'identity', '.']]",[]
"tf.experimental.numpy.iinfo(
    int_type
)
",[],"ii16 = np.iinfo(np.int16)
ii16.min
-32768
ii16.max
32767
ii32 = np.iinfo(np.int32)
ii32.min
-2147483648
ii32.max
2147483647"
"tf.experimental.numpy.imag(
    val
)
","[[""TensorFlow variant of NumPy's "", 'imag', '.']]",[]
"tf.experimental.numpy.inner(
    a, b
)
","[[""TensorFlow variant of NumPy's "", 'inner', '.']]",[]
"tf.experimental.numpy.int16(
    *args, **kwargs
)
","[['Signed integer type, compatible with C ', 'short', '.']]","all()
"
"tf.experimental.numpy.int32(
    *args, **kwargs
)
","[['Signed integer type, compatible with C ', 'int', '.']]","all()
"
"tf.experimental.numpy.int64(
    *args, **kwargs
)
","[['Signed integer type, compatible with Python ', 'int', ' and C ', 'long', '.']]","all()
"
"tf.experimental.numpy.int8(
    *args, **kwargs
)
","[['Signed integer type, compatible with C ', 'char', '.']]","all()
"
"tf.experimental.numpy.int64(
    *args, **kwargs
)
","[['Signed integer type, compatible with Python ', 'int', ' and C ', 'long', '.']]","all()
"
"tf.experimental.numpy.isclose(
    a, b, rtol=1e-05, atol=1e-08, equal_nan=False
)
","[[""TensorFlow variant of NumPy's "", 'isclose', '.']]",[]
"tf.experimental.numpy.iscomplex(
    x
)
","[[""TensorFlow variant of NumPy's "", 'iscomplex', '.']]",[]
"tf.experimental.numpy.iscomplexobj(
    x
)
","[[""TensorFlow variant of NumPy's "", 'iscomplexobj', '.']]",[]
"tf.experimental.numpy.isfinite(
    x
)
","[[""TensorFlow variant of NumPy's "", 'isfinite', '.']]",[]
"tf.experimental.numpy.isinf(
    x
)
","[[""TensorFlow variant of NumPy's "", 'isinf', '.']]",[]
"tf.experimental.numpy.isnan(
    x
)
","[[""TensorFlow variant of NumPy's "", 'isnan', '.']]",[]
"tf.experimental.numpy.isneginf(
    x
)
","[[""TensorFlow variant of NumPy's "", 'isneginf', '.']]",[]
"tf.experimental.numpy.isposinf(
    x
)
","[[""TensorFlow variant of NumPy's "", 'isposinf', '.']]",[]
"tf.experimental.numpy.isreal(
    x
)
","[[""TensorFlow variant of NumPy's "", 'isreal', '.']]",[]
"tf.experimental.numpy.isrealobj(
    x
)
","[[""TensorFlow variant of NumPy's "", 'isrealobj', '.']]",[]
"tf.experimental.numpy.isscalar(
    num
)
","[[""TensorFlow variant of NumPy's "", 'isscalar', '.']]",[]
"tf.experimental.numpy.issubdtype(
    arg1, arg2
)
",[],"ints = np.array([1, 2, 3], dtype=np.int32)
np.issubdtype(ints.dtype, np.integer)
True
np.issubdtype(ints.dtype, np.floating)
False"
"tf.experimental.numpy.ix_(
    *args
)
","[[""TensorFlow variant of NumPy's "", 'ix_', '.']]",[]
"tf.experimental.numpy.kron(
    a, b
)
","[[""TensorFlow variant of NumPy's "", 'kron', '.']]",[]
"tf.experimental.numpy.lcm(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'lcm', '.']]",[]
"tf.experimental.numpy.less(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'less', '.']]",[]
"tf.experimental.numpy.less_equal(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'less_equal', '.']]",[]
"tf.experimental.numpy.linspace(
    start, stop, num=50, endpoint=True, retstep=False, dtype=float, axis=0
)
","[[""TensorFlow variant of NumPy's "", 'linspace', '.']]",[]
"tf.experimental.numpy.log(
    x
)
","[[""TensorFlow variant of NumPy's "", 'log', '.']]",[]
"tf.experimental.numpy.log10(
    x
)
","[[""TensorFlow variant of NumPy's "", 'log10', '.']]",[]
"tf.experimental.numpy.log1p(
    x
)
","[[""TensorFlow variant of NumPy's "", 'log1p', '.']]",[]
"tf.experimental.numpy.log2(
    x
)
","[[""TensorFlow variant of NumPy's "", 'log2', '.']]",[]
"tf.experimental.numpy.logaddexp(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'logaddexp', '.']]",[]
"tf.experimental.numpy.logaddexp2(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'logaddexp2', '.']]",[]
"tf.experimental.numpy.logical_and(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'logical_and', '.']]",[]
"tf.experimental.numpy.logical_not(
    x
)
","[[""TensorFlow variant of NumPy's "", 'logical_not', '.']]",[]
"tf.experimental.numpy.logical_or(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'logical_or', '.']]",[]
"tf.experimental.numpy.logical_xor(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'logical_xor', '.']]",[]
"tf.experimental.numpy.logspace(
    start, stop, num=50, endpoint=True, base=10.0, dtype=None, axis=0
)
","[[""TensorFlow variant of NumPy's "", 'logspace', '.']]",[]
"tf.experimental.numpy.matmul(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'matmul', '.']]",[]
"tf.experimental.numpy.max(
    a, axis=None, keepdims=None
)
","[[""TensorFlow variant of NumPy's "", 'max', '.']]",[]
"tf.experimental.numpy.maximum(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'maximum', '.']]",[]
"tf.experimental.numpy.mean(
    a, axis=None, dtype=None, out=None, keepdims=None
)
","[[""TensorFlow variant of NumPy's "", 'mean', '.']]",[]
"tf.experimental.numpy.meshgrid(
    *xi, **kwargs
)
","[[""TensorFlow variant of NumPy's "", 'meshgrid', '.']]",[]
"tf.experimental.numpy.min(
    a, axis=None, keepdims=None
)
","[[""TensorFlow variant of NumPy's "", 'min', '.']]",[]
"tf.experimental.numpy.minimum(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'minimum', '.']]",[]
"tf.experimental.numpy.mod(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'mod', '.']]",[]
"tf.experimental.numpy.moveaxis(
    a, source, destination
)
","[[""TensorFlow variant of NumPy's "", 'moveaxis', '.']]",[]
"tf.experimental.numpy.multiply(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'multiply', '.']]",[]
"tf.experimental.numpy.nanmean(
    a, axis=None, dtype=None, keepdims=None
)
","[[""TensorFlow variant of NumPy's "", 'nanmean', '.']]",[]
"tf.experimental.numpy.nanprod(
    a, axis=None, dtype=None, keepdims=False
)
","[[""TensorFlow variant of NumPy's "", 'nanprod', '.']]",[]
"tf.experimental.numpy.nansum(
    a, axis=None, dtype=None, keepdims=False
)
","[[""TensorFlow variant of NumPy's "", 'nansum', '.']]",[]
"tf.Tensor(
    op, value_index, dtype
)
","[[None, '\n'], ['A ', 'tf.Tensor', ' represents a multidimensional array of elements.']]","# Compute some values using a Tensor
c = tf.constant([[1.0, 2.0], [3.0, 4.0]])
d = tf.constant([[1.0, 1.0], [0.0, 1.0]])
e = tf.matmul(c, d)
print(e)
tf.Tensor(
[[1. 3.]
 [3. 7.]], shape=(2, 2), dtype=float32)"
"tf.experimental.numpy.ndim(
    a
)
","[[""TensorFlow variant of NumPy's "", 'ndim', '.']]",[]
"tf.experimental.numpy.negative(
    x
)
","[[""TensorFlow variant of NumPy's "", 'negative', '.']]",[]
"tf.experimental.numpy.nextafter(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'nextafter', '.']]",[]
"tf.experimental.numpy.nonzero(
    a
)
","[[""TensorFlow variant of NumPy's "", 'nonzero', '.']]",[]
"tf.experimental.numpy.not_equal(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'not_equal', '.']]",[]
"tf.experimental.numpy.object_(
    *args, **kwargs
)
",[],"all()
"
"tf.experimental.numpy.ones(
    shape, dtype=float
)
","[[""TensorFlow variant of NumPy's "", 'ones', '.']]",[]
"tf.experimental.numpy.ones_like(
    a, dtype=None
)
","[[""TensorFlow variant of NumPy's "", 'ones_like', '.']]",[]
"tf.experimental.numpy.outer(
    a, b
)
","[[""TensorFlow variant of NumPy's "", 'outer', '.']]",[]
"tf.experimental.numpy.pad(
    array, pad_width, mode, **kwargs
)
","[[""TensorFlow variant of NumPy's "", 'pad', '.']]",[]
"tf.experimental.numpy.polyval(
    p, x
)
","[[""TensorFlow variant of NumPy's "", 'polyval', '.']]",[]
"tf.experimental.numpy.positive(
    x
)
","[[""TensorFlow variant of NumPy's "", 'positive', '.']]",[]
"tf.experimental.numpy.power(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'power', '.']]",[]
"tf.experimental.numpy.prod(
    a, axis=None, dtype=None, keepdims=None
)
","[[""TensorFlow variant of NumPy's "", 'prod', '.']]",[]
"tf.experimental.numpy.promote_types(
    type1, type2
)
","[[""TensorFlow variant of NumPy's "", 'promote_types', '.']]",[]
"tf.experimental.numpy.ptp(
    a, axis=None, keepdims=None
)
","[[""TensorFlow variant of NumPy's "", 'ptp', '.']]",[]
"tf.experimental.numpy.rad2deg(
    x
)
","[[""TensorFlow variant of NumPy's "", 'rad2deg', '.']]",[]
"tf.experimental.numpy.random.poisson(
    lam=1.0, size=None
)
","[[""TensorFlow variant of NumPy's "", 'random.poisson', '.']]",[]
"tf.experimental.numpy.random.rand(
    *size
)
","[[""TensorFlow variant of NumPy's "", 'random.rand', '.']]",[]
"tf.experimental.numpy.random.randint(
    low,
    high=None,
    size=None,
    dtype=tf.experimental.numpy.int64
)
","[[""TensorFlow variant of NumPy's "", 'random.randint', '.']]",[]
"tf.experimental.numpy.random.randn(
    *args
)
","[[""TensorFlow variant of NumPy's "", 'random.randn', '.']]",[]
"tf.experimental.numpy.random.random(
    size=None
)
","[[""TensorFlow variant of NumPy's "", 'random.random', '.']]",[]
"tf.experimental.numpy.random.seed(
    s
)
","[[""TensorFlow variant of NumPy's "", 'random.seed', '.']]",[]
"tf.experimental.numpy.random.standard_normal(
    size=None
)
","[[""TensorFlow variant of NumPy's "", 'random.standard_normal', '.']]",[]
"tf.experimental.numpy.random.uniform(
    low=0.0, high=1.0, size=None
)
","[[""TensorFlow variant of NumPy's "", 'random.uniform', '.']]",[]
"tf.experimental.numpy.ravel(
    a
)
","[[""TensorFlow variant of NumPy's "", 'ravel', '.']]",[]
"tf.experimental.numpy.real(
    val
)
","[[""TensorFlow variant of NumPy's "", 'real', '.']]",[]
"tf.experimental.numpy.reciprocal(
    x
)
","[[""TensorFlow variant of NumPy's "", 'reciprocal', '.']]",[]
"tf.experimental.numpy.remainder(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'remainder', '.']]",[]
"tf.experimental.numpy.repeat(
    a, repeats, axis=None
)
","[[""TensorFlow variant of NumPy's "", 'repeat', '.']]",[]
"tf.experimental.numpy.reshape(
    a, newshape, order='C'
)
","[[""TensorFlow variant of NumPy's "", 'reshape', '.']]",[]
"tf.experimental.numpy.result_type(
    *arrays_and_dtypes
)
","[[""TensorFlow variant of NumPy's "", 'result_type', '.']]",[]
"tf.experimental.numpy.roll(
    a, shift, axis=None
)
","[[""TensorFlow variant of NumPy's "", 'roll', '.']]",[]
"tf.experimental.numpy.rot90(
    m, k=1, axes=(0, 1)
)
","[[""TensorFlow variant of NumPy's "", 'rot90', '.']]",[]
"tf.experimental.numpy.round(
    a, decimals=0
)
","[[""TensorFlow variant of NumPy's "", 'round', '.']]",[]
"tf.experimental.numpy.select(
    condlist, choicelist, default=0
)
","[[""TensorFlow variant of NumPy's "", 'select', '.']]",[]
"tf.experimental.numpy.shape(
    a
)
","[[""TensorFlow variant of NumPy's "", 'shape', '.']]",[]
"tf.experimental.numpy.sign(
    x, out=None, where=None, **kwargs
)
","[[""TensorFlow variant of NumPy's "", 'sign', '.']]",[]
"tf.experimental.numpy.signbit(
    x
)
","[[""TensorFlow variant of NumPy's "", 'signbit', '.']]",[]
"tf.experimental.numpy.sin(
    x
)
","[[""TensorFlow variant of NumPy's "", 'sin', '.']]",[]
"tf.experimental.numpy.sinc(
    x
)
","[[""TensorFlow variant of NumPy's "", 'sinc', '.']]",[]
"tf.experimental.numpy.sinh(
    x
)
","[[""TensorFlow variant of NumPy's "", 'sinh', '.']]",[]
"tf.experimental.numpy.size(
    x, axis=None
)
","[[""TensorFlow variant of NumPy's "", 'size', '.']]",[]
"tf.experimental.numpy.sort(
    a, axis=-1, kind='quicksort', order=None
)
","[[""TensorFlow variant of NumPy's "", 'sort', '.']]",[]
"tf.experimental.numpy.split(
    ary, indices_or_sections, axis=0
)
","[[""TensorFlow variant of NumPy's "", 'split', '.']]",[]
"tf.experimental.numpy.sqrt(
    x
)
","[[""TensorFlow variant of NumPy's "", 'sqrt', '.']]",[]
"tf.experimental.numpy.square(
    x
)
","[[""TensorFlow variant of NumPy's "", 'square', '.']]",[]
"tf.experimental.numpy.squeeze(
    a, axis=None
)
","[[""TensorFlow variant of NumPy's "", 'squeeze', '.']]",[]
"tf.experimental.numpy.stack(
    arrays, axis=0
)
","[[""TensorFlow variant of NumPy's "", 'stack', '.']]",[]
"tf.experimental.numpy.std(
    a, axis=None, keepdims=None
)
","[[""TensorFlow variant of NumPy's "", 'std', '.']]",[]
"tf.experimental.numpy.string_(
    *args, **kwargs
)
",[],"all()
"
"tf.experimental.numpy.subtract(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'subtract', '.']]",[]
"tf.experimental.numpy.sum(
    a, axis=None, dtype=None, keepdims=None
)
","[[""TensorFlow variant of NumPy's "", 'sum', '.']]",[]
"tf.experimental.numpy.swapaxes(
    a, axis1, axis2
)
","[[""TensorFlow variant of NumPy's "", 'swapaxes', '.']]",[]
"tf.experimental.numpy.take(
    a, indices, axis=None, out=None, mode='clip'
)
","[[""TensorFlow variant of NumPy's "", 'take', '.']]",[]
"tf.experimental.numpy.take_along_axis(
    arr, indices, axis
)
","[[""TensorFlow variant of NumPy's "", 'take_along_axis', '.']]",[]
"tf.experimental.numpy.tan(
    x
)
","[[""TensorFlow variant of NumPy's "", 'tan', '.']]",[]
"tf.experimental.numpy.tanh(
    x
)
","[[""TensorFlow variant of NumPy's "", 'tanh', '.']]",[]
"tf.experimental.numpy.tensordot(
    a, b, axes=2
)
","[[""TensorFlow variant of NumPy's "", 'tensordot', '.']]",[]
"tf.experimental.numpy.tile(
    a, reps
)
","[[""TensorFlow variant of NumPy's "", 'tile', '.']]",[]
"tf.experimental.numpy.trace(
    a, offset=0, axis1=0, axis2=1, dtype=None
)
","[[""TensorFlow variant of NumPy's "", 'trace', '.']]",[]
"tf.experimental.numpy.transpose(
    a, axes=None
)
","[[""TensorFlow variant of NumPy's "", 'transpose', '.']]",[]
"tf.experimental.numpy.tri(
    N, M=None, k=0, dtype=None
)
","[[""TensorFlow variant of NumPy's "", 'tri', '.']]",[]
"tf.experimental.numpy.tril(
    m, k=0
)
","[[""TensorFlow variant of NumPy's "", 'tril', '.']]",[]
"tf.experimental.numpy.triu(
    m, k=0
)
","[[""TensorFlow variant of NumPy's "", 'triu', '.']]",[]
"tf.experimental.numpy.true_divide(
    x1, x2
)
","[[""TensorFlow variant of NumPy's "", 'true_divide', '.']]",[]
"tf.experimental.numpy.uint16(
    *args, **kwargs
)
","[['Unsigned integer type, compatible with C ', 'unsigned short', '.']]","all()
"
"tf.experimental.numpy.uint32(
    *args, **kwargs
)
","[['Unsigned integer type, compatible with C ', 'unsigned int', '.']]","all()
"
"tf.experimental.numpy.uint64(
    *args, **kwargs
)
","[['Unsigned integer type, compatible with C ', 'unsigned long', '.']]","all()
"
"tf.experimental.numpy.uint8(
    *args, **kwargs
)
","[['Unsigned integer type, compatible with C ', 'unsigned char', '.']]","all()
"
"tf.experimental.numpy.unicode_(
    *args, **kwargs
)
",[],"m = memoryview(np.str_(""abc""))
m.format
'3w'
m.tobytes()
b'a\x00\x00\x00b\x00\x00\x00c\x00\x00\x00'"
"tf.experimental.numpy.vander(
    x, N=None, increasing=False
)
","[[""TensorFlow variant of NumPy's "", 'vander', '.']]",[]
"tf.experimental.numpy.var(
    a, axis=None, dtype=None, out=None, ddof=0, keepdims=None
)
","[[""TensorFlow variant of NumPy's "", 'var', '.']]",[]
"tf.experimental.numpy.vdot(
    a, b
)
","[[""TensorFlow variant of NumPy's "", 'vdot', '.']]",[]
"tf.experimental.numpy.vsplit(
    ary, indices_or_sections
)
","[[""TensorFlow variant of NumPy's "", 'vsplit', '.']]",[]
"tf.experimental.numpy.vstack(
    tup
)
","[[""TensorFlow variant of NumPy's "", 'vstack', '.']]",[]
"tf.experimental.numpy.where(
    condition, x=None, y=None
)
","[[""TensorFlow variant of NumPy's "", 'where', '.']]",[]
"tf.experimental.numpy.zeros(
    shape, dtype=float
)
","[[""TensorFlow variant of NumPy's "", 'zeros', '.']]",[]
"tf.experimental.numpy.zeros_like(
    a, dtype=None
)
","[[""TensorFlow variant of NumPy's "", 'zeros_like', '.']]",[]
"tf.experimental.register_filesystem_plugin(
    plugin_location
)
",[],[]
"tf.experimental.tensorrt.ConversionParams(
    max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
    precision_mode=TrtPrecisionMode.FP32,
    minimum_segment_size=3,
    maximum_cached_engines=1,
    use_calibration=True,
    allow_build_at_runtime=True
)
",[],[]
"tf.experimental.tensorrt.Converter(
    input_saved_model_dir=None,
    input_saved_model_tags=None,
    input_saved_model_signature_key=None,
    use_dynamic_shape=None,
    dynamic_shape_profile_strategy=None,
    max_workspace_size_bytes=DEFAULT_TRT_MAX_WORKSPACE_SIZE_BYTES,
    precision_mode=TrtPrecisionMode.FP32,
    minimum_segment_size=3,
    maximum_cached_engines=1,
    use_calibration=True,
    allow_build_at_runtime=True,
    conversion_params=None
)
",[],"build(
    input_fn
)
"
"tf.experimental.unregister_dispatch_for(
    dispatch_target
)
","[['Unregisters a function that was registered with ', '@dispatch_for_*', '.']]","# Define a type and register a dispatcher to override `tf.abs`:
class MyTensor(tf.experimental.ExtensionType):
  value: tf.Tensor
@tf.experimental.dispatch_for_api(tf.abs)
def my_abs(x: MyTensor):
  return MyTensor(tf.abs(x.value))
tf.abs(MyTensor(5))
MyTensor(value=<tf.Tensor: shape=(), dtype=int32, numpy=5>)"
"tf.extract_volume_patches(
    input, ksizes, strides, padding, name=None
)
","[['Extract ', 'patches', ' from ', 'input', ' and put them in the ', '""depth""', ' output dimension. 3D extension of ', 'extract_image_patches', '.']]","ksizes = [1, ksize_planes, ksize_rows, ksize_cols, 1]
strides = [1, stride_planes, strides_rows, strides_cols, 1]
"
"tf.eye(
    num_rows,
    num_columns=None,
    batch_shape=None,
    dtype=tf.dtypes.float32,
    name=None
)
",[],"# Construct one identity matrix.
tf.eye(2)
==> [[1., 0.],
     [0., 1.]]

# Construct a batch of 3 identity matrices, each 2 x 2.
# batch_identity[i, :, :] is a 2 x 2 identity matrix, i = 0, 1, 2.
batch_identity = tf.eye(2, batch_shape=[3])

# Construct one 2 x 3 ""identity"" matrix
tf.eye(2, num_columns=3)
==> [[ 1.,  0.,  0.],
     [ 0.,  1.,  0.]]
"
"tf.fill(
    dims, value, name=None
)
",[],"tf.fill([2, 3], 9)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[9, 9, 9],
       [9, 9, 9]], dtype=int32)>"
"tf.fingerprint(
    data, method='farmhash64', name=None
)
",[],"tf.fingerprint(data) == tf.fingerprint(tf.reshape(data, ...))
tf.fingerprint(data) == tf.fingerprint(tf.bitcast(data, ...))
"
"tf.math.floor(
    x, name=None
)
",[],"x = tf.constant([1.3324, -1.5, 5.555, -2.532, 0.99, float(""inf"")])
tf.floor(x).numpy()
array([ 1., -2.,  5., -3.,  0., inf], dtype=float32)"
"tf.foldl(
    fn,
    elems,
    initializer=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    name=None
)
","[['foldl on the list of tensors unpacked from ', 'elems', ' on dimension 0. (deprecated argument values)']]","elems = tf.constant([1, 2, 3, 4, 5, 6])
sum = tf.foldl(lambda a, x: a + x, elems)
# sum == 21
"
"tf.foldr(
    fn,
    elems,
    initializer=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    name=None
)
","[['foldr on the list of tensors unpacked from ', 'elems', ' on dimension 0. (deprecated argument values)']]","elems = [1, 2, 3, 4, 5, 6]
sum = tf.foldr(lambda a, x: a + x, elems)
# sum == 21
"
"tf.function(
    func=None,
    input_signature=None,
    autograph=True,
    jit_compile=None,
    reduce_retracing=False,
    experimental_implements=None,
    experimental_autograph_options=None,
    experimental_relax_shapes=None,
    experimental_compile=None,
    experimental_follow_type_hints=None
) -> tf.types.experimental.GenericFunction
",[],"@tf.function
def f(x, y):
  return x ** 2 + y
x = tf.constant([2, 3])
y = tf.constant([3, -2])
f(x, y)
<tf.Tensor: ... numpy=array([7, 7], ...)>"
"tf.gather(
    params, indices, validate_indices=None, axis=None, batch_dims=0, name=None
)
","[['Gather slices from params axis ', 'axis', ' according to indices. (deprecated arguments)']]","params = tf.constant(['p0', 'p1', 'p2', 'p3', 'p4', 'p5'])
params[3].numpy()
b'p3'
tf.gather(params, 3).numpy()
b'p3'"
"tf.gather_nd(
    params, indices, batch_dims=0, name=None
)
","[['Gather slices from ', 'params', ' into a Tensor with shape specified by ', 'indices', '.']]","tf.gather_nd(
    indices=[[0, 0],
             [1, 1]],
    params = [['a', 'b'],
              ['c', 'd']]).numpy()
array([b'a', b'd'], dtype=object)"
"tf.get_static_value(
    tensor, partial=False
)
",[],"a = tf.constant(10)
tf.get_static_value(a)
10
b = tf.constant(20)
tf.get_static_value(tf.add(a, b))
30"
"tf.grad_pass_through(
    f
)
",[],"x = tf.Variable(1.0, name=""x"")
z = tf.Variable(3.0, name=""z"")

with tf.GradientTape() as tape:
  # y will evaluate to 9.0
  y = tf.grad_pass_through(x.assign)(z**2)
# grads will evaluate to 6.0
grads = tape.gradient(y, z)
"
"tf.gradients(
    ys,
    xs,
    grad_ys=None,
    name='gradients',
    gate_gradients=False,
    aggregation_method=None,
    stop_gradients=None,
    unconnected_gradients=tf.UnconnectedGradients.NONE
)
","[['Constructs symbolic derivatives of sum of ', 'ys', ' w.r.t. x in ', 'xs', '.']]","@tf.function
def example():
  a = tf.constant(0.)
  b = 2 * a
  return tf.gradients(a + b, [a, b], stop_gradients=[a, b])
example()
[<tf.Tensor: shape=(), dtype=float32, numpy=1.0>,
<tf.Tensor: shape=(), dtype=float32, numpy=1.0>]"
"tf.math.greater(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5, 2, 5])
tf.math.greater(x, y) ==> [False, True, True]

x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.greater(x, y) ==> [False, False, True]
"
"tf.math.greater_equal(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6, 7])
y = tf.constant([5, 2, 5, 10])
tf.math.greater_equal(x, y) ==> [True, True, True, False]

x = tf.constant([5, 4, 6, 7])
y = tf.constant([5])
tf.math.greater_equal(x, y) ==> [True, False, True, True]
"
"tf.group(
    *inputs, **kwargs
)
",[],"with tf.control_dependencies([a, b]):
    c = tf.no_op()
"
"tf.guarantee_const(
    input, name=None
)
",[],[]
"tf.hessians(
    ys,
    xs,
    gate_gradients=False,
    aggregation_method=None,
    name='hessians'
)
","[['Constructs the Hessian of sum of ', 'ys', ' with respect to ', 'x', ' in ', 'xs', '.']]",[]
"tf.histogram_fixed_width(
    values,
    value_range,
    nbins=100,
    dtype=tf.dtypes.int32,
    name=None
)
",[],"# Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]
hist = tf.histogram_fixed_width(new_values, value_range, nbins=5)
hist.numpy()
array([2, 1, 1, 0, 2], dtype=int32)"
"tf.histogram_fixed_width_bins(
    values,
    value_range,
    nbins=100,
    dtype=tf.dtypes.int32,
    name=None
)
",[],"# Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]
indices = tf.histogram_fixed_width_bins(new_values, value_range, nbins=5)
indices.numpy()
array([0, 0, 1, 2, 4, 4], dtype=int32)"
"tf.identity(
    input, name=None
)
",[],"a = tf.constant([0.78])
a_identity = tf.identity(a)
a.numpy()
array([0.78], dtype=float32)
a_identity.numpy()
array([0.78], dtype=float32)"
"tf.identity_n(
    input, name=None
)
",[],"with tf.get_default_graph().gradient_override_map(
    {'IdentityN': 'OverrideGradientWithG'}):
  y, _ = identity_n([f(x), x])

@tf.RegisterGradient('OverrideGradientWithG')
def ApplyG(op, dy, _):
  return [None, g(dy)]  # Do not backprop to f(x).
"
"tf.image.adjust_brightness(
    image, delta
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_brightness(x, delta=0.1)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.1,  2.1,  3.1],
        [ 4.1,  5.1,  6.1]],
       [[ 7.1,  8.1,  9.1],
        [10.1, 11.1, 12.1]]], dtype=float32)>"
"tf.image.adjust_contrast(
    images, contrast_factor
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_contrast(x, 2.)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[-3.5, -2.5, -1.5],
        [ 2.5,  3.5,  4.5]],
       [[ 8.5,  9.5, 10.5],
        [14.5, 15.5, 16.5]]], dtype=float32)>"
"tf.image.adjust_gamma(
    image, gamma=1, gain=1
)
","[['Performs ', 'Gamma Correction', '.']]","x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_gamma(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[1.       , 1.1486983, 1.2457309],
        [1.319508 , 1.3797297, 1.4309691]],
       [[1.4757731, 1.5157166, 1.5518456],
        [1.5848932, 1.6153942, 1.6437519]]], dtype=float32)>"
"tf.image.adjust_hue(
    image, delta, name=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_hue(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 2.3999996,  1.       ,  3.       ],
        [ 5.3999996,  4.       ,  6.       ]],
      [[ 8.4      ,  7.       ,  9.       ],
        [11.4      , 10.       , 12.       ]]], dtype=float32)>"
"tf.image.adjust_jpeg_quality(
    image, jpeg_quality, name=None
)
",[],"x = [[[0.01, 0.02, 0.03],
      [0.04, 0.05, 0.06]],
     [[0.07, 0.08, 0.09],
      [0.10, 0.11, 0.12]]]
x_jpeg = tf.image.adjust_jpeg_quality(x, 75)
x_jpeg.numpy()
array([[[0.00392157, 0.01960784, 0.03137255],
        [0.02745098, 0.04313726, 0.05490196]],
       [[0.05882353, 0.07450981, 0.08627451],
        [0.08235294, 0.09803922, 0.10980393]]], dtype=float32)"
"tf.image.adjust_saturation(
    image, saturation_factor, name=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_saturation(x, 0.5)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 2. ,  2.5,  3. ],
        [ 5. ,  5.5,  6. ]],
       [[ 8. ,  8.5,  9. ],
        [11. , 11.5, 12. ]]], dtype=float32)>"
"tf.image.central_crop(
    image, central_fraction
)
",[]," --------
|        |
|  XXXX  |
|  XXXX  |
|        |   where ""X"" is the central 50% of the image.
 --------
"
"tf.image.combined_non_max_suppression(
    boxes,
    scores,
    max_output_size_per_class,
    max_total_size,
    iou_threshold=0.5,
    score_threshold=float('-inf'),
    pad_per_class=False,
    clip_boxes=True,
    name=None
)
",[],[]
"tf.image.convert_image_dtype(
    image, dtype, saturate=False, name=None
)
","[['Convert ', 'image', ' to ', 'dtype', ', scaling its values if needed.']]","x = [[[1, 2, 3], [4, 5, 6]],
     [[7, 8, 9], [10, 11, 12]]]
x_int8 = tf.convert_to_tensor(x, dtype=tf.int8)
tf.image.convert_image_dtype(x_int8, dtype=tf.float16, saturate=False)
<tf.Tensor: shape=(2, 2, 3), dtype=float16, numpy=
array([[[0.00787, 0.01575, 0.02362],
        [0.0315 , 0.03937, 0.04724]],
       [[0.0551 , 0.063  , 0.07086],
        [0.07874, 0.0866 , 0.0945 ]]], dtype=float16)>"
"tf.image.crop_and_resize(
    image,
    boxes,
    box_indices,
    crop_size,
    method='bilinear',
    extrapolation_value=0.0,
    name=None
)
",[],"import tensorflow as tf
BATCH_SIZE = 1
NUM_BOXES = 5
IMAGE_HEIGHT = 256
IMAGE_WIDTH = 256
CHANNELS = 3
CROP_SIZE = (24, 24)

image = tf.random.normal(shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH,
CHANNELS) )
boxes = tf.random.uniform(shape=(NUM_BOXES, 4))
box_indices = tf.random.uniform(shape=(NUM_BOXES,), minval=0,
maxval=BATCH_SIZE, dtype=tf.int32)
output = tf.image.crop_and_resize(image, boxes, box_indices, CROP_SIZE)
output.shape  #=> (5, 24, 24, 3)
"
"tf.image.crop_to_bounding_box(
    image, offset_height, offset_width, target_height, target_width
)
","[['Crops an ', 'image', ' to a specified bounding box.']]","image = tf.constant(np.arange(1, 28, dtype=np.float32), shape=[3, 3, 3])
image[:,:,0] # print the first channel of the 3-D tensor
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.,  4.,  7.],
       [10., 13., 16.],
       [19., 22., 25.]], dtype=float32)>
cropped_image = tf.image.crop_to_bounding_box(image, 0, 0, 2, 2)
cropped_image[:,:,0] # print the first channel of the cropped 3-D tensor
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[ 1.,  4.],
       [10., 13.]], dtype=float32)>"
"tf.io.decode_and_crop_jpeg(
    contents,
    crop_window,
    channels=0,
    ratio=1,
    fancy_upscaling=True,
    try_recover_truncated=False,
    acceptable_fraction=1,
    dct_method='',
    name=None
)
",[],[]
"tf.io.decode_bmp(
    contents, channels=0, name=None
)
",[],[]
"tf.io.decode_gif(
    contents, name=None
)
","[[None, '\n']]","convert \\(src.gif -coalesce \\)dst.gif
"
"tf.io.decode_image(
    contents,
    channels=None,
    dtype=tf.dtypes.uint8,
    name=None,
    expand_animations=True
)
","[['Function for ', 'decode_bmp', ', ', 'decode_gif', ', ', 'decode_jpeg', ', and ', 'decode_png', '.']]",[]
"tf.io.decode_jpeg(
    contents,
    channels=0,
    ratio=1,
    fancy_upscaling=True,
    try_recover_truncated=False,
    acceptable_fraction=1,
    dct_method='',
    name=None
)
",[],[]
"tf.io.decode_png(
    contents,
    channels=0,
    dtype=tf.dtypes.uint8,
    name=None
)
",[],[]
"tf.image.draw_bounding_boxes(
    images, boxes, colors, name=None
)
",[],"# create an empty image
img = tf.zeros([1, 3, 3, 3])
# draw a box around the image
box = np.array([0, 0, 1, 1])
boxes = box.reshape([1, 1, 4])
# alternate between red and blue
colors = np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]])
tf.image.draw_bounding_boxes(img, boxes, colors)
<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]],
        [[1., 0., 0.],
        [0., 0., 0.],
        [1., 0., 0.]],
        [[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]]]], dtype=float32)>"
"tf.io.encode_jpeg(
    image,
    format='',
    quality=95,
    progressive=False,
    optimize_size=False,
    chroma_downsampling=True,
    density_unit='in',
    x_density=300,
    y_density=300,
    xmp_metadata='',
    name=None
)
",[],[]
"tf.io.encode_png(
    image, compression=-1, name=None
)
",[],[]
"tf.image.extract_glimpse(
    input,
    size,
    offsets,
    centered=True,
    normalized=True,
    noise='uniform',
    name=None
)
",[],"x = [[[[0.0],
          [1.0],
          [2.0]],
         [[3.0],
          [4.0],
          [5.0]],
         [[6.0],
          [7.0],
          [8.0]]]]
tf.image.extract_glimpse(x, size=(2, 2), offsets=[[1, 1]],
                        centered=False, normalized=False)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
array([[[[4.],
         [5.]],
        [[7.],
         [8.]]]], dtype=float32)>"
"tf.io.extract_jpeg_shape(
    contents,
    output_type=tf.dtypes.int32,
    name=None
)
",[],[]
"tf.image.extract_patches(
    images, sizes, strides, rates, padding, name=None
)
","[['Extract ', 'patches', ' from ', 'images', '.']]","  n = 10
  # images is a 1 x 10 x 10 x 1 array that contains the numbers 1 through 100
  images = [[[[x * n + y + 1] for y in range(n)] for x in range(n)]]

  # We generate two outputs as follows:
  # 1. 3x3 patches with stride length 5
  # 2. Same as above, but the rate is increased to 2
  tf.image.extract_patches(images=images,
                           sizes=[1, 3, 3, 1],
                           strides=[1, 5, 5, 1],
                           rates=[1, 1, 1, 1],
                           padding='VALID')

  # Yields:
  [[[[ 1  2  3 11 12 13 21 22 23]
     [ 6  7  8 16 17 18 26 27 28]]
    [[51 52 53 61 62 63 71 72 73]
     [56 57 58 66 67 68 76 77 78]]]]
"
"tf.image.flip_left_right(
    image
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.flip_left_right(x)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 4.,  5.,  6.],
        [ 1.,  2.,  3.]],
       [[10., 11., 12.],
        [ 7.,  8.,  9.]]], dtype=float32)>"
"tf.image.flip_up_down(
    image
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.flip_up_down(x)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 7.,  8.,  9.],
        [10., 11., 12.]],
       [[ 1.,  2.,  3.],
        [ 4.,  5.,  6.]]], dtype=float32)>"
"tf.image.generate_bounding_box_proposals(
    scores,
    bbox_deltas,
    image_info,
    anchors,
    nms_threshold=0.7,
    pre_nms_topn=6000,
    min_size=16,
    post_nms_topn=300,
    name=None
)
",[],[]
"tf.image.grayscale_to_rgb(
    images, name=None
)
",[],"original = tf.constant([[[1.0], [2.0], [3.0]]])
converted = tf.image.grayscale_to_rgb(original)
print(converted.numpy())
[[[1. 1. 1.]
  [2. 2. 2.]
  [3. 3. 3.]]]"
"tf.image.hsv_to_rgb(
    images, name=None
)
",[],[]
"tf.image.image_gradients(
    image
)
",[],"BATCH_SIZE = 1
IMAGE_HEIGHT = 5
IMAGE_WIDTH = 5
CHANNELS = 1
image = tf.reshape(tf.range(IMAGE_HEIGHT * IMAGE_WIDTH * CHANNELS,
  delta=1, dtype=tf.float32),
  shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))
dy, dx = tf.image.image_gradients(image)
print(image[0, :,:,0])
tf.Tensor(
  [[ 0.  1.  2.  3.  4.]
  [ 5.  6.  7.  8.  9.]
  [10. 11. 12. 13. 14.]
  [15. 16. 17. 18. 19.]
  [20. 21. 22. 23. 24.]], shape=(5, 5), dtype=float32)
print(dy[0, :,:,0])
tf.Tensor(
  [[5. 5. 5. 5. 5.]
  [5. 5. 5. 5. 5.]
  [5. 5. 5. 5. 5.]
  [5. 5. 5. 5. 5.]
  [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)
print(dx[0, :,:,0])
tf.Tensor(
  [[1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]], shape=(5, 5), dtype=float32)
"
"tf.io.is_jpeg(
    contents, name=None
)
",[],[]
"tf.image.non_max_suppression(
    boxes,
    scores,
    max_output_size,
    iou_threshold=0.5,
    score_threshold=float('-inf'),
    name=None
)
",[],"  selected_indices = tf.image.non_max_suppression(
      boxes, scores, max_output_size, iou_threshold)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.non_max_suppression_overlaps(
    overlaps,
    scores,
    max_output_size,
    overlap_threshold=0.5,
    score_threshold=float('-inf'),
    name=None
)
",[],"  selected_indices = tf.image.non_max_suppression_overlaps(
      overlaps, scores, max_output_size, iou_threshold)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.non_max_suppression_padded(
    boxes,
    scores,
    max_output_size,
    iou_threshold=0.5,
    score_threshold=float('-inf'),
    pad_to_max_output_size=False,
    name=None,
    sorted_input=False,
    canonicalized_coordinates=False,
    tile_size=512
)
",[],"  selected_indices_padded, num_valid = tf.image.non_max_suppression_padded(
      boxes, scores, max_output_size, iou_threshold,
      score_threshold, pad_to_max_output_size=True)
  selected_indices = tf.slice(
      selected_indices_padded, tf.constant([0]), num_valid)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.non_max_suppression_with_scores(
    boxes,
    scores,
    max_output_size,
    iou_threshold=0.5,
    score_threshold=float('-inf'),
    soft_nms_sigma=0.0,
    name=None
)
",[],"  selected_indices, selected_scores = tf.image.non_max_suppression_padded(
      boxes, scores, max_output_size, iou_threshold=1.0, score_threshold=0.1,
      soft_nms_sigma=0.5)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.pad_to_bounding_box(
    image, offset_height, offset_width, target_height, target_width
)
","[['Pad ', 'image', ' with zeros to the specified ', 'height', ' and ', 'width', '.']]","x = [[[1., 2., 3.],
      [4., 5., 6.]],
      [[7., 8., 9.],
      [10., 11., 12.]]]
padded_image = tf.image.pad_to_bounding_box(x, 1, 1, 4, 4)
padded_image
<tf.Tensor: shape=(4, 4, 3), dtype=float32, numpy=
array([[[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.]],
[[ 0.,  0.,  0.],
[ 1.,  2.,  3.],
[ 4.,  5.,  6.],
[ 0.,  0.,  0.]],
[[ 0.,  0.,  0.],
[ 7.,  8.,  9.],
[10., 11., 12.],
[ 0.,  0.,  0.]],
[[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.]]], dtype=float32)>"
"tf.image.per_image_standardization(
    image
)
","[['Linearly scales each image in ', 'image', ' to have mean 0 and variance 1.']]","image = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])
image # 3-D tensor
<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=
array([[[ 1,  2,  3],
        [ 4,  5,  6]],
       [[ 7,  8,  9],
        [10, 11, 12]]], dtype=int32)>
new_image = tf.image.per_image_standardization(image)
new_image # 3-D tensor with mean ~= 0 and variance ~= 1
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[-1.593255  , -1.3035723 , -1.0138896 ],
        [-0.7242068 , -0.4345241 , -0.14484136]],
       [[ 0.14484136,  0.4345241 ,  0.7242068 ],
        [ 1.0138896 ,  1.3035723 ,  1.593255  ]]], dtype=float32)>"
"tf.image.psnr(
    a, b, max_val, name=None
)
",[],"    # Read images from file.
    im1 = tf.decode_png('path/to/im1.png')
    im2 = tf.decode_png('path/to/im2.png')
    # Compute PSNR over tf.uint8 Tensors.
    psnr1 = tf.image.psnr(im1, im2, max_val=255)

    # Compute PSNR over tf.float32 Tensors.
    im1 = tf.image.convert_image_dtype(im1, tf.float32)
    im2 = tf.image.convert_image_dtype(im2, tf.float32)
    psnr2 = tf.image.psnr(im1, im2, max_val=1.0)
    # psnr1 and psnr2 both have type tf.float32 and are almost equal.
"
"tf.image.random_brightness(
    image, max_delta, seed=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_brightness(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>"
"tf.image.random_contrast(
    image, lower, upper, seed=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_contrast(x, 0.2, 0.5)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>"
"tf.image.random_crop(
    value, size, seed=None, name=None
)
",[],"image = [[1, 2, 3], [4, 5, 6]]
result = tf.image.random_crop(value=image, size=(1, 3))
result.shape.as_list()
[1, 3]"
"tf.image.random_flip_left_right(
    image, seed=None
)
",[],"image = np.array([[[1], [2]], [[3], [4]]])
tf.image.random_flip_left_right(image, 5).numpy().tolist()
[[[2], [1]], [[4], [3]]]"
"tf.image.random_flip_up_down(
    image, seed=None
)
",[],"image = np.array([[[1], [2]], [[3], [4]]])
tf.image.random_flip_up_down(image, 3).numpy().tolist()
[[[3], [4]], [[1], [2]]]"
"tf.image.random_hue(
    image, max_delta, seed=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_hue(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>"
"tf.image.random_jpeg_quality(
    image, min_jpeg_quality, max_jpeg_quality, seed=None
)
",[],"x = tf.constant([[[1, 2, 3],
                  [4, 5, 6]],
                 [[7, 8, 9],
                  [10, 11, 12]]], dtype=tf.uint8)
tf.image.random_jpeg_quality(x, 75, 95)
<tf.Tensor: shape=(2, 2, 3), dtype=uint8, numpy=...>"
"tf.image.random_saturation(
    image, lower, upper, seed=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_saturation(x, 5, 10)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 0. ,  1.5,  3. ],
        [ 0. ,  3. ,  6. ]],
       [[ 0. ,  4.5,  9. ],
        [ 0. ,  6. , 12. ]]], dtype=float32)>"
"tf.image.resize(
    images,
    size,
    method=ResizeMethod.BILINEAR,
    preserve_aspect_ratio=False,
    antialias=False,
    name=None
)
","[['Resize ', 'images', ' to ', 'size', ' using the specified ', 'method', '.']]","image = tf.constant([
 [1,0,0,0,0],
 [0,1,0,0,0],
 [0,0,1,0,0],
 [0,0,0,1,0],
 [0,0,0,0,1],
])
# Add ""batch"" and ""channels"" dimensions
image = image[tf.newaxis, ..., tf.newaxis]
image.shape.as_list()  # [batch, height, width, channels]
[1, 5, 5, 1]
tf.image.resize(image, [3,5])[0,...,0].numpy()
array([[0.6666667, 0.3333333, 0.       , 0.       , 0.       ],
       [0.       , 0.       , 1.       , 0.       , 0.       ],
       [0.       , 0.       , 0.       , 0.3333335, 0.6666665]],
      dtype=float32)"
"tf.image.resize_with_crop_or_pad(
    image, target_height, target_width
)
",[],"image = np.arange(75).reshape(5, 5, 3)  # create 3-D image input
image[:,:,0]  # print first channel just for demo purposes
array([[ 0,  3,  6,  9, 12],
       [15, 18, 21, 24, 27],
       [30, 33, 36, 39, 42],
       [45, 48, 51, 54, 57],
       [60, 63, 66, 69, 72]])
image = tf.image.resize_with_crop_or_pad(image, 3, 3)  # crop
# print first channel for demo purposes; centrally cropped output
image[:,:,0]
<tf.Tensor: shape=(3, 3), dtype=int64, numpy=
array([[18, 21, 24],
       [33, 36, 39],
       [48, 51, 54]])>"
"tf.image.resize_with_pad(
    image,
    target_height,
    target_width,
    method=ResizeMethod.BILINEAR,
    antialias=False
)
",[],[]
"tf.image.rgb_to_grayscale(
    images, name=None
)
",[],"original = tf.constant([[[1.0, 2.0, 3.0]]])
converted = tf.image.rgb_to_grayscale(original)
print(converted.numpy())
[[[1.81...]]]"
"tf.image.rgb_to_hsv(
    images, name=None
)
",[],"blue_image = tf.stack([
   tf.zeros([5,5]),
   tf.zeros([5,5]),
   tf.ones([5,5])],
   axis=-1)
blue_hsv_image = tf.image.rgb_to_hsv(blue_image)
blue_hsv_image[0,0].numpy()
array([0.6666667, 1. , 1. ], dtype=float32)"
"tf.image.rgb_to_yiq(
    images
)
",[],"x = tf.constant([[[1.0, 2.0, 3.0]]])
tf.image.rgb_to_yiq(x)
<tf.Tensor: shape=(1, 1, 3), dtype=float32,
numpy=array([[[ 1.815     , -0.91724455,  0.09962624]]], dtype=float32)>"
"tf.image.rgb_to_yuv(
    images
)
",[],[]
"tf.image.rot90(
    image, k=1, name=None
)
",[],"a=tf.constant([[[1],[2]],
               [[3],[4]]])
# rotating `a` counter clockwise by 90 degrees
a_rot=tf.image.rot90(a)
print(a_rot[...,0].numpy())
[[2 4]
 [1 3]]
# rotating `a` counter clockwise by 270 degrees
a_rot=tf.image.rot90(a, k=3)
print(a_rot[...,0].numpy())
[[3 1]
 [4 2]]
# rotating `a` clockwise by 180 degrees
a_rot=tf.image.rot90(a, k=-2)
print(a_rot[...,0].numpy())
[[4 3]
 [2 1]]"
"tf.image.sample_distorted_bounding_box(
    image_size,
    bounding_boxes,
    seed=0,
    min_object_covered=0.1,
    aspect_ratio_range=None,
    area_range=None,
    max_attempts=None,
    use_image_if_no_bounding_boxes=None,
    name=None
)
",[],"    # Generate a single distorted bounding box.
    begin, size, bbox_for_draw = tf.image.sample_distorted_bounding_box(
        tf.shape(image),
        bounding_boxes=bounding_boxes,
        min_object_covered=0.1)

    # Draw the bounding box in an image summary.
    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),
                                                  bbox_for_draw)
    tf.compat.v1.summary.image('images_with_box', image_with_box)

    # Employ the bounding box to distort the image.
    distorted_image = tf.slice(image, begin, size)
"
"tf.image.sobel_edges(
    image
)
",[],"image_bytes = tf.io.read_file(path_to_image_file)
image = tf.image.decode_image(image_bytes)
image = tf.cast(image, tf.float32)
image = tf.expand_dims(image, 0)
"
"tf.image.ssim(
    img1,
    img2,
    max_val,
    filter_size=11,
    filter_sigma=1.5,
    k1=0.01,
    k2=0.03,
    return_index_map=False
)
",[],"    # Read images (of size 255 x 255) from file.
    im1 = tf.image.decode_image(tf.io.read_file('path/to/im1.png'))
    im2 = tf.image.decode_image(tf.io.read_file('path/to/im2.png'))
    tf.shape(im1)  # `img1.png` has 3 channels; shape is `(255, 255, 3)`
    tf.shape(im2)  # `img2.png` has 3 channels; shape is `(255, 255, 3)`
    # Add an outer batch for each image.
    im1 = tf.expand_dims(im1, axis=0)
    im2 = tf.expand_dims(im2, axis=0)
    # Compute SSIM over tf.uint8 Tensors.
    ssim1 = tf.image.ssim(im1, im2, max_val=255, filter_size=11,
                          filter_sigma=1.5, k1=0.01, k2=0.03)

    # Compute SSIM over tf.float32 Tensors.
    im1 = tf.image.convert_image_dtype(im1, tf.float32)
    im2 = tf.image.convert_image_dtype(im2, tf.float32)
    ssim2 = tf.image.ssim(im1, im2, max_val=1.0, filter_size=11,
                          filter_sigma=1.5, k1=0.01, k2=0.03)
    # ssim1 and ssim2 both have type tf.float32 and are almost equal.
"
"tf.image.ssim_multiscale(
    img1,
    img2,
    max_val,
    power_factors=_MSSSIM_WEIGHTS,
    filter_size=11,
    filter_sigma=1.5,
    k1=0.01,
    k2=0.03
)
",[],[]
"tf.image.stateless_random_brightness(
    image, max_delta, seed
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
seed = (1, 2)
tf.image.stateless_random_brightness(x, 0.2, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.1376241,  2.1376243,  3.1376243],
        [ 4.1376243,  5.1376243,  6.1376243]],
       [[ 7.1376243,  8.137624 ,  9.137624 ],
        [10.137624 , 11.137624 , 12.137624 ]]], dtype=float32)>"
"tf.image.stateless_random_contrast(
    image, lower, upper, seed
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
seed = (1, 2)
tf.image.stateless_random_contrast(x, 0.2, 0.5, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[3.4605184, 4.4605184, 5.4605184],
        [4.820173 , 5.820173 , 6.820173 ]],
       [[6.179827 , 7.179827 , 8.179828 ],
        [7.5394816, 8.539482 , 9.539482 ]]], dtype=float32)>"
"tf.image.stateless_random_crop(
    value, size, seed, name=None
)
",[],"image = [[[1, 2, 3], [4, 5, 6]], [[7, 8, 9], [10, 11, 12]]]
seed = (1, 2)
tf.image.stateless_random_crop(value=image, size=(1, 2, 3), seed=seed)
<tf.Tensor: shape=(1, 2, 3), dtype=int32, numpy=
array([[[1, 2, 3],
        [4, 5, 6]]], dtype=int32)>"
"tf.image.stateless_random_flip_left_right(
    image, seed
)
",[],"image = np.array([[[1], [2]], [[3], [4]]])
seed = (2, 3)
tf.image.stateless_random_flip_left_right(image, seed).numpy().tolist()
[[[2], [1]], [[4], [3]]]"
"tf.image.stateless_random_flip_up_down(
    image, seed
)
",[],"image = np.array([[[1], [2]], [[3], [4]]])
seed = (2, 3)
tf.image.stateless_random_flip_up_down(image, seed).numpy().tolist()
[[[3], [4]], [[1], [2]]]"
"tf.image.stateless_random_hue(
    image, max_delta, seed
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
seed = (1, 2)
tf.image.stateless_random_hue(x, 0.2, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.6514902,  1.       ,  3.       ],
        [ 4.65149  ,  4.       ,  6.       ]],
       [[ 7.65149  ,  7.       ,  9.       ],
        [10.65149  , 10.       , 12.       ]]], dtype=float32)>"
"tf.image.stateless_random_jpeg_quality(
    image, min_jpeg_quality, max_jpeg_quality, seed
)
",[],"x = tf.constant([[[1, 2, 3],
                  [4, 5, 6]],
                 [[7, 8, 9],
                  [10, 11, 12]]], dtype=tf.uint8)
seed = (1, 2)
tf.image.stateless_random_jpeg_quality(x, 75, 95, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=uint8, numpy=
array([[[ 0,  4,  5],
        [ 1,  5,  6]],
       [[ 5,  9, 10],
        [ 5,  9, 10]]], dtype=uint8)>"
"tf.image.stateless_random_saturation(
    image, lower, upper, seed=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
seed = (1, 2)
tf.image.stateless_random_saturation(x, 0.5, 1.0, seed)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.1559395,  2.0779698,  3.       ],
        [ 4.1559396,  5.07797  ,  6.       ]],
       [[ 7.1559396,  8.07797  ,  9.       ],
        [10.155939 , 11.07797  , 12.       ]]], dtype=float32)>"
"tf.image.stateless_sample_distorted_bounding_box(
    image_size,
    bounding_boxes,
    seed,
    min_object_covered=0.1,
    aspect_ratio_range=None,
    area_range=None,
    max_attempts=None,
    use_image_if_no_bounding_boxes=None,
    name=None
)
",[],"image = np.array([[[1], [2], [3]], [[4], [5], [6]], [[7], [8], [9]]])
bbox = tf.constant(
  [0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
seed = (1, 2)
# Generate a single distorted bounding box.
bbox_begin, bbox_size, bbox_draw = (
  tf.image.stateless_sample_distorted_bounding_box(
    tf.shape(image), bounding_boxes=bbox, seed=seed))
# Employ the bounding box to distort the image.
tf.slice(image, bbox_begin, bbox_size)
<tf.Tensor: shape=(2, 2, 1), dtype=int64, numpy=
array([[[1],
        [2]],
       [[4],
        [5]]])>
# Draw the bounding box in an image summary.
colors = np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]])
tf.image.draw_bounding_boxes(
  tf.expand_dims(tf.cast(image, tf.float32),0), bbox_draw, colors)
<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
array([[[[1.],
         [1.],
         [3.]],
        [[1.],
         [1.],
         [6.]],
        [[7.],
         [8.],
         [9.]]]], dtype=float32)>"
"tf.image.total_variation(
    images, name=None
)
",[],[]
"tf.image.transpose(
    image, name=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.transpose(x)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.,  2.,  3.],
        [ 7.,  8.,  9.]],
       [[ 4.,  5.,  6.],
        [10., 11., 12.]]], dtype=float32)>"
"tf.image.yiq_to_rgb(
    images
)
",[],[]
"tf.image.yuv_to_rgb(
    images
)
",[],"yuv_images = tf.random.uniform(shape=[100, 64, 64, 3], maxval=255)
last_dimension_axis = len(yuv_images.shape) - 1
yuv_tensor_images = tf.truediv(
    tf.subtract(
        yuv_images,
        tf.reduce_min(yuv_images)
    ),
    tf.subtract(
        tf.reduce_max(yuv_images),
        tf.reduce_min(yuv_images)
     )
)
y, u, v = tf.split(yuv_tensor_images, 3, axis=last_dimension_axis)
target_uv_min, target_uv_max = -0.5, 0.5
u = u * (target_uv_max - target_uv_min) + target_uv_min
v = v * (target_uv_max - target_uv_min) + target_uv_min
preprocessed_yuv_images = tf.concat([y, u, v], axis=last_dimension_axis)
rgb_tensor_images = tf.image.yuv_to_rgb(preprocessed_yuv_images)
"
"tf.graph_util.import_graph_def(
    graph_def,
    input_map=None,
    return_elements=None,
    name=None,
    op_dict=None,
    producer_op_list=None
)
","[['Imports the graph from ', 'graph_def', ' into the current default ', 'Graph', '. (deprecated arguments)']]",[]
"tf.keras.initializers.Constant(
    value=0
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotNormal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotUniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeNormal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeUniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Identity(
    gain=1.0
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.Identity()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunNormal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunUniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Orthogonal(
    gain=1.0, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.Orthogonal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomNormal(
    mean=0.0, stddev=0.05, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomUniform(
    minval=-0.05, maxval=0.05, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.TruncatedNormal(
    mean=0.0, stddev=0.05, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.VarianceScaling(
scale=0.1, mode='fan_in', distribution='uniform')
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Constant(
    value=0
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.deserialize(
    config, custom_objects=None
)
","[['Return an ', 'Initializer', ' object from its config.']]",[]
"tf.keras.initializers.get(
    identifier
)
",[],"identifier = 'Ones'
tf.keras.initializers.deserialize(identifier)
<...keras.initializers.initializers_v2.Ones...>"
"tf.keras.initializers.GlorotNormal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotUniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeNormal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeUniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Identity(
    gain=1.0
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.Identity()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunNormal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunUniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Orthogonal(
    gain=1.0, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.Orthogonal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomNormal(
    mean=0.0, stddev=0.05, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomUniform(
    minval=-0.05, maxval=0.05, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.serialize(
    initializer
)
",[],[]
"tf.keras.initializers.TruncatedNormal(
    mean=0.0, stddev=0.05, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.VarianceScaling(
scale=0.1, mode='fan_in', distribution='uniform')
values = initializer(shape=(2, 2))"
"tf.io.FixedLenFeature(
    shape, dtype, default_value=None
)
",[],[]
"tf.io.FixedLenSequenceFeature(
    shape, dtype, allow_missing=False, default_value=None
)
","[['Configuration for parsing a variable-length input feature into a ', 'Tensor', '.']]",[]
"tf.io.RaggedFeature(
    dtype,
    value_key=None,
    partitions=(),
    row_splits_dtype=tf.dtypes.int32,
    validate=False
)
",[],"import google.protobuf.text_format as pbtext
example_batch = [
  pbtext.Merge(r'''
    features {
      feature {key: ""v"" value {int64_list {value: [3, 1, 4, 1, 5, 9]} } }
      feature {key: ""s1"" value {int64_list {value: [0, 2, 3, 3, 6]} } }
      feature {key: ""s2"" value {int64_list {value: [0, 2, 3, 4]} } }
    }''', tf.train.Example()).SerializeToString(),
  pbtext.Merge(r'''
    features {
      feature {key: ""v"" value {int64_list {value: [2, 7, 1, 8, 2, 8, 1]} } }
      feature {key: ""s1"" value {int64_list {value: [0, 3, 4, 5, 7]} } }
      feature {key: ""s2"" value {int64_list {value: [0, 1, 1, 4]} } }
    }''', tf.train.Example()).SerializeToString()]"
"tf.io.RaggedFeature.RowLengths(
    key
)
",[],[]
"tf.io.RaggedFeature.RowLimits(
    key
)
",[],[]
"tf.io.RaggedFeature.RowSplits(
    key
)
",[],[]
"tf.io.RaggedFeature.RowStarts(
    key
)
",[],[]
"tf.io.RaggedFeature.UniformRowLength(
    length
)
",[],[]
"tf.io.RaggedFeature.ValueRowIds(
    key
)
",[],[]
"tf.io.SparseFeature(
    index_key, value_key, dtype, size, already_sorted=False
)
","[['Configuration for parsing a sparse input feature from an ', 'Example', '.']]","SparseTensor(indices=[[3, 1], [20, 0]],
             values=[0.5, -1.0]
             dense_shape=[100, 3])
"
"tf.io.TFRecordOptions(
    compression_type=None,
    flush_mode=None,
    input_buffer_size=None,
    output_buffer_size=None,
    window_bits=None,
    compression_level=None,
    compression_method=None,
    mem_level=None,
    compression_strategy=None
)
",[],"@classmethod
get_compression_type_string(
    options
)
"
"tf.io.TFRecordWriter(
    path, options=None
)
",[],"import tempfile
example_path = os.path.join(tempfile.gettempdir(), ""example.tfrecords"")
np.random.seed(0)"
"tf.io.VarLenFeature(
    dtype
)
",[],[]
"tf.io.decode_and_crop_jpeg(
    contents,
    crop_window,
    channels=0,
    ratio=1,
    fancy_upscaling=True,
    try_recover_truncated=False,
    acceptable_fraction=1,
    dct_method='',
    name=None
)
",[],[]
"tf.io.decode_base64(
    input, name=None
)
",[],[]
"tf.io.decode_bmp(
    contents, channels=0, name=None
)
",[],[]
"tf.io.decode_compressed(
    bytes, compression_type='', name=None
)
",[],[]
"tf.io.decode_csv(
    records,
    record_defaults,
    field_delim=',',
    use_quote_delim=True,
    na_value='',
    select_cols=None,
    name=None
)
",[],[]
"tf.io.decode_gif(
    contents, name=None
)
","[[None, '\n']]","convert \\(src.gif -coalesce \\)dst.gif
"
"tf.io.decode_image(
    contents,
    channels=None,
    dtype=tf.dtypes.uint8,
    name=None,
    expand_animations=True
)
","[['Function for ', 'decode_bmp', ', ', 'decode_gif', ', ', 'decode_jpeg', ', and ', 'decode_png', '.']]",[]
"tf.io.decode_jpeg(
    contents,
    channels=0,
    ratio=1,
    fancy_upscaling=True,
    try_recover_truncated=False,
    acceptable_fraction=1,
    dct_method='',
    name=None
)
",[],[]
"tf.io.decode_json_example(
    json_examples, name=None
)
",[],"example = tf.train.Example(
  features=tf.train.Features(
      feature={
          ""a"": tf.train.Feature(
              int64_list=tf.train.Int64List(
                  value=[1, 1, 3]))}))"
"tf.io.decode_png(
    contents,
    channels=0,
    dtype=tf.dtypes.uint8,
    name=None
)
",[],[]
"tf.io.decode_proto(
    bytes,
    message_type,
    field_names,
    output_types,
    descriptor_source='local://',
    message_format='binary',
    sanitize=False,
    name=None
)
",[],"from google.protobuf import text_format
# A Summary.Value contains: oneof {float simple_value; Image image}
values = [
   ""simple_value: 2.2"",
   ""simple_value: 1.2"",
   ""image { height: 128 width: 512 }"",
   ""image { height: 256 width: 256 }"",]
values = [
   text_format.Parse(v, tf.compat.v1.Summary.Value()).SerializeToString()
   for v in values]"
"tf.io.decode_raw(
    input_bytes, out_type, little_endian=True, fixed_length=None, name=None
)
",[],"tf.io.decode_raw(tf.constant(""1""), tf.uint8)
<tf.Tensor: shape=(1,), dtype=uint8, numpy=array([49], dtype=uint8)>
tf.io.decode_raw(tf.constant(""1,2""), tf.uint8)
<tf.Tensor: shape=(3,), dtype=uint8, numpy=array([49, 44, 50], dtype=uint8)>"
"tf.io.deserialize_many_sparse(
    serialized_sparse, dtype, rank=None, name=None
)
","[['Deserialize and concatenate ', 'SparseTensors', ' from a serialized minibatch.']]","index = [ 0]
        [10]
        [20]
values = [1, 2, 3]
shape = [50]
"
"tf.io.encode_base64(
    input, pad=False, name=None
)
",[],[]
"tf.io.encode_jpeg(
    image,
    format='',
    quality=95,
    progressive=False,
    optimize_size=False,
    chroma_downsampling=True,
    density_unit='in',
    x_density=300,
    y_density=300,
    xmp_metadata='',
    name=None
)
",[],[]
"tf.io.encode_png(
    image, compression=-1, name=None
)
",[],[]
"tf.io.encode_proto(
    sizes,
    values,
    field_names,
    message_type,
    descriptor_source='local://',
    name=None
)
",[],[]
"tf.io.extract_jpeg_shape(
    contents,
    output_type=tf.dtypes.int32,
    name=None
)
",[],[]
"tf.io.gfile.GFile(
    name, mode='r'
)
",[],"with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
with tf.io.gfile.GFile(""/tmp/x"") as f:
  f.read()
'asdf'"
"tf.io.gfile.copy(
    src, dst, overwrite=False
)
","[['Copies data from ', 'src', ' to ', 'dst', '.']]","with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.gfile.exists(""/tmp/x"")
True
tf.io.gfile.copy(""/tmp/x"", ""/tmp/y"")
tf.io.gfile.exists(""/tmp/y"")
True
tf.io.gfile.remove(""/tmp/y"")"
"tf.io.gfile.exists(
    path
)
",[],"with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.gfile.exists(""/tmp/x"")
True"
"tf.io.gfile.glob(
    pattern
)
",[],"tf.io.gfile.glob(""*.py"")
# For example, ['__init__.py']"
"tf.io.gfile.isdir(
    path
)
",[],[]
"tf.io.gfile.join(
    path, *paths
)
",[],">>> tf.io.gfile.join(""gcs://folder"", ""file.py"")
'gcs://folder/file.py'
"
"tf.io.gfile.listdir(
    path
)
",[],[]
"tf.io.gfile.makedirs(
    path
)
",[],[]
"tf.io.gfile.mkdir(
    path
)
","[['Creates a directory with the name given by ', 'path', '.']]",[]
"tf.io.gfile.remove(
    path
)
",[],[]
"tf.io.gfile.rename(
    src, dst, overwrite=False
)
",[],[]
"tf.io.gfile.rmtree(
    path
)
",[],[]
"tf.io.gfile.stat(
    path
)
",[],[]
"tf.io.gfile.walk(
    top, topdown=True, onerror=None
)
",[],[]
"tf.io.is_jpeg(
    contents, name=None
)
",[],[]
"tf.io.match_filenames_once(
    pattern, name=None
)
",[],[]
"tf.io.matching_files(
    pattern, name=None
)
",[],[]
"tf.io.parse_example(
    serialized, features, example_names=None, name=None
)
","[['Parses ', 'Example', ' protos into a ', 'dict', ' of tensors.']]","serialized = [
  features
    { feature { key: ""ft"" value { float_list { value: [1.0, 2.0] } } } },
  features
    { feature []},
  features
    { feature { key: ""ft"" value { float_list { value: [3.0] } } }
]
"
"tf.io.parse_sequence_example(
    serialized,
    context_features=None,
    sequence_features=None,
    example_names=None,
    name=None
)
","[['Parses a batch of ', 'SequenceExample', ' protos.']]",[]
"tf.io.parse_single_example(
    serialized, features, example_names=None, name=None
)
","[['Parses a single ', 'Example', ' proto.']]",[]
"tf.io.parse_single_sequence_example(
    serialized,
    context_features=None,
    sequence_features=None,
    example_name=None,
    name=None
)
","[['Parses a single ', 'SequenceExample', ' proto.']]",[]
"tf.io.parse_tensor(
    serialized, out_type, name=None
)
",[],[]
"tf.io.read_file(
    filename, name=None
)
",[],"with open(""/tmp/file.txt"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.read_file(""/tmp/file.txt"")
<tf.Tensor: shape=(), dtype=string, numpy=b'asdf'>"
"tf.io.serialize_many_sparse(
    sp_input,
    out_type=tf.dtypes.string,
    name=None
)
","[['Serialize ', 'N', '-minibatch ', 'SparseTensor', ' into an ', '[N, 3]', ' ', 'Tensor', '.']]",[]
"tf.io.serialize_sparse(
    sp_input,
    out_type=tf.dtypes.string,
    name=None
)
","[['Serialize a ', 'SparseTensor', ' into a 3-vector (1-D ', 'Tensor', ') object.']]",[]
"tf.io.serialize_tensor(
    tensor, name=None
)
",[],"t = tf.constant(1)
tf.io.serialize_tensor(t)
<tf.Tensor: shape=(), dtype=string, numpy=b'\x08...\x00'>"
"tf.io.write_file(
    filename, contents, name=None
)
","[['Writes ', 'contents', ' to the file at input ', 'filename', '.']]",[]
"tf.io.write_graph(
    graph_or_graph_def, logdir, name, as_text=True
)
",[],"v = tf.Variable(0, name='my_variable')
sess = tf.compat.v1.Session()
tf.io.write_graph(sess.graph_def, '/tmp/my-model', 'train.pbtxt')
"
"tf.is_tensor(
    x
)
","[['Checks whether ', 'x', ' is a TF-native type that can be passed to many TF ops.']]","if not tf.is_tensor(t):
  t = tf.convert_to_tensor(t)
return t.shape, t.dtype
"
"tf.keras.Input(
    shape=None,
    batch_size=None,
    name=None,
    dtype=None,
    sparse=None,
    tensor=None,
    ragged=None,
    type_spec=None,
    **kwargs
)
","[['Input()', ' is used to instantiate a Keras tensor.']]","# this is a logistic regression in Keras
x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)
"
"tf.keras.Model(
    *args, **kwargs
)
","[['Model', ' groups layers into an object with training and inference features.'], ['Inherits From: ', 'Layer', ', ', 'Module']]","import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
"
"tf.keras.Sequential(
    layers=None, name=None
)
","[['Sequential', ' groups a linear stack of layers into a ', 'tf.keras.Model', '.'], ['Inherits From: ', 'Model', ', ', 'Layer', ', ', 'Module']]","# Optionally, the first layer can receive an `input_shape` argument:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
# Afterwards, we do automatic shape inference:
model.add(tf.keras.layers.Dense(4))

# This is identical to the following:
model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(8))

# Note that you can also omit the `input_shape` argument.
# In that case the model doesn't have any weights until the first call
# to a training/evaluation method (since it isn't yet built):
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
# model.weights not created yet

# Whereas if you specify the input shape, the model gets built
# continuously as you are adding layers:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))
len(model.weights)
# Returns ""4""

# When using the delayed-build pattern (no input shape specified), you can
# choose to manually build your model by calling
# `build(batch_input_shape)`:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
model.build((None, 16))
len(model.weights)
# Returns ""4""

# Note that when using the delayed-build pattern (no input shape specified),
# the model gets built the first time you call `fit`, `eval`, or `predict`,
# or the first time you call the model on some input data.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer='sgd', loss='mse')
# This builds the model for the first time:
model.fit(x, y, batch_size=32, epochs=10)
"
"tf.keras.activations.deserialize(
    name, custom_objects=None
)
",[],"tf.keras.activations.deserialize('linear')
 <function linear at 0x1239596a8>
tf.keras.activations.deserialize('sigmoid')
 <function sigmoid at 0x123959510>
tf.keras.activations.deserialize('abcd')
Traceback (most recent call last):
ValueError: Unknown activation function:abcd"
"tf.keras.activations.elu(
    x, alpha=1.0
)
",[],"import tensorflow as tf
model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='elu',
         input_shape=(28, 28, 1)))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))"
"tf.keras.activations.exponential(
    x
)
",[],"a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.exponential(a)
b.numpy()
array([0.04978707,  0.36787945,  1.,  2.7182817 , 20.085537], dtype=float32)"
"tf.keras.activations.gelu(
    x, approximate=False
)
",[],"x = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype=tf.float32)
y = tf.keras.activations.gelu(x)
y.numpy()
array([-0.00404951, -0.15865529,  0.        ,  0.8413447 ,  2.9959507 ],
    dtype=float32)
y = tf.keras.activations.gelu(x, approximate=True)
y.numpy()
array([-0.00363752, -0.15880796,  0.        ,  0.841192  ,  2.9963627 ],
    dtype=float32)"
"tf.keras.activations.get(
    identifier
)
",[],"tf.keras.activations.get('softmax')
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(tf.keras.activations.softmax)
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(None)
 <function linear at 0x1239596a8>
tf.keras.activations.get(abs)
 <built-in function abs>
tf.keras.activations.get('abcd')
Traceback (most recent call last):
ValueError: Unknown activation function:abcd"
"tf.keras.activations.hard_sigmoid(
    x
)
",[],"a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.hard_sigmoid(a)
b.numpy()
array([0. , 0.3, 0.5, 0.7, 1. ], dtype=float32)"
"tf.keras.activations.linear(
    x
)
",[],"a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.linear(a)
b.numpy()
array([-3., -1.,  0.,  1.,  3.], dtype=float32)"
"tf.keras.activations.relu(
    x, alpha=0.0, max_value=None, threshold=0.0
)
",[],"foo = tf.constant([-10, -5, 0.0, 5, 10], dtype = tf.float32)
tf.keras.activations.relu(foo).numpy()
array([ 0.,  0.,  0.,  5., 10.], dtype=float32)
tf.keras.activations.relu(foo, alpha=0.5).numpy()
array([-5. , -2.5,  0. ,  5. , 10. ], dtype=float32)
tf.keras.activations.relu(foo, max_value=5.).numpy()
array([0., 0., 0., 5., 5.], dtype=float32)
tf.keras.activations.relu(foo, threshold=5.).numpy()
array([-0., -0.,  0.,  0., 10.], dtype=float32)"
"tf.keras.activations.selu(
    x
)
",[],"num_classes = 10  # 10-class problem
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(64, kernel_initializer='lecun_normal',
                                activation='selu'))
model.add(tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',
                                activation='selu'))
model.add(tf.keras.layers.Dense(16, kernel_initializer='lecun_normal',
                                activation='selu'))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
"tf.keras.activations.serialize(
    activation
)
",[],"tf.keras.activations.serialize(tf.keras.activations.tanh)
'tanh'
tf.keras.activations.serialize(tf.keras.activations.sigmoid)
'sigmoid'
tf.keras.activations.serialize('abcd')
Traceback (most recent call last):
ValueError: ('Cannot serialize', 'abcd')"
"tf.keras.activations.sigmoid(
    x
)
","[['Sigmoid activation function, ', 'sigmoid(x) = 1 / (1 + exp(-x))', '.']]","a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.sigmoid(a)
b.numpy()
array([2.0611537e-09, 2.6894143e-01, 5.0000000e-01, 7.3105860e-01,
         1.0000000e+00], dtype=float32)"
"tf.keras.activations.softmax(
    x, axis=-1
)
",[],"inputs = tf.random.normal(shape=(32, 10))
outputs = tf.keras.activations.softmax(inputs)
tf.reduce_sum(outputs[0, :])  # Each sample in the batch now sums to 1
<tf.Tensor: shape=(), dtype=float32, numpy=1.0000001>"
"tf.keras.activations.softplus(
    x
)
","[['Softplus activation function, ', 'softplus(x) = log(exp(x) + 1)', '.']]","a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.softplus(a)
b.numpy()
array([2.0611537e-09, 3.1326166e-01, 6.9314718e-01, 1.3132616e+00,
         2.0000000e+01], dtype=float32)"
"tf.keras.activations.softsign(
    x
)
","[['Softsign activation function, ', 'softsign(x) = x / (abs(x) + 1)', '.']]","a = tf.constant([-1.0, 0.0, 1.0], dtype = tf.float32)
b = tf.keras.activations.softsign(a)
b.numpy()
array([-0.5,  0. ,  0.5], dtype=float32)"
"tf.keras.activations.swish(
    x
)
","[['Swish activation function, ', 'swish(x) = x * sigmoid(x)', '.']]","a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.swish(a)
b.numpy()
array([-4.1223075e-08, -2.6894143e-01,  0.0000000e+00,  7.3105860e-01,
          2.0000000e+01], dtype=float32)"
"tf.keras.activations.tanh(
    x
)
",[],"a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.tanh(a)
b.numpy()
array([-0.9950547, -0.7615942,  0.,  0.7615942,  0.9950547], dtype=float32)"
"tf.keras.applications.convnext.ConvNeXtBase(
    model_name='convnext_base',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtLarge(
    model_name='convnext_large',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtSmall(
    model_name='convnext_small',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtTiny(
    model_name='convnext_tiny',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtXLarge(
    model_name='convnext_xlarge',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.densenet.DenseNet121(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.densenet.DenseNet169(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.densenet.DenseNet201(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB0(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB1(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB3(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB4(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB5(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB6(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB7(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B0(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B1(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B3(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2L(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2M(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2S(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.inception_resnet_v2.InceptionResNetV2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.inception_v3.InceptionV3(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.mobilenet.MobileNet(
    input_shape=None,
    alpha=1.0,
    depth_multiplier=1,
    dropout=0.001,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.mobilenet_v2.MobileNetV2(
    input_shape=None,
    alpha=1.0,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.MobileNetV3Large(
    input_shape=None,
    alpha=1.0,
    minimalistic=False,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    classes=1000,
    pooling=None,
    dropout_rate=0.2,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.MobileNetV3Small(
    input_shape=None,
    alpha=1.0,
    minimalistic=False,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    classes=1000,
    pooling=None,
    dropout_rate=0.2,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.nasnet.NASNetLarge(
    input_shape=None,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.nasnet.NASNetMobile(
    input_shape=None,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX002(
    model_name='regnetx002',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX004(
    model_name='regnetx004',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX006(
    model_name='regnetx006',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX008(
    model_name='regnetx008',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX016(
    model_name='regnetx016',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX032(
    model_name='regnetx032',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX040(
    model_name='regnetx040',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX064(
    model_name='regnetx064',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX080(
    model_name='regnetx080',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX120(
    model_name='regnetx120',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX160(
    model_name='regnetx160',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX320(
    model_name='regnetx320',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY002(
    model_name='regnety002',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY004(
    model_name='regnety004',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY006(
    model_name='regnety006',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY008(
    model_name='regnety008',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY016(
    model_name='regnety016',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY032(
    model_name='regnety032',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY040(
    model_name='regnety040',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY064(
    model_name='regnety064',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY080(
    model_name='regnety080',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY120(
    model_name='regnety120',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY160(
    model_name='regnety160',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY320(
    model_name='regnety320',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet.ResNet101(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet_v2.ResNet101V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet.ResNet152(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet_v2.ResNet152V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet50.ResNet50(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet_v2.ResNet50V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS101(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS152(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS200(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS270(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS350(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS420(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS50(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.vgg16.VGG16(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.vgg19.VGG19(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.xception.Xception(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtBase(
    model_name='convnext_base',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtLarge(
    model_name='convnext_large',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtSmall(
    model_name='convnext_small',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtTiny(
    model_name='convnext_tiny',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtXLarge(
    model_name='convnext_xlarge',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.preprocess_input(
    x, data_format=None
)
",[],[]
"tf.keras.applications.densenet.DenseNet121(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.densenet.DenseNet169(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.densenet.DenseNet201(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.densenet.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.densenet.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.efficientnet.EfficientNetB0(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB1(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB3(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB4(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB5(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB6(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB7(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.efficientnet.preprocess_input(
    x, data_format=None
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B0(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B1(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B3(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2L(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2M(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2S(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.efficientnet_v2.preprocess_input(
    x, data_format=None
)
",[],[]
"tf.keras.applications.imagenet_utils.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.imagenet_utils.preprocess_input(
    x, data_format=None, mode='caffe'
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.inception_resnet_v2.InceptionResNetV2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.inception_resnet_v2.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.inception_resnet_v2.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.inception_v3.InceptionV3(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.inception_v3.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.inception_v3.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.mobilenet.MobileNet(
    input_shape=None,
    alpha=1.0,
    depth_multiplier=1,
    dropout=0.001,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.mobilenet.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.mobilenet.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.mobilenet_v2.MobileNetV2(
    input_shape=None,
    alpha=1.0,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.mobilenet_v2.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.mobilenet_v2.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.mobilenet_v3.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.mobilenet_v3.preprocess_input(
    x, data_format=None
)
",[],[]
"tf.keras.applications.nasnet.NASNetLarge(
    input_shape=None,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.nasnet.NASNetMobile(
    input_shape=None,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.nasnet.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.nasnet.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.regnet.RegNetX002(
    model_name='regnetx002',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX004(
    model_name='regnetx004',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX006(
    model_name='regnetx006',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX008(
    model_name='regnetx008',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX016(
    model_name='regnetx016',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX032(
    model_name='regnetx032',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX040(
    model_name='regnetx040',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX064(
    model_name='regnetx064',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX080(
    model_name='regnetx080',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX120(
    model_name='regnetx120',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX160(
    model_name='regnetx160',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX320(
    model_name='regnetx320',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY002(
    model_name='regnety002',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY004(
    model_name='regnety004',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY006(
    model_name='regnety006',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY008(
    model_name='regnety008',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY016(
    model_name='regnety016',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY032(
    model_name='regnety032',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY040(
    model_name='regnety040',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY064(
    model_name='regnety064',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY080(
    model_name='regnety080',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY120(
    model_name='regnety120',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY160(
    model_name='regnety160',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY320(
    model_name='regnety320',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.regnet.preprocess_input(
    x, data_format=None
)
",[],[]
"tf.keras.applications.resnet.ResNet101(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet.ResNet152(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet50.ResNet50(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet50.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.resnet50.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.resnet50.ResNet50(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet50.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.resnet50.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.resnet_rs.ResNetRS101(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS152(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS200(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS270(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS350(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS420(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS50(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.resnet_rs.preprocess_input(
    x, data_format=None
)
",[],[]
"tf.keras.applications.resnet_v2.ResNet101V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet_v2.ResNet152V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet_v2.ResNet50V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet_v2.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.resnet_v2.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.vgg16.VGG16(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.vgg16.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.vgg16.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.vgg19.VGG19(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.vgg19.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.vgg19.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.xception.Xception(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.xception.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.xception.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.backend.get_uid(
    prefix=''
)
",[],"get_uid('dense')
1
get_uid('dense')
2"
"tf.keras.backend.is_keras_tensor(
    x
)
","[['Returns whether ', 'x', ' is a Keras tensor.']]","np_var = np.array([1, 2])
# A numpy array is not a symbolic tensor.
tf.keras.backend.is_keras_tensor(np_var)
Traceback (most recent call last):
ValueError: Unexpectedly found an instance of type
`<class 'numpy.ndarray'>`.
Expected a symbolic tensor instance.
keras_var = tf.keras.backend.variable(np_var)
# A variable created with the keras backend is not a Keras tensor.
tf.keras.backend.is_keras_tensor(keras_var)
False
keras_placeholder = tf.keras.backend.placeholder(shape=(2, 4, 5))
# A placeholder is a Keras tensor.
tf.keras.backend.is_keras_tensor(keras_placeholder)
True
keras_input = tf.keras.layers.Input([10])
# An Input is a Keras tensor.
tf.keras.backend.is_keras_tensor(keras_input)
True
keras_layer_output = tf.keras.layers.Dense(10)(keras_input)
# Any Keras layer output is a Keras tensor.
tf.keras.backend.is_keras_tensor(keras_layer_output)
True"
"tf.keras.backend.rnn(
    step_function,
    inputs,
    initial_states,
    go_backwards=False,
    mask=None,
    constants=None,
    unroll=False,
    input_length=None,
    time_major=False,
    zero_output_for_mask=False,
    return_all_outputs=True
)
",[],"- If `return_all_outputs=True`: a tensor with shape
  `(samples, time, ...)` where each entry `outputs[s, t]` is the
  output of the step function at time `t` for sample `s`
- Else, a tensor equal to `last_output` with shape
  `(samples, 1, ...)`
"
"tf.keras.backend.set_epsilon(
    value
)
",[],"tf.keras.backend.epsilon()
1e-07
tf.keras.backend.set_epsilon(1e-5)
tf.keras.backend.epsilon()
1e-05
tf.keras.backend.set_epsilon(1e-7)"
"tf.keras.backend.set_floatx(
    value
)
",[],"tf.keras.backend.floatx()
'float32'
tf.keras.backend.set_floatx('float64')
tf.keras.backend.floatx()
'float64'
tf.keras.backend.set_floatx('float32')"
"tf.keras.backend.set_image_data_format(
    data_format
)
",[],"tf.keras.backend.image_data_format()
'channels_last'
tf.keras.backend.set_image_data_format('channels_first')
tf.keras.backend.image_data_format()
'channels_first'
tf.keras.backend.set_image_data_format('channels_last')"
"tf.keras.callbacks.BackupAndRestore(
    backup_dir,
    save_freq='epoch',
    delete_checkpoint=True,
    save_before_preemption=False
)
","[['Inherits From: ', 'Callback']]","class InterruptingCallback(tf.keras.callbacks.Callback):
  def on_epoch_begin(self, epoch, logs=None):
    if epoch == 4:
      raise RuntimeError('Interrupting!')
callback = tf.keras.callbacks.BackupAndRestore(backup_dir=""/tmp/backup"")
model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')
try:
  model.fit(np.arange(100).reshape(5, 20), np.zeros(5), epochs=10,
            batch_size=1, callbacks=[callback, InterruptingCallback()],
            verbose=0)
except:
  pass
history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                    epochs=10, batch_size=1, callbacks=[callback],
                    verbose=0)
# Only 6 more epochs are run, since first training got interrupted at
# zero-indexed epoch 4, second training will continue from 4 to 9.
len(history.history['loss'])
6"
"tf.keras.callbacks.BaseLogger(
    stateful_metrics=None
)
","[['Inherits From: ', 'Callback']]","set_model(
    model
)
"
"tf.keras.callbacks.CSVLogger(
    filename, separator=',', append=False
)
","[['Inherits From: ', 'Callback']]","csv_logger = CSVLogger('training.log')
model.fit(X_train, Y_train, callbacks=[csv_logger])
"
"tf.keras.callbacks.CallbackList(
    callbacks=None, add_history=False, add_progbar=False, model=None, **params
)
",[],"append(
    callback
)
"
"tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    min_delta=0,
    patience=0,
    verbose=0,
    mode='auto',
    baseline=None,
    restore_best_weights=False,
    start_from_epoch=0
)
","[['Inherits From: ', 'Callback']]","callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
# This callback will stop the training when there is no improvement in
# the loss for three consecutive epochs.
model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')
history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                    epochs=10, batch_size=1, callbacks=[callback],
                    verbose=0)
len(history.history['loss'])  # Only 4 epochs are run.
4"
"tf.keras.callbacks.LambdaCallback(
    on_epoch_begin=None,
    on_epoch_end=None,
    on_batch_begin=None,
    on_batch_end=None,
    on_train_begin=None,
    on_train_end=None,
    **kwargs
)
","[['Inherits From: ', 'Callback']]","# Print the batch number at the beginning of every batch.
batch_print_callback = LambdaCallback(
    on_batch_begin=lambda batch,logs: print(batch))

# Stream the epoch loss to a file in JSON format. The file content
# is not well-formed JSON but rather has a JSON object per line.
import json
json_log = open('loss_log.json', mode='wt', buffering=1)
json_logging_callback = LambdaCallback(
    on_epoch_end=lambda epoch, logs: json_log.write(
        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\n'),
    on_train_end=lambda logs: json_log.close()
)

# Terminate some processes after having finished model training.
processes = ...
cleanup_callback = LambdaCallback(
    on_train_end=lambda logs: [
        p.terminate() for p in processes if p.is_alive()])

model.fit(...,
          callbacks=[batch_print_callback,
                     json_logging_callback,
                     cleanup_callback])
"
"tf.keras.callbacks.LearningRateScheduler(
    schedule, verbose=0
)
","[['Inherits From: ', 'Callback']]","# This function keeps the initial learning rate for the first ten epochs
# and decreases it exponentially after that.
def scheduler(epoch, lr):
  if epoch < 10:
    return lr
  else:
    return lr * tf.math.exp(-0.1)
model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')
round(model.optimizer.lr.numpy(), 5)
0.01"
"tf.keras.callbacks.ModelCheckpoint(
    filepath,
    monitor: str = 'val_loss',
    verbose: int = 0,
    save_best_only: bool = False,
    save_weights_only: bool = False,
    mode: str = 'auto',
    save_freq='epoch',
    options=None,
    initial_value_threshold=None,
    **kwargs
)
","[['Inherits From: ', 'Callback']]","model.compile(loss=..., optimizer=...,
              metrics=['accuracy'])

EPOCHS = 10
checkpoint_filepath = '/tmp/checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

# Model weights are saved at the end of every epoch, if it's the best seen
# so far.
model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])

# The model weights (that are considered the best) are loaded into the
# model.
model.load_weights(checkpoint_filepath)
"
"tf.keras.callbacks.ProgbarLogger(
    count_mode: str = 'samples', stateful_metrics=None
)
","[['Inherits From: ', 'Callback']]","set_model(
    model
)
"
"tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.1,
    patience=10,
    verbose=0,
    mode='auto',
    min_delta=0.0001,
    cooldown=0,
    min_lr=0,
    **kwargs
)
","[['Inherits From: ', 'Callback']]","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.001)
model.fit(X_train, Y_train, callbacks=[reduce_lr])
"
"tf.keras.callbacks.RemoteMonitor(
    root='http://localhost:9000',
    path='/publish/epoch/end/',
    field='data',
    headers=None,
    send_as_json=False
)
","[['Inherits From: ', 'Callback']]","set_model(
    model
)
"
"tf.keras.callbacks.TensorBoard(
    log_dir='logs',
    histogram_freq=0,
    write_graph=True,
    write_images=False,
    write_steps_per_second=False,
    update_freq='epoch',
    profile_batch=0,
    embeddings_freq=0,
    embeddings_metadata=None,
    **kwargs
)
","[['Inherits From: ', 'Callback']]","tensorboard --logdir=path_to_your_logs
"
"tf.keras.callbacks.experimental.BackupAndRestore(
    *args, **kwargs
)
","[['Deprecated. Please use ', 'tf.keras.callbacks.BackupAndRestore', ' instead.'], ['Inherits From: ', 'BackupAndRestore', ', ', 'Callback']]","set_model(
    model
)
"
"tf.keras.constraints.MaxNorm(
    max_value=2, axis=0
)
","[['Inherits From: ', 'Constraint']]","@classmethod
from_config(
    config
)
"
"tf.keras.constraints.MinMaxNorm(
    min_value=0.0, max_value=1.0, rate=1.0, axis=0
)
","[['Inherits From: ', 'Constraint']]","@classmethod
from_config(
    config
)
"
"tf.keras.constraints.UnitNorm(
    axis=0
)
","[['Inherits From: ', 'Constraint']]","@classmethod
from_config(
    config
)
"
"tf.keras.constraints.deserialize(
    config, custom_objects=None
)
",[],[]
"tf.keras.constraints.get(
    identifier
)
",[],[]
"tf.keras.constraints.MaxNorm(
    max_value=2, axis=0
)
","[['Inherits From: ', 'Constraint']]","@classmethod
from_config(
    config
)
"
"tf.keras.constraints.MinMaxNorm(
    min_value=0.0, max_value=1.0, rate=1.0, axis=0
)
","[['Inherits From: ', 'Constraint']]","@classmethod
from_config(
    config
)
"
"tf.keras.constraints.serialize(
    constraint
)
",[],[]
"tf.keras.constraints.UnitNorm(
    axis=0
)
","[['Inherits From: ', 'Constraint']]","@classmethod
from_config(
    config
)
"
"tf.keras.datasets.boston_housing.load_data(
    path='boston_housing.npz', test_split=0.2, seed=113
)
","[[None, '\n']]",[]
"tf.keras.datasets.cifar100.load_data(
    label_mode='fine'
)
",[],"(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()
assert x_train.shape == (50000, 32, 32, 3)
assert x_test.shape == (10000, 32, 32, 3)
assert y_train.shape == (50000, 1)
assert y_test.shape == (10000, 1)
"
"tf.keras.datasets.imdb.get_word_index(
    path='imdb_word_index.json'
)
",[],"# Use the default parameters to keras.datasets.imdb.load_data
start_char = 1
oov_char = 2
index_from = 3
# Retrieve the training sequences.
(x_train, _), _ = keras.datasets.imdb.load_data(
    start_char=start_char, oov_char=oov_char, index_from=index_from
)
# Retrieve the word index file mapping words to indices
word_index = keras.datasets.imdb.get_word_index()
# Reverse the word index to obtain a dict mapping indices to words
# And add `index_from` to indices to sync with `x_train`
inverted_word_index = dict(
    (i + index_from, word) for (word, i) in word_index.items()
)
# Update `inverted_word_index` to include `start_char` and `oov_char`
inverted_word_index[start_char] = ""[START]""
inverted_word_index[oov_char] = ""[OOV]""
# Decode the first sequence in the dataset
decoded_sequence = "" "".join(inverted_word_index[i] for i in x_train[0])
"
"tf.keras.datasets.imdb.load_data(
    path='imdb.npz',
    num_words=None,
    skip_top=0,
    maxlen=None,
    seed=113,
    start_char=1,
    oov_char=2,
    index_from=3,
    **kwargs
)
","[['Loads the ', 'IMDB dataset', '.']]",[]
"tf.keras.datasets.mnist.load_data(
    path='mnist.npz'
)
",[],"(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
assert x_train.shape == (60000, 28, 28)
assert x_test.shape == (10000, 28, 28)
assert y_train.shape == (60000,)
assert y_test.shape == (10000,)
"
"tf.keras.datasets.reuters.get_word_index(
    path='reuters_word_index.json'
)
",[],[]
"tf.keras.datasets.reuters.load_data(
    path='reuters.npz',
    num_words=None,
    skip_top=0,
    maxlen=None,
    test_split=0.2,
    seed=113,
    start_char=1,
    oov_char=2,
    index_from=3,
    **kwargs
)
",[],[]
"tf.keras.dtensor.experimental.LayoutMap(
    mesh=None
)
","[['A dict-like object that maps string to ', 'Layout', ' instances.']]","map = LayoutMap(mesh=None)
map['.*dense.*kernel'] = layout_2d
map['.*dense.*bias'] = layout_1d
map['.*conv2d.*kernel'] = layout_4d
map['.*conv2d.*bias'] = layout_1d

layout_1 = map['dense_1.kernel']    #   layout_1 == layout_2d
layout_2 = map['dense_1.bias']      #   layout_2 == layout_1d
layout_3 = map['dense_2.kernel']    #   layout_3 == layout_2d
layout_4 = map['dense_2.bias']      #   layout_4 == layout_1d
layout_5 = map['my_model/conv2d_123/kernel']    #   layout_5 == layout_4d
layout_6 = map['my_model/conv2d_123/bias']      #   layout_6 == layout_1d
"
"tf.keras.dtensor.experimental.optimizers.Adadelta(
    learning_rate=0.001,
    rho=0.95,
    epsilon=1e-07,
    gradients_clip_option=None,
    ema_option=None,
    name='Adadelta',
    mesh=None
)
","[['Inherits From: ', 'Adadelta', ', ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.dtensor.experimental.optimizers.Adagrad(
    learning_rate=0.001,
    initial_accumulator_value=0.1,
    epsilon=1e-07,
    gradients_clip_option=None,
    ema_option=None,
    name='Adagrad',
    mesh=None
)
","[['Inherits From: ', 'Adagrad', ', ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.dtensor.experimental.optimizers.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    gradients_clip_option=None,
    ema_option=None,
    name='Adam',
    mesh=None
)
","[['Inherits From: ', 'Adam', ', ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.dtensor.experimental.optimizers.AdamW(
    learning_rate=0.001,
    weight_decay=0.004,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    name='AdamW',
    mesh=None
)
","[['Inherits From: ', 'AdamW', ', ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.dtensor.experimental.optimizers.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    gradients_clip_option=None,
    ema_option=None,
    jit_compile=False,
    name='RMSprop',
    mesh=None
)
","[['Inherits From: ', 'RMSprop', ', ', 'Optimizer']]","opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2) / 2.0  # d(loss) / d(var1) = var1
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.dtensor.experimental.optimizers.SGD(
    learning_rate=0.01,
    momentum=0.0,
    nesterov=False,
    amsgrad=False,
    gradients_clip_option=None,
    ema_option=None,
    jit_compile=False,
    name='SGD',
    mesh=None
)
","[['Inherits From: ', 'SGD', ', ', 'Optimizer']]","w = w - learning_rate * g
"
"tf.keras.estimator.model_to_estimator(
    keras_model=None,
    keras_model_path=None,
    custom_objects=None,
    model_dir=None,
    config=None,
    checkpoint_format='checkpoint',
    metric_names_map=None,
    export_outputs=None
)
","[['Constructs an ', 'Estimator', ' instance from given keras model.']]","keras_model = tf.keras.Model(...)
keras_model.compile(...)

estimator = tf.keras.estimator.model_to_estimator(keras_model)

def input_fn():
  return dataset_ops.Dataset.from_tensors(
      ({'features': features, 'sample_weights': sample_weights},
       targets))

estimator.train(input_fn, steps=1)
"
"tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate, decay_steps, alpha=0.0, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  step = min(step, decay_steps)
  cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))
  decayed = (1 - alpha) * cosine_decay + alpha
  return initial_learning_rate * decayed
"
"tf.keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate,
    first_decay_steps,
    t_mul=2.0,
    m_mul=1.0,
    alpha=0.0,
    name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","first_decay_steps = 1000
lr_decayed_fn = (
  tf.keras.optimizers.schedules.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps))
"
"tf.keras.experimental.LinearModel(
    units=1,
    activation=None,
    use_bias=True,
    kernel_initializer='zeros',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    **kwargs
)
","[[None, '\n'], ['Inherits From: ', 'Model', ', ', 'Layer', ', ', 'Module']]","model = LinearModel()
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, epochs=epochs)
"
"tf.keras.experimental.SequenceFeatures(
    feature_columns, trainable=True, name=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","
import tensorflow as tf

# Behavior of some cells or feature columns may depend on whether we are in
# training or inference mode, e.g. applying dropout.
training = True
rating = tf.feature_column.sequence_numeric_column('rating')
watches = tf.feature_column.sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = tf.feature_column.embedding_column(watches,
                                            dimension=10)
columns = [rating, watches_embedding]

features = {
 'rating': tf.sparse.from_dense([[1.0,1.1, 0, 0, 0],
                                             [2.0,2.1,2.2, 2.3, 2.5]]),
 'watches': tf.sparse.from_dense([[2, 85, 0, 0, 0],[33,78, 2, 73, 1]])
}

sequence_input_layer = tf.keras.experimental.SequenceFeatures(columns)
sequence_input, sequence_length = sequence_input_layer(
   features, training=training)
sequence_length_mask = tf.sequence_mask(sequence_length)
hidden_size = 32
rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.keras.experimental.SidecarEvaluator(
    *args, **kwargs
)
","[['Deprecated. Please use ', 'tf.keras.utils.SidecarEvaluator', ' instead.'], ['Inherits From: ', 'SidecarEvaluator']]","start()
"
"tf.keras.experimental.WideDeepModel(
    linear_model, dnn_model, activation=None, **kwargs
)
","[['Inherits From: ', 'Model', ', ', 'Layer', ', ', 'Module']]","linear_model = LinearModel()
dnn_model = keras.Sequential([keras.layers.Dense(units=64),
                             keras.layers.Dense(units=1)])
combined_model = WideDeepModel(linear_model, dnn_model)
combined_model.compile(optimizer=['sgd', 'adam'],
                       loss='mse', metrics=['mse'])
# define dnn_inputs and linear_inputs as separate numpy arrays or
# a single numpy array if dnn_inputs is same as linear_inputs.
combined_model.fit([linear_inputs, dnn_inputs], y, epochs)
# or define a single `tf.data.Dataset` that contains a single tensor or
# separate tensors for dnn_inputs and linear_inputs.
dataset = tf.data.Dataset.from_tensors(([linear_inputs, dnn_inputs], y))
combined_model.fit(dataset, epochs)
"
"tf.keras.initializers.Constant(
    value=0
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotNormal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotUniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeNormal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeUniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Identity(
    gain=1.0
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.Identity()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunNormal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunUniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Orthogonal(
    gain=1.0, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.Orthogonal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomNormal(
    mean=0.0, stddev=0.05, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomUniform(
    minval=-0.05, maxval=0.05, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.TruncatedNormal(
    mean=0.0, stddev=0.05, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.VarianceScaling(
scale=0.1, mode='fan_in', distribution='uniform')
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Constant(
    value=0
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.Constant(3.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.deserialize(
    config, custom_objects=None
)
","[['Return an ', 'Initializer', ' object from its config.']]",[]
"tf.keras.initializers.get(
    identifier
)
",[],"identifier = 'Ones'
tf.keras.initializers.deserialize(identifier)
<...keras.initializers.initializers_v2.Ones...>"
"tf.keras.initializers.GlorotNormal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.GlorotNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.GlorotUniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.GlorotUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeNormal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.HeNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.HeUniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.HeUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Identity(
    gain=1.0
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.Identity()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunNormal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.LecunNormal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.LecunUniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling', ', ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.LecunUniform()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.Orthogonal(
    gain=1.0, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.Orthogonal()
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomNormal(
    mean=0.0, stddev=0.05, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.RandomNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.RandomUniform(
    minval=-0.05, maxval=0.05, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.RandomUniform(minval=0., maxval=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.serialize(
    initializer
)
",[],[]
"tf.keras.initializers.TruncatedNormal(
    mean=0.0, stddev=0.05, seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.TruncatedNormal(mean=0., stddev=1.)
values = initializer(shape=(2, 2))"
"tf.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None
)
","[['Inherits From: ', 'Initializer']]","# Standalone usage:
initializer = tf.keras.initializers.VarianceScaling(
scale=0.1, mode='fan_in', distribution='uniform')
values = initializer(shape=(2, 2))"
"tf.keras.layers.AbstractRNNCell(
    trainable=True, name=None, dtype=None, dynamic=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","  class MinimalRNNCell(AbstractRNNCell):

    def __init__(self, units, **kwargs):
      self.units = units
      super(MinimalRNNCell, self).__init__(**kwargs)

    @property
    def state_size(self):
      return self.units

    def build(self, input_shape):
      self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                    initializer='uniform',
                                    name='kernel')
      self.recurrent_kernel = self.add_weight(
          shape=(self.units, self.units),
          initializer='uniform',
          name='recurrent_kernel')
      self.built = True

    def call(self, inputs, states):
      prev_output = states[0]
      h = backend.dot(inputs, self.kernel)
      output = h + backend.dot(prev_output, self.recurrent_kernel)
      return output, output
"
"tf.keras.layers.Activation(
    activation, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.Activation('relu')
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]
layer = tf.keras.layers.Activation(tf.nn.relu)
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]"
"tf.keras.layers.ActivityRegularization(
    l1=0.0, l2=0.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Add(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 3, 4)
x1 = tf.random.normal(input_shape)
x2 = tf.random.normal(input_shape)
y = tf.keras.layers.Add()([x1, x2])
print(y.shape)
(2, 3, 4)"
"tf.keras.layers.AdditiveAttention(
    use_scale=True, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# Variable-length int sequences.
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

# Embedding lookup.
token_embedding = tf.keras.layers.Embedding(max_tokens, dimension)
# Query embeddings of shape [batch_size, Tq, dimension].
query_embeddings = token_embedding(query_input)
# Value embeddings of shape [batch_size, Tv, dimension].
value_embeddings = token_embedding(value_input)

# CNN layer.
cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    # Use 'same' padding so outputs have the same shape as inputs.
    padding='same')
# Query encoding of shape [batch_size, Tq, filters].
query_seq_encoding = cnn_layer(query_embeddings)
# Value encoding of shape [batch_size, Tv, filters].
value_seq_encoding = cnn_layer(value_embeddings)

# Query-value attention of shape [batch_size, Tq, filters].
query_value_attention_seq = tf.keras.layers.AdditiveAttention()(
    [query_seq_encoding, value_seq_encoding])

# Reduce over the sequence axis to produce encodings of shape
# [batch_size, filters].
query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

# Concatenate query and document encodings to produce a DNN input layer.
input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])

# Add DNN layers, and create Model.
# ...
"
"tf.keras.layers.AlphaDropout(
    rate, noise_shape=None, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Attention(
    use_scale=False, score_mode='dot', **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# Variable-length int sequences.
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

# Embedding lookup.
token_embedding = tf.keras.layers.Embedding(input_dim=1000, output_dim=64)
# Query embeddings of shape [batch_size, Tq, dimension].
query_embeddings = token_embedding(query_input)
# Value embeddings of shape [batch_size, Tv, dimension].
value_embeddings = token_embedding(value_input)

# CNN layer.
cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    # Use 'same' padding so outputs have the same shape as inputs.
    padding='same')
# Query encoding of shape [batch_size, Tq, filters].
query_seq_encoding = cnn_layer(query_embeddings)
# Value encoding of shape [batch_size, Tv, filters].
value_seq_encoding = cnn_layer(value_embeddings)

# Query-value attention of shape [batch_size, Tq, filters].
query_value_attention_seq = tf.keras.layers.Attention()(
    [query_seq_encoding, value_seq_encoding])

# Reduce over the sequence axis to produce encodings of shape
# [batch_size, filters].
query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

# Concatenate query and document encodings to produce a DNN input layer.
input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])

# Add DNN layers, and create Model.
# ...
"
"tf.keras.layers.Average(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x1 = np.ones((2, 2))
x2 = np.zeros((2, 2))
y = tf.keras.layers.Average()([x1, x2])
y.numpy().tolist()
[[0.5, 0.5], [0.5, 0.5]]"
"tf.keras.layers.AveragePooling1D(
    pool_size=2,
    strides=None,
    padding='valid',
    data_format='channels_last',
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
   strides=1, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5]]], dtype=float32)>"
"tf.keras.layers.AveragePooling2D(
    pool_size=(2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([[1., 2., 3.],
                 [4., 5., 6.],
                 [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
   strides=(1, 1), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.]],
          [[6.],
           [7.]]]], dtype=float32)>"
"tf.keras.layers.AveragePooling3D(
    pool_size=(2, 2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","depth = 30
height = 30
width = 30
input_channels = 3

inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.AveragePooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
"
"tf.keras.layers.AveragePooling1D(
    pool_size=2,
    strides=None,
    padding='valid',
    data_format='channels_last',
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
   strides=1, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5]]], dtype=float32)>"
"tf.keras.layers.AveragePooling2D(
    pool_size=(2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([[1., 2., 3.],
                 [4., 5., 6.],
                 [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
   strides=(1, 1), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.]],
          [[6.],
           [7.]]]], dtype=float32)>"
"tf.keras.layers.AveragePooling3D(
    pool_size=(2, 2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","depth = 30
height = 30
width = 30
input_channels = 3

inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.AveragePooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
"
"tf.keras.layers.BatchNormalization(
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer='zeros',
    gamma_initializer='ones',
    moving_mean_initializer='zeros',
    moving_variance_initializer='ones',
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Bidirectional(
    layer,
    merge_mode='concat',
    weights=None,
    backward_layer=None,
    **kwargs
)
","[['Inherits From: ', 'Wrapper', ', ', 'Layer', ', ', 'Module']]","model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True),
                             input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

# With custom backward layer
model = Sequential()
forward_layer = LSTM(10, return_sequences=True)
backward_layer = LSTM(10, activation='relu', return_sequences=True,
                      go_backwards=True)
model.add(Bidirectional(forward_layer, backward_layer=backward_layer,
                        input_shape=(5, 10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
"
"tf.keras.layers.CategoryEncoding(
    num_tokens=None, output_mode='multi_hot', sparse=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode=""one_hot"")
layer([3, 2, 0, 1])
<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
  array([[0., 0., 0., 1.],
         [0., 0., 1., 0.],
         [1., 0., 0., 0.],
         [0., 1., 0., 0.]], dtype=float32)>"
"tf.keras.layers.CenterCrop(
    height, width, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Concatenate(
    axis=-1, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = np.arange(20).reshape(2, 2, 5)
print(x)
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]
 [[10 11 12 13 14]
  [15 16 17 18 19]]]
y = np.arange(20, 30).reshape(2, 1, 5)
print(y)
[[[20 21 22 23 24]]
 [[25 26 27 28 29]]]
tf.keras.layers.Concatenate(axis=1)([x, y])
<tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=
array([[[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [20, 21, 22, 23, 24]],
       [[10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19],
        [25, 26, 27, 28, 29]]])>"
"tf.keras.layers.Conv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format='channels_last',
    dilation_rate=1,
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# The inputs are 128-length vectors with 10 timesteps, and the
# batch size is 4.
input_shape = (4, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu',input_shape=input_shape[1:])(x)
print(y.shape)
(4, 8, 32)"
"tf.keras.layers.Conv1DTranspose(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    output_padding=None,
    data_format=None,
    dilation_rate=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Conv1D', ', ', 'Layer', ', ', 'Module']]","new_timesteps = ((timesteps - 1) * strides + kernel_size -
2 * padding + output_padding)
"
"tf.keras.layers.Conv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# The inputs are 28x28 RGB images with `channels_last` and the batch
# size is 4.
input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 2)"
"tf.keras.layers.Conv2DTranspose(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    output_padding=None,
    data_format=None,
    dilation_rate=(1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Conv2D', ', ', 'Layer', ', ', 'Module']]","new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +
output_padding[0])
new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +
output_padding[1])
"
"tf.keras.layers.Conv3D(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# The inputs are 28x28x28 volumes with a single channel, and the
# batch size is 4
input_shape =(4, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 26, 2)"
"tf.keras.layers.Conv3DTranspose(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    output_padding=None,
    data_format=None,
    dilation_rate=(1, 1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Conv3D', ', ', 'Layer', ', ', 'Module']]","new_depth = ((depth - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +
output_padding[0])
new_rows = ((rows - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +
output_padding[1])
new_cols = ((cols - 1) * strides[2] + kernel_size[2] - 2 * padding[2] +
output_padding[2])
"
"tf.keras.layers.ConvLSTM1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format=None,
    dilation_rate=1,
    activation='tanh',
    recurrent_activation='hard_sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","reset_states(
    states=None
)
"
"tf.keras.layers.ConvLSTM2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    activation='tanh',
    recurrent_activation='hard_sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","reset_states(
    states=None
)
"
"tf.keras.layers.ConvLSTM3D(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1, 1),
    activation='tanh',
    recurrent_activation='hard_sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","reset_states(
    states=None
)
"
"tf.keras.layers.Conv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format='channels_last',
    dilation_rate=1,
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# The inputs are 128-length vectors with 10 timesteps, and the
# batch size is 4.
input_shape = (4, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu',input_shape=input_shape[1:])(x)
print(y.shape)
(4, 8, 32)"
"tf.keras.layers.Conv1DTranspose(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    output_padding=None,
    data_format=None,
    dilation_rate=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Conv1D', ', ', 'Layer', ', ', 'Module']]","new_timesteps = ((timesteps - 1) * strides + kernel_size -
2 * padding + output_padding)
"
"tf.keras.layers.Conv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# The inputs are 28x28 RGB images with `channels_last` and the batch
# size is 4.
input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 2)"
"tf.keras.layers.Conv2DTranspose(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    output_padding=None,
    data_format=None,
    dilation_rate=(1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Conv2D', ', ', 'Layer', ', ', 'Module']]","new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +
output_padding[0])
new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +
output_padding[1])
"
"tf.keras.layers.Conv3D(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# The inputs are 28x28x28 volumes with a single channel, and the
# batch size is 4
input_shape =(4, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 26, 2)"
"tf.keras.layers.Conv3DTranspose(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    output_padding=None,
    data_format=None,
    dilation_rate=(1, 1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Conv3D', ', ', 'Layer', ', ', 'Module']]","new_depth = ((depth - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +
output_padding[0])
new_rows = ((rows - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +
output_padding[1])
new_cols = ((cols - 1) * strides[2] + kernel_size[2] - 2 * padding[2] +
output_padding[2])
"
"tf.keras.layers.Cropping1D(
    cropping=(1, 1), **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 3, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1]
  [ 2  3]
  [ 4  5]]
 [[ 6  7]
  [ 8  9]
  [10 11]]]
y = tf.keras.layers.Cropping1D(cropping=1)(x)
print(y)
tf.Tensor(
  [[[2 3]]
   [[8 9]]], shape=(2, 1, 2), dtype=int64)"
"tf.keras.layers.Cropping2D(
    cropping=((0, 0), (0, 0)), data_format=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 28, 28, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping2D(cropping=((2, 2), (4, 4)))(x)
print(y.shape)
(2, 24, 20, 3)"
"tf.keras.layers.Cropping3D(
    cropping=((1, 1), (1, 1), (1, 1)), data_format=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 28, 28, 10, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping3D(cropping=(2, 4, 2))(x)
print(y.shape)
(2, 24, 20, 6, 3)"
"tf.keras.layers.Dense(
    units,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# Create a `Sequential` model and add a Dense layer as the first layer.
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(32, activation='relu'))
# Now the model will take as input arrays of shape (None, 16)
# and output arrays of shape (None, 32).
# Note that after the first layer, you don't need to specify
# the size of the input anymore:
model.add(tf.keras.layers.Dense(32))
model.output_shape
(None, 32)"
"tf.keras.layers.DenseFeatures(
    feature_columns, trainable=True, name=None, **kwargs
)
","[['A layer that produces a dense ', 'Tensor', ' based on given ', 'feature_columns', '.'], ['Inherits From: ', 'DenseFeatures', ', ', 'Layer', ', ', 'Module']]","price = tf.feature_column.numeric_column('price')
keywords_embedded = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_hash_bucket(""keywords"",
                                                          10000),
    dimensions=16)
columns = [price, keywords_embedded, ...]
feature_layer = tf.keras.layers.DenseFeatures(columns)

features = tf.io.parse_example(
    ..., features=tf.feature_column.make_parse_example_spec(columns))
dense_tensor = feature_layer(features)
for units in [128, 64, 32]:
  dense_tensor = tf.keras.layers.Dense(units, activation='relu')(
    dense_tensor)
prediction = tf.keras.layers.Dense(1)(dense_tensor)
"
"tf.keras.layers.DepthwiseConv1D(
    kernel_size,
    strides=1,
    padding='valid',
    depth_multiplier=1,
    data_format=None,
    dilation_rate=1,
    activation=None,
    use_bias=True,
    depthwise_initializer='glorot_uniform',
    bias_initializer='zeros',
    depthwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","convolution_op(
    inputs, kernel
)
"
"tf.keras.layers.DepthwiseConv2D(
    kernel_size,
    strides=(1, 1),
    padding='valid',
    depth_multiplier=1,
    data_format=None,
    dilation_rate=(1, 1),
    activation=None,
    use_bias=True,
    depthwise_initializer='glorot_uniform',
    bias_initializer='zeros',
    depthwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","convolution_op(
    inputs, kernel
)
"
"tf.keras.layers.Discretization(
    bin_boundaries=None,
    num_bins=None,
    epsilon=0.01,
    output_mode='int',
    sparse=False,
    **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]",">>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])
>>> layer = tf.keras.layers.Discretization(bin_boundaries=[0., 1., 2.])
>>> layer(input)
<tf.Tensor: shape=(2, 4), dtype=int64, numpy=
array([[0, 2, 3, 1],
       [1, 3, 2, 1]])>
"
"tf.keras.layers.Dot(
    axes, normalize=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = np.arange(10).reshape(1, 5, 2)
print(x)
[[[0 1]
  [2 3]
  [4 5]
  [6 7]
  [8 9]]]
y = np.arange(10, 20).reshape(1, 2, 5)
print(y)
[[[10 11 12 13 14]
  [15 16 17 18 19]]]
tf.keras.layers.Dot(axes=(1, 2))([x, y])
<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=
array([[[260, 360],
        [320, 445]]])>"
"tf.keras.layers.Dropout(
    rate, noise_shape=None, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","tf.random.set_seed(0)
layer = tf.keras.layers.Dropout(.2, input_shape=(2,))
data = np.arange(10).reshape(5, 2).astype(np.float32)
print(data)
[[0. 1.]
 [2. 3.]
 [4. 5.]
 [6. 7.]
 [8. 9.]]
outputs = layer(data, training=True)
print(outputs)
tf.Tensor(
[[ 0.    1.25]
 [ 2.5   3.75]
 [ 5.    6.25]
 [ 7.5   8.75]
 [10.    0.  ]], shape=(5, 2), dtype=float32)"
"tf.keras.layers.ELU(
    alpha=1.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","  f(x) =  alpha * (exp(x) - 1.) for x < 0
  f(x) = x for x >= 0
"
"tf.keras.layers.EinsumDense(
    equation,
    output_shape,
    activation=None,
    bias_axes=None,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['A layer that uses ', 'tf.einsum', ' as the backing computation.'], ['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.EinsumDense(""ab,bc->ac"",
                                    output_shape=64,
                                    bias_axes=""c"")
input_tensor = tf.keras.Input(shape=[32])
output_tensor = layer(input_tensor)
output_tensor
<... shape=(None, 64) dtype=...>"
"tf.keras.layers.Embedding(
    input_dim,
    output_dim,
    embeddings_initializer='uniform',
    embeddings_regularizer=None,
    activity_regularizer=None,
    embeddings_constraint=None,
    mask_zero=False,
    input_length=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(1000, 64, input_length=10))
# The model will take as input an integer matrix of size (batch,
# input_length), and the largest integer (i.e. word index) in the input
# should be no larger than 999 (vocabulary size).
# Now model.output_shape is (None, 10, 64), where `None` is the batch
# dimension.
input_array = np.random.randint(1000, size=(32, 10))
model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)
print(output_array.shape)
(32, 10, 64)"
"tf.keras.layers.Flatten(
    data_format=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(64, 3, 3, input_shape=(3, 32, 32)))
model.output_shape
(None, 1, 10, 64)"
"tf.keras.layers.GRU(
    units,
    activation='tanh',
    recurrent_activation='sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    unroll=False,
    time_major=False,
    reset_after=True,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","inputs = tf.random.normal([32, 10, 8])
gru = tf.keras.layers.GRU(4)
output = gru(inputs)
print(output.shape)
(32, 4)
gru = tf.keras.layers.GRU(4, return_sequences=True, return_state=True)
whole_sequence_output, final_state = gru(inputs)
print(whole_sequence_output.shape)
(32, 10, 4)
print(final_state.shape)
(32, 4)"
"tf.keras.layers.GRUCell(
    units,
    activation='tanh',
    recurrent_activation='sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    reset_after=True,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","inputs = tf.random.normal([32, 10, 8])
rnn = tf.keras.layers.RNN(tf.keras.layers.GRUCell(4))
output = rnn(inputs)
print(output.shape)
(32, 4)
rnn = tf.keras.layers.RNN(
   tf.keras.layers.GRUCell(4),
   return_sequences=True,
   return_state=True)
whole_sequence_output, final_state = rnn(inputs)
print(whole_sequence_output.shape)
(32, 10, 4)
print(final_state.shape)
(32, 4)"
"tf.keras.layers.GaussianDropout(
    rate, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.GaussianNoise(
    stddev, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.GlobalAveragePooling1D(
    data_format='channels_last', **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 3, 4)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling1D()(x)
print(y.shape)
(2, 4)"
"tf.keras.layers.GlobalAveragePooling2D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalAveragePooling3D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.GlobalAveragePooling1D(
    data_format='channels_last', **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 3, 4)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling1D()(x)
print(y.shape)
(2, 4)"
"tf.keras.layers.GlobalAveragePooling2D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalAveragePooling3D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.GlobalMaxPool1D(
    data_format='channels_last', keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])
x = tf.reshape(x, [3, 3, 1])
x
<tf.Tensor: shape=(3, 3, 1), dtype=float32, numpy=
array([[[1.], [2.], [3.]],
       [[4.], [5.], [6.]],
       [[7.], [8.], [9.]]], dtype=float32)>
max_pool_1d = tf.keras.layers.GlobalMaxPooling1D()
max_pool_1d(x)
<tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[3.],
       [6.],
       [9.], dtype=float32)>"
"tf.keras.layers.GlobalMaxPool2D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalMaxPool2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalMaxPool3D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.GlobalMaxPool1D(
    data_format='channels_last', keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])
x = tf.reshape(x, [3, 3, 1])
x
<tf.Tensor: shape=(3, 3, 1), dtype=float32, numpy=
array([[[1.], [2.], [3.]],
       [[4.], [5.], [6.]],
       [[7.], [8.], [9.]]], dtype=float32)>
max_pool_1d = tf.keras.layers.GlobalMaxPooling1D()
max_pool_1d(x)
<tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[3.],
       [6.],
       [9.], dtype=float32)>"
"tf.keras.layers.GlobalMaxPool2D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalMaxPool2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalMaxPool3D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.GroupNormalization(
    groups=32,
    axis=-1,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer='zeros',
    gamma_initializer='ones',
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Hashing(
    num_bins,
    mask_value=None,
    salt=None,
    output_mode='int',
    sparse=False,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.Hashing(num_bins=3)
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
  array([[1],
         [0],
         [1],
         [1],
         [2]])>"
"tf.keras.Input(
    shape=None,
    batch_size=None,
    name=None,
    dtype=None,
    sparse=None,
    tensor=None,
    ragged=None,
    type_spec=None,
    **kwargs
)
","[['Input()', ' is used to instantiate a Keras tensor.']]","# this is a logistic regression in Keras
x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)
"
"tf.keras.layers.InputLayer(
    input_shape=None,
    batch_size=None,
    dtype=None,
    input_tensor=None,
    sparse=None,
    name=None,
    ragged=None,
    type_spec=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# With explicit InputLayer.
model = tf.keras.Sequential([
  tf.keras.layers.InputLayer(input_shape=(4,)),
  tf.keras.layers.Dense(8)])
model.compile(tf.keras.optimizers.RMSprop(0.001), loss='mse')
model.fit(np.zeros((10, 4)),
          np.ones((10, 8)))

# Without InputLayer and let the first layer to have the input_shape.
# Keras will add a input for the model behind the scene.
model = tf.keras.Sequential([
  tf.keras.layers.Dense(8, input_shape=(4,))])
model.compile(tf.keras.optimizers.RMSprop(0.001), loss='mse')
model.fit(np.zeros((10, 4)),
          np.ones((10, 8)))
"
"tf.keras.layers.InputSpec(
    dtype=None,
    shape=None,
    ndim=None,
    max_ndim=None,
    min_ndim=None,
    axes=None,
    allow_last_axis_squeeze=False,
    name=None
)
",[],"class MyLayer(Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        # The layer will accept inputs with
        # shape (?, 28, 28) & (?, 28, 28, 1)
        # and raise an appropriate error message otherwise.
        self.input_spec = InputSpec(
            shape=(None, 28, 28, 1),
            allow_last_axis_squeeze=True)
"
"tf.keras.layers.IntegerLookup(
    max_tokens=None,
    num_oov_indices=1,
    mask_token=None,
    oov_token=-1,
    vocabulary=None,
    vocabulary_dtype='int64',
    idf_weights=None,
    invert=False,
    output_mode='int',
    sparse=False,
    pad_to_max_tokens=False,
    **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]","vocab = [12, 36, 1138, 42]
data = tf.constant([[12, 1138, 42], [42, 1000, 36]])  # Note OOV tokens
layer = tf.keras.layers.IntegerLookup(vocabulary=vocab)
layer(data)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
array([[1, 3, 4],
       [4, 0, 2]])>"
"tf.keras.layers.LSTM(
    units,
    activation='tanh',
    recurrent_activation='sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    time_major=False,
    unroll=False,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","inputs = tf.random.normal([32, 10, 8])
lstm = tf.keras.layers.LSTM(4)
output = lstm(inputs)
print(output.shape)
(32, 4)
lstm = tf.keras.layers.LSTM(4, return_sequences=True, return_state=True)
whole_seq_output, final_memory_state, final_carry_state = lstm(inputs)
print(whole_seq_output.shape)
(32, 10, 4)
print(final_memory_state.shape)
(32, 4)
print(final_carry_state.shape)
(32, 4)"
"tf.keras.layers.LSTMCell(
    units,
    activation='tanh',
    recurrent_activation='sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","inputs = tf.random.normal([32, 10, 8])
rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4))
output = rnn(inputs)
print(output.shape)
(32, 4)
rnn = tf.keras.layers.RNN(
   tf.keras.layers.LSTMCell(4),
   return_sequences=True,
   return_state=True)
whole_seq_output, final_memory_state, final_carry_state = rnn(inputs)
print(whole_seq_output.shape)
(32, 10, 4)
print(final_memory_state.shape)
(32, 4)
print(final_carry_state.shape)
(32, 4)"
"tf.keras.layers.Lambda(
    function, output_shape=None, mask=None, arguments=None, **kwargs
)
","[['Wraps arbitrary expressions as a ', 'Layer', ' object.'], ['Inherits From: ', 'Layer', ', ', 'Module']]","# add a x -> x^2 layer
model.add(Lambda(lambda x: x ** 2))
"
"tf.keras.layers.Layer(
    trainable=True, name=None, dtype=None, dynamic=False, **kwargs
)
","[['Inherits From: ', 'Module']]","class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):  # Create the state of the layer (weights)
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

  def call(self, inputs):  # Defines the computation from inputs to outputs
      return tf.matmul(inputs, self.w) + self.b

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2
"
"tf.keras.layers.LayerNormalization(
    axis=-1,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer='zeros',
    gamma_initializer='ones',
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","data = tf.constant(np.arange(10).reshape(5, 2) * 10, dtype=tf.float32)
print(data)
tf.Tensor(
[[ 0. 10.]
 [20. 30.]
 [40. 50.]
 [60. 70.]
 [80. 90.]], shape=(5, 2), dtype=float32)"
"tf.keras.layers.LeakyReLU(
    alpha=0.3, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","  f(x) = alpha * x if x < 0
  f(x) = x if x >= 0
"
"tf.keras.layers.LocallyConnected1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format=None,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    implementation=1,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","    # apply a unshared weight convolution 1d of length 3 to a sequence with
    # 10 timesteps, with 64 output filters
    model = Sequential()
    model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))
    # now model.output_shape == (None, 8, 64)
    # add a new conv1d on top
    model.add(LocallyConnected1D(32, 3))
    # now model.output_shape == (None, 6, 32)
"
"tf.keras.layers.LocallyConnected2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    implementation=1,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","    # apply a 3x3 unshared weights convolution with 64 output filters on a
    32x32 image
    # with `data_format=""channels_last""`:
    model = Sequential()
    model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))
    # now model.output_shape == (None, 30, 30, 64)
    # notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64
    parameters

    # add a 3x3 unshared weights convolution on top, with 32 output filters:
    model.add(LocallyConnected2D(32, (3, 3)))
    # now model.output_shape == (None, 28, 28, 32)
"
"tf.keras.layers.Masking(
    mask_value=0.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","samples, timesteps, features = 32, 10, 8
inputs = np.random.random([samples, timesteps, features]).astype(np.float32)
inputs[:, 3, :] = 0.
inputs[:, 5, :] = 0.

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Masking(mask_value=0.,
                                  input_shape=(timesteps, features)))
model.add(tf.keras.layers.LSTM(32))

output = model(inputs)
# The time step 3 and 5 will be skipped from LSTM calculation.
"
"tf.keras.layers.MaxPool1D(
    pool_size=2,
    strides=None,
    padding='valid',
    data_format='channels_last',
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,
   strides=1, padding='valid')
max_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[2.],
        [3.],
        [4.],
        [5.]]], dtype=float32)>"
"tf.keras.layers.MaxPool2D(
    pool_size=(2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([[1., 2., 3.],
                 [4., 5., 6.],
                 [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
   strides=(1, 1), padding='valid')
max_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[5.],
           [6.]],
          [[8.],
           [9.]]]], dtype=float32)>"
"tf.keras.layers.MaxPool3D(
    pool_size=(2, 2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","depth = 30
height = 30
width = 30
input_channels = 3

inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.MaxPooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
"
"tf.keras.layers.MaxPool1D(
    pool_size=2,
    strides=None,
    padding='valid',
    data_format='channels_last',
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,
   strides=1, padding='valid')
max_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[2.],
        [3.],
        [4.],
        [5.]]], dtype=float32)>"
"tf.keras.layers.MaxPool2D(
    pool_size=(2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([[1., 2., 3.],
                 [4., 5., 6.],
                 [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
   strides=(1, 1), padding='valid')
max_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[5.],
           [6.]],
          [[8.],
           [9.]]]], dtype=float32)>"
"tf.keras.layers.MaxPool3D(
    pool_size=(2, 2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","depth = 30
height = 30
width = 30
input_channels = 3

inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.MaxPooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
"
"tf.keras.layers.Maximum(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","tf.keras.layers.Maximum()([np.arange(5).reshape(5, 1),
                           np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[5],
     [6],
     [7],
     [8],
     [9]])>"
"tf.keras.layers.Minimum(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","tf.keras.layers.Minimum()([np.arange(5).reshape(5, 1),
                           np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[0],
     [1],
     [2],
     [3],
     [4]])>"
"tf.keras.layers.MultiHeadAttention(
    num_heads,
    key_dim,
    value_dim=None,
    dropout=0.0,
    use_bias=True,
    output_shape=None,
    attention_axes=None,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = MultiHeadAttention(num_heads=2, key_dim=2)
target = tf.keras.Input(shape=[8, 16])
source = tf.keras.Input(shape=[4, 16])
output_tensor, weights = layer(target, source,
                               return_attention_scores=True)
print(output_tensor.shape)
(None, 8, 16)
print(weights.shape)
(None, 2, 8, 4)"
"tf.keras.layers.Multiply(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","tf.keras.layers.Multiply()([np.arange(5).reshape(5, 1),
                            np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[ 0],
     [ 6],
     [14],
     [24],
     [36]])>"
"tf.keras.layers.Normalization(
    axis=-1, mean=None, variance=None, invert=False, **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]","adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')
input_data = np.array([1., 2., 3.], dtype='float32')
layer = tf.keras.layers.Normalization(axis=None)
layer.adapt(adapt_data)
layer(input_data)
<tf.Tensor: shape=(3,), dtype=float32, numpy=
array([-1.4142135, -0.70710677, 0.], dtype=float32)>"
"tf.keras.layers.PReLU(
    alpha_initializer='zeros',
    alpha_regularizer=None,
    alpha_constraint=None,
    shared_axes=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","  f(x) = alpha * x for x < 0
  f(x) = x for x >= 0
"
"tf.keras.layers.Permute(
    dims, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","model = Sequential()
model.add(Permute((2, 1), input_shape=(10, 64)))
# now: model.output_shape == (None, 64, 10)
# note: `None` is the batch dimension
"
"tf.keras.layers.RNN(
    cell,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    unroll=False,
    time_major=False,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","- Specify `stateful=True` in the layer constructor.
- Specify a fixed batch size for your model, by passing
  If sequential model:
    `batch_input_shape=(...)` to the first layer in your model.
  Else for functional model with 1 or more Input layers:
    `batch_shape=(...)` to all the first layers in your model.
  This is the expected shape of your inputs
  *including the batch size*.
  It should be a tuple of integers, e.g. `(32, 10, 100)`.
- Specify `shuffle=False` when calling `fit()`.
"
"tf.keras.layers.RandomBrightness(
    factor, value_range=(0, 255), seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","random_bright = tf.keras.layers.RandomBrightness(factor=0.2)

# An image with shape [2, 2, 3]
image = [[[1, 2, 3], [4 ,5 ,6]], [[7, 8, 9], [10, 11, 12]]]

# Assume we randomly select the factor to be 0.1, then it will apply
# 0.1 * 255 to all the channel
output = random_bright(image, training=True)

# output will be int64 with 25.5 added to each channel and round down.
tf.Tensor([[[26.5, 27.5, 28.5]
            [29.5, 30.5, 31.5]]
           [[32.5, 33.5, 34.5]
            [35.5, 36.5, 37.5]]],
          shape=(2, 2, 3), dtype=int64)
"
"tf.keras.layers.RandomContrast(
    factor, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomCrop(
    height, width, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomFlip(
    mode=HORIZONTAL_AND_VERTICAL, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomHeight(
    factor, interpolation='bilinear', seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomRotation(
    factor,
    fill_mode='reflect',
    interpolation='bilinear',
    seed=None,
    fill_value=0.0,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomTranslation(
    height_factor,
    width_factor,
    fill_mode='reflect',
    interpolation='bilinear',
    seed=None,
    fill_value=0.0,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomWidth(
    factor, interpolation='bilinear', seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomZoom(
    height_factor,
    width_factor=None,
    fill_mode='reflect',
    interpolation='bilinear',
    seed=None,
    fill_value=0.0,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_img = np.random.random((32, 224, 224, 3))
layer = tf.keras.layers.RandomZoom(.5, .2)
out_img = layer(input_img)
out_img.shape
TensorShape([32, 224, 224, 3])"
"tf.keras.layers.ReLU(
    max_value=None, negative_slope=0.0, threshold=0.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","  f(x) = max_value if x >= max_value
  f(x) = x if threshold <= x < max_value
  f(x) = negative_slope * (x - threshold) otherwise
"
"tf.keras.layers.RepeatVector(
    n, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","model = Sequential()
model.add(Dense(32, input_dim=32))
# now: model.output_shape == (None, 32)
# note: `None` is the batch dimension

model.add(RepeatVector(3))
# now: model.output_shape == (None, 3, 32)
"
"tf.keras.layers.Rescaling(
    scale, offset=0.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Reshape(
    target_shape, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# as first layer in a Sequential model
model = tf.keras.Sequential()
model.add(tf.keras.layers.Reshape((3, 4), input_shape=(12,)))
# model.output_shape == (None, 3, 4), `None` is the batch size.
model.output_shape
(None, 3, 4)"
"tf.keras.layers.Resizing(
    height,
    width,
    interpolation='bilinear',
    crop_to_aspect_ratio=False,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.SeparableConv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format=None,
    dilation_rate=1,
    depth_multiplier=1,
    activation=None,
    use_bias=True,
    depthwise_initializer='glorot_uniform',
    pointwise_initializer='glorot_uniform',
    bias_initializer='zeros',
    depthwise_regularizer=None,
    pointwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    pointwise_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","convolution_op(
    inputs, kernel
)
"
"tf.keras.layers.SeparableConv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    depth_multiplier=1,
    activation=None,
    use_bias=True,
    depthwise_initializer='glorot_uniform',
    pointwise_initializer='glorot_uniform',
    bias_initializer='zeros',
    depthwise_regularizer=None,
    pointwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    pointwise_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","convolution_op(
    inputs, kernel
)
"
"tf.keras.layers.SeparableConv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format=None,
    dilation_rate=1,
    depth_multiplier=1,
    activation=None,
    use_bias=True,
    depthwise_initializer='glorot_uniform',
    pointwise_initializer='glorot_uniform',
    bias_initializer='zeros',
    depthwise_regularizer=None,
    pointwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    pointwise_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","convolution_op(
    inputs, kernel
)
"
"tf.keras.layers.SeparableConv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    depth_multiplier=1,
    activation=None,
    use_bias=True,
    depthwise_initializer='glorot_uniform',
    pointwise_initializer='glorot_uniform',
    bias_initializer='zeros',
    depthwise_regularizer=None,
    pointwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    pointwise_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","convolution_op(
    inputs, kernel
)
"
"tf.keras.layers.SimpleRNN(
    units,
    activation='tanh',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    unroll=False,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","inputs = np.random.random([32, 10, 8]).astype(np.float32)
simple_rnn = tf.keras.layers.SimpleRNN(4)

output = simple_rnn(inputs)  # The output has shape `[32, 4]`.

simple_rnn = tf.keras.layers.SimpleRNN(
    4, return_sequences=True, return_state=True)

# whole_sequence_output has shape `[32, 10, 4]`.
# final_state has shape `[32, 4]`.
whole_sequence_output, final_state = simple_rnn(inputs)
"
"tf.keras.layers.SimpleRNNCell(
    units,
    activation='tanh',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","inputs = np.random.random([32, 10, 8]).astype(np.float32)
rnn = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(4))

output = rnn(inputs)  # The output has shape `[32, 4]`.

rnn = tf.keras.layers.RNN(
    tf.keras.layers.SimpleRNNCell(4),
    return_sequences=True,
    return_state=True)

# whole_sequence_output has shape `[32, 10, 4]`.
# final_state has shape `[32, 4]`.
whole_sequence_output, final_state = rnn(inputs)
"
"tf.keras.layers.Softmax(
    axis=-1, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","inp = np.asarray([1., 2., 1.])
layer = tf.keras.layers.Softmax()
layer(inp).numpy()
array([0.21194157, 0.5761169 , 0.21194157], dtype=float32)
mask = np.asarray([True, False, True], dtype=bool)
layer(inp, mask).numpy()
array([0.5, 0. , 0.5], dtype=float32)"
"tf.keras.layers.SpatialDropout1D(
    rate, **kwargs
)
","[['Inherits From: ', 'Dropout', ', ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.SpatialDropout2D(
    rate, data_format=None, **kwargs
)
","[['Inherits From: ', 'Dropout', ', ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.SpatialDropout3D(
    rate, data_format=None, **kwargs
)
","[['Inherits From: ', 'Dropout', ', ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.StackedRNNCells(
    cells, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","batch_size = 3
sentence_max_length = 5
n_features = 2
new_shape = (batch_size, sentence_max_length, n_features)
x = tf.constant(np.reshape(np.arange(30), new_shape), dtype = tf.float32)

rnn_cells = [tf.keras.layers.LSTMCell(128) for _ in range(2)]
stacked_lstm = tf.keras.layers.StackedRNNCells(rnn_cells)
lstm_layer = tf.keras.layers.RNN(stacked_lstm)

result = lstm_layer(x)
"
"tf.keras.layers.StringLookup(
    max_tokens=None,
    num_oov_indices=1,
    mask_token=None,
    oov_token='[UNK]',
    vocabulary=None,
    idf_weights=None,
    encoding='utf-8',
    invert=False,
    output_mode='int',
    sparse=False,
    pad_to_max_tokens=False,
    **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]","vocab = [""a"", ""b"", ""c"", ""d""]
data = tf.constant([[""a"", ""c"", ""d""], [""d"", ""z"", ""b""]])
layer = tf.keras.layers.StringLookup(vocabulary=vocab)
layer(data)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
array([[1, 3, 4],
       [4, 0, 2]])>"
"tf.keras.layers.Subtract(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    # Equivalent to subtracted = keras.layers.subtract([x1, x2])
    subtracted = keras.layers.Subtract()([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
"
"tf.keras.layers.TextVectorization(
    max_tokens=None,
    standardize='lower_and_strip_punctuation',
    split='whitespace',
    ngrams=None,
    output_mode='int',
    output_sequence_length=None,
    pad_to_max_tokens=False,
    vocabulary=None,
    idf_weights=None,
    sparse=False,
    ragged=False,
    encoding='utf-8',
    **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]","text_dataset = tf.data.Dataset.from_tensor_slices([""foo"", ""bar"", ""baz""])
max_features = 5000  # Maximum vocab size.
max_len = 4  # Sequence length to pad the outputs to.
# Create the layer.
vectorize_layer = tf.keras.layers.TextVectorization(
 max_tokens=max_features,
 output_mode='int',
 output_sequence_length=max_len)
# Now that the vocab layer has been created, call `adapt` on the
# text-only dataset to create the vocabulary. You don't have to batch,
# but for large datasets this means we're not keeping spare copies of
# the dataset.
vectorize_layer.adapt(text_dataset.batch(64))
# Create the model that uses the vectorize text layer
model = tf.keras.models.Sequential()
# Start by creating an explicit input layer. It needs to have a shape of
# (1,) (because we need to guarantee that there is exactly one string
# input per batch), and the dtype needs to be 'string'.
model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
# The first layer in our model is the vectorization layer. After this
# layer, we have a tensor of shape (batch_size, max_len) containing
# vocab indices.
model.add(vectorize_layer)
# Now, the model can map strings to integers, and you can add an
# embedding layer to map these integers to learned embeddings.
input_data = [[""foo qux bar""], [""qux baz""]]
model.predict(input_data)
array([[2, 1, 4, 0],
       [1, 3, 0, 0]])"
"tf.keras.layers.ThresholdedReLU(
    theta=1.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","  f(x) = x for x > theta
  f(x) = 0 otherwise`
"
"tf.keras.layers.TimeDistributed(
    layer, **kwargs
)
","[['Inherits From: ', 'Wrapper', ', ', 'Layer', ', ', 'Module']]","inputs = tf.keras.Input(shape=(10, 128, 128, 3))
conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))
outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)
outputs.shape
TensorShape([None, 10, 126, 126, 64])"
"tf.keras.layers.UnitNormalization(
    axis=-1, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","data = tf.constant(np.arange(6).reshape(2, 3), dtype=tf.float32)
normalized_data = tf.keras.layers.UnitNormalization()(data)
print(tf.reduce_sum(normalized_data[0, :] ** 2).numpy())
1.0"
"tf.keras.layers.UpSampling1D(
    size=2, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1  2]
  [ 3  4  5]]
 [[ 6  7  8]
  [ 9 10 11]]]
y = tf.keras.layers.UpSampling1D(size=2)(x)
print(y)
tf.Tensor(
  [[[ 0  1  2]
    [ 0  1  2]
    [ 3  4  5]
    [ 3  4  5]]
   [[ 6  7  8]
    [ 6  7  8]
    [ 9 10 11]
    [ 9 10 11]]], shape=(2, 4, 3), dtype=int64)"
"tf.keras.layers.UpSampling2D(
    size=(2, 2), data_format=None, interpolation='nearest', **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 2, 1, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[[ 0  1  2]]
  [[ 3  4  5]]]
 [[[ 6  7  8]]
  [[ 9 10 11]]]]
y = tf.keras.layers.UpSampling2D(size=(1, 2))(x)
print(y)
tf.Tensor(
  [[[[ 0  1  2]
     [ 0  1  2]]
    [[ 3  4  5]
     [ 3  4  5]]]
   [[[ 6  7  8]
     [ 6  7  8]]
    [[ 9 10 11]
     [ 9 10 11]]]], shape=(2, 2, 2, 3), dtype=int64)"
"tf.keras.layers.UpSampling3D(
    size=(2, 2, 2), data_format=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 1, 2, 1, 3)
x = tf.constant(1, shape=input_shape)
y = tf.keras.layers.UpSampling3D(size=2)(x)
print(y.shape)
(2, 2, 4, 2, 3)"
"tf.keras.layers.Wrapper(
    layer, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.ZeroPadding1D(
    padding=1, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1  2]
  [ 3  4  5]]
 [[ 6  7  8]
  [ 9 10 11]]]
y = tf.keras.layers.ZeroPadding1D(padding=2)(x)
print(y)
tf.Tensor(
  [[[ 0  0  0]
    [ 0  0  0]
    [ 0  1  2]
    [ 3  4  5]
    [ 0  0  0]
    [ 0  0  0]]
   [[ 0  0  0]
    [ 0  0  0]
    [ 6  7  8]
    [ 9 10 11]
    [ 0  0  0]
    [ 0  0  0]]], shape=(2, 6, 3), dtype=int64)"
"tf.keras.layers.ZeroPadding2D(
    padding=(1, 1), data_format=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (1, 1, 2, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[[0 1]
   [2 3]]]]
y = tf.keras.layers.ZeroPadding2D(padding=1)(x)
print(y)
tf.Tensor(
  [[[[0 0]
     [0 0]
     [0 0]
     [0 0]]
    [[0 0]
     [0 1]
     [2 3]
     [0 0]]
    [[0 0]
     [0 0]
     [0 0]
     [0 0]]]], shape=(1, 3, 4, 2), dtype=int64)"
"tf.keras.layers.ZeroPadding3D(
    padding=(1, 1, 1), data_format=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (1, 1, 2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.ZeroPadding3D(padding=2)(x)
print(y.shape)
(1, 5, 6, 6, 3)"
"tf.keras.layers.add(
    inputs, **kwargs
)
","[['Functional interface to the ', 'tf.keras.layers.Add', ' layer.']]","input_shape = (2, 3, 4)
x1 = tf.random.normal(input_shape)
x2 = tf.random.normal(input_shape)
y = tf.keras.layers.add([x1, x2])
print(y.shape)
(2, 3, 4)"
"tf.keras.layers.average(
    inputs, **kwargs
)
","[['Functional interface to the ', 'tf.keras.layers.Average', ' layer.']]","x1 = np.ones((2, 2))
x2 = np.zeros((2, 2))
y = tf.keras.layers.Average()([x1, x2])
y.numpy().tolist()
[[0.5, 0.5], [0.5, 0.5]]"
"tf.keras.layers.concatenate(
    inputs, axis=-1, **kwargs
)
","[['Functional interface to the ', 'Concatenate', ' layer.']]","x = np.arange(20).reshape(2, 2, 5)
print(x)
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]
 [[10 11 12 13 14]
  [15 16 17 18 19]]]
y = np.arange(20, 30).reshape(2, 1, 5)
print(y)
[[[20 21 22 23 24]]
 [[25 26 27 28 29]]]
tf.keras.layers.concatenate([x, y],
                            axis=1)
<tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=
array([[[ 0,  1,  2,  3,  4],
      [ 5,  6,  7,  8,  9],
      [20, 21, 22, 23, 24]],
     [[10, 11, 12, 13, 14],
      [15, 16, 17, 18, 19],
      [25, 26, 27, 28, 29]]])>"
"tf.keras.layers.deserialize(
    config, custom_objects=None
)
",[],"# Configuration of Dense(32, activation='relu')
config = {
  'class_name': 'Dense',
  'config': {
    'activation': 'relu',
    'activity_regularizer': None,
    'bias_constraint': None,
    'bias_initializer': {'class_name': 'Zeros', 'config': {} },
    'bias_regularizer': None,
    'dtype': 'float32',
    'kernel_constraint': None,
    'kernel_initializer': {'class_name': 'GlorotUniform',
                           'config': {'seed': None} },
    'kernel_regularizer': None,
    'name': 'dense',
    'trainable': True,
    'units': 32,
    'use_bias': True
  }
}
dense_layer = tf.keras.layers.deserialize(config)
"
"tf.keras.layers.dot(
    inputs, axes, normalize=False, **kwargs
)
","[['Functional interface to the ', 'Dot', ' layer.']]",[]
"tf.keras.layers.EinsumDense(
    equation,
    output_shape,
    activation=None,
    bias_axes=None,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['A layer that uses ', 'tf.einsum', ' as the backing computation.'], ['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.EinsumDense(""ab,bc->ac"",
                                    output_shape=64,
                                    bias_axes=""c"")
input_tensor = tf.keras.Input(shape=[32])
output_tensor = layer(input_tensor)
output_tensor
<... shape=(None, 64) dtype=...>"
"tf.keras.layers.experimental.RandomFourierFeatures(
    output_dim,
    kernel_initializer='gaussian',
    scale=None,
    trainable=False,
    name=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","model = keras.Sequential([
  keras.Input(shape=(784,)),
  RandomFourierFeatures(
      output_dim=4096,
      scale=10.,
      kernel_initializer='gaussian'),
  layers.Dense(units=10, activation='softmax'),
])
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['categorical_accuracy']
)
"
"tf.keras.layers.experimental.SyncBatchNormalization(
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer='zeros',
    gamma_initializer='ones',
    moving_mean_initializer='zeros',
    moving_variance_initializer='ones',
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","strategy = tf.distribute.MirroredStrategy()

with strategy.scope():
  model = tf.keras.Sequential()
  model.add(tf.keras.layers.Dense(16))
  model.add(tf.keras.layers.experimental.SyncBatchNormalization())
"
"tf.keras.layers.CategoryEncoding(
    num_tokens=None, output_mode='multi_hot', sparse=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode=""one_hot"")
layer([3, 2, 0, 1])
<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
  array([[0., 0., 0., 1.],
         [0., 0., 1., 0.],
         [1., 0., 0., 0.],
         [0., 1., 0., 0.]], dtype=float32)>"
"tf.keras.layers.CenterCrop(
    height, width, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Discretization(
    bin_boundaries=None,
    num_bins=None,
    epsilon=0.01,
    output_mode='int',
    sparse=False,
    **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]",">>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])
>>> layer = tf.keras.layers.Discretization(bin_boundaries=[0., 1., 2.])
>>> layer(input)
<tf.Tensor: shape=(2, 4), dtype=int64, numpy=
array([[0, 2, 3, 1],
       [1, 3, 2, 1]])>
"
"tf.keras.layers.experimental.preprocessing.HashedCrossing(
    num_bins, output_mode='int', sparse=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(
    num_bins=5)
feat1 = tf.constant(['A', 'B', 'A', 'B', 'A'])
feat2 = tf.constant([101, 101, 101, 102, 102])
layer((feat1, feat2))
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 4, 1, 1, 3])>"
"tf.keras.layers.Hashing(
    num_bins,
    mask_value=None,
    salt=None,
    output_mode='int',
    sparse=False,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.Hashing(num_bins=3)
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
  array([[1],
         [0],
         [1],
         [1],
         [2]])>"
"tf.keras.layers.IntegerLookup(
    max_tokens=None,
    num_oov_indices=1,
    mask_token=None,
    oov_token=-1,
    vocabulary=None,
    vocabulary_dtype='int64',
    idf_weights=None,
    invert=False,
    output_mode='int',
    sparse=False,
    pad_to_max_tokens=False,
    **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]","vocab = [12, 36, 1138, 42]
data = tf.constant([[12, 1138, 42], [42, 1000, 36]])  # Note OOV tokens
layer = tf.keras.layers.IntegerLookup(vocabulary=vocab)
layer(data)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
array([[1, 3, 4],
       [4, 0, 2]])>"
"tf.keras.layers.Normalization(
    axis=-1, mean=None, variance=None, invert=False, **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]","adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')
input_data = np.array([1., 2., 3.], dtype='float32')
layer = tf.keras.layers.Normalization(axis=None)
layer.adapt(adapt_data)
layer(input_data)
<tf.Tensor: shape=(3,), dtype=float32, numpy=
array([-1.4142135, -0.70710677, 0.], dtype=float32)>"
"tf.keras.layers.experimental.preprocessing.PreprocessingLayer(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","adapt(
    data, batch_size=None, steps=None
)
"
"tf.keras.layers.RandomContrast(
    factor, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomCrop(
    height, width, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomFlip(
    mode=HORIZONTAL_AND_VERTICAL, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomHeight(
    factor, interpolation='bilinear', seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomRotation(
    factor,
    fill_mode='reflect',
    interpolation='bilinear',
    seed=None,
    fill_value=0.0,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomTranslation(
    height_factor,
    width_factor,
    fill_mode='reflect',
    interpolation='bilinear',
    seed=None,
    fill_value=0.0,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomWidth(
    factor, interpolation='bilinear', seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","class SubclassLayer(BaseImageAugmentationLayer):
  def __init__(self):
    super().__init__()
    self.auto_vectorize = False
"
"tf.keras.layers.RandomZoom(
    height_factor,
    width_factor=None,
    fill_mode='reflect',
    interpolation='bilinear',
    seed=None,
    fill_value=0.0,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_img = np.random.random((32, 224, 224, 3))
layer = tf.keras.layers.RandomZoom(.5, .2)
out_img = layer(input_img)
out_img.shape
TensorShape([32, 224, 224, 3])"
"tf.keras.layers.Rescaling(
    scale, offset=0.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Resizing(
    height,
    width,
    interpolation='bilinear',
    crop_to_aspect_ratio=False,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.StringLookup(
    max_tokens=None,
    num_oov_indices=1,
    mask_token=None,
    oov_token='[UNK]',
    vocabulary=None,
    idf_weights=None,
    encoding='utf-8',
    invert=False,
    output_mode='int',
    sparse=False,
    pad_to_max_tokens=False,
    **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]","vocab = [""a"", ""b"", ""c"", ""d""]
data = tf.constant([[""a"", ""c"", ""d""], [""d"", ""z"", ""b""]])
layer = tf.keras.layers.StringLookup(vocabulary=vocab)
layer(data)
<tf.Tensor: shape=(2, 3), dtype=int64, numpy=
array([[1, 3, 4],
       [4, 0, 2]])>"
"tf.keras.layers.TextVectorization(
    max_tokens=None,
    standardize='lower_and_strip_punctuation',
    split='whitespace',
    ngrams=None,
    output_mode='int',
    output_sequence_length=None,
    pad_to_max_tokens=False,
    vocabulary=None,
    idf_weights=None,
    sparse=False,
    ragged=False,
    encoding='utf-8',
    **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]","text_dataset = tf.data.Dataset.from_tensor_slices([""foo"", ""bar"", ""baz""])
max_features = 5000  # Maximum vocab size.
max_len = 4  # Sequence length to pad the outputs to.
# Create the layer.
vectorize_layer = tf.keras.layers.TextVectorization(
 max_tokens=max_features,
 output_mode='int',
 output_sequence_length=max_len)
# Now that the vocab layer has been created, call `adapt` on the
# text-only dataset to create the vocabulary. You don't have to batch,
# but for large datasets this means we're not keeping spare copies of
# the dataset.
vectorize_layer.adapt(text_dataset.batch(64))
# Create the model that uses the vectorize text layer
model = tf.keras.models.Sequential()
# Start by creating an explicit input layer. It needs to have a shape of
# (1,) (because we need to guarantee that there is exactly one string
# input per batch), and the dtype needs to be 'string'.
model.add(tf.keras.Input(shape=(1,), dtype=tf.string))
# The first layer in our model is the vectorization layer. After this
# layer, we have a tensor of shape (batch_size, max_len) containing
# vocab indices.
model.add(vectorize_layer)
# Now, the model can map strings to integers, and you can add an
# embedding layer to map these integers to learned embeddings.
input_data = [[""foo qux bar""], [""qux baz""]]
model.predict(input_data)
array([[2, 1, 4, 0],
       [1, 3, 0, 0]])"
"tf.keras.layers.maximum(
    inputs, **kwargs
)
","[['Functional interface to compute maximum (element-wise) list of ', 'inputs', '.']]","input1 = tf.keras.layers.Input(shape=(16,))
x1 = tf.keras.layers.Dense(8, activation='relu')(input1) #shape=(None, 8)
input2 = tf.keras.layers.Input(shape=(32,))
x2 = tf.keras.layers.Dense(8, activation='relu')(input2) #shape=(None, 8)
max_inp=tf.keras.layers.maximum([x1,x2]) #shape=(None, 8)
out = tf.keras.layers.Dense(4)(max_inp)
model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)
"
"tf.keras.layers.minimum(
    inputs, **kwargs
)
","[['Functional interface to the ', 'Minimum', ' layer.']]",[]
"tf.keras.layers.multiply(
    inputs, **kwargs
)
","[['Functional interface to the ', 'Multiply', ' layer.']]","x1 = np.arange(3.0)
x2 = np.arange(3.0)
tf.keras.layers.multiply([x1, x2])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 4.], ...)>"
"tf.keras.layers.serialize(
    layer
)
","[['Serializes a ', 'Layer', ' object into a JSON-compatible representation.']]","from pprint import pprint
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(32, activation='relu'))

pprint(tf.keras.layers.serialize(model))
# prints the configuration of the model, as a dict.
"
"tf.keras.layers.subtract(
    inputs, **kwargs
)
","[['Functional interface to the ', 'Subtract', ' layer.']]","    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    subtracted = keras.layers.subtract([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
"
"tf.keras.losses.BinaryCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_crossentropy'
)
","[['Inherits From: ', 'Loss']]","model.compile(
  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
  ....
)
"
"tf.keras.losses.BinaryFocalCrossentropy(
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_focal_crossentropy'
)
","[['Inherits From: ', 'Loss']]","model.compile(
  loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True),
  ....
)
"
"tf.keras.losses.CategoricalCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='categorical_crossentropy'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy()
cce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.CategoricalHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='categorical_hinge'
)
","[['Computes the categorical hinge loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
# Using 'auto'/'sum_over_batch_size' reduction type.
h = tf.keras.losses.CategoricalHinge()
h(y_true, y_pred).numpy()
1.4"
"tf.keras.losses.CosineSimilarity(
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='cosine_similarity'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)
# l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]
# l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]
# l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]
# loss = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))
#       = -((0. + 0.) +  (0.5 + 0.5)) / 2
cosine_loss(y_true, y_pred).numpy()
-0.5"
"tf.keras.losses.Hinge(
    reduction=losses_utils.ReductionV2.AUTO, name='hinge'
)
","[['Computes the hinge loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
# Using 'auto'/'sum_over_batch_size' reduction type.
h = tf.keras.losses.Hinge()
h(y_true, y_pred).numpy()
1.3"
"tf.keras.losses.Huber(
    delta=1.0,
    reduction=losses_utils.ReductionV2.AUTO,
    name='huber_loss'
)
","[['Computes the Huber loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","loss = 0.5 * x^2                  if |x| <= d
loss = 0.5 * d^2 + d * (|x| - d)  if |x| > d
"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.losses.KLDivergence(
    reduction=losses_utils.ReductionV2.AUTO, name='kl_divergence'
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
# Using 'auto'/'sum_over_batch_size' reduction type.
kl = tf.keras.losses.KLDivergence()
kl(y_true, y_pred).numpy()
0.458"
"tf.keras.losses.LogCosh(
    reduction=losses_utils.ReductionV2.AUTO, name='log_cosh'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
l = tf.keras.losses.LogCosh()
l(y_true, y_pred).numpy()
0.108"
"tf.keras.losses.Loss(
    reduction=losses_utils.ReductionV2.AUTO, name=None
)
",[],"class MeanSquaredError(Loss):

  def call(self, y_true, y_pred):
    return tf.reduce_mean(tf.math.square(y_pred - y_true), axis=-1)
"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.losses.MeanAbsoluteError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_error'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
mae = tf.keras.losses.MeanAbsoluteError()
mae(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanAbsolutePercentageError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_percentage_error'
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[2., 1.], [2., 3.]]
y_pred = [[1., 1.], [1., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
mape = tf.keras.losses.MeanAbsolutePercentageError()
mape(y_true, y_pred).numpy()
50."
"tf.keras.losses.MeanSquaredError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_error'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
mse = tf.keras.losses.MeanSquaredError()
mse(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_logarithmic_error'
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
msle = tf.keras.losses.MeanSquaredLogarithmicError()
msle(y_true, y_pred).numpy()
0.240"
"tf.keras.losses.Poisson(
    reduction=losses_utils.ReductionV2.AUTO, name='poisson'
)
","[['Computes the Poisson loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
p = tf.keras.losses.Poisson()
p(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=False,
    ignore_class=None,
    reduction=losses_utils.ReductionV2.AUTO,
    name='sparse_categorical_crossentropy'
)
","[['Inherits From: ', 'Loss']]","y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
scce = tf.keras.losses.SparseCategoricalCrossentropy()
scce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.SquaredHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='squared_hinge'
)
","[['Computes the squared hinge loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
# Using 'auto'/'sum_over_batch_size' reduction type.
h = tf.keras.losses.SquaredHinge()
h(y_true, y_pred).numpy()
1.86"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
",[],"y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
",[],"y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
",[],"y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.losses.categorical_hinge(
    y_true, y_pred
)
","[['Computes the categorical hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 3, size=(2,))
y_true = tf.keras.utils.to_categorical(y_true, num_classes=3)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.categorical_hinge(y_true, y_pred)
assert loss.shape == (2,)
pos = np.sum(y_true * y_pred, axis=-1)
neg = np.amax((1. - y_true) * y_pred, axis=-1)
assert np.array_equal(loss.numpy(), np.maximum(0., neg - pos + 1.))"
"tf.keras.losses.cosine_similarity(
    y_true, y_pred, axis=-1
)
",[],"y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)
loss.numpy()
array([-0., -0.999, 0.999], dtype=float32)"
"tf.keras.losses.deserialize(
    name, custom_objects=None
)
",[],[]
"tf.keras.losses.get(
    identifier
)
","[['Retrieves a Keras loss as a ', 'function', '/', 'Loss', ' class instance.']]","loss = tf.keras.losses.get(""categorical_crossentropy"")
type(loss)
<class 'function'>
loss = tf.keras.losses.get(""CategoricalCrossentropy"")
type(loss)
<class '...keras.losses.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","[['Computes the hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.losses.huber(
    y_true, y_pred, delta=1.0
)
",[],"loss = 0.5 * x^2                  if |x| <= d
loss = d * |x| - 0.5 * d^2        if |x| > d
"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.losses.log_cosh(
    y_true, y_pred
)
",[],"y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.logcosh(y_true, y_pred)
assert loss.shape == (2,)
x = y_pred - y_true
assert np.allclose(
    loss.numpy(),
    np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),
            axis=-1),
    atol=1e-5)"
"tf.keras.losses.log_cosh(
    y_true, y_pred
)
",[],"y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.logcosh(y_true, y_pred)
assert loss.shape == (2,)
x = y_pred - y_true
assert np.allclose(
    loss.numpy(),
    np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),
            axis=-1),
    atol=1e-5)"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.losses.serialize(
    loss
)
","[['Serializes loss function or ', 'Loss', ' instance.']]",[]
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
",[],"y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","[['Computes the squared hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.keras.metrics.AUC(
    num_thresholds=200,
    curve='ROC',
    summation_method='interpolation',
    name=None,
    dtype=None,
    thresholds=None,
    multi_label=False,
    num_labels=None,
    label_weights=None,
    from_logits=False
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.AUC(num_thresholds=3)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
# threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]
# tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]
# tp_rate = recall = [1, 0.5, 0], fp_rate = [1, 0, 0]
# auc = ((((1+0.5)/2)*(1-0)) + (((0.5+0)/2)*(0-0))) = 0.75
m.result().numpy()
0.75"
"tf.keras.metrics.Accuracy(
    name='accuracy', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Accuracy()
m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryAccuracy(
    name='binary_accuracy', dtype=None, threshold=0.5
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.BinaryAccuracy()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryCrossentropy(
    name='binary_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.BinaryCrossentropy()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.81492424"
"tf.keras.metrics.BinaryIoU(
    target_class_ids: Union[List[int], Tuple[int, ...]] = (0, 1),
    threshold=0.5,
    name=None,
    dtype=None
)
","[['Inherits From: ', 'IoU', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.CategoricalAccuracy(
    name='categorical_accuracy', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.CategoricalAccuracy()
m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
                [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.CategoricalCrossentropy(
    name='categorical_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0,
    axis=-1
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","# EPSILON = 1e-7, y = y_true, y` = y_pred
# y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)
# y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]
# xent = -sum(y * log(y'), axis = -1)
#      = -((log 0.95), (log 0.1))
#      = [0.051, 2.302]
# Reduced xent = (0.051 + 2.302) / 2
m = tf.keras.metrics.CategoricalCrossentropy()
m.update_state([[0, 1, 0], [0, 0, 1]],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.CategoricalHinge(
    name='categorical_hinge', dtype=None
)
","[['Computes the categorical hinge metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.CategoricalHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.4000001"
"tf.keras.metrics.CosineSimilarity(
    name='cosine_similarity', dtype=None, axis=-1
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","# l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]
# l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]
# l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]
# result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))
#        = ((0. + 0.) +  (0.5 + 0.5)) / 2
m = tf.keras.metrics.CosineSimilarity(axis=1)
m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.FalseNegatives(
    thresholds=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.FalseNegatives()
m.update_state([0, 1, 1, 1], [0, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.FalsePositives(
    thresholds=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.FalsePositives()
m.update_state([0, 1, 0, 0], [0, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.Hinge(
    name='hinge', dtype=None
)
","[['Computes the hinge metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Hinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.3"
"tf.keras.metrics.IoU(
    num_classes: int,
    target_class_ids: Union[List[int], Tuple[int, ...]],
    name: Optional[str] = None,
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    ignore_class: Optional[int] = None,
    sparse_y_true: bool = True,
    sparse_y_pred: bool = True,
    axis: int = -1
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.KLDivergence(
    name='kullback_leibler_divergence', dtype=None
)
","[['Computes Kullback-Leibler divergence metric between ', 'y_true', ' and'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.KLDivergence()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.45814306"
"tf.keras.metrics.LogCoshError(
    name='logcosh', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.LogCoshError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.10844523"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.Mean(
    name='mean', dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Mean()
m.update_state([1, 3, 5, 7])
m.result().numpy()
4.0
m.reset_state()
m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.MeanAbsoluteError(
    name='mean_absolute_error', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanAbsoluteError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanAbsolutePercentageError(
    name='mean_absolute_percentage_error', dtype=None
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' and'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanAbsolutePercentageError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
250000000.0"
"tf.keras.metrics.MeanIoU(
    num_classes: int,
    name: Optional[str] = None,
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    ignore_class: Optional[int] = None,
    sparse_y_true: bool = True,
    sparse_y_pred: bool = True,
    axis: int = -1
)
","[['Inherits From: ', 'IoU', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.MeanMetricWrapper(
    fn, name=None, dtype=None, **kwargs
)
","[['Inherits From: ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","def accuracy(y_true, y_pred):
  return tf.cast(tf.math.equal(y_true, y_pred), tf.float32)

accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=accuracy)

keras_model.compile(..., metrics=accuracy_metric)
"
"tf.keras.metrics.MeanRelativeError(
    normalizer, name=None, dtype=None
)
","[['Inherits From: ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3])
m.update_state([1, 3, 2, 3], [2, 4, 6, 8])"
"tf.keras.metrics.MeanSquaredError(
    name='mean_squared_error', dtype=None
)
","[['Computes the mean squared error between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanSquaredLogarithmicError(
    name='mean_squared_logarithmic_error', dtype=None
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' and'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanSquaredLogarithmicError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.12011322"
"tf.keras.metrics.MeanTensor(
    name='mean_tensor', dtype=None, shape=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanTensor()
m.update_state([0, 1, 2, 3])
m.update_state([4, 5, 6, 7])
m.result().numpy()
array([2., 3., 4., 5.], dtype=float32)"
"tf.keras.metrics.Metric(
    name=None, dtype=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","m = SomeMetric(...)
for input in ...:
  m.update_state(input)
print('Final result: ', m.result().numpy())
"
"tf.keras.metrics.OneHotIoU(
    num_classes: int,
    target_class_ids: Union[List[int], Tuple[int, ...]],
    name=None,
    dtype=None,
    ignore_class: Optional[int] = None,
    sparse_y_pred: bool = False,
    axis: int = -1
)
","[['Inherits From: ', 'IoU', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.OneHotMeanIoU(
    num_classes: int,
    name: str = None,
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    ignore_class: Optional[int] = None,
    sparse_y_pred: bool = False,
    axis: int = -1
)
","[['Inherits From: ', 'MeanIoU', ', ', 'IoU', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.Poisson(
    name='poisson', dtype=None
)
","[['Computes the Poisson metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Poisson()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.Precision(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Precision()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.PrecisionAtRecall(
    recall, num_thresholds=200, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.PrecisionAtRecall(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.Recall(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Recall()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.RecallAtPrecision(
    precision, num_thresholds=200, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.RecallAtPrecision(0.8)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
m.result().numpy()
0.5"
"tf.keras.metrics.RootMeanSquaredError(
    name='root_mean_squared_error', dtype=None
)
","[['Computes root mean squared error metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.RootMeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SensitivityAtSpecificity(
    specificity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.SensitivityAtSpecificity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.SparseCategoricalAccuracy(
    name='sparse_categorical_accuracy', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","acc = np.dot(sample_weight, np.equal(y_true, np.argmax(y_pred, axis=1))
"
"tf.keras.metrics.SparseCategoricalCrossentropy(
    name: str = 'sparse_categorical_crossentropy',
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    from_logits: bool = False,
    ignore_class: Optional[int] = None,
    axis: int = -1
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","# y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]]
# logits = log(y_pred)
# softmax = exp(logits) / sum(exp(logits), axis=-1)
# softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]
# xent = -sum(y * log(softmax), 1)
# log(softmax) = [[-2.9957, -0.0513, -16.1181],
#                [-2.3026, -0.2231, -2.3026]]
# y_true * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]
# xent = [0.0513, 2.3026]
# Reduced xent = (0.0513 + 2.3026) / 2
m = tf.keras.metrics.SparseCategoricalCrossentropy()
m.update_state([1, 2],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.SparseTopKCategoricalAccuracy(
    k=5, name='sparse_top_k_categorical_accuracy', dtype=None
)
","[['Computes how often integer targets are in the top ', 'K', ' predictions.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)
m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SpecificityAtSensitivity(
    sensitivity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.SpecificityAtSensitivity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.66666667"
"tf.keras.metrics.SquaredHinge(
    name='squared_hinge', dtype=None
)
","[['Computes the squared hinge metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.SquaredHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.86"
"tf.keras.metrics.Sum(
    name='sum', dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Sum()
m.update_state([1, 3, 5, 7])
m.result().numpy()
16.0"
"tf.keras.metrics.TopKCategoricalAccuracy(
    k=5, name='top_k_categorical_accuracy', dtype=None
)
","[['Computes how often targets are in the top ', 'K', ' predictions.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.TopKCategoricalAccuracy(k=1)
m.update_state([[0, 0, 1], [0, 1, 0]],
               [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.TrueNegatives(
    thresholds=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.TrueNegatives()
m.update_state([0, 1, 0, 0], [1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.TruePositives(
    thresholds=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.TruePositives()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.binary_accuracy(
    y_true, y_pred, threshold=0.5
)
",[],"y_true = [[1], [1], [0], [0]]
y_pred = [[1], [1], [0], [0]]
m = tf.keras.metrics.binary_accuracy(y_true, y_pred)
assert m.shape == (4,)
m.numpy()
array([1., 1., 1., 1.], dtype=float32)"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
",[],"y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
",[],"y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_accuracy(
    y_true, y_pred
)
",[],"y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
",[],"y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.deserialize(
    config, custom_objects=None
)
",[],[]
"tf.keras.metrics.get(
    identifier
)
","[['Retrieves a Keras metric as a ', 'function', '/', 'Metric', ' class instance.']]","metric = tf.keras.metrics.get(""categorical_crossentropy"")
type(metric)
<class 'function'>
metric = tf.keras.metrics.get(""CategoricalCrossentropy"")
type(metric)
<class '...metrics.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","[['Computes the hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.losses.log_cosh(
    y_true, y_pred
)
",[],"y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.logcosh(y_true, y_pred)
assert loss.shape == (2,)
x = y_pred - y_true
assert np.allclose(
    loss.numpy(),
    np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),
            axis=-1),
    atol=1e-5)"
"tf.keras.losses.log_cosh(
    y_true, y_pred
)
",[],"y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.logcosh(y_true, y_pred)
assert loss.shape == (2,)
x = y_pred - y_true
assert np.allclose(
    loss.numpy(),
    np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),
            axis=-1),
    atol=1e-5)"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.metrics.serialize(
    metric
)
","[['Serializes metric function or ', 'Metric', ' instance.']]",[]
"tf.keras.metrics.sparse_categorical_accuracy(
    y_true, y_pred
)
",[],"y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
",[],"y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","[['Computes how often integer targets are in the top ', 'K', ' predictions.']]","y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","[['Computes the squared hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.keras.metrics.top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","[['Computes how often targets are in the top ', 'K', ' predictions.']]","y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.keras.mixed_precision.Policy(
    name
)
",[],"tf.keras.mixed_precision.set_global_policy('mixed_float16')
layer1 = tf.keras.layers.Dense(10)
layer1.dtype_policy  # `layer1` will automatically use mixed precision
<Policy ""mixed_float16"">
# Can optionally override layer to use float32
# instead of mixed precision.
layer2 = tf.keras.layers.Dense(10, dtype='float32')
layer2.dtype_policy
<Policy ""float32"">
# Set policy back to initial float32 for future examples.
tf.keras.mixed_precision.set_global_policy('float32')"
"tf.keras.mixed_precision.set_global_policy(
    policy
)
",[],"tf.keras.mixed_precision.set_global_policy('mixed_float16')
tf.keras.mixed_precision.global_policy()
<Policy ""mixed_float16"">
tf.keras.layers.Dense(10).dtype_policy
<Policy ""mixed_float16"">
# Global policy is not used if a policy
# is directly passed to constructor
tf.keras.layers.Dense(10, dtype='float64').dtype_policy
<Policy ""float64"">
tf.keras.mixed_precision.set_global_policy('float32')"
"tf.keras.Model(
    *args, **kwargs
)
","[['Model', ' groups layers into an object with training and inference features.'], ['Inherits From: ', 'Layer', ', ', 'Module']]","import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
"
"tf.keras.Sequential(
    layers=None, name=None
)
","[['Sequential', ' groups a linear stack of layers into a ', 'tf.keras.Model', '.'], ['Inherits From: ', 'Model', ', ', 'Layer', ', ', 'Module']]","# Optionally, the first layer can receive an `input_shape` argument:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
# Afterwards, we do automatic shape inference:
model.add(tf.keras.layers.Dense(4))

# This is identical to the following:
model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(8))

# Note that you can also omit the `input_shape` argument.
# In that case the model doesn't have any weights until the first call
# to a training/evaluation method (since it isn't yet built):
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
# model.weights not created yet

# Whereas if you specify the input shape, the model gets built
# continuously as you are adding layers:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))
len(model.weights)
# Returns ""4""

# When using the delayed-build pattern (no input shape specified), you can
# choose to manually build your model by calling
# `build(batch_input_shape)`:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
model.build((None, 16))
len(model.weights)
# Returns ""4""

# Note that when using the delayed-build pattern (no input shape specified),
# the model gets built the first time you call `fit`, `eval`, or `predict`,
# or the first time you call the model on some input data.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer='sgd', loss='mse')
# This builds the model for the first time:
model.fit(x, y, batch_size=32, epochs=10)
"
"tf.keras.models.clone_model(
    model, input_tensors=None, clone_function=None
)
","[['Clone a Functional or Sequential ', 'Model', ' instance.']]","# Create a test Sequential model.
model = keras.Sequential([
    keras.Input(shape=(728,)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid'),
])
# Create a copy of the test model (with freshly initialized weights).
new_model = clone_model(model)
"
"tf.keras.models.experimental.SharpnessAwareMinimization(
    model, rho=0.05, num_batch_splits=None, name=None
)
","[['Inherits From: ', 'Model', ', ', 'Layer', ', ', 'Module']]","inputs = tf.keras.layers.Input(shape=(3,))
outputs = tf.keras.layers.Dense(2)(inputs)
model = tf.keras.models.Model(inputs=inputs, outputs=outputs)
model.compile(optimizer=""Adam"", loss=""mse"", metrics=[""mae""])
model.metrics_names
[]"
"tf.keras.models.load_model(
    filepath, custom_objects=None, compile=True, options=None
)
","[['Loads a model saved via ', 'model.save()', '.']]","model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
model.save('/tmp/model')
loaded_model = tf.keras.models.load_model('/tmp/model')
x = tf.random.uniform((10, 3))
assert np.allclose(model.predict(x), loaded_model.predict(x))"
"tf.keras.models.model_from_config(
    config, custom_objects=None
)
",[],"# for a Functional API model
tf.keras.Model().from_config(model.get_config())

# for a Sequential model
tf.keras.Sequential().from_config(model.get_config())
"
"tf.keras.models.model_from_json(
    json_string, custom_objects=None
)
",[],"model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
config = model.to_json()
loaded_model = tf.keras.models.model_from_json(config)"
"tf.keras.models.model_from_yaml(
    yaml_string, custom_objects=None
)
",[],[]
"tf.keras.models.save_model(
    model,
    filepath,
    overwrite=True,
    include_optimizer=True,
    save_format=None,
    signatures=None,
    options=None,
    save_traces=True
)
",[],"model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
model.save('/tmp/model')
loaded_model = tf.keras.models.load_model('/tmp/model')
x = tf.random.uniform((10, 3))
assert np.allclose(model.predict(x), loaded_model.predict(x))"
"tf.keras.optimizers.experimental.Adadelta(
    learning_rate=0.001,
    rho=0.95,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adadelta',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.experimental.Adagrad(
    learning_rate=0.001,
    initial_accumulator_value=0.1,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adagrad',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.experimental.Adamax(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adamax',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","m = 0  # Initialize initial 1st moment vector
u = 0  # Initialize the exponentially weighted infinity norm
t = 0  # Initialize timestep
"
"tf.keras.optimizers.experimental.Ftrl(
    learning_rate=0.001,
    learning_rate_power=-0.5,
    initial_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    l2_shrinkage_regularization_strength=0.0,
    beta=0.0,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Ftrl',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","n = 0
sigma = 0
z = 0
"
"tf.keras.optimizers.experimental.Nadam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Nadam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.Optimizer(
    name,
    weight_decay=0,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    **kwargs
)
",[],"# Create an optimizer with the desired parameters.
opt = tf.keras.optimizers.experimental.SGD(learning_rate=0.1)
var1, var2 = tf.Variable(1.0), tf.Variable(2.0)
# `loss` is a callable that takes no argument and returns the value
# to minimize.
loss = lambda: 3 * var1 * var1 + 2 * var2 * var2
# Call minimize to update the list of variables.
opt.minimize(loss, var_list=[var1, var2])
"
"tf.keras.optimizers.experimental.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=100,
    jit_compile=True,
    name='RMSprop',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2) / 2.0  # d(loss) / d(var1) = var1
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.optimizers.experimental.SGD(
    learning_rate=0.01,
    momentum=0.0,
    nesterov=False,
    amsgrad=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='SGD',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","w = w - learning_rate * g
"
"tf.keras.optimizers.deserialize(
    config, custom_objects=None, **kwargs
)
","[['Inverse of the ', 'serialize', ' function.']]",[]
"tf.keras.optimizers.experimental.Adadelta(
    learning_rate=0.001,
    rho=0.95,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adadelta',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.experimental.Adafactor(
    learning_rate=0.001,
    beta_2_decay=-0.8,
    epsilon_1=1e-30,
    epsilon_2=0.001,
    clip_threshold=1.0,
    relative_step=True,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adafactor',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.experimental.Adagrad(
    learning_rate=0.001,
    initial_accumulator_value=0.1,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adagrad',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.experimental.AdamW(
    learning_rate=0.001,
    weight_decay=0.004,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='AdamW',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.experimental.Adamax(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adamax',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","m = 0  # Initialize initial 1st moment vector
u = 0  # Initialize the exponentially weighted infinity norm
t = 0  # Initialize timestep
"
"tf.keras.optimizers.experimental.Ftrl(
    learning_rate=0.001,
    learning_rate_power=-0.5,
    initial_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    l2_shrinkage_regularization_strength=0.0,
    beta=0.0,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Ftrl',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","n = 0
sigma = 0
z = 0
"
"tf.keras.optimizers.experimental.Nadam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Nadam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.Optimizer(
    name,
    weight_decay=0,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    **kwargs
)
",[],"# Create an optimizer with the desired parameters.
opt = tf.keras.optimizers.experimental.SGD(learning_rate=0.1)
var1, var2 = tf.Variable(1.0), tf.Variable(2.0)
# `loss` is a callable that takes no argument and returns the value
# to minimize.
loss = lambda: 3 * var1 * var1 + 2 * var2 * var2
# Call minimize to update the list of variables.
opt.minimize(loss, var_list=[var1, var2])
"
"tf.keras.optimizers.experimental.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=100,
    jit_compile=True,
    name='RMSprop',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2) / 2.0  # d(loss) / d(var1) = var1
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.optimizers.experimental.SGD(
    learning_rate=0.01,
    momentum=0.0,
    nesterov=False,
    amsgrad=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='SGD',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","w = w - learning_rate * g
"
"tf.keras.optimizers.get(
    identifier, **kwargs
)
",[],[]
"tf.keras.optimizers.legacy.Adadelta(
    learning_rate=0.001,
    rho=0.95,
    epsilon=1e-07,
    name='Adadelta',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_slot(
    var, slot_name, initializer='zeros', shape=None
)
"
"tf.keras.optimizers.legacy.Adagrad(
    learning_rate=0.001,
    initial_accumulator_value=0.1,
    epsilon=1e-07,
    name='Adagrad',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_slot(
    var, slot_name, initializer='zeros', shape=None
)
"
"tf.keras.optimizers.legacy.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    name='Adam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1
step_count = opt.minimize(loss, [var1]).numpy()
# The first step is `-learning_rate*sign(grad)`
var1.numpy()
9.9"
"tf.keras.optimizers.legacy.Adamax(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    name='Adamax',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","m = 0  # Initialize initial 1st moment vector
v = 0  # Initialize the exponentially weighted infinity norm
t = 0  # Initialize timestep
"
"tf.keras.optimizers.legacy.Ftrl(
    learning_rate=0.001,
    learning_rate_power=-0.5,
    initial_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    name='Ftrl',
    l2_shrinkage_regularization_strength=0.0,
    beta=0.0,
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","n = 0
sigma = 0
z = 0
"
"tf.keras.optimizers.legacy.Nadam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    name='Nadam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]",">>> opt = tf.keras.optimizers.legacy.Nadam(learning_rate=0.2)
>>> var1 = tf.Variable(10.0)
>>> loss = lambda: (var1 ** 2) / 2.0
>>> step_count = opt.minimize(loss, [var1]).numpy()
>>> ""{:.1f}"".format(var1.numpy())
9.8
"
"tf.keras.optimizers.legacy.Optimizer(
    name, gradient_aggregator=None, gradient_transformers=None, **kwargs
)
",[],"# Create an optimizer with the desired parameters.
opt = tf.keras.optimizers.legacy.SGD(learning_rate=0.1)
# `loss` is a callable that takes no argument and returns the value
# to minimize.
var1 = tf.Variable(2.0)
var2 = tf.Variable(5.0)
loss = lambda: 3 * var1 * var1 + 2 * var2 * var2
# In graph mode, returns op that minimizes the loss by updating the listed
# variables.
opt_op = opt.minimize(loss, var_list=[var1, var2])
opt_op.run()
# In eager mode, simply call minimize to update the list of variables.
opt.minimize(loss, var_list=[var1, var2])
"
"tf.keras.optimizers.legacy.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    name='RMSprop',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2) / 2.0    # d(loss) / d(var1) = var1
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
9.683772"
"tf.keras.optimizers.legacy.SGD(
    learning_rate=0.01,
    momentum=0.0,
    nesterov=False,
    name='SGD',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","w = w - learning_rate * g
"
"tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate, decay_steps, alpha=0.0, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  step = min(step, decay_steps)
  cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))
  decayed = (1 - alpha) * cosine_decay + alpha
  return initial_learning_rate * decayed
"
"tf.keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate,
    first_decay_steps,
    t_mul=2.0,
    m_mul=1.0,
    alpha=0.0,
    name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","first_decay_steps = 1000
lr_decayed_fn = (
  tf.keras.optimizers.schedules.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps))
"
"tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  return initial_learning_rate * decay_rate ^ (step / decay_steps)
"
"tf.keras.optimizers.schedules.InverseTimeDecay(
    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  return initial_learning_rate / (1 + decay_rate * step / decay_step)
"
"tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","step = tf.Variable(0, trainable=False)
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values)

# Later, whenever we perform an optimization step, we pass in the step.
learning_rate = learning_rate_fn(step)
"
"tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate,
    decay_steps,
    end_learning_rate=0.0001,
    power=1.0,
    cycle=False,
    name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  step = min(step, decay_steps)
  return ((initial_learning_rate - end_learning_rate) *
          (1 - step / decay_steps) ^ (power)
         ) + end_learning_rate
"
"tf.keras.optimizers.schedules.deserialize(
    config, custom_objects=None
)
","[['Instantiates a ', 'LearningRateSchedule', ' object from a serialized form.']]","# Configuration for PolynomialDecay
config = {
  'class_name': 'PolynomialDecay',
  'config': {'cycle': False,
    'decay_steps': 10000,
    'end_learning_rate': 0.01,
    'initial_learning_rate': 0.1,
    'name': None,
    'power': 0.5} }
lr_schedule = tf.keras.optimizers.schedules.deserialize(config)
"
"tf.keras.optimizers.schedules.serialize(
    learning_rate_schedule
)
","[['Serializes a ', 'LearningRateSchedule', ' into a JSON-compatible representation.']]","lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
  0.1, decay_steps=100000, decay_rate=0.96, staircase=True)
tf.keras.optimizers.schedules.serialize(lr_schedule)
{'class_name': 'ExponentialDecay', 'config': {...} }"
"tf.keras.optimizers.serialize(
    optimizer
)
",[],"tf.keras.optimizers.serialize(tf.keras.optimizers.legacy.SGD())
{'class_name': 'SGD', 'config': {'name': 'SGD', 'learning_rate': 0.01,
                                 'decay': 0.0, 'momentum': 0.0,
                                 'nesterov': False} }"
"tf.keras.preprocessing.image.DirectoryIterator(
    directory,
    image_data_generator,
    target_size=(256, 256),
    color_mode='rgb',
    classes=None,
    class_mode='categorical',
    batch_size=32,
    shuffle=True,
    seed=None,
    data_format=None,
    save_to_dir=None,
    save_prefix='',
    save_format='png',
    follow_links=False,
    subset=None,
    interpolation='nearest',
    keep_aspect_ratio=False,
    dtype=None
)
","[['Inherits From: ', 'Iterator', ', ', 'Sequence']]","next()
"
"tf.keras.preprocessing.image.ImageDataGenerator(
    featurewise_center=False,
    samplewise_center=False,
    featurewise_std_normalization=False,
    samplewise_std_normalization=False,
    zca_whitening=False,
    zca_epsilon=1e-06,
    rotation_range=0,
    width_shift_range=0.0,
    height_shift_range=0.0,
    brightness_range=None,
    shear_range=0.0,
    zoom_range=0.0,
    channel_shift_range=0.0,
    fill_mode='nearest',
    cval=0.0,
    horizontal_flip=False,
    vertical_flip=False,
    rescale=None,
    preprocessing_function=None,
    data_format=None,
    validation_split=0.0,
    interpolation_order=1,
    dtype=None
)
",[],"(x_train, y_train), (x_test, y_test) = cifar10.load_data()
y_train = utils.to_categorical(y_train, num_classes)
y_test = utils.to_categorical(y_test, num_classes)
datagen = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    validation_split=0.2)
# compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)
datagen.fit(x_train)
# fits the model on batches with real-time data augmentation:
model.fit(datagen.flow(x_train, y_train, batch_size=32,
         subset='training'),
         validation_data=datagen.flow(x_train, y_train,
         batch_size=8, subset='validation'),
         steps_per_epoch=len(x_train) / 32, epochs=epochs)
# here's a more ""manual"" example
for e in range(epochs):
    print('Epoch', e)
    batches = 0
    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):
        model.fit(x_batch, y_batch)
        batches += 1
        if batches >= len(x_train) / 32:
            # we need to break the loop by hand because
            # the generator loops indefinitely
            break
"
"tf.keras.preprocessing.image.Iterator(
    n, batch_size, shuffle, seed
)
","[['Inherits From: ', 'Sequence']]","next()
"
"tf.keras.preprocessing.image.NumpyArrayIterator(
    x,
    y,
    image_data_generator,
    batch_size=32,
    shuffle=False,
    sample_weight=None,
    seed=None,
    data_format=None,
    save_to_dir=None,
    save_prefix='',
    save_format='png',
    subset=None,
    ignore_class_split=False,
    dtype=None
)
","[['Inherits From: ', 'Iterator', ', ', 'Sequence']]","next()
"
"tf.keras.preprocessing.image.apply_affine_transform(
    x,
    theta=0,
    tx=0,
    ty=0,
    shear=0,
    zx=1,
    zy=1,
    row_axis=1,
    col_axis=2,
    channel_axis=0,
    fill_mode='nearest',
    cval=0.0,
    order=1
)
",[],[]
"tf.keras.preprocessing.image.apply_brightness_shift(
    x, brightness, scale=True
)
",[],[]
"tf.keras.preprocessing.image.apply_channel_shift(
    x, intensity, channel_axis=0
)
",[],[]
"tf.keras.utils.array_to_img(
    x, data_format=None, scale=True, dtype=None
)
",[],"from PIL import Image
img = np.random.random(size=(100, 100, 3))
pil_img = tf.keras.utils.array_to_img(img)
"
"tf.keras.utils.img_to_array(
    img, data_format=None, dtype=None
)
",[],"from PIL import Image
img_data = np.random.random(size=(100, 100, 3))
img = tf.keras.utils.array_to_img(img_data)
array = tf.keras.utils.image.img_to_array(img)
"
"tf.keras.utils.load_img(
    path,
    grayscale=False,
    color_mode='rgb',
    target_size=None,
    interpolation='nearest',
    keep_aspect_ratio=False
)
",[],"image = tf.keras.utils.load_img(image_path)
input_arr = tf.keras.utils.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)
"
"tf.keras.preprocessing.image.random_brightness(
    x, brightness_range, scale=True
)
",[],[]
"tf.keras.preprocessing.image.random_channel_shift(
    x, intensity_range, channel_axis=0
)
",[],[]
"tf.keras.preprocessing.image.random_rotation(
    x,
    rg,
    row_axis=1,
    col_axis=2,
    channel_axis=0,
    fill_mode='nearest',
    cval=0.0,
    interpolation_order=1
)
",[],[]
"tf.keras.preprocessing.image.random_shear(
    x,
    intensity,
    row_axis=1,
    col_axis=2,
    channel_axis=0,
    fill_mode='nearest',
    cval=0.0,
    interpolation_order=1
)
",[],[]
"tf.keras.preprocessing.image.random_shift(
    x,
    wrg,
    hrg,
    row_axis=1,
    col_axis=2,
    channel_axis=0,
    fill_mode='nearest',
    cval=0.0,
    interpolation_order=1
)
",[],[]
"tf.keras.preprocessing.image.random_zoom(
    x,
    zoom_range,
    row_axis=1,
    col_axis=2,
    channel_axis=0,
    fill_mode='nearest',
    cval=0.0,
    interpolation_order=1
)
",[],[]
"tf.keras.utils.save_img(
    path, x, data_format=None, file_format=None, scale=True, **kwargs
)
",[],[]
"tf.keras.preprocessing.image.smart_resize(
    x, size, interpolation='bilinear'
)
",[],"size = (200, 200)
ds = ds.map(lambda img: tf.image.resize(img, size))
"
"tf.keras.utils.image_dataset_from_directory(
    directory,
    labels='inferred',
    label_mode='int',
    class_names=None,
    color_mode='rgb',
    batch_size=32,
    image_size=(256, 256),
    shuffle=True,
    seed=None,
    validation_split=None,
    subset=None,
    interpolation='bilinear',
    follow_links=False,
    crop_to_aspect_ratio=False,
    **kwargs
)
","[['Generates a ', 'tf.data.Dataset', ' from image files in a directory.']]","main_directory/
...class_a/
......a_image_1.jpg
......a_image_2.jpg
...class_b/
......b_image_1.jpg
......b_image_2.jpg
"
"tf.keras.preprocessing.sequence.TimeseriesGenerator(
    data,
    targets,
    length,
    sampling_rate=1,
    stride=1,
    start_index=0,
    end_index=None,
    shuffle=False,
    reverse=False,
    batch_size=128
)
","[['Inherits From: ', 'Sequence']]","from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np
data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])
data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20
batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
"
"tf.keras.preprocessing.sequence.make_sampling_table(
    size, sampling_factor=1e-05
)
",[],"p(word) = (min(1, sqrt(word_frequency / sampling_factor) /
    (word_frequency / sampling_factor)))
"
"tf.keras.utils.pad_sequences(
    sequences,
    maxlen=None,
    dtype='int32',
    padding='pre',
    truncating='pre',
    value=0.0
)
",[],"sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)
array([[0, 0, 1],
       [0, 2, 3],
       [4, 5, 6]], dtype=int32)"
"tf.keras.preprocessing.sequence.skipgrams(
    sequence,
    vocabulary_size,
    window_size=4,
    negative_samples=1.0,
    shuffle=True,
    categorical=False,
    sampling_table=None,
    seed=None
)
",[],[]
"tf.keras.preprocessing.text.Tokenizer(
    num_words=None,
    filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
    lower=True,
    split=' ',
    char_level=False,
    oov_token=None,
    analyzer=None,
    **kwargs
)
","[[None, '\n']]","fit_on_sequences(
    sequences
)
"
"tf.keras.preprocessing.text.hashing_trick(
    text,
    n,
    hash_function=None,
    filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
    lower=True,
    split=' ',
    analyzer=None
)
","[[None, '\n']]",[]
"tf.keras.preprocessing.text.one_hot(
    input_text,
    n,
    filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
    lower=True,
    split=' ',
    analyzer=None
)
","[[None, '\n'], ['One-hot encodes a text into a list of word indexes of size ', 'n', '.']]",[]
"tf.keras.preprocessing.text.text_to_word_sequence(
    input_text,
    filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
    lower=True,
    split=' '
)
","[[None, '\n']]","sample_text = 'This is a sample sentence.'
tf.keras.preprocessing.text.text_to_word_sequence(sample_text)
['this', 'is', 'a', 'sample', 'sentence']"
"tf.keras.preprocessing.text.tokenizer_from_json(
    json_string
)
",[],[]
"tf.keras.utils.text_dataset_from_directory(
    directory,
    labels='inferred',
    label_mode='int',
    class_names=None,
    batch_size=32,
    max_length=None,
    shuffle=True,
    seed=None,
    validation_split=None,
    subset=None,
    follow_links=False
)
","[['Generates a ', 'tf.data.Dataset', ' from text files in a directory.']]","main_directory/
...class_a/
......a_text_1.txt
......a_text_2.txt
...class_b/
......b_text_1.txt
......b_text_2.txt
"
"tf.keras.utils.timeseries_dataset_from_array(
    data,
    targets,
    sequence_length,
    sequence_stride=1,
    sampling_rate=1,
    batch_size=128,
    shuffle=False,
    seed=None,
    start_index=None,
    end_index=None
)
",[],"First sequence:  [0  2  4  6  8 10 12 14 16 18]
Second sequence: [3  5  7  9 11 13 15 17 19 21]
Third sequence:  [6  8 10 12 14 16 18 20 22 24]
...
Last sequence:   [78 80 82 84 86 88 90 92 94 96]
"
"tf.keras.regularizers.L1(
    l1=0.01, **kwargs
)
","[['Inherits From: ', 'Regularizer']]","dense = tf.keras.layers.Dense(3, kernel_regularizer='l1')"
"tf.keras.regularizers.L1L2(
    l1=0.0, l2=0.0
)
","[['Inherits From: ', 'Regularizer']]","dense = tf.keras.layers.Dense(3, kernel_regularizer='l1_l2')"
"tf.keras.regularizers.L2(
    l2=0.01, **kwargs
)
","[['Inherits From: ', 'Regularizer']]","dense = tf.keras.layers.Dense(3, kernel_regularizer='l2')"
"tf.keras.regularizers.OrthogonalRegularizer(
    factor=0.01, mode='rows'
)
","[['Inherits From: ', 'Regularizer']]","regularizer = tf.keras.regularizers.OrthogonalRegularizer(factor=0.01)
layer = tf.keras.layers.Dense(units=4, kernel_regularizer=regularizer)"
"tf.keras.regularizers.deserialize(
    config, custom_objects=None
)
",[],[]
"tf.keras.regularizers.get(
    identifier
)
",[],[]
"tf.keras.regularizers.L1(
    l1=0.01, **kwargs
)
","[['Inherits From: ', 'Regularizer']]","dense = tf.keras.layers.Dense(3, kernel_regularizer='l1')"
"tf.keras.regularizers.l1_l2(
    l1=0.01, l2=0.01
)
",[],[]
"tf.keras.regularizers.L2(
    l2=0.01, **kwargs
)
","[['Inherits From: ', 'Regularizer']]","dense = tf.keras.layers.Dense(3, kernel_regularizer='l2')"
"tf.keras.regularizers.OrthogonalRegularizer(
    factor=0.01, mode='rows'
)
","[['Inherits From: ', 'Regularizer']]","regularizer = tf.keras.regularizers.OrthogonalRegularizer(factor=0.01)
layer = tf.keras.layers.Dense(units=4, kernel_regularizer=regularizer)"
"tf.keras.regularizers.serialize(
    regularizer
)
",[],[]
"tf.keras.utils.custom_object_scope(
    *args
)
",[],"layer = Dense(3, kernel_regularizer=my_regularizer)
# Config contains a reference to `my_regularizer`
config = layer.get_config()
...
# Later:
with custom_object_scope({'my_regularizer': my_regularizer}):
  layer = Dense.from_config(config)
"
"tf.keras.utils.GeneratorEnqueuer(
    generator, use_multiprocessing=False, random_seed=None
)
","[['Inherits From: ', 'SequenceEnqueuer']]","get()
"
"tf.keras.utils.OrderedEnqueuer(
    sequence, use_multiprocessing=False, shuffle=False
)
","[['Inherits From: ', 'SequenceEnqueuer']]","get()
"
"tf.keras.utils.Progbar(
    target,
    width=30,
    verbose=1,
    interval=0.05,
    stateful_metrics=None,
    unit_name='step'
)
",[],"add(
    n, values=None
)
"
"tf.keras.utils.SequenceEnqueuer(
    sequence, use_multiprocessing=False
)
",[],"    enqueuer = SequenceEnqueuer(...)
    enqueuer.start()
    datas = enqueuer.get()
    for data in datas:
        # Use the inputs; training, evaluating, predicting.
        # ... stop sometime.
    enqueuer.stop()
"
"tf.keras.utils.SidecarEvaluator(
    model,
    data,
    checkpoint_dir,
    steps=None,
    max_evaluations=None,
    callbacks=None
)
","[[None, '\n']]","model = tf.keras.models.Sequential(...)
model.compile(metrics=tf.keras.metrics.SparseCategoricalAccuracy(
    name=""eval_metrics""))
data = tf.data.Dataset.from_tensor_slices(...)

tf.keras.SidecarEvaluator(
    model=model,
    data=data,
    # dir for training-saved checkpoint
    checkpoint_dir='/tmp/checkpoint_dir',
    steps=None,  # Eval until dataset is exhausted
    max_evaluations=None,  # The evaluation needs to be stopped manually
    callbacks=[tf.keras.callbacks.TensorBoard(log_dir='/tmp/log_dir')]
).start()
"
"tf.keras.utils.array_to_img(
    x, data_format=None, scale=True, dtype=None
)
",[],"from PIL import Image
img = np.random.random(size=(100, 100, 3))
pil_img = tf.keras.utils.array_to_img(img)
"
"tf.keras.utils.audio_dataset_from_directory(
    directory,
    labels='inferred',
    label_mode='int',
    class_names=None,
    batch_size=32,
    sampling_rate=None,
    output_sequence_length=None,
    ragged=False,
    shuffle=True,
    seed=None,
    validation_split=None,
    subset=None,
    follow_links=False
)
","[['Generates a ', 'tf.data.Dataset', ' from audio files in a directory.']]","main_directory/
...class_a/
......a_audio_1.wav
......a_audio_2.wav
...class_b/
......b_audio_1.wav
......b_audio_2.wav
"
"tf.keras.utils.custom_object_scope(
    *args
)
",[],"layer = Dense(3, kernel_regularizer=my_regularizer)
# Config contains a reference to `my_regularizer`
config = layer.get_config()
...
# Later:
with custom_object_scope({'my_regularizer': my_regularizer}):
  layer = Dense.from_config(config)
"
"tf.keras.utils.deserialize_keras_object(
    identifier,
    module_objects=None,
    custom_objects=None,
    printable_module_name='object'
)
",[],"def deserialize(config, custom_objects=None):
   return deserialize_keras_object(
     identifier,
     module_objects=globals(),
     custom_objects=custom_objects,
     name=""MyObjectType"",
   )
"
"tf.keras.utils.experimental.DatasetCreator(
    dataset_fn, input_options=None
)
","[['Object that returns a ', 'tf.data.Dataset', ' upon invoking.']]","model = tf.keras.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss=""mse"")

def dataset_fn(input_context):
  global_batch_size = 64
  batch_size = input_context.get_per_replica_batch_size(global_batch_size)
  dataset = tf.data.Dataset.from_tensors(([1.], [1.])).repeat()
  dataset = dataset.shard(
      input_context.num_input_pipelines, input_context.input_pipeline_id)
  dataset = dataset.batch(batch_size)
  dataset = dataset.prefetch(2)
  return dataset

input_options = tf.distribute.InputOptions(
    experimental_fetch_to_device=True,
    experimental_per_replica_buffer_size=2)
model.fit(tf.keras.utils.experimental.DatasetCreator(
    dataset_fn, input_options=input_options), epochs=10, steps_per_epoch=10)
"
"tf.keras.utils.get_file(
    fname=None,
    origin=None,
    untar=False,
    md5_hash=None,
    file_hash=None,
    cache_subdir='datasets',
    hash_algorithm='auto',
    extract=False,
    archive_format='auto',
    cache_dir=None
)
",[],"path_to_downloaded_file = tf.keras.utils.get_file(
    ""flower_photos"",
    ""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"",
    untar=True)
"
"tf.keras.utils.get_registered_name(
    obj
)
",[],[]
"tf.keras.utils.get_registered_object(
    name, custom_objects=None, module_objects=None
)
","[['Returns the class associated with ', 'name', ' if it is registered with Keras.']]","def from_config(cls, config, custom_objects=None):
  if 'my_custom_object_name' in config:
    config['hidden_cls'] = tf.keras.utils.get_registered_object(
        config['my_custom_object_name'], custom_objects=custom_objects)
"
"tf.keras.utils.get_source_inputs(
    tensor, layer=None, node_index=None
)
","[['Returns the list of input tensors necessary to compute ', 'tensor', '.']]",[]
"tf.keras.utils.image_dataset_from_directory(
    directory,
    labels='inferred',
    label_mode='int',
    class_names=None,
    color_mode='rgb',
    batch_size=32,
    image_size=(256, 256),
    shuffle=True,
    seed=None,
    validation_split=None,
    subset=None,
    interpolation='bilinear',
    follow_links=False,
    crop_to_aspect_ratio=False,
    **kwargs
)
","[['Generates a ', 'tf.data.Dataset', ' from image files in a directory.']]","main_directory/
...class_a/
......a_image_1.jpg
......a_image_2.jpg
...class_b/
......b_image_1.jpg
......b_image_2.jpg
"
"tf.keras.utils.img_to_array(
    img, data_format=None, dtype=None
)
",[],"from PIL import Image
img_data = np.random.random(size=(100, 100, 3))
img = tf.keras.utils.array_to_img(img_data)
array = tf.keras.utils.image.img_to_array(img)
"
"tf.keras.utils.load_img(
    path,
    grayscale=False,
    color_mode='rgb',
    target_size=None,
    interpolation='nearest',
    keep_aspect_ratio=False
)
",[],"image = tf.keras.utils.load_img(image_path)
input_arr = tf.keras.utils.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)
"
"tf.keras.utils.model_to_dot(
    model,
    show_shapes=False,
    show_dtype=False,
    show_layer_names=True,
    rankdir='TB',
    expand_nested=False,
    dpi=96,
    subgraph=False,
    layer_range=None,
    show_layer_activations=False
)
",[],[]
"tf.keras.utils.normalize(
    x, axis=-1, order=2
)
",[],[]
"tf.keras.utils.pack_x_y_sample_weight(
    x, y=None, sample_weight=None
)
",[],"x = tf.ones((10, 1))
data = tf.keras.utils.pack_x_y_sample_weight(x)
isinstance(data, tf.Tensor)
True
y = tf.ones((10, 1))
data = tf.keras.utils.pack_x_y_sample_weight(x, y)
isinstance(data, tuple)
True
x, y = data"
"tf.keras.utils.pad_sequences(
    sequences,
    maxlen=None,
    dtype='int32',
    padding='pre',
    truncating='pre',
    value=0.0
)
",[],"sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)
array([[0, 0, 1],
       [0, 2, 3],
       [4, 5, 6]], dtype=int32)"
"tf.keras.utils.plot_model(
    model,
    to_file='model.png',
    show_shapes=False,
    show_dtype=False,
    show_layer_names=True,
    rankdir='TB',
    expand_nested=False,
    dpi=96,
    layer_range=None,
    show_layer_activations=False
)
",[],"input = tf.keras.Input(shape=(100,), dtype='int32', name='input')
x = tf.keras.layers.Embedding(
    output_dim=512, input_dim=10000, input_length=100)(input)
x = tf.keras.layers.LSTM(32)(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)
model = tf.keras.Model(inputs=[input], outputs=[output])
dot_img_file = '/tmp/model_1.png'
tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)
"
"tf.keras.utils.register_keras_serializable(
    package='Custom', name=None
)
",[],"# Note that `'my_package'` is used as the `package` argument here, and since
# the `name` argument is not provided, `'MyDense'` is used as the `name`.
@keras.utils.register_keras_serializable('my_package')
class MyDense(keras.layers.Dense):
  pass

assert keras.utils.get_registered_object('my_package>MyDense') == MyDense
assert keras.utils.get_registered_name(MyDense) == 'my_package>MyDense'
"
"tf.keras.utils.save_img(
    path, x, data_format=None, file_format=None, scale=True, **kwargs
)
",[],[]
"tf.keras.utils.serialize_keras_object(
    instance
)
",[],[]
"tf.keras.utils.set_random_seed(
    seed
)
",[],"import random
import numpy as np
import tensorflow as tf
random.seed(seed)
np.random.seed(seed)
tf.random.set_seed(seed)
"
"tf.keras.utils.split_dataset(
    dataset, left_size=None, right_size=None, shuffle=False, seed=None
)
",[],"data = np.random.random(size=(1000, 4))
left_ds, right_ds = tf.keras.utils.split_dataset(data, left_size=0.8)
int(left_ds.cardinality())
800
int(right_ds.cardinality())
200"
"tf.keras.utils.text_dataset_from_directory(
    directory,
    labels='inferred',
    label_mode='int',
    class_names=None,
    batch_size=32,
    max_length=None,
    shuffle=True,
    seed=None,
    validation_split=None,
    subset=None,
    follow_links=False
)
","[['Generates a ', 'tf.data.Dataset', ' from text files in a directory.']]","main_directory/
...class_a/
......a_text_1.txt
......a_text_2.txt
...class_b/
......b_text_1.txt
......b_text_2.txt
"
"tf.keras.utils.timeseries_dataset_from_array(
    data,
    targets,
    sequence_length,
    sequence_stride=1,
    sampling_rate=1,
    batch_size=128,
    shuffle=False,
    seed=None,
    start_index=None,
    end_index=None
)
",[],"First sequence:  [0  2  4  6  8 10 12 14 16 18]
Second sequence: [3  5  7  9 11 13 15 17 19 21]
Third sequence:  [6  8 10 12 14 16 18 20 22 24]
...
Last sequence:   [78 80 82 84 86 88 90 92 94 96]
"
"tf.keras.utils.to_categorical(
    y, num_classes=None, dtype='float32'
)
",[],"a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)
a = tf.constant(a, shape=[4, 4])
print(a)
tf.Tensor(
  [[1. 0. 0. 0.]
   [0. 1. 0. 0.]
   [0. 0. 1. 0.]
   [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)"
"tf.keras.utils.unpack_x_y_sample_weight(
    data
)
",[],"features_batch = tf.ones((10, 5))
labels_batch = tf.zeros((10, 5))
data = (features_batch, labels_batch)
# `y` and `sample_weight` will default to `None` if not provided.
x, y, sample_weight = tf.keras.utils.unpack_x_y_sample_weight(data)
sample_weight is None
True"
"tf.keras.utils.warmstart_embedding_matrix(
    base_vocabulary,
    new_vocabulary,
    base_embeddings,
    new_embeddings_initializer='uniform'
)
",[],">>> import keras
>>> vocab_base = tf.convert_to_tensor([""unk"", ""a"", ""b"", ""c""])
>>> vocab_new = tf.convert_to_tensor(
...        [""unk"", ""unk"", ""a"", ""b"", ""c"", ""d"", ""e""])
>>> vectorized_vocab_base = np.random.rand(vocab_base.shape[0], 3)
>>> vectorized_vocab_new = np.random.rand(vocab_new.shape[0], 3)
>>> warmstarted_embedding_matrix = warmstart_embedding_matrix(
...       base_vocabulary=vocab_base,
...       new_vocabulary=vocab_new,
...       base_embeddings=vectorized_vocab_base,
...       new_embeddings_initializer=keras.initializers.Constant(
...         vectorized_vocab_new))
"
"tf.math.less(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less(x, y) ==> [False, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 7])
tf.math.less(x, y) ==> [False, True, True]
"
"tf.math.less_equal(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less_equal(x, y) ==> [True, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 6])
tf.math.less_equal(x, y) ==> [True, True, True]
"
"tf.linalg.LinearOperator(
    dtype,
    graph_parents=None,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None,
    parameters=None
)
","[['Inherits From: ', 'Module']]","operator.shape = [B1,...,Bb] + [M, N],  b >= 0,
x.shape =   [B1,...,Bb] + [N, R]
"
"tf.linalg.LinearOperatorAdjoint(
    operator,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","[['LinearOperator', ' representing the adjoint of another operator.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 linear operator.
operator = LinearOperatorFullMatrix([[1 - i., 3.], [0., 1. + i]])
operator_adjoint = LinearOperatorAdjoint(operator)

operator_adjoint.to_dense()
==> [[1. + i, 0.]
     [3., 1 - i]]

operator_adjoint.shape
==> [2, 2]

operator_adjoint.log_abs_determinant()
==> - log(2)

x = ... Shape [2, 4] Tensor
operator_adjoint.matmul(x)
==> Shape [2, 4] Tensor, equal to operator.matmul(x, adjoint=True)
"
"tf.linalg.LinearOperatorBlockDiag(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=True,
    name=None
)
","[['Combines one or more ', 'LinearOperators', ' in to a Block Diagonal matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 4 x 4 linear operator combined of two 2 x 2 operators.
operator_1 = LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorFullMatrix([[1., 0.], [0., 1.]])
operator = LinearOperatorBlockDiag([operator_1, operator_2])

operator.to_dense()
==> [[1., 2., 0., 0.],
     [3., 4., 0., 0.],
     [0., 0., 1., 0.],
     [0., 0., 0., 1.]]

operator.shape
==> [4, 4]

operator.log_abs_determinant()
==> scalar Tensor

x1 = ... # Shape [2, 2] Tensor
x2 = ... # Shape [2, 2] Tensor
x = tf.concat([x1, x2], 0)  # Shape [2, 4] Tensor
operator.matmul(x)
==> tf.concat([operator_1.matmul(x1), operator_2.matmul(x2)])

# Create a 5 x 4 linear operator combining three blocks.
operator_1 = LinearOperatorFullMatrix([[1.], [3.]])
operator_2 = LinearOperatorFullMatrix([[1., 6.]])
operator_3 = LinearOperatorFullMatrix([[2.], [7.]])
operator = LinearOperatorBlockDiag([operator_1, operator_2, operator_3])

operator.to_dense()
==> [[1., 0., 0., 0.],
     [3., 0., 0., 0.],
     [0., 1., 6., 0.],
     [0., 0., 0., 2.]]
     [0., 0., 0., 7.]]

operator.shape
==> [5, 4]


# Create a [2, 3] batch of 4 x 4 linear operators.
matrix_44 = tf.random.normal(shape=[2, 3, 4, 4])
operator_44 = LinearOperatorFullMatrix(matrix)

# Create a [1, 3] batch of 5 x 5 linear operators.
matrix_55 = tf.random.normal(shape=[1, 3, 5, 5])
operator_55 = LinearOperatorFullMatrix(matrix_55)

# Combine to create a [2, 3] batch of 9 x 9 operators.
operator_99 = LinearOperatorBlockDiag([operator_44, operator_55])

# Create a shape [2, 3, 9] vector.
x = tf.random.normal(shape=[2, 3, 9])
operator_99.matmul(x)
==> Shape [2, 3, 9] Tensor

# Create a blockwise list of vectors.
x = [tf.random.normal(shape=[2, 3, 4]), tf.random.normal(shape=[2, 3, 5])]
operator_99.matmul(x)
==> [Shape [2, 3, 4] Tensor, Shape [2, 3, 5] Tensor]
"
"tf.linalg.LinearOperatorBlockLowerTriangular(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorBlockLowerTriangular'
)
","[['Combines ', 'LinearOperators', ' into a blockwise lower-triangular matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]",">>> operator_0 = tf.linalg.LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
>>> operator_1 = tf.linalg.LinearOperatorFullMatrix([[1., 0.], [0., 1.]])
>>> operator_2 = tf.linalg.LinearOperatorLowerTriangular([[5., 6.], [7., 8]])
>>> operator = LinearOperatorBlockLowerTriangular(
...   [[operator_0], [operator_1, operator_2]])
"
"tf.linalg.LinearOperatorCirculant(
    spectrum,
    input_output_dtype=tf.dtypes.complex64,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=True,
    name='LinearOperatorCirculant'
)
","[['LinearOperator', ' acting like a circulant matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","A = |w z y x|
    |x w z y|
    |y x w z|
    |z y x w|
"
"tf.linalg.LinearOperatorCirculant2D(
    spectrum,
    input_output_dtype=tf.dtypes.complex64,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=True,
    name='LinearOperatorCirculant2D'
)
","[['LinearOperator', ' acting like a block circulant matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","A = |W Z Y X|
    |X W Z Y|
    |Y X W Z|
    |Z Y X W|
"
"tf.linalg.LinearOperatorCirculant3D(
    spectrum,
    input_output_dtype=tf.dtypes.complex64,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=True,
    name='LinearOperatorCirculant3D'
)
","[['LinearOperator', ' acting like a nested block circulant matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","A = |W Z Y X|
    |X W Z Y|
    |Y X W Z|
    |Z Y X W|
"
"tf.linalg.LinearOperatorComposition(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","[['Composes one or more ', 'LinearOperators', '.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","op_composed(x) := op1(op2(...(opJ(x)...))
"
"tf.linalg.LinearOperatorDiag(
    diag,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorDiag'
)
","[['LinearOperator', ' acting like a [batch] square diagonal matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 diagonal linear operator.
diag = [1., -1.]
operator = LinearOperatorDiag(diag)

operator.to_dense()
==> [[1.,  0.]
     [0., -1.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
diag = tf.random.normal(shape=[2, 3, 4])
operator = LinearOperatorDiag(diag)

# Create a shape [2, 1, 4, 2] vector.  Note that this shape is compatible
# since the batch dimensions, [2, 1], are broadcast to
# operator.batch_shape = [2, 3].
y = tf.random.normal(shape=[2, 1, 4, 2])
x = operator.solve(y)
==> operator.matmul(x) = y
"
"tf.linalg.LinearOperatorFullMatrix(
    matrix,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorFullMatrix'
)
","[['LinearOperator', ' that wraps a [batch] matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 linear operator.
matrix = [[1., 2.], [3., 4.]]
operator = LinearOperatorFullMatrix(matrix)

operator.to_dense()
==> [[1., 2.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
matrix = tf.random.normal(shape=[2, 3, 4, 4])
operator = LinearOperatorFullMatrix(matrix)
"
"tf.linalg.LinearOperatorHouseholder(
    reflection_axis,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorHouseholder'
)
","[['LinearOperator', ' acting like a [batch] of Householder transformations.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 householder transform.
vec = [1 / np.sqrt(2), 1. / np.sqrt(2)]
operator = LinearOperatorHouseholder(vec)

operator.to_dense()
==> [[0.,  -1.]
     [-1., -0.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor
"
"tf.linalg.LinearOperatorIdentity(
    num_rows,
    batch_shape=None,
    dtype=None,
    is_non_singular=True,
    is_self_adjoint=True,
    is_positive_definite=True,
    is_square=True,
    assert_proper_shapes=False,
    name='LinearOperatorIdentity'
)
","[['LinearOperator', ' acting like a [batch] square identity matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 identity matrix.
operator = LinearOperatorIdentity(num_rows=2, dtype=tf.float32)

operator.to_dense()
==> [[1., 0.]
     [0., 1.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> 0.

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor, same as x.

y = tf.random.normal(shape=[3, 2, 4])
# Note that y.shape is compatible with operator.shape because operator.shape
# is broadcast to [3, 2, 2].
# This broadcast does NOT require copying data, since we can infer that y
# will be passed through without changing shape.  We are always able to infer
# this if the operator has no batch_shape.
x = operator.solve(y)
==> Shape [3, 2, 4] Tensor, same as y.

# Create a 2-batch of 2x2 identity matrices
operator = LinearOperatorIdentity(num_rows=2, batch_shape=[2])
operator.to_dense()
==> [[[1., 0.]
      [0., 1.]],
     [[1., 0.]
      [0., 1.]]]

# Here, even though the operator has a batch shape, the input is the same as
# the output, so x can be passed through without a copy.  The operator is able
# to detect that no broadcast is necessary because both x and the operator
# have statically defined shape.
x = ... Shape [2, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, same as x

# Here the operator and x have different batch_shape, and are broadcast.
# This requires a copy, since the output is different size than the input.
x = ... Shape [1, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, equal to [x, x]
"
"tf.linalg.LinearOperatorInversion(
    operator,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","[['LinearOperator', ' representing the inverse of another operator.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 linear operator.
operator = LinearOperatorFullMatrix([[1., 0.], [0., 2.]])
operator_inv = LinearOperatorInversion(operator)

operator_inv.to_dense()
==> [[1., 0.]
     [0., 0.5]]

operator_inv.shape
==> [2, 2]

operator_inv.log_abs_determinant()
==> - log(2)

x = ... Shape [2, 4] Tensor
operator_inv.matmul(x)
==> Shape [2, 4] Tensor, equal to operator.solve(x)
"
"tf.linalg.LinearOperatorKronecker(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","[['Kronecker product between two ', 'LinearOperators', '.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 4 x 4 linear operator composed of two 2 x 2 operators.
operator_1 = LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorFullMatrix([[1., 0.], [2., 1.]])
operator = LinearOperatorKronecker([operator_1, operator_2])

operator.to_dense()
==> [[1., 0., 2., 0.],
     [2., 1., 4., 2.],
     [3., 0., 4., 0.],
     [6., 3., 8., 4.]]

operator.shape
==> [4, 4]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [4, 2] Tensor
operator.matmul(x)
==> Shape [4, 2] Tensor

# Create a [2, 3] batch of 4 x 5 linear operators.
matrix_45 = tf.random.normal(shape=[2, 3, 4, 5])
operator_45 = LinearOperatorFullMatrix(matrix)

# Create a [2, 3] batch of 5 x 6 linear operators.
matrix_56 = tf.random.normal(shape=[2, 3, 5, 6])
operator_56 = LinearOperatorFullMatrix(matrix_56)

# Compose to create a [2, 3] batch of 20 x 30 operators.
operator_large = LinearOperatorKronecker([operator_45, operator_56])

# Create a shape [2, 3, 20, 2] vector.
x = tf.random.normal(shape=[2, 3, 6, 2])
operator_large.matmul(x)
==> Shape [2, 3, 30, 2] Tensor
"
"tf.linalg.LinearOperatorLowRankUpdate(
    base_operator,
    u,
    diag_update=None,
    v=None,
    is_diag_update_positive=None,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorLowRankUpdate'
)
","[['Perturb a ', 'LinearOperator', ' with a rank ', 'K', ' update.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","L, is a LinearOperator representing [batch] M x N matrices
U, is a [batch] M x K matrix.  Typically K << M.
D, is a [batch] K x K matrix.
V, is a [batch] N x K matrix.  Typically K << N.
V^H is the Hermitian transpose (adjoint) of V.
"
"tf.linalg.LinearOperatorLowerTriangular(
    tril,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorLowerTriangular'
)
","[['LinearOperator', ' acting like a [batch] square lower triangular matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 lower-triangular linear operator.
tril = [[1., 2.], [3., 4.]]
operator = LinearOperatorLowerTriangular(tril)

# The upper triangle is ignored.
operator.to_dense()
==> [[1., 0.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
tril = tf.random.normal(shape=[2, 3, 4, 4])
operator = LinearOperatorLowerTriangular(tril)
"
"tf.linalg.LinearOperatorPermutation(
    perm,
    dtype=tf.dtypes.float32,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorPermutation'
)
","[['LinearOperator', ' acting like a [batch] of permutation matrices.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 3 x 3 permutation matrix that swaps the last two columns.
vec = [0, 2, 1]
operator = LinearOperatorPermutation(vec)

operator.to_dense()
==> [[1., 0., 0.]
     [0., 0., 1.]
     [0., 1., 0.]]

operator.shape
==> [3, 3]

# This will be zero.
operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [3, 4] Tensor
operator.matmul(x)
==> Shape [3, 4] Tensor
"
"tf.linalg.LinearOperatorScaledIdentity(
    num_rows,
    multiplier,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=True,
    assert_proper_shapes=False,
    name='LinearOperatorScaledIdentity'
)
","[['LinearOperator', ' acting like a scaled [batch] identity matrix ', 'A = c I', '.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 scaled identity matrix.
operator = LinearOperatorIdentity(num_rows=2, multiplier=3.)

operator.to_dense()
==> [[3., 0.]
     [0., 3.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> 2 * Log[3]

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> 3 * x

y = tf.random.normal(shape=[3, 2, 4])
# Note that y.shape is compatible with operator.shape because operator.shape
# is broadcast to [3, 2, 2].
x = operator.solve(y)
==> 3 * x

# Create a 2-batch of 2x2 identity matrices
operator = LinearOperatorIdentity(num_rows=2, multiplier=5.)
operator.to_dense()
==> [[[5., 0.]
      [0., 5.]],
     [[5., 0.]
      [0., 5.]]]

x = ... Shape [2, 2, 3]
operator.matmul(x)
==> 5 * x

# Here the operator and x have different batch_shape, and are broadcast.
x = ... Shape [1, 2, 3]
operator.matmul(x)
==> 5 * x
"
"tf.linalg.LinearOperatorToeplitz(
    col,
    row,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorToeplitz'
)
","[['LinearOperator', ' acting like a [batch] of toeplitz matrices.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","A = |a b c d|
    |e a b c|
    |f e a b|
    |g f e a|
"
"tf.linalg.LinearOperatorTridiag(
    diagonals,
    diagonals_format=_COMPACT,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorTridiag'
)
","[['LinearOperator', ' acting like a [batch] square tridiagonal matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","superdiag = [3., 4., 5.]
diag = [1., -1., 2.]
subdiag = [6., 7., 8]
operator = tf.linalg.LinearOperatorTridiag(
   [superdiag, diag, subdiag],
   diagonals_format='sequence')
operator.to_dense()
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.,  3.,  0.],
       [ 7., -1.,  4.],
       [ 0.,  8.,  2.]], dtype=float32)>
operator.shape
TensorShape([3, 3])"
"tf.linalg.LinearOperatorZeros(
    num_rows,
    num_columns=None,
    batch_shape=None,
    dtype=None,
    is_non_singular=False,
    is_self_adjoint=True,
    is_positive_definite=False,
    is_square=True,
    assert_proper_shapes=False,
    name='LinearOperatorZeros'
)
","[['LinearOperator', ' acting like a [batch] zero matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 zero matrix.
operator = LinearOperatorZero(num_rows=2, dtype=tf.float32)

operator.to_dense()
==> [[0., 0.]
     [0., 0.]]

operator.shape
==> [2, 2]

operator.determinant()
==> 0.

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor, same as x.

# Create a 2-batch of 2x2 zero matrices
operator = LinearOperatorZeros(num_rows=2, batch_shape=[2])
operator.to_dense()
==> [[[0., 0.]
      [0., 0.]],
     [[0., 0.]
      [0., 0.]]]

# Here, even though the operator has a batch shape, the input is the same as
# the output, so x can be passed through without a copy.  The operator is able
# to detect that no broadcast is necessary because both x and the operator
# have statically defined shape.
x = ... Shape [2, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, same as tf.zeros_like(x)

# Here the operator and x have different batch_shape, and are broadcast.
# This requires a copy, since the output is different size than the input.
x = ... Shape [1, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, equal to tf.zeros_like([x, x])
"
"tf.linalg.adjoint(
    matrix, name=None
)
","[['Transposes the last two dimensions of and conjugates tensor ', 'matrix', '.']]","x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],
                 [4 + 4j, 5 + 5j, 6 + 6j]])
tf.linalg.adjoint(x)  # [[1 - 1j, 4 - 4j],
                      #  [2 - 2j, 5 - 5j],
                      #  [3 - 3j, 6 - 6j]]
"
"tf.linalg.band_part(
    input, num_lower, num_upper, name=None
)
",[],"# if 'input' is [[ 0,  1,  2, 3]
#                [-1,  0,  1, 2]
#                [-2, -1,  0, 1]
#                [-3, -2, -1, 0]],

tf.linalg.band_part(input, 1, -1) ==> [[ 0,  1,  2, 3]
                                       [-1,  0,  1, 2]
                                       [ 0, -1,  0, 1]
                                       [ 0,  0, -1, 0]],

tf.linalg.band_part(input, 2, 1) ==> [[ 0,  1,  0, 0]
                                      [-1,  0,  1, 0]
                                      [-2, -1,  0, 1]
                                      [ 0, -2, -1, 0]]
"
"tf.linalg.banded_triangular_solve(
    bands, rhs, lower=True, adjoint=False, name=None
)
",[],"x = [[2., 3., 4.], [1., 2., 3.]]
x2 = [[2., 3., 4.], [10000., 2., 3.]]
y = tf.zeros([3, 3])
z = tf.linalg.set_diag(y, x, align='LEFT_RIGHT', k=(-1, 0))
z
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[2., 0., 0.],
       [2., 3., 0.],
       [0., 3., 4.]], dtype=float32)>
soln = tf.linalg.banded_triangular_solve(x, tf.ones([3, 1]))
soln
<tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[0.5 ],
       [0.  ],
       [0.25]], dtype=float32)>
are_equal = soln == tf.linalg.banded_triangular_solve(x2, tf.ones([3, 1]))
tf.reduce_all(are_equal).numpy()
True
are_equal = soln == tf.linalg.triangular_solve(z, tf.ones([3, 1]))
tf.reduce_all(are_equal).numpy()
True"
"tf.linalg.cholesky(
    input, name=None
)
",[],[]
"tf.linalg.cholesky_solve(
    chol, rhs, name=None
)
","[['Solves systems of linear eqns ', 'A X = RHS', ', given Cholesky factorizations.']]","# Solve 10 separate 2x2 linear systems:
A = ... # shape 10 x 2 x 2
RHS = ... # shape 10 x 2 x 1
chol = tf.linalg.cholesky(A)  # shape 10 x 2 x 2
X = tf.linalg.cholesky_solve(chol, RHS)  # shape 10 x 2 x 1
# tf.matmul(A, X) ~ RHS
X[3, :, 0]  # Solution to the linear system A[3, :, :] x = RHS[3, :, 0]

# Solve five linear systems (K = 5) for every member of the length 10 batch.
A = ... # shape 10 x 2 x 2
RHS = ... # shape 10 x 2 x 5
...
X[3, :, 2]  # Solution to the linear system A[3, :, :] x = RHS[3, :, 2]
"
"tf.linalg.cross(
    a, b, name=None
)
",[],[]
"tf.linalg.det(
    input, name=None
)
",[],[]
"tf.linalg.diag(
    diagonal,
    name='diag',
    k=0,
    num_rows=-1,
    num_cols=-1,
    padding_value=0,
    align='RIGHT_LEFT'
)
",[],"output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper
    padding_value                             ; otherwise
"
"tf.linalg.diag_part(
    input,
    name='diag_part',
    k=0,
    padding_value=0,
    align='RIGHT_LEFT'
)
",[],"diagonal[i, j, ..., l, n]
  = input[i, j, ..., l, n+y, n+x] ; if 0 <= n+y < M and 0 <= n+x < N,
    padding_value                 ; otherwise.
"
"tf.linalg.eig(
    tensor, name=None
)
",[],[]
"tf.linalg.eigh(
    tensor, name=None
)
",[],[]
"tf.linalg.eigh_tridiagonal(
    alpha,
    beta,
    eigvals_only=True,
    select='a',
    select_range=None,
    tol=None,
    name=None
)
",[],"import numpy
eigvals = tf.linalg.eigh_tridiagonal([0.0, 0.0, 0.0], [1.0, 1.0])
eigvals_expected = [-numpy.sqrt(2.0), 0.0, numpy.sqrt(2.0)]
tf.assert_near(eigvals_expected, eigvals)
# ==> True
"
"tf.linalg.eigvals(
    tensor, name=None
)
",[],[]
"tf.linalg.eigvalsh(
    tensor, name=None
)
",[],[]
"tf.einsum(
    equation, *inputs, **kwargs
)
","[[None, '\n']]","C[i,k] = sum_j A[i,j] * B[j,k]
"
"tf.linalg.experimental.conjugate_gradient(
    operator,
    rhs,
    preconditioner=None,
    x=None,
    tol=1e-05,
    max_iter=20,
    name='conjugate_gradient'
)
","[[None, '\n']]",[]
"tf.linalg.expm(
    input, name=None
)
","[[None, '\n']]",[]
"tf.eye(
    num_rows,
    num_columns=None,
    batch_shape=None,
    dtype=tf.dtypes.float32,
    name=None
)
",[],"# Construct one identity matrix.
tf.eye(2)
==> [[1., 0.],
     [0., 1.]]

# Construct a batch of 3 identity matrices, each 2 x 2.
# batch_identity[i, :, :] is a 2 x 2 identity matrix, i = 0, 1, 2.
batch_identity = tf.eye(2, batch_shape=[3])

# Construct one 2 x 3 ""identity"" matrix
tf.eye(2, num_columns=3)
==> [[ 1.,  0.,  0.],
     [ 0.,  1.,  0.]]
"
"tf.linalg.global_norm(
    t_list, name=None
)
",[],[]
"tf.linalg.inv(
    input, adjoint=False, name=None
)
",[],[]
"tf.math.l2_normalize(
    x, axis=None, epsilon=1e-12, name=None, dim=None
)
","[['Normalizes along dimension ', 'axis', ' using an L2 norm. (deprecated arguments)']]","output = x / sqrt(max(sum(x**2), epsilon))
"
"tf.linalg.logdet(
    matrix, name=None
)
",[],"# Compute the determinant of a matrix while reducing the chance of over- or
underflow:
A = ... # shape 10 x 10
det = tf.exp(tf.linalg.logdet(A))  # scalar
"
"tf.linalg.logm(
    input, name=None
)
","[[None, '\n']]",[]
"tf.linalg.lu(
    input,
    output_idx_type=tf.dtypes.int32,
    name=None
)
",[],[]
"tf.linalg.lu_matrix_inverse(
    lower_upper, perm, validate_args=False, name=None
)
",[],"inv_X = tf.lu_matrix_inverse(*tf.linalg.lu(X))
tf.assert_near(tf.matrix_inverse(X), inv_X)
# ==> True
"
"tf.linalg.lu_reconstruct(
    lower_upper, perm, validate_args=False, name=None
)
",[],"import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp

x = [[[3., 4], [1, 2]],
     [[7., 8], [3, 4]]]
x_reconstructed = tf.linalg.lu_reconstruct(*tf.linalg.lu(x))
tf.assert_near(x, x_reconstructed)
# ==> True
"
"tf.linalg.lu_solve(
    lower_upper, perm, rhs, validate_args=False, name=None
)
","[['Solves systems of linear eqns ', 'A X = RHS', ', given LU factorizations.']]","import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp

x = [[[1., 2],
      [3, 4]],
     [[7, 8],
      [3, 4]]]
inv_x = tf.linalg.lu_solve(*tf.linalg.lu(x), rhs=tf.eye(2))
tf.assert_near(tf.matrix_inverse(x), inv_x)
# ==> True
"
"tf.linalg.matmul(
    a,
    b,
    transpose_a=False,
    transpose_b=False,
    adjoint_a=False,
    adjoint_b=False,
    a_is_sparse=False,
    b_is_sparse=False,
    output_type=None,
    name=None
)
","[['Multiplies matrix ', 'a', ' by matrix ', 'b', ', producing ', 'a', ' * ', 'b', '.']]","a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
a  # 2-D tensor
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)>
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
b  # 2-D tensor
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 7,  8],
       [ 9, 10],
       [11, 12]], dtype=int32)>
c = tf.matmul(a, b)
c  # `a` * `b`
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 58,  64],
       [139, 154]], dtype=int32)>"
"tf.linalg.matrix_rank(
    a, tol=None, validate_args=False, name=None
)
",[],[]
"tf.linalg.matrix_transpose(
    a, name='matrix_transpose', conjugate=False
)
","[['Transposes last two dimensions of tensor ', 'a', '.']]","x = tf.constant([[1, 2, 3], [4, 5, 6]])
tf.linalg.matrix_transpose(x)  # [[1, 4],
                               #  [2, 5],
                               #  [3, 6]]

x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],
                 [4 + 4j, 5 + 5j, 6 + 6j]])
tf.linalg.matrix_transpose(x, conjugate=True)  # [[1 - 1j, 4 - 4j],
                                               #  [2 - 2j, 5 - 5j],
                                               #  [3 - 3j, 6 - 6j]]

# Matrix with two batch dimensions.
# x.shape is [1, 2, 3, 4]
# tf.linalg.matrix_transpose(x) is shape [1, 2, 4, 3]
"
"tf.linalg.matvec(
    a,
    b,
    transpose_a=False,
    adjoint_a=False,
    a_is_sparse=False,
    b_is_sparse=False,
    name=None
)
","[['Multiplies matrix ', 'a', ' by vector ', 'b', ', producing ', 'a', ' * ', 'b', '.']]","# 2-D tensor `a`
# [[1, 2, 3],
#  [4, 5, 6]]
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

# 1-D tensor `b`
# [7, 9, 11]
b = tf.constant([7, 9, 11], shape=[3])

# `a` * `b`
# [ 58,  64]
c = tf.linalg.matvec(a, b)


# 3-D tensor `a`
# [[[ 1,  2,  3],
#   [ 4,  5,  6]],
#  [[ 7,  8,  9],
#   [10, 11, 12]]]
a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])

# 2-D tensor `b`
# [[13, 14, 15],
#  [16, 17, 18]]
b = tf.constant(np.arange(13, 19, dtype=np.int32),
                shape=[2, 3])

# `a` * `b`
# [[ 86, 212],
#  [410, 563]]
c = tf.linalg.matvec(a, b)
"
"tf.norm(
    tensor, ord='euclidean', axis=None, keepdims=None, name=None
)
",[],[]
"tf.linalg.normalize(
    tensor, ord='euclidean', axis=None, name=None
)
","[['Normalizes ', 'tensor', ' along dimension ', 'axis', ' using specified norm.']]",[]
"tf.linalg.pinv(
    a, rcond=None, validate_args=False, name=None
)
",[],"import tensorflow as tf
import tensorflow_probability as tfp

a = tf.constant([[1.,  0.4,  0.5],
                 [0.4, 0.2,  0.25],
                 [0.5, 0.25, 0.35]])
tf.matmul(tf.linalg.pinv(a), a)
# ==> array([[1., 0., 0.],
             [0., 1., 0.],
             [0., 0., 1.]], dtype=float32)

a = tf.constant([[1.,  0.4,  0.5,  1.],
                 [0.4, 0.2,  0.25, 2.],
                 [0.5, 0.25, 0.35, 3.]])
tf.matmul(tf.linalg.pinv(a), a)
# ==> array([[ 0.76,  0.37,  0.21, -0.02],
             [ 0.37,  0.43, -0.33,  0.02],
             [ 0.21, -0.33,  0.81,  0.01],
             [-0.02,  0.02,  0.01,  1.  ]], dtype=float32)
"
"tf.linalg.qr(
    input, full_matrices=False, name=None
)
",[],"# a is a tensor.
# q is a tensor of orthonormal matrices.
# r is a tensor of upper triangular matrices.
q, r = qr(a)
q_full, r_full = qr(a, full_matrices=True)
"
"tf.linalg.set_diag(
    input,
    diagonal,
    name='set_diag',
    k=0,
    align='RIGHT_LEFT'
)
",[],"output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]
    input[i, j, ..., l, m, n]              ; otherwise
"
"tf.linalg.slogdet(
    input, name=None
)
",[],[]
"tf.linalg.solve(
    matrix, rhs, adjoint=False, name=None
)
",[],[]
"tf.linalg.sqrtm(
    input, name=None
)
",[],[]
"tf.linalg.svd(
    tensor, full_matrices=False, compute_uv=True, name=None
)
","[[None, '\n']]","# a is a tensor.
# s is a tensor of singular values.
# u is a tensor of left singular vectors.
# v is a tensor of right singular vectors.
s, u, v = svd(a)
s = svd(a, compute_uv=False)
"
"tf.linalg.tensor_diag(
    diagonal, name=None
)
",[],"# 'diagonal' is [1, 2, 3, 4]
tf.diag(diagonal) ==> [[1, 0, 0, 0]
                       [0, 2, 0, 0]
                       [0, 0, 3, 0]
                       [0, 0, 0, 4]]
"
"tf.linalg.tensor_diag_part(
    input, name=None
)
",[],"x = [[[[1111,1112],[1121,1122]],
      [[1211,1212],[1221,1222]]],
     [[[2111, 2112], [2121, 2122]],
      [[2211, 2212], [2221, 2222]]]
     ]
tf.linalg.tensor_diag_part(x)
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[1111, 1212],
       [2121, 2222]], dtype=int32)>
tf.linalg.diag_part(x).shape
TensorShape([2, 2, 2])"
"tf.tensordot(
    a, b, axes, name=None
)
","[[None, '\n']]",[]
"tf.linalg.trace(
    x, name=None
)
","[['Compute the trace of a tensor ', 'x', '.']]","x = tf.constant([[1, 2], [3, 4]])
tf.linalg.trace(x)  # 5

x = tf.constant([[1, 2, 3],
                 [4, 5, 6],
                 [7, 8, 9]])
tf.linalg.trace(x)  # 15

x = tf.constant([[[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9]],
                 [[-1, -2, -3],
                  [-4, -5, -6],
                  [-7, -8, -9]]])
tf.linalg.trace(x)  # [15, -15]
"
"tf.linalg.triangular_solve(
    matrix, rhs, lower=True, adjoint=False, name=None
)
",[],"a = tf.constant([[3,  0,  0,  0],
  [2,  1,  0,  0],
  [1,  0,  1,  0],
  [1,  1,  1,  1]], dtype=tf.float32)"
"tf.linalg.tridiagonal_matmul(
    diagonals, rhs, diagonals_format='compact', name=None
)
",[],"superdiag = tf.constant([-1, -1, 0], dtype=tf.float64)
maindiag = tf.constant([2, 2, 2], dtype=tf.float64)
subdiag = tf.constant([0, -1, -1], dtype=tf.float64)
diagonals = [superdiag, maindiag, subdiag]
rhs = tf.constant([[1, 1], [1, 1], [1, 1]], dtype=tf.float64)
x = tf.linalg.tridiagonal_matmul(diagonals, rhs, diagonals_format='sequence')
"
"tf.linalg.tridiagonal_solve(
    diagonals,
    rhs,
    diagonals_format='compact',
    transpose_rhs=False,
    conjugate_rhs=False,
    name=None,
    partial_pivoting=True,
    perturb_singular=False
)
",[],"rhs = tf.constant([...])
matrix = tf.constant([[...]])
m = matrix.shape[0]
dummy_idx = [0, 0]  # An arbitrary element to use as a dummy
indices = [[[i, i + 1] for i in range(m - 1)] + [dummy_idx],  # Superdiagonal
         [[i, i] for i in range(m)],                          # Diagonal
         [dummy_idx] + [[i + 1, i] for i in range(m - 1)]]    # Subdiagonal
diagonals=tf.gather_nd(matrix, indices)
x = tf.linalg.tridiagonal_solve(diagonals, rhs)
"
"tf.linspace(
    start, stop, num, name=None, axis=0
)
",[],"tf.linspace(10.0, 12.0, 3, name=""linspace"") => [ 10.0  11.0  12.0]
"
"tf.lite.Interpreter(
    model_path=None,
    model_content=None,
    experimental_delegates=None,
    num_threads=None,
    experimental_op_resolver_type=tf.lite.experimental.OpResolverType.AUTO,
    experimental_preserve_all_tensors=False
)
",[],"x = np.array([[1.], [2.]])
y = np.array([[2.], [4.]])
model = tf.keras.models.Sequential([
          tf.keras.layers.Dropout(0.2),
          tf.keras.layers.Dense(units=1, input_shape=[1])
        ])
model.compile(optimizer='sgd', loss='mean_squared_error')
model.fit(x, y, epochs=1)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()"
"tf.lite.RepresentativeDataset(
    input_gen
)
",[],[]
"tf.lite.TFLiteConverter(
    funcs, trackable_obj=None
)
",[],"# Converting a SavedModel to a TensorFlow Lite model.
  converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)
  tflite_model = converter.convert()

# Converting a tf.Keras model to a TensorFlow Lite model.
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()

# Converting ConcreteFunctions to a TensorFlow Lite model.
converter = tf.lite.TFLiteConverter.from_concrete_functions([func], model)
tflite_model = converter.convert()

# Converting a Jax model to a TensorFlow Lite model.
converter = tf.lite.TFLiteConverter.experimental_from_jax([func], [[
    ('input1', input1), ('input2', input2)]])
tflite_model = converter.convert()
"
"tf.lite.TargetSpec(
    supported_ops=None,
    supported_types=None,
    experimental_select_user_tf_ops=None,
    experimental_supported_backends=None
)
",[],[]
"tf.lite.experimental.QuantizationDebugOptions(
    layer_debug_metrics: Optional[Mapping[str, Callable[[np.ndarray], float]]] = None,
    model_debug_metrics: Optional[Mapping[str, Callable[[Sequence[np.ndarray], Sequence[np.ndarray]],
        float]]] = None,
    layer_direct_compare_metrics: Optional[Mapping[str, Callable[[Sequence[np.ndarray], Sequence[np.ndarray],
        float, int], float]]] = None,
    denylisted_ops: Optional[List[str]] = None,
    denylisted_nodes: Optional[List[str]] = None,
    fully_quantize: bool = False
) -> None
",[],[]
"tf.lite.experimental.QuantizationDebugger(
    quant_debug_model_path: Optional[str] = None,
    quant_debug_model_content: Optional[bytes] = None,
    float_model_path: Optional[str] = None,
    float_model_content: Optional[bytes] = None,
    debug_dataset: Optional[Callable[[], Iterable[Sequence[np.ndarray]]]] = None,
    debug_options: Optional[tf.lite.experimental.QuantizationDebugOptions] = None,
    converter: Optional[TFLiteConverter] = None
) -> None
",[],"get_debug_quantized_model() -> bytes
"
"tf.lite.experimental.authoring.compatible(
    target=None, converter_target_spec=None, **kwargs
)
","[['Wraps ', 'tf.function', ' into a callable function with TFLite compatibility checking.']]","@tf.lite.experimental.authoring.compatible
@tf.function(input_signature=[
    tf.TensorSpec(shape=[None], dtype=tf.float32)
])
def f(x):
    return tf.cosh(x)

result = f(tf.constant([0.0]))
# COMPATIBILITY WARNING: op 'tf.Cosh' require(s) ""Select TF Ops"" for model
# conversion for TensorFlow Lite.
# Op: tf.Cosh
#   - tensorflow/python/framework/op_def_library.py:748
#   - tensorflow/python/ops/gen_math_ops.py:2458
#   - <stdin>:6
"
"tf.lite.experimental.load_delegate(
    library, options=None
)
",[],"import tensorflow as tf

try:
  delegate = tf.lite.experimental.load_delegate('delegate.so')
except ValueError:
  // Fallback to CPU

if delegate:
  interpreter = tf.lite.Interpreter(
      model_path='model.tflite',
      experimental_delegates=[delegate])
else:
  interpreter = tf.lite.Interpreter(model_path='model.tflite')
"
"tf.load_library(
    library_location
)
",[],[]
"tf.load_op_library(
    library_filename
)
",[],[]
"tf.math.logical_and(
    x, y, name=None
)
",[],">>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_and(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
>>> a & b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
"
"tf.math.logical_not(
    x, name=None
)
","[['Returns the truth value of ', 'NOT x', ' element-wise.']]","tf.math.logical_not(tf.constant([True, False]))
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.math.logical_or(
    x, y, name=None
)
",[],">>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_or(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
>>> a | b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
"
"tf.lookup.KeyValueTensorInitializer(
    keys, values, key_dtype=None, value_dtype=None, name=None
)
","[['Table initializers given ', 'keys', ' and ', 'values', ' tensors.']]","keys_tensor = tf.constant(['a', 'b', 'c'])
vals_tensor = tf.constant([7, 8, 9])
input_tensor = tf.constant(['a', 'f'])
init = tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor)
table = tf.lookup.StaticHashTable(
    init,
    default_value=-1)
table.lookup(input_tensor).numpy()
array([ 7, -1], dtype=int32)"
"tf.lookup.StaticHashTable(
    initializer, default_value, name=None, experimental_is_anonymous=False
)
","[['Inherits From: ', 'TrackableResource']]","keys_tensor = tf.constant(['a', 'b', 'c'])
vals_tensor = tf.constant([7, 8, 9])
input_tensor = tf.constant(['a', 'f'])
table = tf.lookup.StaticHashTable(
    tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor),
    default_value=-1)
table.lookup(input_tensor).numpy()
array([ 7, -1], dtype=int32)"
"tf.lookup.StaticVocabularyTable(
    initializer,
    num_oov_buckets,
    lookup_key_dtype=None,
    name=None,
    experimental_is_anonymous=False
)
","[['Inherits From: ', 'TrackableResource']]","init = tf.lookup.KeyValueTensorInitializer(
    keys=tf.constant(['emerson', 'lake', 'palmer']),
    values=tf.constant([0, 1, 2], dtype=tf.int64))
table = tf.lookup.StaticVocabularyTable(
   init,
   num_oov_buckets=5)"
"tf.lookup.TextFileInitializer(
    filename,
    key_dtype,
    key_index,
    value_dtype,
    value_index,
    vocab_size=None,
    delimiter='\t',
    name=None,
    value_index_offset=0
)
",[],"import tempfile
f = tempfile.NamedTemporaryFile(delete=False)
content='\n'.join([""emerson 10"", ""lake 20"", ""palmer 30"",])
f.file.write(content.encode('utf-8'))
f.file.close()"
"tf.lookup.experimental.DenseHashTable(
    key_dtype,
    value_dtype,
    default_value,
    empty_key,
    deleted_key,
    initial_num_buckets=None,
    name='MutableDenseHashTable',
    checkpoint=True,
    experimental_is_anonymous=False
)
","[[None, '\n'], ['Inherits From: ', 'TrackableResource']]","table = tf.lookup.experimental.DenseHashTable(
    key_dtype=tf.string,
    value_dtype=tf.int64,
    default_value=-1,
    empty_key='',
    deleted_key='$')
keys = tf.constant(['a', 'b', 'c'])
values = tf.constant([0, 1, 2], dtype=tf.int64)
table.insert(keys, values)
table.remove(tf.constant(['c']))
table.lookup(tf.constant(['a', 'b', 'c','d'])).numpy()
array([ 0,  1, -1, -1])"
"tf.lookup.experimental.MutableHashTable(
    key_dtype,
    value_dtype,
    default_value,
    name='MutableHashTable',
    checkpoint=True,
    experimental_is_anonymous=False
)
","[['Inherits From: ', 'TrackableResource']]","table = tf.lookup.experimental.MutableHashTable(key_dtype=tf.string,
                                                value_dtype=tf.int64,
                                                default_value=-1)
keys_tensor = tf.constant(['a', 'b', 'c'])
vals_tensor = tf.constant([7, 8, 9], dtype=tf.int64)
input_tensor = tf.constant(['a', 'f'])
table.insert(keys_tensor, vals_tensor)
table.lookup(input_tensor).numpy()
array([ 7, -1])
table.remove(tf.constant(['c']))
table.lookup(keys_tensor).numpy()
array([ 7, 8, -1])
sorted(table.export()[0].numpy())
[b'a', b'b']
sorted(table.export()[1].numpy())
[7, 8]"
"tf.keras.losses.BinaryCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_crossentropy'
)
","[['Inherits From: ', 'Loss']]","model.compile(
  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
  ....
)
"
"tf.keras.losses.BinaryFocalCrossentropy(
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_focal_crossentropy'
)
","[['Inherits From: ', 'Loss']]","model.compile(
  loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True),
  ....
)
"
"tf.keras.losses.CategoricalCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='categorical_crossentropy'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy()
cce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.CategoricalHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='categorical_hinge'
)
","[['Computes the categorical hinge loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
# Using 'auto'/'sum_over_batch_size' reduction type.
h = tf.keras.losses.CategoricalHinge()
h(y_true, y_pred).numpy()
1.4"
"tf.keras.losses.CosineSimilarity(
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='cosine_similarity'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)
# l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]
# l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]
# l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]
# loss = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))
#       = -((0. + 0.) +  (0.5 + 0.5)) / 2
cosine_loss(y_true, y_pred).numpy()
-0.5"
"tf.keras.losses.Hinge(
    reduction=losses_utils.ReductionV2.AUTO, name='hinge'
)
","[['Computes the hinge loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
# Using 'auto'/'sum_over_batch_size' reduction type.
h = tf.keras.losses.Hinge()
h(y_true, y_pred).numpy()
1.3"
"tf.keras.losses.Huber(
    delta=1.0,
    reduction=losses_utils.ReductionV2.AUTO,
    name='huber_loss'
)
","[['Computes the Huber loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","loss = 0.5 * x^2                  if |x| <= d
loss = 0.5 * d^2 + d * (|x| - d)  if |x| > d
"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.losses.KLDivergence(
    reduction=losses_utils.ReductionV2.AUTO, name='kl_divergence'
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
# Using 'auto'/'sum_over_batch_size' reduction type.
kl = tf.keras.losses.KLDivergence()
kl(y_true, y_pred).numpy()
0.458"
"tf.keras.losses.LogCosh(
    reduction=losses_utils.ReductionV2.AUTO, name='log_cosh'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
l = tf.keras.losses.LogCosh()
l(y_true, y_pred).numpy()
0.108"
"tf.keras.losses.Loss(
    reduction=losses_utils.ReductionV2.AUTO, name=None
)
",[],"class MeanSquaredError(Loss):

  def call(self, y_true, y_pred):
    return tf.reduce_mean(tf.math.square(y_pred - y_true), axis=-1)
"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.losses.MeanAbsoluteError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_error'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
mae = tf.keras.losses.MeanAbsoluteError()
mae(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanAbsolutePercentageError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_percentage_error'
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[2., 1.], [2., 3.]]
y_pred = [[1., 1.], [1., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
mape = tf.keras.losses.MeanAbsolutePercentageError()
mape(y_true, y_pred).numpy()
50."
"tf.keras.losses.MeanSquaredError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_error'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
mse = tf.keras.losses.MeanSquaredError()
mse(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_logarithmic_error'
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
msle = tf.keras.losses.MeanSquaredLogarithmicError()
msle(y_true, y_pred).numpy()
0.240"
"tf.keras.losses.Poisson(
    reduction=losses_utils.ReductionV2.AUTO, name='poisson'
)
","[['Computes the Poisson loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
p = tf.keras.losses.Poisson()
p(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=False,
    ignore_class=None,
    reduction=losses_utils.ReductionV2.AUTO,
    name='sparse_categorical_crossentropy'
)
","[['Inherits From: ', 'Loss']]","y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
scce = tf.keras.losses.SparseCategoricalCrossentropy()
scce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.SquaredHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='squared_hinge'
)
","[['Computes the squared hinge loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
# Using 'auto'/'sum_over_batch_size' reduction type.
h = tf.keras.losses.SquaredHinge()
h(y_true, y_pred).numpy()
1.86"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
",[],"y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
",[],"y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
",[],"y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.losses.categorical_hinge(
    y_true, y_pred
)
","[['Computes the categorical hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 3, size=(2,))
y_true = tf.keras.utils.to_categorical(y_true, num_classes=3)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.categorical_hinge(y_true, y_pred)
assert loss.shape == (2,)
pos = np.sum(y_true * y_pred, axis=-1)
neg = np.amax((1. - y_true) * y_pred, axis=-1)
assert np.array_equal(loss.numpy(), np.maximum(0., neg - pos + 1.))"
"tf.keras.losses.cosine_similarity(
    y_true, y_pred, axis=-1
)
",[],"y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)
loss.numpy()
array([-0., -0.999, 0.999], dtype=float32)"
"tf.keras.losses.deserialize(
    name, custom_objects=None
)
",[],[]
"tf.keras.losses.get(
    identifier
)
","[['Retrieves a Keras loss as a ', 'function', '/', 'Loss', ' class instance.']]","loss = tf.keras.losses.get(""categorical_crossentropy"")
type(loss)
<class 'function'>
loss = tf.keras.losses.get(""CategoricalCrossentropy"")
type(loss)
<class '...keras.losses.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","[['Computes the hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.losses.huber(
    y_true, y_pred, delta=1.0
)
",[],"loss = 0.5 * x^2                  if |x| <= d
loss = d * |x| - 0.5 * d^2        if |x| > d
"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.losses.log_cosh(
    y_true, y_pred
)
",[],"y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.logcosh(y_true, y_pred)
assert loss.shape == (2,)
x = y_pred - y_true
assert np.allclose(
    loss.numpy(),
    np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),
            axis=-1),
    atol=1e-5)"
"tf.keras.losses.log_cosh(
    y_true, y_pred
)
",[],"y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.logcosh(y_true, y_pred)
assert loss.shape == (2,)
x = y_pred - y_true
assert np.allclose(
    loss.numpy(),
    np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),
            axis=-1),
    atol=1e-5)"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.losses.serialize(
    loss
)
","[['Serializes loss function or ', 'Loss', ' instance.']]",[]
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
",[],"y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","[['Computes the squared hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.make_ndarray(
    tensor
)
",[],"# Tensor a has shape (2,3)
a = tf.constant([[1,2,3],[4,5,6]])
proto_tensor = tf.make_tensor_proto(a)  # convert `tensor a` to a proto tensor
tf.make_ndarray(proto_tensor) # output: array([[1, 2, 3],
#                                              [4, 5, 6]], dtype=int32)
# output has shape (2,3)
"
"tf.make_tensor_proto(
    values, dtype=None, shape=None, verify_shape=False, allow_broadcast=False
)
",[],"  request = tensorflow_serving.apis.predict_pb2.PredictRequest()
  request.model_spec.name = ""my_model""
  request.model_spec.signature_name = ""serving_default""
  request.inputs[""images""].CopyFrom(tf.make_tensor_proto(X_new))
"
"tf.map_fn(
    fn,
    elems,
    dtype=None,
    parallel_iterations=None,
    back_prop=True,
    swap_memory=False,
    infer_shape=True,
    name=None,
    fn_output_signature=None
)
","[['Transforms ', 'elems', ' by applying ', 'fn', ' to each element unstacked on axis 0. (deprecated arguments)']]","tf.map_fn(fn=lambda t: tf.range(t, t + 3), elems=tf.constant([3, 5, 2]))
<tf.Tensor: shape=(3, 3), dtype=int32, numpy=
  array([[3, 4, 5],
         [5, 6, 7],
         [2, 3, 4]], dtype=int32)>"
"tf.math.abs(
    x, name=None
)
","[[None, '\n']]","# real number
x = tf.constant([-2.25, 3.25])
tf.abs(x)
<tf.Tensor: shape=(2,), dtype=float32,
numpy=array([2.25, 3.25], dtype=float32)>"
"tf.math.accumulate_n(
    inputs, shape=None, tensor_dtype=None, name=None
)
",[],"a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[5, 0], [0, 6]])
tf.math.accumulate_n([a, b, a]).numpy()
array([[ 7, 4],
       [ 6, 14]], dtype=int32)"
"tf.math.acos(
    x, name=None
)
",[],"x = tf.constant([1.0, -0.5, 3.4, 0.2, 0.0, -2], dtype = tf.float32)
tf.math.acos(x)
<tf.Tensor: shape=(6,), dtype=float32,
numpy= array([0. , 2.0943952, nan, 1.3694383, 1.5707964, nan],
dtype=float32)>"
"tf.math.acosh(
    x, name=None
)
",[],"x = tf.constant([-2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
tf.math.acosh(x) ==> [nan nan 0. 0.62236255 5.9914584 9.903487 inf]
"
"tf.math.add(
    x, y, name=None
)
",[],"x = [1, 2, 3, 4, 5]
y = 1
tf.add(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],
dtype=int32)>"
"tf.math.add_n(
    inputs, name=None
)
",[],"a = tf.constant([[3, 5], [4, 8]])
b = tf.constant([[1, 6], [2, 9]])
tf.math.add_n([a, b, a]).numpy()
array([[ 7, 16],
       [10, 25]], dtype=int32)"
"tf.math.angle(
    input, name=None
)
","[[None, '\n']]","input = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j], dtype=tf.complex64)
tf.math.angle(input).numpy()
# ==> array([2.0131705, 1.056345 ], dtype=float32)
"
"tf.math.approx_max_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","[['Returns max ', 'k', ' values and their indices of the input ', 'operand', ' in an approximate manner.']]","import tensorflow as tf
@tf.function(jit_compile=True)
def mips(qy, db, k=10, recall_target=0.95):
  dists = tf.einsum('ik,jk->ij', qy, db)
  # returns (f32[qy_size, k], i32[qy_size, k])
  return tf.nn.approx_max_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
dot_products, neighbors = mips(qy, db, k=20)"
"tf.math.approx_min_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","[['Returns min ', 'k', ' values and their indices of the input ', 'operand', ' in an approximate manner.']]","import tensorflow as tf
@tf.function(jit_compile=True)
def l2_ann(qy, db, half_db_norms, k=10, recall_target=0.95):
  dists = half_db_norms - tf.einsum('ik,jk->ij', qy, db)
  return tf.nn.approx_min_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
half_db_norms = tf.norm(db, axis=1) / 2
dists, neighbors = l2_ann(qy, db, half_db_norms)"
"tf.math.argmax(
    input,
    axis=None,
    output_type=tf.dtypes.int64,
    name=None
)
",[],"A = tf.constant([2, 20, 30, 3, 6])
tf.math.argmax(A)  # A[2] is maximum in tensor A
<tf.Tensor: shape=(), dtype=int64, numpy=2>
B = tf.constant([[2, 20, 30, 3, 6], [3, 11, 16, 1, 8],
                 [14, 45, 23, 5, 27]])
tf.math.argmax(B, 0)
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([2, 2, 0, 2, 2])>
tf.math.argmax(B, 1)
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 2, 1])>
C = tf.constant([0, 0, 0, 0])
tf.math.argmax(C) # Returns smallest index in case of ties
<tf.Tensor: shape=(), dtype=int64, numpy=0>"
"tf.math.argmin(
    input,
    axis=None,
    output_type=tf.dtypes.int64,
    name=None
)
",[],"import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmin(input = a)
c = tf.keras.backend.eval(b)
# c = 0
# here a[0] = 1 which is the smallest element of a across axis 0
"
"tf.math.asin(
    x, name=None
)
",[],"# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]
x = tf.constant([1.047, 0.785])
y = tf.math.sin(x) # [0.8659266, 0.7068252]

tf.math.asin(y) # [1.047, 0.785] = x
"
"tf.math.asinh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.asinh(x) ==> [-inf -1.4436355 -0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf]
"
"tf.math.atan(
    x, name=None
)
",[],"# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]
x = tf.constant([1.047, 0.785])
y = tf.math.tan(x) # [1.731261, 0.99920404]

tf.math.atan(y) # [1.047, 0.785] = x
"
"tf.math.atan2(
    y, x, name=None
)
","[[None, '\n'], ['Computes arctangent of ', 'y/x', ' element-wise, respecting signs of the arguments.']]","x = [1., 1.]
y = [1., -1.]
print((tf.math.atan2(y,x) * (180 / np.pi)).numpy())
[ 45. -45.]"
"tf.math.atanh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -1, -0.5, 1, 0, 0.5, 10, float(""inf"")])
  tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]
"
"tf.math.bessel_i0(
    x, name=None
)
","[['Computes the Bessel i0 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i0([-1., -0.5, 0.5, 1.]).numpy()
array([1.26606588, 1.06348337, 1.06348337, 1.26606588], dtype=float32)"
"tf.math.bessel_i0e(
    x, name=None
)
","[['Computes the Bessel i0e function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i0e([-1., -0.5, 0.5, 1.]).numpy()
array([0.46575961, 0.64503527, 0.64503527, 0.46575961], dtype=float32)"
"tf.math.bessel_i1(
    x, name=None
)
","[['Computes the Bessel i1 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i1([-1., -0.5, 0.5, 1.]).numpy()
array([-0.5651591 , -0.25789431,  0.25789431,  0.5651591 ], dtype=float32)"
"tf.math.bessel_i1e(
    x, name=None
)
","[['Computes the Bessel i1e function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i1e([-1., -0.5, 0.5, 1.]).numpy()
array([-0.20791042, -0.15642083,  0.15642083,  0.20791042], dtype=float32)"
"tf.math.betainc(
    a, b, x, name=None
)
","[[None, '\n']]",[]
"tf.math.bincount(
    arr,
    weights=None,
    minlength=None,
    maxlength=None,
    dtype=tf.dtypes.int32,
    name=None,
    axis=None,
    binary_output=False
)
",[],"values = tf.constant([1,1,2,3,2,4,4,5])
tf.math.bincount(values) #[0 2 2 1 2 1]
"
"tf.math.ceil(
    x, name=None
)
",[],"tf.math.ceil([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])
<tf.Tensor: shape=(7,), dtype=float32,
numpy=array([-1., -1., -0.,  1.,  2.,  2.,  2.], dtype=float32)>"
"tf.math.confusion_matrix(
    labels,
    predictions,
    num_classes=None,
    weights=None,
    dtype=tf.dtypes.int32,
    name=None
)
",[],"  tf.math.confusion_matrix([1, 2, 4], [2, 2, 4]) ==>
      [[0 0 0 0 0]
       [0 0 1 0 0]
       [0 0 1 0 0]
       [0 0 0 0 0]
       [0 0 0 0 1]]
"
"tf.math.conj(
    x, name=None
)
","[[None, '\n']]","x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
tf.math.conj(x)
<tf.Tensor: shape=(2,), dtype=complex128,
numpy=array([-2.25-4.75j,  3.25-5.75j])>"
"tf.math.cos(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]
"
"tf.math.cosh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.cosh(x) ==> [inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf]
"
"tf.math.count_nonzero(
    input,
    axis=None,
    keepdims=None,
    dtype=tf.dtypes.int64,
    name=None
)
",[],"x = tf.constant([[0, 1, 0], [1, 1, 0]])
tf.math.count_nonzero(x)  # 3
tf.math.count_nonzero(x, 0)  # [1, 2, 0]
tf.math.count_nonzero(x, 1)  # [1, 2]
tf.math.count_nonzero(x, 1, keepdims=True)  # [[1], [2]]
tf.math.count_nonzero(x, [0, 1])  # 3
"
"tf.math.cumprod(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","[['Compute the cumulative product of the tensor ', 'x', ' along ', 'axis', '.']]","tf.math.cumprod([a, b, c])  # [a, a * b, a * b * c]
"
"tf.math.cumsum(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","[['Compute the cumulative sum of the tensor ', 'x', ' along ', 'axis', '.']]","# tf.cumsum([a, b, c])   # [a, a + b, a + b + c]
x = tf.constant([2, 4, 6, 8])
tf.cumsum(x)
<tf.Tensor: shape=(4,), dtype=int32,
numpy=array([ 2,  6, 12, 20], dtype=int32)>"
"tf.math.cumulative_logsumexp(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","[['Compute the cumulative log-sum-exp of the tensor ', 'x', ' along ', 'axis', '.']]","log(sum(exp(x))) == log(sum(exp(x - max(x)))) + max(x)
"
"tf.math.digamma(
    x, name=None
)
",[],[]
"tf.math.divide(
    x, y, name=None
)
","[['Computes Python style division of ', 'x', ' by ', 'y', '.']]","x = tf.constant([16, 12, 11])
y = tf.constant([4, 6, 2])
tf.divide(x,y)
<tf.Tensor: shape=(3,), dtype=float64,
numpy=array([4. , 2. , 5.5])>"
"tf.math.divide_no_nan(
    x, y, name=None
)
","[['Computes a safe divide which returns 0 if ', 'y', ' (denominator) is zero.']]","tf.constant(3.0) / 0.0
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
tf.math.divide_no_nan(3.0, 0.0)
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
"tf.math.equal(
    x, y, name=None
)
",[],"x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>"
"tf.math.erf(
    x, name=None
)
","[[None, '\n'], ['Computes the ', 'Gauss error function', ' of ', 'x', ' element-wise. In statistics, for non-negative values of \\(x\\), the error function has the following interpretation: for a random variable \\(Y\\) that is normally distributed with mean 0 and variance \\(1/\\sqrt{2}\\), \\(erf(x)\\) is the probability that \\(Y\\) falls in the range \\([x, x]\\).']]","tf.math.erf([[1.0, 2.0, 3.0], [0.0, -1.0, -2.0]])
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.8427007,  0.9953223,  0.999978 ],
       [ 0.       , -0.8427007, -0.9953223]], dtype=float32)>"
"tf.math.erfc(
    x, name=None
)
","[['Computes the complementary error function of ', 'x', ' element-wise.']]",[]
"tf.math.erfcinv(
    x, name=None
)
",[],"tf.math.erfcinv([0., 0.2, 1., 1.5, 2.])
<tf.Tensor: shape=(5,), dtype=float32, numpy=
array([       inf,  0.9061935, -0.       , -0.4769363,       -inf],
      dtype=float32)>"
"tf.math.erfinv(
    x, name=None
)
",[],[]
"tf.math.exp(
    x, name=None
)
","[[None, '\n']]","x = tf.constant(2.0)
tf.math.exp(x)
<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>"
"tf.math.expm1(
    x, name=None
)
","[['Computes ', 'exp(x) - 1', ' element-wise.']]","  x = tf.constant(2.0)
  tf.math.expm1(x) ==> 6.389056

  x = tf.constant([2.0, 8.0])
  tf.math.expm1(x) ==> array([6.389056, 2979.958], dtype=float32)

  x = tf.constant(1 + 1j)
  tf.math.expm1(x) ==> (0.46869393991588515+2.2873552871788423j)
"
"tf.math.floor(
    x, name=None
)
",[],"x = tf.constant([1.3324, -1.5, 5.555, -2.532, 0.99, float(""inf"")])
tf.floor(x).numpy()
array([ 1., -2.,  5., -3.,  0., inf], dtype=float32)"
"tf.math.floordiv(
    x, y, name=None
)
","[['Divides ', 'x / y', ' elementwise, rounding toward the most negative integer.']]",[]
"tf.math.floormod(
    x, y, name=None
)
",[],[]
"tf.math.greater(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5, 2, 5])
tf.math.greater(x, y) ==> [False, True, True]

x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.greater(x, y) ==> [False, False, True]
"
"tf.math.greater_equal(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6, 7])
y = tf.constant([5, 2, 5, 10])
tf.math.greater_equal(x, y) ==> [True, True, True, False]

x = tf.constant([5, 4, 6, 7])
y = tf.constant([5])
tf.math.greater_equal(x, y) ==> [True, False, True, True]
"
"tf.math.igamma(
    a, x, name=None
)
","[[None, '\n'], ['Compute the lower regularized incomplete Gamma function ', 'P(a, x)', '.']]",[]
"tf.math.igammac(
    a, x, name=None
)
","[[None, '\n'], ['Compute the upper regularized incomplete Gamma function ', 'Q(a, x)', '.']]",[]
"tf.math.imag(
    input, name=None
)
",[],"x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
tf.math.imag(x)  # [4.75, 5.75]
"
"tf.math.in_top_k(
    targets, predictions, k, name=None
)
","[[None, '\n'], ['Says whether the targets are in the top ', 'K', ' predictions.']]",[]
"tf.math.invert_permutation(
    x, name=None
)
",[],"# tensor `x` is [3, 4, 0, 2, 1]
invert_permutation(x) ==> [2, 4, 3, 0, 1]
"
"tf.math.is_finite(
    x, name=None
)
",[],"x = tf.constant([5.0, 4.8, 6.8, np.inf, np.nan])
tf.math.is_finite(x) ==> [True, True, True, False, False]
"
"tf.math.is_inf(
    x, name=None
)
",[],"x = tf.constant([5.0, np.inf, 6.8, np.inf])
tf.math.is_inf(x) ==> [False, True, False, True]
"
"tf.math.is_nan(
    x, name=None
)
",[],"x = tf.constant([5.0, np.nan, 6.8, np.nan, np.inf])
tf.math.is_nan(x) ==> [False, True, False, True, False]
"
"tf.math.is_non_decreasing(
    x, name=None
)
","[['Returns ', 'True', ' if ', 'x', ' is non-decreasing.']]","x1 = tf.constant([1.0, 1.0, 3.0])
tf.math.is_non_decreasing(x1)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
x2 = tf.constant([3.0, 1.0, 2.0])
tf.math.is_non_decreasing(x2)
<tf.Tensor: shape=(), dtype=bool, numpy=False>"
"tf.math.is_strictly_increasing(
    x, name=None
)
","[['Returns ', 'True', ' if ', 'x', ' is strictly increasing.']]","x1 = tf.constant([1.0, 2.0, 3.0])
tf.math.is_strictly_increasing(x1)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
x2 = tf.constant([3.0, 1.0, 2.0])
tf.math.is_strictly_increasing(x2)
<tf.Tensor: shape=(), dtype=bool, numpy=False>"
"tf.math.l2_normalize(
    x, axis=None, epsilon=1e-12, name=None, dim=None
)
","[['Normalizes along dimension ', 'axis', ' using an L2 norm. (deprecated arguments)']]","output = x / sqrt(max(sum(x**2), epsilon))
"
"tf.math.lbeta(
    x, name=None
)
","[[None, '\n']]",[]
"tf.math.less(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less(x, y) ==> [False, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 7])
tf.math.less(x, y) ==> [False, True, True]
"
"tf.math.less_equal(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less_equal(x, y) ==> [True, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 6])
tf.math.less_equal(x, y) ==> [True, True, True]
"
"tf.math.lgamma(
    x, name=None
)
","[['Computes the log of the absolute value of ', 'Gamma(x)', ' element-wise.']]","x = tf.constant([0, 0.5, 1, 4.5, -4, -5.6])
tf.math.lgamma(x) ==> [inf, 0.5723649, 0., 2.4537368, inf, -4.6477685]
"
"tf.math.log(
    x, name=None
)
","[[None, '\n']]","x = tf.constant([0, 0.5, 1, 5])
tf.math.log(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([      -inf, -0.6931472,  0.       ,  1.609438 ], dtype=float32)>"
"tf.math.log1p(
    x, name=None
)
","[[None, '\n']]","x = tf.constant([0, 0.5, 1, 5])
tf.math.log1p(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.       , 0.4054651, 0.6931472, 1.7917595], dtype=float32)>"
"tf.math.log_sigmoid(
    x, name=None
)
","[['Computes log sigmoid of ', 'x', ' element-wise.']]","x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.log_sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=
array([-6.9314718e-01, -3.1326169e-01, -1.9287499e-22, -0.0000000e+00],
      dtype=float32)>"
"tf.nn.log_softmax(
    logits, axis=None, name=None
)
",[],"logsoftmax = logits - log(reduce_sum(exp(logits), axis))
"
"tf.math.logical_and(
    x, y, name=None
)
",[],">>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_and(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
>>> a & b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
"
"tf.math.logical_not(
    x, name=None
)
","[['Returns the truth value of ', 'NOT x', ' element-wise.']]","tf.math.logical_not(tf.constant([True, False]))
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.math.logical_or(
    x, y, name=None
)
",[],">>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_or(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
>>> a | b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
"
"tf.math.logical_xor(
    x, y, name='LogicalXor'
)
",[],"a = tf.constant([True])
b = tf.constant([False])
tf.math.logical_xor(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>"
"tf.math.maximum(
    x, y, name=None
)
",[],"x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-2., 0., 2., 5.])
tf.math.maximum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>"
"tf.math.minimum(
    x, y, name=None
)
",[],"x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-5., -2., 0., 3.])
tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>"
"tf.math.floormod(
    x, y, name=None
)
",[],[]
"tf.math.multiply(
    x, y, name=None
)
",[],"x = tf.constant(([1, 2, 3, 4]))
tf.math.multiply(x, x)
<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>"
"tf.math.multiply_no_nan(
    x, y, name=None
)
",[],[]
"tf.math.ndtri(
    x, name=None
)
",[],[]
"tf.math.negative(
    x, name=None
)
","[[None, '\n']]",[]
"tf.math.nextafter(
    x1, x2, name=None
)
","[['Returns the next representable value of ', 'x1', ' in the direction of ', 'x2', ', element-wise.']]",[]
"tf.math.not_equal(
    x, y, name=None
)
",[],"x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.not_equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.math.polygamma(
    a, x, name=None
)
","[[None, '\n']]",[]
"tf.math.polyval(
    coeffs, x, name=None
)
",[],"p(x) = coeffs[n-1] + x * (coeffs[n-2] + ... + x * (coeffs[1] + x * coeffs[0]))
"
"tf.math.pow(
    x, y, name=None
)
","[[None, '\n']]","x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]
"
"tf.math.real(
    input, name=None
)
",[],"x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
tf.math.real(x)  # [-2.25, 3.25]
"
"tf.math.reciprocal(
    x, name=None
)
","[[None, '\n']]",[]
"tf.math.reciprocal_no_nan(
    x, name=None
)
",[],"x = tf.constant([2.0, 0.5, 0, 1], dtype=tf.float32)
tf.math.reciprocal_no_nan(x)  # [ 0.5, 2, 0.0, 1.0 ]
"
"tf.math.reduce_all(
    input_tensor, axis=None, keepdims=False, name=None
)
","[['Computes ', 'tf.math.logical_and', ' of elements across dimensions of a tensor.']]",">>> x = tf.constant([[True,  True], [False, False]])
>>> tf.math.reduce_all(x)
<tf.Tensor: shape=(), dtype=bool, numpy=False>
>>> tf.math.reduce_all(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False, False])>
>>> tf.math.reduce_all(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.math.reduce_any(
    input_tensor, axis=None, keepdims=False, name=None
)
","[['Computes ', 'tf.math.logical_or', ' of elements across dimensions of a tensor.']]",">>> x = tf.constant([[True,  True], [False, False]])
>>> tf.reduce_any(x)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
>>> tf.reduce_any(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>
>>> tf.reduce_any(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.math.reduce_euclidean_norm(
    input_tensor, axis=None, keepdims=False, name=None
)
",[],"x = tf.constant([[1, 2, 3], [1, 1, 1]]) # x.dtype is tf.int32
tf.math.reduce_euclidean_norm(x)  # returns 4 as dtype is tf.int32
y = tf.constant([[1, 2, 3], [1, 1, 1]], dtype = tf.float32)
tf.math.reduce_euclidean_norm(y)  # returns 4.1231055 which is sqrt(17)
tf.math.reduce_euclidean_norm(y, 0)  # [sqrt(2), sqrt(5), sqrt(10)]
tf.math.reduce_euclidean_norm(y, 1)  # [sqrt(14), sqrt(3)]
tf.math.reduce_euclidean_norm(y, 1, keepdims=True)  # [[sqrt(14)], [sqrt(3)]]
tf.math.reduce_euclidean_norm(y, [0, 1])  # sqrt(17)
"
"tf.math.reduce_logsumexp(
    input_tensor, axis=None, keepdims=False, name=None
)
",[],"x = tf.constant([[0., 0., 0.], [0., 0., 0.]])
tf.reduce_logsumexp(x)  # log(6)
tf.reduce_logsumexp(x, 0)  # [log(2), log(2), log(2)]
tf.reduce_logsumexp(x, 1)  # [log(3), log(3)]
tf.reduce_logsumexp(x, 1, keepdims=True)  # [[log(3)], [log(3)]]
tf.reduce_logsumexp(x, [0, 1])  # log(6)
"
"tf.math.reduce_max(
    input_tensor, axis=None, keepdims=False, name=None
)
","[['Computes ', 'tf.math.maximum', ' of elements across dimensions of a tensor.']]",">>> x = tf.constant([5, 1, 2, 4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=5>
>>> x = tf.constant([-5, -1, -2, -4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=-1>
>>> x = tf.constant([4, float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('nan'), float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('-inf'), float('inf')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
"
"tf.math.reduce_mean(
    input_tensor, axis=None, keepdims=False, name=None
)
",[],"x = tf.constant([[1., 1.], [2., 2.]])
tf.reduce_mean(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.5>
tf.reduce_mean(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>
tf.reduce_mean(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
"tf.math.reduce_min(
    input_tensor, axis=None, keepdims=False, name=None
)
","[['Computes the ', 'tf.math.minimum', ' of elements across dimensions of a tensor.']]","a = tf.constant([
  [[1, 2], [3, 4]],
  [[1, 2], [3, 4]]
])
tf.reduce_min(a)
<tf.Tensor: shape=(), dtype=int32, numpy=1>"
"tf.math.reduce_prod(
    input_tensor, axis=None, keepdims=False, name=None
)
","[['Computes ', 'tf.math.multiply', ' of elements across dimensions of a tensor.']]",">>> x = tf.constant([[1., 2.], [3., 4.]])
>>> tf.math.reduce_prod(x)
<tf.Tensor: shape=(), dtype=float32, numpy=24.>
>>> tf.math.reduce_prod(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 8.], dtype=float32)>
>>> tf.math.reduce_prod(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 12.],
dtype=float32)>
"
"tf.math.reduce_std(
    input_tensor, axis=None, keepdims=False, name=None
)
",[],"x = tf.constant([[1., 2.], [3., 4.]])
tf.math.reduce_std(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.118034>
tf.math.reduce_std(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>
tf.math.reduce_std(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5, 0.5], dtype=float32)>"
"tf.math.reduce_sum(
    input_tensor, axis=None, keepdims=False, name=None
)
",[],">>> # x has a shape of (2, 3) (two rows and three columns):
>>> x = tf.constant([[1, 1, 1], [1, 1, 1]])
>>> x.numpy()
array([[1, 1, 1],
       [1, 1, 1]], dtype=int32)
>>> # sum all the elements
>>> # 1 + 1 + 1 + 1 + 1+ 1 = 6
>>> tf.reduce_sum(x).numpy()
6
>>> # reduce along the first dimension
>>> # the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]
>>> tf.reduce_sum(x, 0).numpy()
array([2, 2, 2], dtype=int32)
>>> # reduce along the second dimension
>>> # the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]
>>> tf.reduce_sum(x, 1).numpy()
array([3, 3], dtype=int32)
>>> # keep the original dimensions
>>> tf.reduce_sum(x, 1, keepdims=True).numpy()
array([[3],
       [3]], dtype=int32)
>>> # reduce along both dimensions
>>> # the result is 1 + 1 + 1 + 1 + 1 + 1 = 6
>>> # or, equivalently, reduce along rows, then reduce the resultant array
>>> # [1, 1, 1] + [1, 1, 1] = [2, 2, 2]
>>> # 2 + 2 + 2 = 6
>>> tf.reduce_sum(x, [0, 1]).numpy()
6
"
"tf.math.reduce_variance(
    input_tensor, axis=None, keepdims=False, name=None
)
",[],"x = tf.constant([[1., 2.], [3., 4.]])
tf.math.reduce_variance(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.25>
tf.math.reduce_variance(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], ...)>
tf.math.reduce_variance(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.25, 0.25], ...)>"
"tf.math.rint(
    x, name=None
)
",[],"rint(-1.5) ==> -2.0
rint(0.5000001) ==> 1.0
rint([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0]) ==> [-2., -2., -0., 0., 2., 2., 2.]
"
"tf.math.round(
    x, name=None
)
",[],"x = tf.constant([0.9, 2.5, 2.3, 1.5, -4.5])
tf.round(x)  # [ 1.0, 2.0, 2.0, 2.0, -4.0 ]
"
"tf.math.rsqrt(
    x, name=None
)
",[],"x = tf.constant([2., 0., -2.])
tf.math.rsqrt(x)
<tf.Tensor: shape=(3,), dtype=float32,
numpy=array([0.707, inf, nan], dtype=float32)>"
"tf.math.scalar_mul(
    scalar, x, name=None
)
","[['Multiplies a scalar times a ', 'Tensor', ' or ', 'IndexedSlices', ' object.']]","x = tf.reshape(tf.range(30, dtype=tf.float32), [10, 3])
with tf.GradientTape() as g:
  g.watch(x)
  y = tf.gather(x, [1, 2])  # IndexedSlices
  z = tf.math.scalar_mul(10.0, y)"
"tf.math.segment_max(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_max(c, tf.constant([0, 0, 1])).numpy()
array([[4, 3, 3, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_mean(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1.0,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_mean(c, tf.constant([0, 0, 1])).numpy()
array([[2.5, 2.5, 2.5, 2.5],
       [5., 6., 7., 8.]], dtype=float32)"
"tf.math.segment_min(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_min(c, tf.constant([0, 0, 1])).numpy()
array([[1, 2, 2, 1],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_prod(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_prod(c, tf.constant([0, 0, 1])).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_sum(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_sum(c, tf.constant([0, 0, 1])).numpy()
array([[5, 5, 5, 5],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.sigmoid(
    x, name=None
)
","[[None, '\n'], ['Computes sigmoid of ', 'x', ' element-wise.']]","x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32,
numpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>"
"tf.math.sign(
    x, name=None
)
",[],"# real number
tf.math.sign([0., 2., -3.])
<tf.Tensor: shape=(3,), dtype=float32,
numpy=array([ 0.,  1., -1.], dtype=float32)>"
"tf.math.sin(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10, float(""inf"")])
  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]
"
"tf.math.sinh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.sinh(x) ==> [-inf -4.0515420e+03 -5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf]
"
"tf.math.sobol_sample(
    dim,
    num_results,
    skip=0,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.nn.softmax(
    logits, axis=None, name=None
)
",[],"softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis, keepdims=True)
"
"tf.math.softplus(
    features, name=None
)
","[['Computes elementwise softplus: ', 'softplus(x) = log(exp(x) + 1)', '.']]","import tensorflow as tf
tf.math.softplus(tf.range(0, 2, dtype=tf.float32)).numpy()
array([0.6931472, 1.3132616], dtype=float32)"
"tf.nn.softsign(
    features, name=None
)
","[['Computes softsign: ', 'features / (abs(features) + 1)', '.']]",[]
"tf.math.bessel_i0(
    x, name=None
)
","[['Computes the Bessel i0 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i0([-1., -0.5, 0.5, 1.]).numpy()
array([1.26606588, 1.06348337, 1.06348337, 1.26606588], dtype=float32)"
"tf.math.bessel_i0e(
    x, name=None
)
","[['Computes the Bessel i0e function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i0e([-1., -0.5, 0.5, 1.]).numpy()
array([0.46575961, 0.64503527, 0.64503527, 0.46575961], dtype=float32)"
"tf.math.bessel_i1(
    x, name=None
)
","[['Computes the Bessel i1 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i1([-1., -0.5, 0.5, 1.]).numpy()
array([-0.5651591 , -0.25789431,  0.25789431,  0.5651591 ], dtype=float32)"
"tf.math.bessel_i1e(
    x, name=None
)
","[['Computes the Bessel i1e function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i1e([-1., -0.5, 0.5, 1.]).numpy()
array([-0.20791042, -0.15642083,  0.15642083,  0.20791042], dtype=float32)"
"tf.math.special.bessel_j0(
    x, name=None
)
","[['Computes the Bessel j0 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_j0([0.5, 1., 2., 4.]).numpy()
array([ 0.93846981,  0.76519769,  0.22389078, -0.39714981], dtype=float32)"
"tf.math.special.bessel_j1(
    x, name=None
)
","[['Computes the Bessel j1 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_j1([0.5, 1., 2., 4.]).numpy()
array([ 0.24226846,  0.44005059,  0.57672481, -0.06604333], dtype=float32)"
"tf.math.special.bessel_k0(
    x, name=None
)
","[['Computes the Bessel k0 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_k0([0.5, 1., 2., 4.]).numpy()
array([0.92441907, 0.42102444, 0.11389387, 0.01115968], dtype=float32)"
"tf.math.special.bessel_k0e(
    x, name=None
)
","[['Computes the Bessel k0e function of ', 'x', ' element-wise.']]","tf.math.special.bessel_k0e([0.5, 1., 2., 4.]).numpy()
array([1.52410939, 1.14446308, 0.84156822, 0.60929767], dtype=float32)"
"tf.math.special.bessel_k1(
    x, name=None
)
","[['Computes the Bessel k1 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_k1([0.5, 1., 2., 4.]).numpy()
array([1.65644112, 0.60190723, 0.13986588, 0.0124835 ], dtype=float32)"
"tf.math.special.bessel_k1e(
    x, name=None
)
","[['Computes the Bessel k1e function of ', 'x', ' element-wise.']]","tf.math.special.bessel_k1e([0.5, 1., 2., 4.]).numpy()
array([2.73100971, 1.63615349, 1.03347685, 0.68157595], dtype=float32)"
"tf.math.special.bessel_y0(
    x, name=None
)
","[['Computes the Bessel y0 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_y0([0.5, 1., 2., 4.]).numpy()
array([-0.44451873,  0.08825696,  0.51037567, -0.01694074], dtype=float32)"
"tf.math.special.bessel_y1(
    x, name=None
)
","[['Computes the Bessel y1 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_y1([0.5, 1., 2., 4.]).numpy()
array([-1.47147239, -0.78121282, -0.10703243,  0.39792571], dtype=float32)"
"tf.math.special.dawsn(
    x, name=None
)
","[[""Computes Dawson's integral of "", 'x', ' element-wise.']]",">>> tf.math.special.dawsn([-1., -0.5, 0.5, 1.]).numpy()
array([-0.5380795, -0.4244364, 0.4244364,  0.5380795], dtype=float32)
"
"tf.math.special.expint(
    x, name=None
)
","[['Computes the Exponential integral of ', 'x', ' element-wise.']]","tf.math.special.expint([1., 1.1, 2.1, 4.1]).numpy()
array([ 1.8951179,  2.1673784,  5.3332353, 21.048464], dtype=float32)"
"tf.math.special.fresnel_cos(
    x, name=None
)
","[[""Computes Fresnel's cosine integral of "", 'x', ' element-wise.']]",">>> tf.math.special.fresnel_cos([-1., -0.1, 0.1, 1.]).numpy()
array([-0.7798934 , -0.09999753,  0.09999753,  0.7798934 ], dtype=float32)
"
"tf.math.special.fresnel_sin(
    x, name=None
)
","[[""Computes Fresnel's sine integral of "", 'x', ' element-wise.']]","tf.math.special.fresnel_sin([-1., -0.1, 0.1, 1.]).numpy()
array([-0.43825912, -0.00052359,  0.00052359,  0.43825912], dtype=float32)"
"tf.math.special.spence(
    x, name=None
)
","[[""Computes Spence's integral of "", 'x', ' element-wise.']]","tf.math.special.spence([0.5, 1., 2., 3.]).numpy()
array([ 0.58224034,  0.        , -0.82246685, -1.4367464], dtype=float32)"
"tf.math.sqrt(
    x, name=None
)
",[],"x = tf.constant([[4.0], [16.0]])
tf.sqrt(x)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[2.],
         [4.]], dtype=float32)>
y = tf.constant([[-4.0], [16.0]])
tf.sqrt(y)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[nan],
         [ 4.]], dtype=float32)>
z = tf.constant([[-1.0], [16.0]], dtype=tf.complex128)
tf.sqrt(z)
<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=
  array([[0.0+1.j],
         [4.0+0.j]])>"
"tf.math.square(
    x, name=None
)
","[[None, '\n']]","tf.math.square([-2., 0., 3.])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>"
"tf.math.squared_difference(
    x, y, name=None
)
",[],[]
"tf.math.subtract(
    x, y, name=None
)
",[],"x = [1, 2, 3, 4, 5]
y = 1
tf.subtract(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>
tf.subtract(y, x)
<tf.Tensor: shape=(5,), dtype=int32,
numpy=array([ 0, -1, -2, -3, -4], dtype=int32)>"
"tf.math.tan(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.tan(x) ==> [nan 0.45231566 -0.5463025 1.5574077 2.572152 -1.7925274 0.32097113 nan]
"
"tf.math.tanh(
    x, name=None
)
","[['Computes hyperbolic tangent of ', 'x', ' element-wise.']]",[]
"tf.math.top_k(
    input, k=1, sorted=True, name=None
)
","[['Finds values and indices of the ', 'k', ' largest entries for the last dimension.']]","result = tf.math.top_k([1, 2, 98, 1, 1, 99, 3, 1, 3, 96, 4, 1],
                        k=3)
result.values.numpy()
array([99, 98, 96], dtype=int32)
result.indices.numpy()
array([5, 2, 9], dtype=int32)"
"tf.math.truediv(
    x, y, name=None
)
",[],[]
"tf.math.unsorted_segment_max(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_max(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[4, 3, 3, 4],
       [5,  6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_mean(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]",[]
"tf.math.unsorted_segment_min(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_min(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[1, 2, 2, 1],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_prod(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_prod(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_sqrt_n(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]",[]
"tf.math.unsorted_segment_sum(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = [[1,2,3,4], [5,6,7,8], [4,3,2,1]]
tf.math.unsorted_segment_sum(c, [0, 1, 0], num_segments=2).numpy()
array([[5, 5, 5, 5],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.xdivy(
    x, y, name=None
)
",[],[]
"tf.math.xlog1py(
    x, y, name=None
)
",[],"tf.math.xlog1py(0., 1.)
<tf.Tensor: shape=(), dtype=float32, numpy=0.>
tf.math.xlog1py(1., 1.)
<tf.Tensor: shape=(), dtype=float32, numpy=0.6931472>
tf.math.xlog1py(2., 2.)
<tf.Tensor: shape=(), dtype=float32, numpy=2.1972246>
tf.math.xlog1py(0., -1.)
<tf.Tensor: shape=(), dtype=float32, numpy=0.>"
"tf.math.xlogy(
    x, y, name=None
)
",[],[]
"tf.math.zero_fraction(
    value, name=None
)
","[['Returns the fraction of zeros in ', 'value', '.']]","    z = tf.nn.relu(...)
    summ = tf.compat.v1.summary.scalar('sparsity', tf.nn.zero_fraction(z))
"
"tf.math.zeta(
    x, q, name=None
)
","[[None, '\n']]",[]
"tf.linalg.matmul(
    a,
    b,
    transpose_a=False,
    transpose_b=False,
    adjoint_a=False,
    adjoint_b=False,
    a_is_sparse=False,
    b_is_sparse=False,
    output_type=None,
    name=None
)
","[['Multiplies matrix ', 'a', ' by matrix ', 'b', ', producing ', 'a', ' * ', 'b', '.']]","a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
a  # 2-D tensor
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)>
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
b  # 2-D tensor
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 7,  8],
       [ 9, 10],
       [11, 12]], dtype=int32)>
c = tf.matmul(a, b)
c  # `a` * `b`
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 58,  64],
       [139, 154]], dtype=int32)>"
"tf.linalg.sqrtm(
    input, name=None
)
",[],[]
"tf.math.maximum(
    x, y, name=None
)
",[],"x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-2., 0., 2., 5.])
tf.math.maximum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>"
"tf.meshgrid(
    *args, **kwargs
)
",[],"x = [1, 2, 3]
y = [4, 5, 6]
X, Y = tf.meshgrid(x, y)
# X = [[1, 2, 3],
#      [1, 2, 3],
#      [1, 2, 3]]
# Y = [[4, 4, 4],
#      [5, 5, 5],
#      [6, 6, 6]]
"
"tf.keras.metrics.AUC(
    num_thresholds=200,
    curve='ROC',
    summation_method='interpolation',
    name=None,
    dtype=None,
    thresholds=None,
    multi_label=False,
    num_labels=None,
    label_weights=None,
    from_logits=False
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.AUC(num_thresholds=3)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
# threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]
# tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]
# tp_rate = recall = [1, 0.5, 0], fp_rate = [1, 0, 0]
# auc = ((((1+0.5)/2)*(1-0)) + (((0.5+0)/2)*(0-0))) = 0.75
m.result().numpy()
0.75"
"tf.keras.metrics.Accuracy(
    name='accuracy', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Accuracy()
m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryAccuracy(
    name='binary_accuracy', dtype=None, threshold=0.5
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.BinaryAccuracy()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryCrossentropy(
    name='binary_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.BinaryCrossentropy()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.81492424"
"tf.keras.metrics.BinaryIoU(
    target_class_ids: Union[List[int], Tuple[int, ...]] = (0, 1),
    threshold=0.5,
    name=None,
    dtype=None
)
","[['Inherits From: ', 'IoU', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.CategoricalAccuracy(
    name='categorical_accuracy', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.CategoricalAccuracy()
m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
                [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.CategoricalCrossentropy(
    name='categorical_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0,
    axis=-1
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","# EPSILON = 1e-7, y = y_true, y` = y_pred
# y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)
# y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]
# xent = -sum(y * log(y'), axis = -1)
#      = -((log 0.95), (log 0.1))
#      = [0.051, 2.302]
# Reduced xent = (0.051 + 2.302) / 2
m = tf.keras.metrics.CategoricalCrossentropy()
m.update_state([[0, 1, 0], [0, 0, 1]],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.CategoricalHinge(
    name='categorical_hinge', dtype=None
)
","[['Computes the categorical hinge metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.CategoricalHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.4000001"
"tf.keras.metrics.CosineSimilarity(
    name='cosine_similarity', dtype=None, axis=-1
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","# l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]
# l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]
# l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]
# result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))
#        = ((0. + 0.) +  (0.5 + 0.5)) / 2
m = tf.keras.metrics.CosineSimilarity(axis=1)
m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.FalseNegatives(
    thresholds=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.FalseNegatives()
m.update_state([0, 1, 1, 1], [0, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.FalsePositives(
    thresholds=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.FalsePositives()
m.update_state([0, 1, 0, 0], [0, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.Hinge(
    name='hinge', dtype=None
)
","[['Computes the hinge metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Hinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.3"
"tf.keras.metrics.IoU(
    num_classes: int,
    target_class_ids: Union[List[int], Tuple[int, ...]],
    name: Optional[str] = None,
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    ignore_class: Optional[int] = None,
    sparse_y_true: bool = True,
    sparse_y_pred: bool = True,
    axis: int = -1
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.KLDivergence(
    name='kullback_leibler_divergence', dtype=None
)
","[['Computes Kullback-Leibler divergence metric between ', 'y_true', ' and'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.KLDivergence()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.45814306"
"tf.keras.metrics.LogCoshError(
    name='logcosh', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.LogCoshError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.10844523"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.Mean(
    name='mean', dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Mean()
m.update_state([1, 3, 5, 7])
m.result().numpy()
4.0
m.reset_state()
m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.MeanAbsoluteError(
    name='mean_absolute_error', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanAbsoluteError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanAbsolutePercentageError(
    name='mean_absolute_percentage_error', dtype=None
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' and'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanAbsolutePercentageError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
250000000.0"
"tf.keras.metrics.MeanIoU(
    num_classes: int,
    name: Optional[str] = None,
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    ignore_class: Optional[int] = None,
    sparse_y_true: bool = True,
    sparse_y_pred: bool = True,
    axis: int = -1
)
","[['Inherits From: ', 'IoU', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.MeanMetricWrapper(
    fn, name=None, dtype=None, **kwargs
)
","[['Inherits From: ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","def accuracy(y_true, y_pred):
  return tf.cast(tf.math.equal(y_true, y_pred), tf.float32)

accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=accuracy)

keras_model.compile(..., metrics=accuracy_metric)
"
"tf.keras.metrics.MeanRelativeError(
    normalizer, name=None, dtype=None
)
","[['Inherits From: ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3])
m.update_state([1, 3, 2, 3], [2, 4, 6, 8])"
"tf.keras.metrics.MeanSquaredError(
    name='mean_squared_error', dtype=None
)
","[['Computes the mean squared error between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanSquaredLogarithmicError(
    name='mean_squared_logarithmic_error', dtype=None
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' and'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanSquaredLogarithmicError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.12011322"
"tf.keras.metrics.MeanTensor(
    name='mean_tensor', dtype=None, shape=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanTensor()
m.update_state([0, 1, 2, 3])
m.update_state([4, 5, 6, 7])
m.result().numpy()
array([2., 3., 4., 5.], dtype=float32)"
"tf.keras.metrics.Metric(
    name=None, dtype=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","m = SomeMetric(...)
for input in ...:
  m.update_state(input)
print('Final result: ', m.result().numpy())
"
"tf.keras.metrics.OneHotIoU(
    num_classes: int,
    target_class_ids: Union[List[int], Tuple[int, ...]],
    name=None,
    dtype=None,
    ignore_class: Optional[int] = None,
    sparse_y_pred: bool = False,
    axis: int = -1
)
","[['Inherits From: ', 'IoU', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.OneHotMeanIoU(
    num_classes: int,
    name: str = None,
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    ignore_class: Optional[int] = None,
    sparse_y_pred: bool = False,
    axis: int = -1
)
","[['Inherits From: ', 'MeanIoU', ', ', 'IoU', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.Poisson(
    name='poisson', dtype=None
)
","[['Computes the Poisson metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Poisson()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.Precision(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Precision()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.PrecisionAtRecall(
    recall, num_thresholds=200, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.PrecisionAtRecall(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.Recall(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Recall()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.RecallAtPrecision(
    precision, num_thresholds=200, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.RecallAtPrecision(0.8)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
m.result().numpy()
0.5"
"tf.keras.metrics.RootMeanSquaredError(
    name='root_mean_squared_error', dtype=None
)
","[['Computes root mean squared error metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.RootMeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SensitivityAtSpecificity(
    specificity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.SensitivityAtSpecificity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.SparseCategoricalAccuracy(
    name='sparse_categorical_accuracy', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","acc = np.dot(sample_weight, np.equal(y_true, np.argmax(y_pred, axis=1))
"
"tf.keras.metrics.SparseCategoricalCrossentropy(
    name: str = 'sparse_categorical_crossentropy',
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    from_logits: bool = False,
    ignore_class: Optional[int] = None,
    axis: int = -1
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","# y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]]
# logits = log(y_pred)
# softmax = exp(logits) / sum(exp(logits), axis=-1)
# softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]
# xent = -sum(y * log(softmax), 1)
# log(softmax) = [[-2.9957, -0.0513, -16.1181],
#                [-2.3026, -0.2231, -2.3026]]
# y_true * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]
# xent = [0.0513, 2.3026]
# Reduced xent = (0.0513 + 2.3026) / 2
m = tf.keras.metrics.SparseCategoricalCrossentropy()
m.update_state([1, 2],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.SparseTopKCategoricalAccuracy(
    k=5, name='sparse_top_k_categorical_accuracy', dtype=None
)
","[['Computes how often integer targets are in the top ', 'K', ' predictions.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)
m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SpecificityAtSensitivity(
    sensitivity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.SpecificityAtSensitivity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.66666667"
"tf.keras.metrics.SquaredHinge(
    name='squared_hinge', dtype=None
)
","[['Computes the squared hinge metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.SquaredHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.86"
"tf.keras.metrics.Sum(
    name='sum', dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Sum()
m.update_state([1, 3, 5, 7])
m.result().numpy()
16.0"
"tf.keras.metrics.TopKCategoricalAccuracy(
    k=5, name='top_k_categorical_accuracy', dtype=None
)
","[['Computes how often targets are in the top ', 'K', ' predictions.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.TopKCategoricalAccuracy(k=1)
m.update_state([[0, 0, 1], [0, 1, 0]],
               [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.TrueNegatives(
    thresholds=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.TrueNegatives()
m.update_state([0, 1, 0, 0], [1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.TruePositives(
    thresholds=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.TruePositives()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.binary_accuracy(
    y_true, y_pred, threshold=0.5
)
",[],"y_true = [[1], [1], [0], [0]]
y_pred = [[1], [1], [0], [0]]
m = tf.keras.metrics.binary_accuracy(y_true, y_pred)
assert m.shape == (4,)
m.numpy()
array([1., 1., 1., 1.], dtype=float32)"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
",[],"y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
",[],"y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_accuracy(
    y_true, y_pred
)
",[],"y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
",[],"y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.deserialize(
    config, custom_objects=None
)
",[],[]
"tf.keras.metrics.get(
    identifier
)
","[['Retrieves a Keras metric as a ', 'function', '/', 'Metric', ' class instance.']]","metric = tf.keras.metrics.get(""categorical_crossentropy"")
type(metric)
<class 'function'>
metric = tf.keras.metrics.get(""CategoricalCrossentropy"")
type(metric)
<class '...metrics.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","[['Computes the hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.losses.log_cosh(
    y_true, y_pred
)
",[],"y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.logcosh(y_true, y_pred)
assert loss.shape == (2,)
x = y_pred - y_true
assert np.allclose(
    loss.numpy(),
    np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),
            axis=-1),
    atol=1e-5)"
"tf.keras.losses.log_cosh(
    y_true, y_pred
)
",[],"y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.logcosh(y_true, y_pred)
assert loss.shape == (2,)
x = y_pred - y_true
assert np.allclose(
    loss.numpy(),
    np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),
            axis=-1),
    atol=1e-5)"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.metrics.serialize(
    metric
)
","[['Serializes metric function or ', 'Metric', ' instance.']]",[]
"tf.keras.metrics.sparse_categorical_accuracy(
    y_true, y_pred
)
",[],"y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
",[],"y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","[['Computes how often integer targets are in the top ', 'K', ' predictions.']]","y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","[['Computes the squared hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.keras.metrics.top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","[['Computes how often targets are in the top ', 'K', ' predictions.']]","y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.math.minimum(
    x, y, name=None
)
",[],"x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-5., -2., 0., 3.])
tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>"
"tf.mlir.experimental.convert_function(
    concrete_function,
    pass_pipeline='tf-standard-pipeline',
    show_debug_info=False
)
",[],"@tf.function
def add(a, b):
  return a + b"
"tf.mlir.experimental.convert_graph_def(
    graph_def,
    pass_pipeline='tf-standard-pipeline',
    show_debug_info=False
)
",[],[]
"tf.math.multiply(
    x, y, name=None
)
",[],"x = tf.constant(([1, 2, 3, 4]))
tf.math.multiply(x, x)
<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>"
"tf.name_scope(
    name
)
",[],"def my_op(a, b, c, name=None):
  with tf.name_scope(""MyOp"") as scope:
    a = tf.convert_to_tensor(a, name=""a"")
    b = tf.convert_to_tensor(b, name=""b"")
    c = tf.convert_to_tensor(c, name=""c"")
    # Define some computation that uses `a`, `b`, and `c`.
    return foo_op(..., name=scope)
"
"tf.math.negative(
    x, name=None
)
","[[None, '\n']]",[]
"tf.nest.assert_same_structure(
    nest1, nest2, check_types=True, expand_composites=False
)
",[],[]
"tf.nest.flatten(
    structure, expand_composites=False
)
",[],[]
"tf.nest.is_nested(
    seq
)
",[],[]
"tf.nest.map_structure(
    func, *structure, **kwargs
)
","[['Creates a new structure by applying ', 'func', ' to each atom in ', 'structure', '.']]","a = {""hello"": 24, ""world"": 76}
tf.nest.map_structure(lambda p: p * 2, a)
{'hello': 48, 'world': 152}"
"tf.nest.pack_sequence_as(
    structure, flat_sequence, expand_composites=False
)
",[],[]
"tf.nn.RNNCellDeviceWrapper(
    *args, **kwargs
)
","[['Inherits From: ', 'Module']]","self.input_spec = tf.keras.layers.InputSpec(ndim=4)
"
"tf.nn.RNNCellDropoutWrapper(
    *args, **kwargs
)
","[['Inherits From: ', 'Module']]","self.input_spec = tf.keras.layers.InputSpec(ndim=4)
"
"tf.nn.RNNCellResidualWrapper(
    *args, **kwargs
)
","[['Inherits From: ', 'Module']]","self.input_spec = tf.keras.layers.InputSpec(ndim=4)
"
"tf.random.all_candidate_sampler(
    true_classes, num_true, num_sampled, unique, seed=None, name=None
)
",[],[]
"tf.math.approx_max_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","[['Returns max ', 'k', ' values and their indices of the input ', 'operand', ' in an approximate manner.']]","import tensorflow as tf
@tf.function(jit_compile=True)
def mips(qy, db, k=10, recall_target=0.95):
  dists = tf.einsum('ik,jk->ij', qy, db)
  # returns (f32[qy_size, k], i32[qy_size, k])
  return tf.nn.approx_max_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
dot_products, neighbors = mips(qy, db, k=20)"
"tf.math.approx_min_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","[['Returns min ', 'k', ' values and their indices of the input ', 'operand', ' in an approximate manner.']]","import tensorflow as tf
@tf.function(jit_compile=True)
def l2_ann(qy, db, half_db_norms, k=10, recall_target=0.95):
  dists = half_db_norms - tf.einsum('ik,jk->ij', qy, db)
  return tf.nn.approx_min_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
half_db_norms = tf.norm(db, axis=1) / 2
dists, neighbors = l2_ann(qy, db, half_db_norms)"
"tf.nn.atrous_conv2d(
    value, filters, rate, padding, name=None
)
",[],"output[batch, height, width, out_channel] =
    sum_{dheight, dwidth, in_channel} (
        filters[dheight, dwidth, in_channel, out_channel] *
        value[batch, height + rate*dheight, width + rate*dwidth, in_channel]
    )
"
"tf.nn.atrous_conv2d_transpose(
    value, filters, output_shape, rate, padding, name=None
)
","[['The transpose of ', 'atrous_conv2d', '.']]",[]
"tf.nn.avg_pool(
    input, ksize, strides, padding, data_format=None, name=None
)
",[],[]
"tf.nn.avg_pool1d(
    input, ksize, strides, padding, data_format='NWC', name=None
)
",[],[]
"tf.nn.avg_pool2d(
    input, ksize, strides, padding, data_format='NHWC', name=None
)
",[],[]
"tf.nn.avg_pool3d(
    input, ksize, strides, padding, data_format='NDHWC', name=None
)
",[],[]
"tf.nn.batch_norm_with_global_normalization(
    input,
    mean,
    variance,
    beta,
    gamma,
    variance_epsilon,
    scale_after_normalization,
    name=None
)
",[],[]
"tf.nn.batch_normalization(
    x, mean, variance, offset, scale, variance_epsilon, name=None
)
","[[None, '\n']]",[]
"tf.nn.bias_add(
    value, bias, data_format=None, name=None
)
","[['Adds ', 'bias', ' to ', 'value', '.']]",[]
"tf.nn.collapse_repeated(
    labels, seq_length, name=None
)
",[],[]
"tf.nn.compute_accidental_hits(
    true_classes, sampled_candidates, num_true, seed=None, name=None
)
","[['Compute the position ids in ', 'sampled_candidates', ' matching ', 'true_classes', '.']]",[]
"tf.nn.compute_average_loss(
    per_example_loss, sample_weight=None, global_batch_size=None
)
",[],"with strategy.scope():
  def compute_loss(labels, predictions, sample_weight=None):

    # If you are using a `Loss` class instead, set reduction to `NONE` so that
    # we can do the reduction afterwards and divide by global batch size.
    per_example_loss = tf.keras.losses.sparse_categorical_crossentropy(
        labels, predictions)

    # Compute loss that is scaled by sample_weight and by global batch size.
    return tf.nn.compute_average_loss(
        per_example_loss,
        sample_weight=sample_weight,
        global_batch_size=GLOBAL_BATCH_SIZE)
"
"tf.nn.conv1d(
    input,
    filters,
    stride,
    padding,
    data_format='NWC',
    dilations=None,
    name=None
)
","[[None, '\n']]",[]
"tf.nn.conv1d_transpose(
    input,
    filters,
    output_shape,
    strides,
    padding='SAME',
    data_format='NWC',
    dilations=None,
    name=None
)
","[['The transpose of ', 'conv1d', '.']]",[]
"tf.nn.conv2d(
    input,
    filters,
    strides,
    padding,
    data_format='NHWC',
    dilations=None,
    name=None
)
","[['Computes a 2-D convolution given ', 'input', ' and 4-D ', 'filters', ' tensors.']]","output[b, i, j, k] =
    sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] *
                    filter[di, dj, q, k]
"
"tf.nn.conv2d_transpose(
    input,
    filters,
    output_shape,
    strides,
    padding='SAME',
    data_format='NHWC',
    dilations=None,
    name=None
)
","[['The transpose of ', 'conv2d', '.']]",[]
"tf.nn.conv3d(
    input,
    filters,
    strides,
    padding,
    data_format='NDHWC',
    dilations=None,
    name=None
)
","[['Computes a 3-D convolution given 5-D ', 'input', ' and ', 'filters', ' tensors.']]",[]
"tf.nn.conv3d_transpose(
    input,
    filters,
    output_shape,
    strides,
    padding='SAME',
    data_format='NDHWC',
    dilations=None,
    name=None
)
","[['The transpose of ', 'conv3d', '.']]",[]
"tf.nn.conv_transpose(
    input,
    filters,
    output_shape,
    strides,
    padding='SAME',
    data_format=None,
    dilations=None,
    name=None
)
","[['The transpose of ', 'convolution', '.']]",[]
"tf.nn.convolution(
    input,
    filters,
    strides=None,
    padding='VALID',
    data_format=None,
    dilations=None,
    name=None
)
",[],"output[b, x[0], ..., x[N-1], k] =
    sum_{z[0], ..., z[N-1], q}
        filter[z[0], ..., z[N-1], q, k] *
        padded_input[b,
                     x[0]*strides[0] + dilation_rate[0]*z[0],
                     ...,
                     x[N-1]*strides[N-1] + dilation_rate[N-1]*z[N-1],
                     q]
"
"tf.nn.crelu(
    features, axis=-1, name=None
)
",[],[]
"tf.nn.ctc_beam_search_decoder(
    inputs, sequence_length, beam_width=100, top_paths=1
)
",[],[]
"tf.nn.ctc_greedy_decoder(
    inputs, sequence_length, merge_repeated=True, blank_index=None
)
",[],"inf = float(""inf"")
logits = tf.constant([[[   0., -inf, -inf],
                       [ -2.3, -inf, -0.1]],
                      [[ -inf, -0.5, -inf],
                       [ -inf, -inf, -0.1]],
                      [[ -inf, -inf, -inf],
                       [ -0.1, -inf, -2.3]]])
seq_lens = tf.constant([2, 3])
outputs = tf.nn.ctc_greedy_decoder(
    logits,
    seq_lens,
    blank_index=1)"
"tf.nn.ctc_loss(
    labels,
    logits,
    label_length,
    logit_length,
    logits_time_major=True,
    unique=None,
    blank_index=None,
    name=None
)
",[],[]
"tf.nn.ctc_unique_labels(
    labels, name=None
)
","[['Get unique labels and indices for batched labels for ', 'tf.nn.ctc_loss', '.']]",[]
"tf.nn.depth_to_space(
    input, block_size, data_format='NHWC', name=None
)
",[],"x = [[[[1, 2, 3, 4]]]]

"
"tf.nn.depthwise_conv2d(
    input,
    filter,
    strides,
    padding,
    data_format=None,
    dilations=None,
    name=None
)
",[],"output[b, i, j, k * channel_multiplier + q] =
    sum_{di, dj} filter[di, dj, k, q] *
                 input[b, strides[1] * i + dilations[0] * di,
                          strides[2] * j + dilations[1] * dj, k]
"
"tf.nn.depthwise_conv2d_backprop_filter(
    input,
    filter_sizes,
    out_backprop,
    strides,
    padding,
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.nn.depthwise_conv2d_backprop_input(
    input_sizes,
    filter,
    out_backprop,
    strides,
    padding,
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.nn.dilation2d(
    input, filters, strides, padding, data_format, dilations, name=None
)
","[['Computes the grayscale dilation of 4-D ', 'input', ' and 3-D ', 'filters', ' tensors.']]","output[b, y, x, c] =
   max_{dy, dx} input[b,
                      strides[1] * y + rates[1] * dy,
                      strides[2] * x + rates[2] * dx,
                      c] +
                filters[dy, dx, c]
"
"tf.nn.dropout(
    x, rate, noise_shape=None, seed=None, name=None
)
",[],"tf.random.set_seed(0)
x = tf.ones([3,5])
tf.nn.dropout(x, rate = 0.5, seed = 1).numpy()
array([[2., 0., 0., 2., 2.],
     [2., 2., 2., 2., 2.],
     [2., 0., 2., 0., 2.]], dtype=float32)"
"tf.nn.elu(
    features, name=None
)
","[[None, '\n']]","tf.nn.elu(1.0)
<tf.Tensor: shape=(), dtype=float32, numpy=1.0>
tf.nn.elu(0.0)
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>
tf.nn.elu(-1000.0)
<tf.Tensor: shape=(), dtype=float32, numpy=-1.0>"
"tf.nn.embedding_lookup(
    params, ids, max_norm=None, name=None
)
","[['Looks up embeddings for the given ', 'ids', ' from a list of tensors.']]","[[1, 2], [3, 4], [5, 6], [7, 8], [9, 10]]
"
"tf.nn.embedding_lookup_sparse(
    params, sp_ids, sp_weights, combiner=None, max_norm=None, name=None
)
",[],"  [0, 0]: id 1, weight 2.0
  [0, 1]: id 3, weight 0.5
  [1, 0]: id 0, weight 1.0
  [2, 3]: id 1, weight 3.0
"
"tf.nn.erosion2d(
    value, filters, strides, padding, data_format, dilations, name=None
)
","[['Computes the grayscale erosion of 4-D ', 'value', ' and 3-D ', 'filters', ' tensors.']]","output[b, y, x, c] =
   min_{dy, dx} value[b,
                      strides[1] * y - dilations[1] * dy,
                      strides[2] * x - dilations[2] * dx,
                      c] -
                filters[dy, dx, c]
"
"tf.nn.experimental.stateless_dropout(
    x, rate, seed, rng_alg=None, noise_shape=None, name=None
)
",[],"x = tf.ones([3,5])
tf.nn.experimental.stateless_dropout(x, rate=0.5, seed=[1, 0])
<tf.Tensor: shape=(3, 5), dtype=float32, numpy=
array([[2., 0., 2., 0., 0.],
       [0., 0., 2., 0., 2.],
       [0., 0., 0., 0., 2.]], dtype=float32)>"
"tf.random.fixed_unigram_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    vocab_file='',
    distortion=1.0,
    num_reserved_ids=0,
    num_shards=1,
    shard=0,
    unigrams=(),
    seed=None,
    name=None
)
",[],[]
"tf.nn.fractional_avg_pool(
    value,
    pooling_ratio,
    pseudo_random=False,
    overlapping=False,
    seed=0,
    name=None
)
",[],[]
"tf.nn.fractional_max_pool(
    value,
    pooling_ratio,
    pseudo_random=False,
    overlapping=False,
    seed=0,
    name=None
)
",[],[]
"tf.nn.gelu(
    features, approximate=False, name=None
)
",[],"x = tf.constant([-3.0, -1.0, 0.0, 1.0, 3.0], dtype=tf.float32)
y = tf.nn.gelu(x)
y.numpy()
array([-0.00404951, -0.15865529,  0.        ,  0.8413447 ,  2.9959507 ],
    dtype=float32)
y = tf.nn.gelu(x, approximate=True)
y.numpy()
array([-0.00363752, -0.15880796,  0.        ,  0.841192  ,  2.9963627 ],
    dtype=float32)"
"tf.math.in_top_k(
    targets, predictions, k, name=None
)
","[[None, '\n'], ['Says whether the targets are in the top ', 'K', ' predictions.']]",[]
"tf.nn.isotonic_regression(
    inputs, decreasing=True, axis=-1
)
","[[None, '\n']]",">>> x = tf.constant([[3, 1, 2], [1, 3, 4]], dtype=tf.float32)
>>> y, segments = tf.nn.isotonic_regression(x, axis=1)
>>> y  # The solution.
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[3.       , 1.5      , 1.5      ],
       [2.6666667, 2.6666667, 2.6666667]], dtype=float32)>
"
"tf.nn.l2_loss(
    t, name=None
)
",[],"output = sum(t ** 2) / 2
"
"tf.math.l2_normalize(
    x, axis=None, epsilon=1e-12, name=None, dim=None
)
","[['Normalizes along dimension ', 'axis', ' using an L2 norm. (deprecated arguments)']]","output = x / sqrt(max(sum(x**2), epsilon))
"
"tf.nn.leaky_relu(
    features, alpha=0.2, name=None
)
",[],[]
"tf.random.learned_unigram_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=None,
    name=None
)
",[],[]
"tf.nn.local_response_normalization(
    input, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None
)
",[],"sqr_sum[a, b, c, d] =
    sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)
output = input / (bias + alpha * sqr_sum) ** beta
"
"tf.nn.log_poisson_loss(
    targets, log_input, compute_full_loss=False, name=None
)
","[['Computes log Poisson loss given ', 'log_input', '.']]","  -log(exp(-x) * (x^z) / z!)
= -log(exp(-x) * (x^z)) + log(z!)
~ -log(exp(-x)) - log(x^z) [+ z * log(z) - z + 0.5 * log(2 * pi * z)]
    [ Note the second term is the Stirling's Approximation for log(z!).
      It is invariant to x and does not affect optimization, though
      important for correct relative loss comparisons. It is only
      computed when compute_full_loss == True. ]
= x - z * log(x) [+ z * log(z) - z + 0.5 * log(2 * pi * z)]
= exp(c) - z * c [+ z * log(z) - z + 0.5 * log(2 * pi * z)]
"
"tf.nn.log_softmax(
    logits, axis=None, name=None
)
",[],"logsoftmax = logits - log(reduce_sum(exp(logits), axis))
"
"tf.nn.local_response_normalization(
    input, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None
)
",[],"sqr_sum[a, b, c, d] =
    sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)
output = input / (bias + alpha * sqr_sum) ** beta
"
"tf.nn.max_pool(
    input, ksize, strides, padding, data_format=None, name=None
)
",[],"matrix = tf.constant([
    [0, 0, 1, 7],
    [0, 2, 0, 0],
    [5, 2, 0, 0],
    [0, 0, 9, 8],
])
reshaped = tf.reshape(matrix, (1, 4, 4, 1))
tf.nn.max_pool(reshaped, ksize=2, strides=2, padding=""SAME"")
<tf.Tensor: shape=(1, 2, 2, 1), dtype=int32, numpy=
array([[[[2],
         [7]],
        [[5],
         [9]]]], dtype=int32)>"
"tf.nn.max_pool1d(
    input, ksize, strides, padding, data_format='NWC', name=None
)
",[],[]
"tf.nn.max_pool2d(
    input, ksize, strides, padding, data_format='NHWC', name=None
)
",[],"x = tf.constant([[1., 2., 3., 4.],
                 [5., 6., 7., 8.],
                 [9., 10., 11., 12.]])
# Add the `batch` and `channels` dimensions.
x = x[tf.newaxis, :, :, tf.newaxis]
result = tf.nn.max_pool2d(x, ksize=(2, 2), strides=(2, 2),
                          padding=""VALID"")
result[0, :, :, 0]
<tf.Tensor: shape=(1, 2), dtype=float32, numpy=
array([[6., 8.]], dtype=float32)>"
"tf.nn.max_pool3d(
    input, ksize, strides, padding, data_format='NDHWC', name=None
)
",[],[]
"tf.nn.max_pool_with_argmax(
    input,
    ksize,
    strides,
    padding,
    data_format='NHWC',
    output_dtype=tf.dtypes.int64,
    include_batch_in_index=False,
    name=None
)
",[],[]
"tf.nn.moments(
    x, axes, shift=None, keepdims=False, name=None
)
","[['Calculates the mean and variance of ', 'x', '.']]",[]
"tf.nn.nce_loss(
    weights,
    biases,
    labels,
    inputs,
    num_sampled,
    num_classes,
    num_true=1,
    sampled_values=None,
    remove_accidental_hits=False,
    name='nce_loss'
)
",[],"if mode == ""train"":
  loss = tf.nn.nce_loss(
      weights=weights,
      biases=biases,
      labels=labels,
      inputs=inputs,
      ...)
elif mode == ""eval"":
  logits = tf.matmul(inputs, tf.transpose(weights))
  logits = tf.nn.bias_add(logits, biases)
  labels_one_hot = tf.one_hot(labels, n_classes)
  loss = tf.nn.sigmoid_cross_entropy_with_logits(
      labels=labels_one_hot,
      logits=logits)
  loss = tf.reduce_sum(loss, axis=1)
"
"tf.nn.normalize_moments(
    counts, mean_ss, variance_ss, shift, name=None
)
",[],[]
"tf.nn.pool(
    input,
    window_shape,
    pooling_type,
    strides=None,
    padding='VALID',
    data_format=None,
    dilations=None,
    name=None
)
",[],"output[b, x[0], ..., x[N-1], c] =
  REDUCE_{z[0], ..., z[N-1]}
    input[b,
          x[0] * strides[0] - pad_before[0] + dilation_rate[0]*z[0],
          ...
          x[N-1]*strides[N-1] - pad_before[N-1] + dilation_rate[N-1]*z[N-1],
          c],
"
"tf.nn.relu(
    features, name=None
)
","[['Computes rectified linear: ', 'max(features, 0)', '.']]",">>> tf.nn.relu([-2., 0., 3.]).numpy()
array([0., 0., 3.], dtype=float32)
"
"tf.nn.relu6(
    features, name=None
)
","[['Computes Rectified Linear 6: ', 'min(max(features, 0), 6)', '.']]","x = tf.constant([-3.0, -1.0, 0.0, 6.0, 10.0], dtype=tf.float32)
y = tf.nn.relu6(x)
y.numpy()
array([0., 0., 0., 6., 6.], dtype=float32)"
"tf.nn.safe_embedding_lookup_sparse(
    embedding_weights,
    sparse_ids,
    sparse_weights=None,
    combiner='mean',
    default_id=None,
    max_norm=None,
    name=None
)
",[],"  [0, 0]: id 1, weight 2.0
  [0, 1]: id 3, weight 0.5
  [1, 0]: id -1, weight 1.0
  [2, 3]: id 1, weight 3.0
"
"tf.nn.sampled_softmax_loss(
    weights,
    biases,
    labels,
    inputs,
    num_sampled,
    num_classes,
    num_true=1,
    sampled_values=None,
    remove_accidental_hits=True,
    seed=None,
    name='sampled_softmax_loss'
)
",[],"if mode == ""train"":
  loss = tf.nn.sampled_softmax_loss(
      weights=weights,
      biases=biases,
      labels=labels,
      inputs=inputs,
      ...)
elif mode == ""eval"":
  logits = tf.matmul(inputs, tf.transpose(weights))
  logits = tf.nn.bias_add(logits, biases)
  labels_one_hot = tf.one_hot(labels, n_classes)
  loss = tf.nn.softmax_cross_entropy_with_logits(
      labels=labels_one_hot,
      logits=logits)
"
"tf.nn.scale_regularization_loss(
    regularization_loss
)
",[],"with strategy.scope():
  def compute_loss(self, label, predictions):
    per_example_loss = tf.keras.losses.sparse_categorical_crossentropy(
        labels, predictions)

    # Compute loss that is scaled by sample_weight and by global batch size.
    loss = tf.nn.compute_average_loss(
        per_example_loss,
        sample_weight=sample_weight,
        global_batch_size=GLOBAL_BATCH_SIZE)

    # Add scaled regularization losses.
    loss += tf.nn.scale_regularization_loss(tf.nn.l2_loss(weights))
    return loss
"
"tf.nn.selu(
    features, name=None
)
","[['Computes scaled exponential linear: ', 'scale * alpha * (exp(features) - 1)']]",[]
"tf.nn.separable_conv2d(
    input,
    depthwise_filter,
    pointwise_filter,
    strides,
    padding,
    data_format=None,
    dilations=None,
    name=None
)
",[],"output[b, i, j, k] = sum_{di, dj, q, r}
    input[b, strides[1] * i + di, strides[2] * j + dj, q] *
    depthwise_filter[di, dj, q, r] *
    pointwise_filter[0, 0, q * channel_multiplier + r, k]
"
"tf.math.sigmoid(
    x, name=None
)
","[[None, '\n'], ['Computes sigmoid of ', 'x', ' element-wise.']]","x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32,
numpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>"
"tf.nn.sigmoid_cross_entropy_with_logits(
    labels=None, logits=None, name=None
)
","[[None, '\n'], ['Computes sigmoid cross entropy given ', 'logits', '.']]","  z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
= z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
= (1 - z) * x + log(1 + exp(-x))
= x - x * z + log(1 + exp(-x))
"
"tf.nn.silu(
    features, beta=1.0
)
","[['Computes the SiLU or Swish activation function: ', 'x * sigmoid(beta * x)', '.']]",[]
"tf.nn.softmax(
    logits, axis=None, name=None
)
",[],"softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis, keepdims=True)
"
"tf.nn.softmax_cross_entropy_with_logits(
    labels, logits, axis=-1, name=None
)
","[['Computes softmax cross entropy between ', 'logits', ' and ', 'labels', '.']]","logits = [[4.0, 2.0, 1.0], [0.0, 5.0, 1.0]]
labels = [[1.0, 0.0, 0.0], [0.0, 0.8, 0.2]]
tf.nn.softmax_cross_entropy_with_logits(labels=labels, logits=logits)
<tf.Tensor: shape=(2,), dtype=float32,
numpy=array([0.16984604, 0.82474494], dtype=float32)>"
"tf.math.softplus(
    features, name=None
)
","[['Computes elementwise softplus: ', 'softplus(x) = log(exp(x) + 1)', '.']]","import tensorflow as tf
tf.math.softplus(tf.range(0, 2, dtype=tf.float32)).numpy()
array([0.6931472, 1.3132616], dtype=float32)"
"tf.nn.softsign(
    features, name=None
)
","[['Computes softsign: ', 'features / (abs(features) + 1)', '.']]",[]
"tf.space_to_batch(
    input, block_shape, paddings, name=None
)
",[],"x = [[[[1], [2]], [[3], [4]]]]
"
"tf.nn.space_to_depth(
    input, block_size, data_format='NHWC', name=None
)
",[],"x = [[[[1], [2]],
      [[3], [4]]]]
"
"tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels, logits, name=None
)
","[['Computes sparse softmax cross entropy between ', 'logits', ' and ', 'labels', '.']]","logits = tf.constant([[2., -5., .5, -.1],
                      [0., 0., 1.9, 1.4],
                      [-100., 100., -100., -100.]])
labels = tf.constant([0, 3, 1])
tf.nn.sparse_softmax_cross_entropy_with_logits(
    labels=labels, logits=logits).numpy()
array([0.29750752, 1.1448325 , 0.        ], dtype=float32)"
"tf.nn.sufficient_statistics(
    x, axes, shift=None, keepdims=False, name=None
)
","[['Calculate the sufficient statistics for the mean and variance of ', 'x', '.']]",[]
"tf.nn.silu(
    features, beta=1.0
)
","[['Computes the SiLU or Swish activation function: ', 'x * sigmoid(beta * x)', '.']]",[]
"tf.math.tanh(
    x, name=None
)
","[['Computes hyperbolic tangent of ', 'x', ' element-wise.']]",[]
"tf.math.top_k(
    input, k=1, sorted=True, name=None
)
","[['Finds values and indices of the ', 'k', ' largest entries for the last dimension.']]","result = tf.math.top_k([1, 2, 98, 1, 1, 99, 3, 1, 3, 96, 4, 1],
                        k=3)
result.values.numpy()
array([99, 98, 96], dtype=int32)
result.indices.numpy()
array([5, 2, 9], dtype=int32)"
"tf.nn.weighted_cross_entropy_with_logits(
    labels, logits, pos_weight, name=None
)
",[],"labels * -log(sigmoid(logits)) +
    (1 - labels) * -log(1 - sigmoid(logits))
"
"tf.nn.weighted_moments(
    x, axes, frequency_weights, keepdims=False, name=None
)
","[['Returns the frequency-weighted mean and variance of ', 'x', '.']]",[]
"tf.nn.with_space_to_batch(
    input,
    dilation_rate,
    padding,
    op,
    filter_shape=None,
    spatial_dims=None,
    data_format=None
)
","[['Performs ', 'op', ' on the space-to-batch representation of ', 'input', '.']]",[]
"tf.math.zero_fraction(
    value, name=None
)
","[['Returns the fraction of zeros in ', 'value', '.']]","    z = tf.nn.relu(...)
    summ = tf.compat.v1.summary.scalar('sparsity', tf.nn.zero_fraction(z))
"
"tf.no_gradient(
    op_type
)
","[['Specifies that ops of type ', 'op_type', ' is not differentiable.']]","tf.no_gradient(""Size"")
"
"tf.no_op(
    name=None
)
",[],[]
"tf.nondifferentiable_batch_function(
    num_batch_threads,
    max_batch_size,
    batch_timeout_micros,
    allowed_batch_sizes=None,
    max_enqueued_batches=10,
    autograph=True,
    enable_large_batch_splitting=True
)
",[],"@batch_function(1, 2, 3)
def layer(a):
  return tf.matmul(a, a)

b = layer(w)
"
"tf.norm(
    tensor, ord='euclidean', axis=None, keepdims=None, name=None
)
",[],[]
"tf.math.not_equal(
    x, y, name=None
)
",[],"x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.not_equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.numpy_function(
    func, inp, Tout, stateful=True, name=None
)
",[],"def my_numpy_func(x):
  # x will be a numpy array with the contents of the input to the
  # tf.function
  return np.sinh(x)
@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])
def tf_function(input):
  y = tf.numpy_function(my_numpy_func, [input], tf.float32)
  return y * y
tf_function(tf.constant(1.))
<tf.Tensor: shape=(), dtype=float32, numpy=1.3810978>"
"tf.one_hot(
    indices,
    depth,
    on_value=None,
    off_value=None,
    axis=None,
    dtype=None,
    name=None
)
",[],"  features x depth if axis == -1
  depth x features if axis == 0
"
"tf.ones(
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
",[],"tf.ones([3, 4], tf.int32)
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
array([[1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1]], dtype=int32)>"
"tf.ones_like(
    input, dtype=None, name=None
)
",[],"tensor = tf.constant([[1, 2, 3], [4, 5, 6]])
tf.ones_like(tensor)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
  array([[1, 1, 1],
         [1, 1, 1]], dtype=int32)>"
"tf.keras.optimizers.experimental.Adadelta(
    learning_rate=0.001,
    rho=0.95,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adadelta',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.experimental.Adagrad(
    learning_rate=0.001,
    initial_accumulator_value=0.1,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adagrad',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.experimental.Adamax(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adamax',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","m = 0  # Initialize initial 1st moment vector
u = 0  # Initialize the exponentially weighted infinity norm
t = 0  # Initialize timestep
"
"tf.keras.optimizers.experimental.Ftrl(
    learning_rate=0.001,
    learning_rate_power=-0.5,
    initial_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    l2_shrinkage_regularization_strength=0.0,
    beta=0.0,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Ftrl',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","n = 0
sigma = 0
z = 0
"
"tf.keras.optimizers.experimental.Nadam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Nadam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.Optimizer(
    name,
    weight_decay=0,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    **kwargs
)
",[],"# Create an optimizer with the desired parameters.
opt = tf.keras.optimizers.experimental.SGD(learning_rate=0.1)
var1, var2 = tf.Variable(1.0), tf.Variable(2.0)
# `loss` is a callable that takes no argument and returns the value
# to minimize.
loss = lambda: 3 * var1 * var1 + 2 * var2 * var2
# Call minimize to update the list of variables.
opt.minimize(loss, var_list=[var1, var2])
"
"tf.keras.optimizers.experimental.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=100,
    jit_compile=True,
    name='RMSprop',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2) / 2.0  # d(loss) / d(var1) = var1
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.optimizers.experimental.SGD(
    learning_rate=0.01,
    momentum=0.0,
    nesterov=False,
    amsgrad=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='SGD',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","w = w - learning_rate * g
"
"tf.keras.optimizers.deserialize(
    config, custom_objects=None, **kwargs
)
","[['Inverse of the ', 'serialize', ' function.']]",[]
"tf.keras.optimizers.experimental.Adadelta(
    learning_rate=0.001,
    rho=0.95,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adadelta',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.experimental.Adafactor(
    learning_rate=0.001,
    beta_2_decay=-0.8,
    epsilon_1=1e-30,
    epsilon_2=0.001,
    clip_threshold=1.0,
    relative_step=True,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adafactor',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.experimental.Adagrad(
    learning_rate=0.001,
    initial_accumulator_value=0.1,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adagrad',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.experimental.AdamW(
    learning_rate=0.001,
    weight_decay=0.004,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='AdamW',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.experimental.Adamax(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Adamax',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","m = 0  # Initialize initial 1st moment vector
u = 0  # Initialize the exponentially weighted infinity norm
t = 0  # Initialize timestep
"
"tf.keras.optimizers.experimental.Ftrl(
    learning_rate=0.001,
    learning_rate_power=-0.5,
    initial_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    l2_shrinkage_regularization_strength=0.0,
    beta=0.0,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Ftrl',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","n = 0
sigma = 0
z = 0
"
"tf.keras.optimizers.experimental.Nadam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='Nadam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_variable(
    shape, dtype=None, initializer='zeros', name=None
)
"
"tf.keras.optimizers.Optimizer(
    name,
    weight_decay=0,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    **kwargs
)
",[],"# Create an optimizer with the desired parameters.
opt = tf.keras.optimizers.experimental.SGD(learning_rate=0.1)
var1, var2 = tf.Variable(1.0), tf.Variable(2.0)
# `loss` is a callable that takes no argument and returns the value
# to minimize.
loss = lambda: 3 * var1 * var1 + 2 * var2 * var2
# Call minimize to update the list of variables.
opt.minimize(loss, var_list=[var1, var2])
"
"tf.keras.optimizers.experimental.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=100,
    jit_compile=True,
    name='RMSprop',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","opt = tf.keras.optimizers.experimental.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2) / 2.0  # d(loss) / d(var1) = var1
opt.minimize(loss, [var1])
var1.numpy()
9.683772"
"tf.keras.optimizers.experimental.SGD(
    learning_rate=0.01,
    momentum=0.0,
    nesterov=False,
    amsgrad=False,
    weight_decay=None,
    clipnorm=None,
    clipvalue=None,
    global_clipnorm=None,
    use_ema=False,
    ema_momentum=0.99,
    ema_overwrite_frequency=None,
    jit_compile=True,
    name='SGD',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","w = w - learning_rate * g
"
"tf.keras.optimizers.get(
    identifier, **kwargs
)
",[],[]
"tf.keras.optimizers.legacy.Adadelta(
    learning_rate=0.001,
    rho=0.95,
    epsilon=1e-07,
    name='Adadelta',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_slot(
    var, slot_name, initializer='zeros', shape=None
)
"
"tf.keras.optimizers.legacy.Adagrad(
    learning_rate=0.001,
    initial_accumulator_value=0.1,
    epsilon=1e-07,
    name='Adagrad',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_slot(
    var, slot_name, initializer='zeros', shape=None
)
"
"tf.keras.optimizers.legacy.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    name='Adam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1
step_count = opt.minimize(loss, [var1]).numpy()
# The first step is `-learning_rate*sign(grad)`
var1.numpy()
9.9"
"tf.keras.optimizers.legacy.Adamax(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    name='Adamax',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","m = 0  # Initialize initial 1st moment vector
v = 0  # Initialize the exponentially weighted infinity norm
t = 0  # Initialize timestep
"
"tf.keras.optimizers.legacy.Ftrl(
    learning_rate=0.001,
    learning_rate_power=-0.5,
    initial_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    name='Ftrl',
    l2_shrinkage_regularization_strength=0.0,
    beta=0.0,
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","n = 0
sigma = 0
z = 0
"
"tf.keras.optimizers.legacy.Nadam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    name='Nadam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]",">>> opt = tf.keras.optimizers.legacy.Nadam(learning_rate=0.2)
>>> var1 = tf.Variable(10.0)
>>> loss = lambda: (var1 ** 2) / 2.0
>>> step_count = opt.minimize(loss, [var1]).numpy()
>>> ""{:.1f}"".format(var1.numpy())
9.8
"
"tf.keras.optimizers.legacy.Optimizer(
    name, gradient_aggregator=None, gradient_transformers=None, **kwargs
)
",[],"# Create an optimizer with the desired parameters.
opt = tf.keras.optimizers.legacy.SGD(learning_rate=0.1)
# `loss` is a callable that takes no argument and returns the value
# to minimize.
var1 = tf.Variable(2.0)
var2 = tf.Variable(5.0)
loss = lambda: 3 * var1 * var1 + 2 * var2 * var2
# In graph mode, returns op that minimizes the loss by updating the listed
# variables.
opt_op = opt.minimize(loss, var_list=[var1, var2])
opt_op.run()
# In eager mode, simply call minimize to update the list of variables.
opt.minimize(loss, var_list=[var1, var2])
"
"tf.keras.optimizers.legacy.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    name='RMSprop',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2) / 2.0    # d(loss) / d(var1) = var1
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
9.683772"
"tf.keras.optimizers.legacy.SGD(
    learning_rate=0.01,
    momentum=0.0,
    nesterov=False,
    name='SGD',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","w = w - learning_rate * g
"
"tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate, decay_steps, alpha=0.0, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  step = min(step, decay_steps)
  cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))
  decayed = (1 - alpha) * cosine_decay + alpha
  return initial_learning_rate * decayed
"
"tf.keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate,
    first_decay_steps,
    t_mul=2.0,
    m_mul=1.0,
    alpha=0.0,
    name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","first_decay_steps = 1000
lr_decayed_fn = (
  tf.keras.optimizers.schedules.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps))
"
"tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  return initial_learning_rate * decay_rate ^ (step / decay_steps)
"
"tf.keras.optimizers.schedules.InverseTimeDecay(
    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  return initial_learning_rate / (1 + decay_rate * step / decay_step)
"
"tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","step = tf.Variable(0, trainable=False)
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values)

# Later, whenever we perform an optimization step, we pass in the step.
learning_rate = learning_rate_fn(step)
"
"tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate,
    decay_steps,
    end_learning_rate=0.0001,
    power=1.0,
    cycle=False,
    name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  step = min(step, decay_steps)
  return ((initial_learning_rate - end_learning_rate) *
          (1 - step / decay_steps) ^ (power)
         ) + end_learning_rate
"
"tf.keras.optimizers.schedules.deserialize(
    config, custom_objects=None
)
","[['Instantiates a ', 'LearningRateSchedule', ' object from a serialized form.']]","# Configuration for PolynomialDecay
config = {
  'class_name': 'PolynomialDecay',
  'config': {'cycle': False,
    'decay_steps': 10000,
    'end_learning_rate': 0.01,
    'initial_learning_rate': 0.1,
    'name': None,
    'power': 0.5} }
lr_schedule = tf.keras.optimizers.schedules.deserialize(config)
"
"tf.keras.optimizers.schedules.serialize(
    learning_rate_schedule
)
","[['Serializes a ', 'LearningRateSchedule', ' into a JSON-compatible representation.']]","lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
  0.1, decay_steps=100000, decay_rate=0.96, staircase=True)
tf.keras.optimizers.schedules.serialize(lr_schedule)
{'class_name': 'ExponentialDecay', 'config': {...} }"
"tf.keras.optimizers.serialize(
    optimizer
)
",[],"tf.keras.optimizers.serialize(tf.keras.optimizers.legacy.SGD())
{'class_name': 'SGD', 'config': {'name': 'SGD', 'learning_rate': 0.01,
                                 'decay': 0.0, 'momentum': 0.0,
                                 'nesterov': False} }"
"tf.pad(
    tensor, paddings, mode='CONSTANT', constant_values=0, name=None
)
",[],"t = tf.constant([[1, 2, 3], [4, 5, 6]])
paddings = tf.constant([[1, 1,], [2, 2]])
# 'constant_values' is 0.
# rank of 't' is 2.
tf.pad(t, paddings, ""CONSTANT"")  # [[0, 0, 0, 0, 0, 0, 0],
                                 #  [0, 0, 1, 2, 3, 0, 0],
                                 #  [0, 0, 4, 5, 6, 0, 0],
                                 #  [0, 0, 0, 0, 0, 0, 0]]

tf.pad(t, paddings, ""REFLECT"")  # [[6, 5, 4, 5, 6, 5, 4],
                                #  [3, 2, 1, 2, 3, 2, 1],
                                #  [6, 5, 4, 5, 6, 5, 4],
                                #  [3, 2, 1, 2, 3, 2, 1]]

tf.pad(t, paddings, ""SYMMETRIC"")  # [[2, 1, 1, 2, 3, 3, 2],
                                  #  [2, 1, 1, 2, 3, 3, 2],
                                  #  [5, 4, 4, 5, 6, 6, 5],
                                  #  [5, 4, 4, 5, 6, 6, 5]]
"
"tf.parallel_stack(
    values, name='parallel_stack'
)
","[['Stacks a list of rank-', 'R', ' tensors into one rank-', '(R+1)', ' tensor in parallel.']]","x = tf.constant([1, 4])
y = tf.constant([2, 5])
z = tf.constant([3, 6])
tf.parallel_stack([x, y, z])  # [[1, 4], [2, 5], [3, 6]]
"
"tf.math.pow(
    x, y, name=None
)
","[[None, '\n']]","x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]
"
"tf.print(
    *inputs, **kwargs
)
",[],"tensor = tf.range(10)
tf.print(tensor, output_stream=sys.stderr)
"
"tf.profiler.experimental.Profile(
    logdir, options=None
)
",[],"with tf.profiler.experimental.Profile(""/path/to/logdir""):
  # do some work
"
"tf.profiler.experimental.ProfilerOptions(
    host_tracer_level=2,
    python_tracer_level=0,
    device_tracer_level=1,
    delay_ms=None
)
",[],[]
"tf.profiler.experimental.Trace(
    name, **kwargs
)
",[],"tf.profiler.experimental.start('logdir')
for step in range(num_steps):
  # Creates a trace event for each training step with the step number.
  with tf.profiler.experimental.Trace(""Train"", step_num=step, _r=1):
    train_fn()
tf.profiler.experimental.stop()
"
"tf.profiler.experimental.client.monitor(
    service_addr, duration_ms, level=1
)
",[],"  # Continuously send gRPC requests to the Cloud TPU to monitor the model
  # execution.

  for query in range(0, 100):
    print(
      tf.profiler.experimental.client.monitor('grpc://10.0.0.2:8466', 1000))
"
"tf.profiler.experimental.client.trace(
    service_addr,
    logdir,
    duration_ms,
    worker_list='',
    num_tracing_attempts=3,
    options=None
)
","[[None, '\n']]","  # Start a profiler server before your model runs.
  tf.profiler.experimental.server.start(6009)
  # (Model code goes here).
  # Send gRPC request to the profiler server to collect a trace of your model.
  tf.profiler.experimental.client.trace('grpc://localhost:6009',
                                        '/nfs/tb_log', 2000)
"
"tf.profiler.experimental.server.start(
    port
)
",[],[]
"tf.profiler.experimental.start(
    logdir, options=None
)
",[],"options = tf.profiler.experimental.ProfilerOptions(host_tracer_level = 3,
                                                   python_tracer_level = 1,
                                                   device_tracer_level = 1)
tf.profiler.experimental.start('logdir_path', options = options)
# Training code here
tf.profiler.experimental.stop()
"
"tf.profiler.experimental.stop(
    save=True
)
",[],[]
"tf.py_function(
    func, inp, Tout, name=None
)
",[],"def log_huber(x, m):
  if tf.abs(x) <= m:
    return x**2
  else:
    return m**2 * (1 - 2 * tf.math.log(m) + tf.math.log(x**2))

x = tf.constant(1.0)
m = tf.constant(2.0)

with tf.GradientTape() as t:
  t.watch([x, m])
  y = tf.py_function(func=log_huber, inp=[x, m], Tout=tf.float32)

dy_dx = t.gradient(y, x)
assert dy_dx.numpy() == 2.0
"
"tf.quantization.dequantize(
    input,
    min_range,
    max_range,
    mode='MIN_COMBINED',
    name=None,
    axis=None,
    narrow_range=False,
    dtype=tf.dtypes.float32
)
",[],"if T == qint8: in[i] += (range(T) + 1)/ 2.0
out[i] = min_range + (in[i]* (max_range - min_range) / range(T))
"
"tf.quantization.fake_quant_with_min_max_args(
    inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_args_gradient(
    gradients, inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_vars(
    inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_vars_gradient(
    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_vars_per_channel(
    inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient(
    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.quantize(
    input,
    min_range,
    max_range,
    T,
    mode='MIN_COMBINED',
    round_mode='HALF_AWAY_FROM_ZERO',
    name=None,
    narrow_range=False,
    axis=None,
    ensure_minimum_range=0.01
)
",[],"out[i] = (in[i] - min_range) * range(T) / (max_range - min_range)
if T == qint8: out[i] -= (range(T) + 1) / 2.0
"
"tf.quantization.quantize_and_dequantize(
    input,
    input_min,
    input_max,
    signed_input=True,
    num_bits=8,
    range_given=False,
    round_mode='HALF_TO_EVEN',
    name=None,
    narrow_range=False,
    axis=None
)
",[],[]
"tf.quantization.quantize_and_dequantize_v2(
    input,
    input_min,
    input_max,
    signed_input=True,
    num_bits=8,
    range_given=False,
    round_mode='HALF_TO_EVEN',
    name=None,
    narrow_range=False,
    axis=None
)
",[],"def getQuantizeOp(input):
    input_tensor = tf.placeholder(tf.float32, shape=[4, 4])
    net = tf.quantization.quantize_and_dequantize(input,
                                                  input_min=min_threshold,
                                                  input_max=max_threshold,
                                                  range_given=True)

To simulate v1 behavior:

def testDecomposeQuantizeDequantize(self):
    def f(input_tensor):
      return tf.quantization.quantize_and_dequantize_v2(input_tensor,
                                                        input_min = 5.0,
                                                        input_max= -10.0,
                                                        range_given=True)
    input_tensor = tf.placeholder(tf.float32, shape=[4, 4])
    net = tf.grad_pass_through(f)(input_tensor)
"
"tf.quantization.quantized_concat(
    concat_dim, values, input_mins, input_maxes, name=None
)
",[],[]
"tf.queue.FIFOQueue(
    capacity,
    dtypes,
    shapes=None,
    names=None,
    shared_name=None,
    name='fifo_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.queue.PaddingFIFOQueue(
    capacity,
    dtypes,
    shapes,
    names=None,
    shared_name=None,
    name='padding_fifo_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.queue.PriorityQueue(
    capacity,
    types,
    shapes=None,
    names=None,
    shared_name=None,
    name='priority_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.queue.QueueBase(
    dtypes, shapes, names, queue_ref
)
",[],"close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.queue.RandomShuffleQueue(
    capacity,
    min_after_dequeue,
    dtypes,
    shapes=None,
    names=None,
    seed=None,
    shared_name=None,
    name='random_shuffle_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.ragged.boolean_mask(
    data, mask, name=None
)
","[['Applies a boolean mask to ', 'data', ' without flattening the mask dimensions.']]","# Aliases for True & False so data and mask line up.
T, F = (True, False)"
"tf.ragged.constant(
    pylist,
    dtype=None,
    ragged_rank=None,
    inner_shape=None,
    name=None,
    row_splits_dtype=tf.dtypes.int64
)
",[],"tf.ragged.constant([[1, 2], [3], [4, 5, 6]])
<tf.RaggedTensor [[1, 2], [3], [4, 5, 6]]>"
"tf.ragged.cross(
    inputs, name=None
)
",[],"tf.ragged.cross([tf.ragged.constant([['a'], ['b', 'c']]),
                 tf.ragged.constant([['d'], ['e']]),
                 tf.ragged.constant([['f'], ['g']])])
<tf.RaggedTensor [[b'a_X_d_X_f'], [b'b_X_e_X_g', b'c_X_e_X_g']]>"
"tf.ragged.cross_hashed(
    inputs, num_buckets=0, hash_key=None, name=None
)
",[],"tf.ragged.cross_hashed([tf.ragged.constant([['a'], ['b', 'c']]),
                        tf.ragged.constant([['d'], ['e']]),
                        tf.ragged.constant([['f'], ['g']])],
                       num_buckets=100)
<tf.RaggedTensor [[78], [66, 74]]>"
"tf.ragged.map_flat_values(
    op, *args, **kwargs
)
","[['Applies ', 'op', ' to the ', 'flat_values', ' of one or more RaggedTensors.']]","rt = tf.ragged.constant([[1, 2, 3], [], [4, 5], [6]])
tf.ragged.map_flat_values(tf.ones_like, rt)
<tf.RaggedTensor [[1, 1, 1], [], [1, 1], [1]]>
tf.ragged.map_flat_values(tf.multiply, rt, rt)
<tf.RaggedTensor [[1, 4, 9], [], [16, 25], [36]]>
tf.ragged.map_flat_values(tf.add, rt, 5)
<tf.RaggedTensor [[6, 7, 8], [], [9, 10], [11]]>"
"tf.ragged.range(
    starts,
    limits=None,
    deltas=1,
    dtype=None,
    name=None,
    row_splits_dtype=tf.dtypes.int64
)
","[['Returns a ', 'RaggedTensor', ' containing the specified sequences of numbers.']]","ragged.range(starts, limits, deltas)[i] ==
    tf.range(starts[i], limits[i], deltas[i])
"
"tf.ragged.row_splits_to_segment_ids(
    splits, name=None, out_type=None
)
","[['Generates the segmentation corresponding to a RaggedTensor ', 'row_splits', '.']]","print(tf.ragged.row_splits_to_segment_ids([0, 3, 3, 5, 6, 9]))
 tf.Tensor([0 0 0 2 2 3 4 4 4], shape=(9,), dtype=int64)"
"tf.ragged.segment_ids_to_row_splits(
    segment_ids, num_segments=None, out_type=None, name=None
)
","[['Generates the RaggedTensor ', 'row_splits', ' corresponding to a segmentation.']]","print(tf.ragged.segment_ids_to_row_splits([0, 0, 0, 2, 2, 3, 4, 4, 4]))
tf.Tensor([0 3 3 5 6 9], shape=(6,), dtype=int64)"
"tf.ragged.stack(
    values: typing.List[ragged_tensor.RaggedOrDense], axis=0, name=None
)
","[['Stacks a list of rank-', 'R', ' tensors into one rank-', '(R+1)', ' ', 'RaggedTensor', '.']]","# Stacking two ragged tensors.
t1 = tf.ragged.constant([[1, 2], [3, 4, 5]])
t2 = tf.ragged.constant([[6], [7, 8, 9]])
tf.ragged.stack([t1, t2], axis=0)
<tf.RaggedTensor [[[1, 2], [3, 4, 5]], [[6], [7, 8, 9]]]>
tf.ragged.stack([t1, t2], axis=1)
<tf.RaggedTensor [[[1, 2], [6]], [[3, 4, 5], [7, 8, 9]]]>"
"tf.ragged.stack_dynamic_partitions(
    data, partitions, num_partitions, name=None
)
",[],"data           = ['a', 'b', 'c', 'd', 'e']
partitions     = [  3,   0,   2,   2,   3]
num_partitions = 5
tf.ragged.stack_dynamic_partitions(data, partitions, num_partitions)
<tf.RaggedTensor [[b'b'], [], [b'c', b'd'], [b'a', b'e'], []]>"
"tf.random.Generator(
    copy_from=None, state=None, alg=None
)
",[],"g = tf.random.Generator.from_seed(1234)
g.normal(shape=(2, 3))
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.9356609 ,  1.0854305 , -0.93788373],
       [-0.5061547 ,  1.3169702 ,  0.7137579 ]], dtype=float32)>"
"tf.random.all_candidate_sampler(
    true_classes, num_true, num_sampled, unique, seed=None, name=None
)
",[],[]
"tf.random.categorical(
    logits, num_samples, dtype=None, seed=None, name=None
)
",[],"# samples has shape [1, 5], where each value is either 0 or 1 with equal
# probability.
samples = tf.random.categorical(tf.math.log([[0.5, 0.5]]), 5)
"
"tf.random.create_rng_state(
    seed, alg
)
",[],"tf.random.create_rng_state(
    1234, ""philox"")
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1234,    0,    0])>
tf.random.create_rng_state(
    [12, 34], ""threefry"")
<tf.Tensor: shape=(2,), dtype=int64, numpy=array([12, 34])>"
"tf.random.Generator(
    copy_from=None, state=None, alg=None
)
",[],"g = tf.random.Generator.from_seed(1234)
g.normal(shape=(2, 3))
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.9356609 ,  1.0854305 , -0.93788373],
       [-0.5061547 ,  1.3169702 ,  0.7137579 ]], dtype=float32)>"
"tf.random.create_rng_state(
    seed, alg
)
",[],"tf.random.create_rng_state(
    1234, ""philox"")
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1234,    0,    0])>
tf.random.create_rng_state(
    [12, 34], ""threefry"")
<tf.Tensor: shape=(2,), dtype=int64, numpy=array([12, 34])>"
"tf.random.experimental.index_shuffle(
    index, seed, max_index
)
","[['Outputs the position of ', 'index', ' in a permutation of [0, ..., max_index].']]","vector = tf.constant(['e0', 'e1', 'e2', 'e3'])
indices = tf.random.experimental.index_shuffle(tf.range(4), [5, 9], 3)
shuffled_vector = tf.gather(vector, indices)
print(shuffled_vector)
tf.Tensor([b'e2' b'e0' b'e1' b'e3'], shape=(4,), dtype=string)"
"tf.random.set_global_generator(
    generator
)
","[['Replaces the global generator with another ', 'Generator', ' object.']]","rng = tf.random.get_global_generator()
rng.reset_from_seed(30)"
"tf.random.experimental.stateless_fold_in(
    seed, data, alg='auto_select'
)
",[],"master_seed = [1, 2]
replica_id = 3
replica_seed = tf.random.experimental.stateless_fold_in(
  master_seed, replica_id)
print(replica_seed)
tf.Tensor([1105988140          3], shape=(2,), dtype=int32)
tf.random.stateless_normal(shape=[3], seed=replica_seed)
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.03197195, 0.8979765 ,
0.13253039], dtype=float32)>"
"tf.random.experimental.stateless_shuffle(
    value, seed, alg='auto_select', name=None
)
",[],"[[1, 2],       [[5, 6],
 [3, 4],  ==>   [1, 2],
 [5, 6]]        [3, 4]]
"
"tf.random.experimental.stateless_split(
    seed, num=2, alg='auto_select'
)
","[['Splits an RNG seed into ', 'num', ' new seeds by adding a leading axis.']]","seed = [1, 2]
new_seeds = tf.random.experimental.stateless_split(seed, num=3)
print(new_seeds)
tf.Tensor(
[[1105988140 1738052849]
 [-335576002  370444179]
 [  10670227 -246211131]], shape=(3, 2), dtype=int32)
tf.random.stateless_normal(shape=[3], seed=new_seeds[0, :])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.59835213, -0.9578608 ,
0.9002807 ], dtype=float32)>"
"tf.random.fixed_unigram_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    vocab_file='',
    distortion=1.0,
    num_reserved_ids=0,
    num_shards=1,
    shard=0,
    unigrams=(),
    seed=None,
    name=None
)
",[],[]
"tf.random.gamma(
    shape,
    alpha,
    beta=None,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","[['Draws ', 'shape', ' samples from each of the given Gamma distribution(s).']]","samples = tf.random.gamma([10], [0.5, 1.5])
# samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents
# the samples drawn from each distribution

samples = tf.random.gamma([7, 5], [0.5, 1.5])
# samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1]
# represents the 7x5 samples drawn from each of the two distributions

alpha = tf.constant([[1.],[3.],[5.]])
beta = tf.constant([[3., 4.]])
samples = tf.random.gamma([30], alpha=alpha, beta=beta)
# samples has shape [30, 3, 2], with 30 samples each of 3x2 distributions.

loss = tf.reduce_mean(tf.square(samples))
dloss_dalpha, dloss_dbeta = tf.gradients(loss, [alpha, beta])
# unbiased stochastic derivatives of the loss function
alpha.shape == dloss_dalpha.shape  # True
beta.shape == dloss_dbeta.shape  # True
"
"tf.random.learned_unigram_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=None,
    name=None
)
",[],[]
"tf.random.log_uniform_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=None,
    name=None
)
",[],[]
"tf.random.normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
",[],"tf.random.set_seed(5);
tf.random.normal([4], 0, 1, tf.float32)
<tf.Tensor: shape=(4,), dtype=float32, numpy=..., dtype=float32)>"
"tf.random.poisson(
    shape,
    lam,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","[['Draws ', 'shape', ' samples from each of the given Poisson distribution(s).']]","samples = tf.random.poisson([10], [0.5, 1.5])
# samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents
# the samples drawn from each distribution

samples = tf.random.poisson([7, 5], [12.2, 3.3])
# samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1]
# represents the 7x5 samples drawn from each of the two distributions
"
"tf.random.set_global_generator(
    generator
)
","[['Replaces the global generator with another ', 'Generator', ' object.']]","rng = tf.random.get_global_generator()
rng.reset_from_seed(30)"
"tf.random.set_seed(
    seed
)
",[],"print(tf.random.uniform([1]))  # generates 'A1'
print(tf.random.uniform([1]))  # generates 'A2'
"
"tf.random.shuffle(
    value, seed=None, name=None
)
",[],"[[1, 2],       [[5, 6],
 [3, 4],  ==>   [1, 2],
 [5, 6]]        [3, 4]]
"
"tf.random.stateless_binomial(
    shape,
    seed,
    counts,
    probs,
    output_dtype=tf.dtypes.int32,
    name=None
)
",[],"counts = [10., 20.]
# Probability of success.
probs = [0.8]

binomial_samples = tf.random.stateless_binomial(
    shape=[2], seed=[123, 456], counts=counts, probs=probs)

counts = ... # Shape [3, 1, 2]
probs = ...  # Shape [1, 4, 2]
shape = [3, 4, 3, 4, 2]
# Sample shape will be [3, 4, 3, 4, 2]
binomial_samples = tf.random.stateless_binomial(
    shape=shape, seed=[123, 456], counts=counts, probs=probs)
"
"tf.random.stateless_categorical(
    logits,
    num_samples,
    seed,
    dtype=tf.dtypes.int64,
    name=None
)
",[],"# samples has shape [1, 5], where each value is either 0 or 1 with equal
# probability.
samples = tf.random.stateless_categorical(
    tf.math.log([[0.5, 0.5]]), 5, seed=[7, 17])
"
"tf.random.stateless_gamma(
    shape,
    seed,
    alpha,
    beta=None,
    dtype=tf.dtypes.float32,
    name=None
)
",[],"samples = tf.random.stateless_gamma([10, 2], seed=[12, 34], alpha=[0.5, 1.5])
# samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents
# the samples drawn from each distribution

samples = tf.random.stateless_gamma([7, 5, 2], seed=[12, 34], alpha=[.5, 1.5])
# samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1]
# represents the 7x5 samples drawn from each of the two distributions

alpha = tf.constant([[1.], [3.], [5.]])
beta = tf.constant([[3., 4.]])
samples = tf.random.stateless_gamma(
    [30, 3, 2], seed=[12, 34], alpha=alpha, beta=beta)
# samples has shape [30, 3, 2], with 30 samples each of 3x2 distributions.

with tf.GradientTape() as tape:
  tape.watch([alpha, beta])
  loss = tf.reduce_mean(tf.square(tf.random.stateless_gamma(
      [30, 3, 2], seed=[12, 34], alpha=alpha, beta=beta)))
dloss_dalpha, dloss_dbeta = tape.gradient(loss, [alpha, beta])
# unbiased stochastic derivatives of the loss function
alpha.shape == dloss_dalpha.shape  # True
beta.shape == dloss_dbeta.shape  # True
"
"tf.random.stateless_normal(
    shape,
    seed,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    name=None,
    alg='auto_select'
)
",[],[]
"tf.random.stateless_parameterized_truncated_normal(
    shape, seed, means=0.0, stddevs=1.0, minvals=-2.0, maxvals=2.0, name=None
)
",[],"means = 0.
stddevs = tf.math.exp(tf.random.uniform(shape=[2, 3]))
minvals = [-1., -2., -1000.]
maxvals = [[10000.], [1.]]
y = tf.random.stateless_parameterized_truncated_normal(
  shape=[10, 2, 3], seed=[7, 17],
  means=means, stddevs=stddevs, minvals=minvals, maxvals=maxvals)
y.shape
TensorShape([10, 2, 3])"
"tf.random.stateless_poisson(
    shape,
    seed,
    lam,
    dtype=tf.dtypes.int32,
    name=None
)
",[],"samples = tf.random.stateless_poisson([10, 2], seed=[12, 34], lam=[5, 15])
# samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents
# the samples drawn from each distribution

samples = tf.random.stateless_poisson([7, 5, 2], seed=[12, 34], lam=[5, 15])
# samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1]
# represents the 7x5 samples drawn from each of the two distributions

rate = tf.constant([[1.], [3.], [5.]])
samples = tf.random.stateless_poisson([30, 3, 1], seed=[12, 34], lam=rate)
# samples has shape [30, 3, 1], with 30 samples each of 3x1 distributions.
"
"tf.random.stateless_truncated_normal(
    shape,
    seed,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    name=None,
    alg='auto_select'
)
",[],[]
"tf.random.stateless_uniform(
    shape,
    seed,
    minval=0,
    maxval=None,
    dtype=tf.dtypes.float32,
    name=None,
    alg='auto_select'
)
",[],"ints = tf.random.stateless_uniform(
    [10], seed=(2, 3), minval=None, maxval=None, dtype=tf.int32)
"
"tf.random.truncated_normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
",[],"tf.random.truncated_normal(shape=[2])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>"
"tf.random.uniform(
    shape,
    minval=0,
    maxval=None,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
",[],"tf.random.uniform(shape=[2])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>
tf.random.uniform(shape=[], minval=-1., maxval=0.)
<tf.Tensor: shape=(), dtype=float32, numpy=-...>
tf.random.uniform(shape=[], minval=5, maxval=10, dtype=tf.int64)
<tf.Tensor: shape=(), dtype=int64, numpy=...>"
"tf.random.uniform_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=None,
    name=None
)
",[],[]
"tf.random_index_shuffle(
    index, seed, max_index, rounds=4, name=None
)
","[['Outputs the position of ', 'value', ' in a permutation of [0, ..., max_index].']]",[]
"tf.random_normal_initializer(
    mean=0.0, stddev=0.05, seed=None
)
",[],"def make_variables(k, initializer):
  return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),
          tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))
v1, v2 = make_variables(3,
                        tf.random_normal_initializer(mean=1., stddev=2.))
v1
<tf.Variable ... shape=(3,) ... numpy=array([...], dtype=float32)>
v2
<tf.Variable ... shape=(3, 3) ... numpy=
make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ..."
"tf.random_uniform_initializer(
    minval=-0.05, maxval=0.05, seed=None
)
",[],"def make_variables(k, initializer):
  return (tf.Variable(initializer(shape=[k], dtype=tf.float32)),
          tf.Variable(initializer(shape=[k, k], dtype=tf.float32)))
v1, v2 = make_variables(3, tf.ones_initializer())
v1
<tf.Variable ... shape=(3,) ... numpy=array([1., 1., 1.], dtype=float32)>
v2
<tf.Variable ... shape=(3, 3) ... numpy=
array([[1., 1., 1.],
       [1., 1., 1.],
       [1., 1., 1.]], dtype=float32)>
make_variables(4, tf.random_uniform_initializer(minval=-1., maxval=1.))
(<tf.Variable...shape=(4,) dtype=float32...>, <tf.Variable...shape=(4, 4) ..."
"tf.rank(
    input, name=None
)
",[],"# shape of tensor 't' is [2, 2, 3]
t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])
tf.rank(t)  # 3
"
"tf.raw_ops.Abort(
    error_msg='', exit_without_error=False, name=None
)
",[],[]
"tf.raw_ops.Abs(
    x, name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.AccumulateNV2(
    inputs, shape, name=None
)
",[],[]
"tf.raw_ops.AccumulatorApplyGradient(
    handle, local_step, gradient, name=None
)
",[],[]
"tf.raw_ops.AccumulatorNumAccumulated(
    handle, name=None
)
",[],[]
"tf.raw_ops.AccumulatorSetGlobalStep(
    handle, new_global_step, name=None
)
",[],[]
"tf.raw_ops.AccumulatorTakeGradient(
    handle, num_required, dtype, name=None
)
",[],[]
"tf.raw_ops.Acos(
    x, name=None
)
",[],[]
"tf.raw_ops.Acosh(
    x, name=None
)
",[],"x = tf.constant([-2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
tf.math.acosh(x) ==> [nan nan 0. 0.62236255 5.9914584 9.903487 inf]
"
"tf.raw_ops.Add(
    x, y, name=None
)
",[],[]
"tf.raw_ops.AddManySparseToTensorsMap(
    sparse_indices,
    sparse_values,
    sparse_shape,
    container='',
    shared_name='',
    name=None
)
","[['Add an ', 'N', '-minibatch ', 'SparseTensor', ' to a ', 'SparseTensorsMap', ', return ', 'N', ' handles.']]",[]
"tf.raw_ops.AddN(
    inputs, name=None
)
",[],"  x = [9, 7, 10]
  tf.math.add_n(x) ==> 26
"
"tf.raw_ops.AddSparseToTensorsMap(
    sparse_indices,
    sparse_values,
    sparse_shape,
    container='',
    shared_name='',
    name=None
)
","[['Add a ', 'SparseTensor', ' to a ', 'SparseTensorsMap', ' return its handle.']]",[]
"tf.raw_ops.AddV2(
    x, y, name=None
)
",[],[]
"tf.raw_ops.AdjustContrast(
    images, contrast_factor, min_value, max_value, name=None
)
",[],[]
"tf.raw_ops.AdjustContrastv2(
    images, contrast_factor, name=None
)
",[],[]
"tf.raw_ops.AdjustHue(
    images, delta, name=None
)
",[],[]
"tf.raw_ops.AdjustSaturation(
    images, scale, name=None
)
",[],[]
"tf.raw_ops.All(
    input, axis, keep_dims=False, name=None
)
",[],[]
"tf.raw_ops.AllCandidateSampler(
    true_classes, num_true, num_sampled, unique, seed=0, seed2=0, name=None
)
",[],[]
"tf.raw_ops.AllToAll(
    input,
    group_assignment,
    concat_dimension,
    split_dimension,
    split_count,
    name=None
)
",[],[]
"tf.raw_ops.Angle(
    input,
    Tout=tf.dtypes.float32,
    name=None
)
","[[None, '\n']]","# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]
tf.angle(input) ==> [2.0132, 1.056]
"
"tf.raw_ops.AnonymousHashTable(
    key_dtype, value_dtype, name=None
)
",[],[]
"tf.raw_ops.AnonymousIterator(
    output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.AnonymousIteratorV2(
    output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.AnonymousIteratorV3(
    output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.AnonymousMemoryCache(
    name=None
)
",[],[]
"tf.raw_ops.AnonymousMultiDeviceIterator(
    devices, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.AnonymousMultiDeviceIteratorV3(
    devices, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.AnonymousMutableDenseHashTable(
    empty_key,
    deleted_key,
    value_dtype,
    value_shape=[],
    initial_num_buckets=131072,
    max_load_factor=0.8,
    name=None
)
",[],[]
"tf.raw_ops.AnonymousMutableHashTable(
    key_dtype, value_dtype, name=None
)
",[],[]
"tf.raw_ops.AnonymousMutableHashTableOfTensors(
    key_dtype, value_dtype, value_shape=[], name=None
)
",[],[]
"tf.raw_ops.AnonymousRandomSeedGenerator(
    seed, seed2, name=None
)
",[],[]
"tf.raw_ops.AnonymousSeedGenerator(
    seed, seed2, reshuffle, name=None
)
",[],[]
"tf.raw_ops.Any(
    input, axis, keep_dims=False, name=None
)
",[],[]
"tf.raw_ops.ApplyAdaMax(
    var,
    m,
    v,
    beta1_power,
    lr,
    beta1,
    beta2,
    epsilon,
    grad,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.ApplyAdadelta(
    var,
    accum,
    accum_update,
    lr,
    rho,
    epsilon,
    grad,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.ApplyAdagrad(
    var, accum, lr, grad, use_locking=False, update_slots=True, name=None
)
",[],[]
"tf.raw_ops.ApplyAdagradDA(
    var,
    gradient_accumulator,
    gradient_squared_accumulator,
    grad,
    lr,
    l1,
    l2,
    global_step,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.ApplyAdagradV2(
    var,
    accum,
    lr,
    epsilon,
    grad,
    use_locking=False,
    update_slots=True,
    name=None
)
",[],[]
"tf.raw_ops.ApplyAdam(
    var,
    m,
    v,
    beta1_power,
    beta2_power,
    lr,
    beta1,
    beta2,
    epsilon,
    grad,
    use_locking=False,
    use_nesterov=False,
    name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.ApplyAddSign(
    var, m, lr, alpha, sign_decay, beta, grad, use_locking=False, name=None
)
",[],[]
"tf.raw_ops.ApplyCenteredRMSProp(
    var,
    mg,
    ms,
    mom,
    lr,
    rho,
    momentum,
    epsilon,
    grad,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.ApplyFtrl(
    var,
    accum,
    linear,
    grad,
    lr,
    l1,
    l2,
    lr_power,
    use_locking=False,
    multiply_linear_by_lr=False,
    name=None
)
",[],[]
"tf.raw_ops.ApplyFtrlV2(
    var,
    accum,
    linear,
    grad,
    lr,
    l1,
    l2,
    l2_shrinkage,
    lr_power,
    use_locking=False,
    multiply_linear_by_lr=False,
    name=None
)
",[],[]
"tf.raw_ops.ApplyGradientDescent(
    var, alpha, delta, use_locking=False, name=None
)
",[],[]
"tf.raw_ops.ApplyMomentum(
    var,
    accum,
    lr,
    grad,
    momentum,
    use_locking=False,
    use_nesterov=False,
    name=None
)
",[],[]
"tf.raw_ops.ApplyPowerSign(
    var, m, lr, logbase, sign_decay, beta, grad, use_locking=False, name=None
)
",[],[]
"tf.raw_ops.ApplyProximalAdagrad(
    var, accum, lr, l1, l2, grad, use_locking=False, name=None
)
","[[""Update '"", ""var' and '"", ""accum' according to FOBOS with Adagrad learning rate.""]]",[]
"tf.raw_ops.ApplyProximalGradientDescent(
    var, alpha, l1, l2, delta, use_locking=False, name=None
)
",[],[]
"tf.raw_ops.ApplyRMSProp(
    var,
    ms,
    mom,
    lr,
    rho,
    momentum,
    epsilon,
    grad,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.ApproxTopK(
    input,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    is_max_k=True,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
",[],[]
"tf.raw_ops.ApproximateEqual(
    x, y, tolerance=1e-05, name=None
)
",[],[]
"tf.raw_ops.ArgMax(
    input,
    dimension,
    output_type=tf.dtypes.int64,
    name=None
)
",[],"import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmax(input = a)
c = tf.keras.backend.eval(b)
# c = 4
# here a[4] = 166.32 which is the largest element of a across axis 0
"
"tf.raw_ops.ArgMin(
    input,
    dimension,
    output_type=tf.dtypes.int64,
    name=None
)
",[],"import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmin(input = a)
c = tf.keras.backend.eval(b)
# c = 0
# here a[0] = 1 which is the smallest element of a across axis 0
"
"tf.raw_ops.AsString(
    input,
    precision=-1,
    scientific=False,
    shortest=False,
    width=-1,
    fill='',
    name=None
)
",[],"tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)"
"tf.raw_ops.Asin(
    x, name=None
)
",[],"# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]
x = tf.constant([1.047, 0.785])
y = tf.math.sin(x) # [0.8659266, 0.7068252]

tf.math.asin(y) # [1.047, 0.785] = x
"
"tf.raw_ops.Asinh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.asinh(x) ==> [-inf -1.4436355 -0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf]
"
"tf.raw_ops.Assert(
    condition, data, summarize=3, name=None
)
",[],[]
"tf.raw_ops.AssertCardinalityDataset(
    input_dataset, cardinality, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.AssertNextDataset(
    input_dataset, transformations, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.AssertPrevDataset(
    input_dataset, transformations, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.Assign(
    ref, value, validate_shape=True, use_locking=True, name=None
)
",[],[]
"tf.raw_ops.AssignAdd(
    ref, value, use_locking=False, name=None
)
",[],[]
"tf.raw_ops.AssignAddVariableOp(
    resource, value, name=None
)
",[],[]
"tf.raw_ops.AssignSub(
    ref, value, use_locking=False, name=None
)
",[],[]
"tf.raw_ops.AssignSubVariableOp(
    resource, value, name=None
)
",[],[]
"tf.raw_ops.AssignVariableOp(
    resource, value, validate_shape=False, name=None
)
",[],[]
"tf.raw_ops.AssignVariableXlaConcatND(
    resource, inputs, num_concats, paddings=[], name=None
)
",[],"[[0, 1],
 [4, 5]]
[[2, 3],
 [6, 7]]
[[8, 9],
 [12, 13]]
[[10, 11],
 [14, 15]]
"
"tf.raw_ops.Atan(
    x, name=None
)
",[],"# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]
x = tf.constant([1.047, 0.785])
y = tf.math.tan(x) # [1.731261, 0.99920404]

tf.math.atan(y) # [1.047, 0.785] = x
"
"tf.raw_ops.Atan2(
    y, x, name=None
)
","[[None, '\n'], ['Computes arctangent of ', 'y/x', ' element-wise, respecting signs of the arguments.']]","x = [1., 1.]
y = [1., -1.]
print((tf.math.atan2(y,x) * (180 / np.pi)).numpy())
[ 45. -45.]"
"tf.raw_ops.Atanh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -1, -0.5, 1, 0, 0.5, 10, float(""inf"")])
  tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]
"
"tf.raw_ops.AudioSpectrogram(
    input, window_size, stride, magnitude_squared=False, name=None
)
",[],[]
"tf.raw_ops.AudioSummary(
    tag, tensor, sample_rate, max_outputs=3, name=None
)
","[['Outputs a ', 'Summary', ' protocol buffer with audio.']]",[]
"tf.raw_ops.AudioSummaryV2(
    tag, tensor, sample_rate, max_outputs=3, name=None
)
","[['Outputs a ', 'Summary', ' protocol buffer with audio.']]",[]
"tf.raw_ops.AutoShardDataset(
    input_dataset,
    num_workers,
    index,
    output_types,
    output_shapes,
    auto_shard_policy=0,
    num_replicas=0,
    name=None
)
",[],[]
"tf.raw_ops.AvgPool(
    value, ksize, strides, padding, data_format='NHWC', name=None
)
",[],[]
"tf.raw_ops.AvgPool3D(
    input, ksize, strides, padding, data_format='NDHWC', name=None
)
",[],[]
"tf.raw_ops.AvgPool3DGrad(
    orig_input_shape,
    grad,
    ksize,
    strides,
    padding,
    data_format='NDHWC',
    name=None
)
",[],[]
"tf.raw_ops.AvgPoolGrad(
    orig_input_shape,
    grad,
    ksize,
    strides,
    padding,
    data_format='NHWC',
    name=None
)
",[],[]
"tf.raw_ops.BandedTriangularSolve(
    matrix, rhs, lower=True, adjoint=False, name=None
)
",[],[]
"tf.raw_ops.Barrier(
    component_types,
    shapes=[],
    capacity=-1,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.BarrierClose(
    handle, cancel_pending_enqueues=False, name=None
)
",[],[]
"tf.raw_ops.BarrierIncompleteSize(
    handle, name=None
)
",[],[]
"tf.raw_ops.BarrierInsertMany(
    handle, keys, values, component_index, name=None
)
",[],[]
"tf.raw_ops.BarrierReadySize(
    handle, name=None
)
",[],[]
"tf.raw_ops.BarrierTakeMany(
    handle,
    num_elements,
    component_types,
    allow_small_batch=False,
    wait_for_incomplete=False,
    timeout_ms=-1,
    name=None
)
",[],[]
"tf.raw_ops.Batch(
    in_tensors,
    num_batch_threads,
    max_batch_size,
    batch_timeout_micros,
    grad_timeout_micros,
    max_enqueued_batches=10,
    allowed_batch_sizes=[],
    container='',
    shared_name='',
    batching_queue='',
    name=None
)
",[],[]
"tf.raw_ops.BatchCholesky(
    input, name=None
)
",[],[]
"tf.raw_ops.BatchCholeskyGrad(
    l, grad, name=None
)
",[],[]
"tf.raw_ops.BatchDataset(
    input_dataset,
    batch_size,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset that batches ', 'batch_size', ' elements from ', 'input_dataset', '.']]",[]
"tf.raw_ops.BatchDatasetV2(
    input_dataset,
    batch_size,
    drop_remainder,
    output_types,
    output_shapes,
    parallel_copy=False,
    metadata='',
    name=None
)
","[['Creates a dataset that batches ', 'batch_size', ' elements from ', 'input_dataset', '.']]",[]
"tf.raw_ops.BatchFFT(
    input, name=None
)
",[],[]
"tf.raw_ops.BatchFFT2D(
    input, name=None
)
",[],[]
"tf.raw_ops.BatchFFT3D(
    input, name=None
)
",[],[]
"tf.raw_ops.BatchFunction(
    in_tensors,
    captured_tensors,
    f,
    num_batch_threads,
    max_batch_size,
    batch_timeout_micros,
    Tout,
    max_enqueued_batches=10,
    allowed_batch_sizes=[],
    container='',
    shared_name='',
    batching_queue='',
    enable_large_batch_splitting=False,
    name=None
)
",[],"
  # This input will be captured.
  y = tf.placeholder_with_default(1.0, shape=[])

  @tf.Defun(tf.float32)
  def computation(a):
    return tf.matmul(a, a) + y

  b = gen_batch_ops.batch_function(
          f=computation
          in_tensors=[a],
          captured_tensors=computation.captured_inputs,
          Tout=[o.type for o in computation.definition.signature.output_arg],
          num_batch_threads=1,
          max_batch_size=10,
          batch_timeout_micros=100000,  # 100ms
          allowed_batch_sizes=[3, 10],
          batching_queue="""")
"
"tf.raw_ops.BatchIFFT(
    input, name=None
)
",[],[]
"tf.raw_ops.BatchIFFT2D(
    input, name=None
)
",[],[]
"tf.raw_ops.BatchIFFT3D(
    input, name=None
)
",[],[]
"tf.raw_ops.BatchMatMul(
    x, y, adj_x=False, adj_y=False, name=None
)
",[],"r_o = c_x if adj_x else r_x
c_o = r_y if adj_y else c_y
"
"tf.raw_ops.BatchMatMulV2(
    x, y, adj_x=False, adj_y=False, name=None
)
",[],"r_o = c_x if adj_x else r_x
c_o = r_y if adj_y else c_y
"
"tf.raw_ops.BatchMatMulV3(
    x, y, Tout, adj_x=False, adj_y=False, name=None
)
",[],"r_o = c_x if adj_x else r_x
c_o = r_y if adj_y else c_y
"
"tf.raw_ops.BatchMatrixBandPart(
    input, num_lower, num_upper, name=None
)
",[],[]
"tf.raw_ops.BatchMatrixDeterminant(
    input, name=None
)
",[],[]
"tf.raw_ops.BatchMatrixDiag(
    diagonal, name=None
)
",[],[]
"tf.raw_ops.BatchMatrixDiagPart(
    input, name=None
)
",[],[]
"tf.raw_ops.BatchMatrixInverse(
    input, adjoint=False, name=None
)
",[],[]
"tf.raw_ops.BatchMatrixSetDiag(
    input, diagonal, name=None
)
",[],[]
"tf.raw_ops.BatchMatrixSolve(
    matrix, rhs, adjoint=False, name=None
)
",[],[]
"tf.raw_ops.BatchMatrixSolveLs(
    matrix, rhs, l2_regularizer, fast=True, name=None
)
",[],[]
"tf.raw_ops.BatchMatrixTriangularSolve(
    matrix, rhs, lower=True, adjoint=False, name=None
)
",[],[]
"tf.raw_ops.BatchNormWithGlobalNormalization(
    t,
    m,
    v,
    beta,
    gamma,
    variance_epsilon,
    scale_after_normalization,
    name=None
)
",[],[]
"tf.raw_ops.BatchNormWithGlobalNormalizationGrad(
    t,
    m,
    v,
    gamma,
    backprop,
    variance_epsilon,
    scale_after_normalization,
    name=None
)
",[],[]
"tf.raw_ops.BatchSelfAdjointEig(
    input, name=None
)
",[],[]
"tf.raw_ops.BatchSelfAdjointEigV2(
    input, compute_v=True, name=None
)
",[],[]
"tf.raw_ops.BatchSvd(
    input, compute_uv=True, full_matrices=False, name=None
)
",[],[]
"tf.raw_ops.BatchToSpace(
    input, crops, block_size, name=None
)
",[],"crops = [[crop_top, crop_bottom], [crop_left, crop_right]]
"
"tf.raw_ops.BatchToSpaceND(
    input, block_shape, crops, name=None
)
",[],"[[[[1]]], [[[2]]], [[[3]]], [[[4]]]]
"
"tf.raw_ops.BesselI0(
    x, name=None
)
",[],[]
"tf.raw_ops.BesselI0e(
    x, name=None
)
",[],[]
"tf.raw_ops.BesselI1(
    x, name=None
)
",[],[]
"tf.raw_ops.BesselI1e(
    x, name=None
)
",[],[]
"tf.raw_ops.BesselJ0(
    x, name=None
)
",[],[]
"tf.raw_ops.BesselJ1(
    x, name=None
)
",[],[]
"tf.raw_ops.BesselK0(
    x, name=None
)
",[],[]
"tf.raw_ops.BesselK0e(
    x, name=None
)
",[],[]
"tf.raw_ops.BesselK1(
    x, name=None
)
",[],[]
"tf.raw_ops.BesselK1e(
    x, name=None
)
",[],[]
"tf.raw_ops.BesselY0(
    x, name=None
)
",[],[]
"tf.raw_ops.BesselY1(
    x, name=None
)
",[],[]
"tf.raw_ops.Betainc(
    a, b, x, name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.BiasAdd(
    value, bias, data_format='NHWC', name=None
)
","[['Adds ', 'bias', ' to ', 'value', '.']]",[]
"tf.raw_ops.BiasAddGrad(
    out_backprop, data_format='NHWC', name=None
)
",[],[]
"tf.raw_ops.BiasAddV1(
    value, bias, name=None
)
","[['Adds ', 'bias', ' to ', 'value', '.']]",[]
"tf.raw_ops.Bincount(
    arr, size, weights, name=None
)
",[],[]
"tf.raw_ops.Bitcast(
    input, type, name=None
)
",[],"a = [1., 2., 3.]
equality_bitcast = tf.bitcast(a, tf.complex128)
Traceback (most recent call last):
InvalidArgumentError: Cannot bitcast from 1 to 18 [Op:Bitcast]
equality_cast = tf.cast(a, tf.complex128)
print(equality_cast)
tf.Tensor([1.+0.j 2.+0.j 3.+0.j], shape=(3,), dtype=complex128)"
"tf.raw_ops.BitwiseAnd(
    x, y, name=None
)
","[['Elementwise computes the bitwise AND of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([0, 0, 3, 10], dtype=tf.float32)

  res = bitwise_ops.bitwise_and(lhs, rhs)
  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE
"
"tf.raw_ops.BitwiseOr(
    x, y, name=None
)
","[['Elementwise computes the bitwise OR of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([5, 5, 7, 15], dtype=tf.float32)

  res = bitwise_ops.bitwise_or(lhs, rhs)
  tf.assert_equal(tf.cast(res,  tf.float32), exp)  # TRUE
"
"tf.raw_ops.BitwiseXor(
    x, y, name=None
)
","[['Elementwise computes the bitwise XOR of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([5, 5, 4, 5],  dtype=tf.float32)

  res = bitwise_ops.bitwise_xor(lhs, rhs)
  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE
"
"tf.raw_ops.BlockLSTM(
    seq_len_max,
    x,
    cs_prev,
    h_prev,
    w,
    wci,
    wcf,
    wco,
    b,
    forget_bias=1,
    cell_clip=3,
    use_peephole=False,
    name=None
)
",[],"for x1 in unpack(x):
  i1, cs1, f1, o1, ci1, co1, h1 = LSTMBlock(
    x1, cs_prev, h_prev, w, wci, wcf, wco, b)
  cs_prev = cs1
  h_prev = h1
  i.append(i1)
  cs.append(cs1)
  f.append(f1)
  o.append(o1)
  ci.append(ci1)
  co.append(co1)
  h.append(h1)
return pack(i), pack(cs), pack(f), pack(o), pack(ci), pack(ch), pack(h)
"
"tf.raw_ops.BlockLSTMGrad(
    seq_len_max,
    x,
    cs_prev,
    h_prev,
    w,
    wci,
    wcf,
    wco,
    b,
    i,
    cs,
    f,
    o,
    ci,
    co,
    h,
    cs_grad,
    h_grad,
    use_peephole,
    name=None
)
",[],[]
"tf.raw_ops.BlockLSTMGradV2(
    seq_len_max,
    x,
    cs_prev,
    h_prev,
    w,
    wci,
    wcf,
    wco,
    b,
    i,
    cs,
    f,
    o,
    ci,
    co,
    h,
    cs_grad,
    h_grad,
    use_peephole,
    name=None
)
",[],[]
"tf.raw_ops.BlockLSTMV2(
    seq_len_max,
    x,
    cs_prev,
    h_prev,
    w,
    wci,
    wcf,
    wco,
    b,
    cell_clip=0,
    use_peephole=False,
    name=None
)
",[],"for x1 in unpack(x):
  i1, cs1, f1, o1, ci1, co1, h1 = LSTMBlock(
    x1, cs_prev, h_prev, w, wci, wcf, wco, b)
  cs_prev = cs1
  h_prev = h1
  i.append(i1)
  cs.append(cs1)
  f.append(f1)
  o.append(o1)
  ci.append(ci1)
  co.append(co1)
  h.append(h1)
return pack(i), pack(cs), pack(f), pack(o), pack(ci), pack(ch), pack(h)

Note that unlike LSTMBlockCell (and BlockLSTM) which uses ICFO gate layout,
this op uses IFCO. So in order for the following snippet to be equivalent
all gate-related outputs should be reordered.
"
"tf.raw_ops.BoostedTreesAggregateStats(
    node_ids, gradients, hessians, feature, max_splits, num_buckets, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesBucketize(
    float_values, bucket_boundaries, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesCalculateBestFeatureSplit(
    node_id_range,
    stats_summary,
    l1,
    l2,
    tree_complexity,
    min_node_weight,
    logits_dimension,
    split_type='inequality',
    name=None
)
",[],[]
"tf.raw_ops.BoostedTreesCalculateBestFeatureSplitV2(
    node_id_range,
    stats_summaries_list,
    split_types,
    candidate_feature_ids,
    l1,
    l2,
    tree_complexity,
    min_node_weight,
    logits_dimension,
    name=None
)
",[],[]
"tf.raw_ops.BoostedTreesCalculateBestGainsPerFeature(
    node_id_range,
    stats_summary_list,
    l1,
    l2,
    tree_complexity,
    min_node_weight,
    max_splits,
    name=None
)
",[],[]
"tf.raw_ops.BoostedTreesCenterBias(
    tree_ensemble_handle, mean_gradients, mean_hessians, l1, l2, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesCreateEnsemble(
    tree_ensemble_handle, stamp_token, tree_ensemble_serialized, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesCreateQuantileStreamResource(
    quantile_stream_resource_handle,
    epsilon,
    num_streams,
    max_elements=1099511627776,
    name=None
)
",[],[]
"tf.raw_ops.BoostedTreesDeserializeEnsemble(
    tree_ensemble_handle, stamp_token, tree_ensemble_serialized, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesEnsembleResourceHandleOp(
    container='', shared_name='', name=None
)
",[],[]
"tf.raw_ops.BoostedTreesExampleDebugOutputs(
    tree_ensemble_handle, bucketized_features, logits_dimension, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesFlushQuantileSummaries(
    quantile_stream_resource_handle, num_features, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesGetEnsembleStates(
    tree_ensemble_handle, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesMakeQuantileSummaries(
    float_values, example_weights, epsilon, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesMakeStatsSummary(
    node_ids,
    gradients,
    hessians,
    bucketized_features_list,
    max_splits,
    num_buckets,
    name=None
)
",[],[]
"tf.raw_ops.BoostedTreesPredict(
    tree_ensemble_handle, bucketized_features, logits_dimension, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesQuantileStreamResourceAddSummaries(
    quantile_stream_resource_handle, summaries, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesQuantileStreamResourceDeserialize(
    quantile_stream_resource_handle, bucket_boundaries, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesQuantileStreamResourceFlush(
    quantile_stream_resource_handle,
    num_buckets,
    generate_quantiles=False,
    name=None
)
",[],[]
"tf.raw_ops.BoostedTreesQuantileStreamResourceGetBucketBoundaries(
    quantile_stream_resource_handle, num_features, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesQuantileStreamResourceHandleOp(
    container='', shared_name='', name=None
)
",[],[]
"tf.raw_ops.BoostedTreesSerializeEnsemble(
    tree_ensemble_handle, name=None
)
",[],[]
"tf.raw_ops.BoostedTreesSparseAggregateStats(
    node_ids,
    gradients,
    hessians,
    feature_indices,
    feature_values,
    feature_shape,
    max_splits,
    num_buckets,
    name=None
)
",[],[]
"tf.raw_ops.BoostedTreesSparseCalculateBestFeatureSplit(
    node_id_range,
    stats_summary_indices,
    stats_summary_values,
    stats_summary_shape,
    l1,
    l2,
    tree_complexity,
    min_node_weight,
    logits_dimension,
    split_type='inequality',
    name=None
)
",[],[]
"tf.raw_ops.BoostedTreesTrainingPredict(
    tree_ensemble_handle,
    cached_tree_ids,
    cached_node_ids,
    bucketized_features,
    logits_dimension,
    name=None
)
",[],[]
"tf.raw_ops.BoostedTreesUpdateEnsemble(
    tree_ensemble_handle,
    feature_ids,
    node_ids,
    gains,
    thresholds,
    left_node_contribs,
    right_node_contribs,
    max_depth,
    learning_rate,
    pruning_mode,
    name=None
)
",[],[]
"tf.raw_ops.BoostedTreesUpdateEnsembleV2(
    tree_ensemble_handle,
    feature_ids,
    dimension_ids,
    node_ids,
    gains,
    thresholds,
    left_node_contribs,
    right_node_contribs,
    split_types,
    max_depth,
    learning_rate,
    pruning_mode,
    logits_dimension=1,
    name=None
)
",[],[]
"tf.raw_ops.BroadcastArgs(
    s0, s1, name=None
)
",[],[]
"tf.raw_ops.BroadcastGradientArgs(
    s0, s1, name=None
)
",[],[]
"tf.raw_ops.BroadcastTo(
    input, shape, name=None
)
",[],"x = tf.constant([[1, 2, 3]])   # Shape (1, 3,)
y = tf.broadcast_to(x, [2, 3])
print(y)
tf.Tensor(
    [[1 2 3]
     [1 2 3]], shape=(2, 3), dtype=int32)"
"tf.raw_ops.Bucketize(
    input, boundaries, name=None
)
",[],[]
"tf.raw_ops.BytesProducedStatsDataset(
    input_dataset, tag, output_types, output_shapes, name=None
)
","[['Records the bytes size of each element of ', 'input_dataset', ' in a StatsAggregator.']]",[]
"tf.raw_ops.CSRSparseMatrixComponents(
    csr_sparse_matrix, index, type, name=None
)
","[['Reads out the CSR components at batch ', 'index', '.']]",[]
"tf.raw_ops.CSRSparseMatrixToDense(
    sparse_input, type, name=None
)
",[],[]
"tf.raw_ops.CSRSparseMatrixToSparseTensor(
    sparse_matrix, type, name=None
)
",[],[]
"tf.raw_ops.CSVDataset(
    filenames,
    compression_type,
    buffer_size,
    header,
    field_delim,
    use_quote_delim,
    na_value,
    select_cols,
    record_defaults,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.CSVDatasetV2(
    filenames,
    compression_type,
    buffer_size,
    header,
    field_delim,
    use_quote_delim,
    na_value,
    select_cols,
    record_defaults,
    exclude_cols,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.CTCBeamSearchDecoder(
    inputs,
    sequence_length,
    beam_width,
    top_paths,
    merge_repeated=True,
    name=None
)
",[],[]
"tf.raw_ops.CTCGreedyDecoder(
    inputs, sequence_length, merge_repeated=False, blank_index=-1, name=None
)
",[],[]
"tf.raw_ops.CTCLoss(
    inputs,
    labels_indices,
    labels_values,
    sequence_length,
    preprocess_collapse_repeated=False,
    ctc_merge_repeated=True,
    ignore_longer_outputs_than_inputs=False,
    name=None
)
",[],[]
"tf.raw_ops.CTCLossV2(
    inputs,
    labels_indices,
    labels_values,
    sequence_length,
    preprocess_collapse_repeated=False,
    ctc_merge_repeated=True,
    ignore_longer_outputs_than_inputs=False,
    name=None
)
",[],[]
"tf.raw_ops.CacheDataset(
    input_dataset,
    filename,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset that caches elements from ', 'input_dataset', '.']]",[]
"tf.raw_ops.CacheDatasetV2(
    input_dataset,
    filename,
    cache,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.Case(
    branch_index, input, Tout, branches, output_shapes=[], name=None
)
",[],"An n-way switch statement, implementing the following:

```
switch (branch_index) {
  case 0:
    output = branches[0](input);
    break;
  case 1:
    output = branches[1](input);
    break;
  ...
  case [[nbranches-1]]:
  default:
    output = branches[nbranches-1](input);
    break;
}
```
"
"tf.raw_ops.Cast(
    x, DstT, Truncate=False, name=None
)
",[],[]
"tf.raw_ops.Ceil(
    x, name=None
)
",[],[]
"tf.raw_ops.CheckNumerics(
    tensor, message, name=None
)
",[],"a = tf.Variable(1.0)
tf.debugging.check_numerics(a, message='')

b = tf.Variable(np.nan)
try:
  tf.debugging.check_numerics(b, message='Checking b')
except Exception as e:
  assert ""Checking b : Tensor had NaN values"" in e.message

c = tf.Variable(np.inf)
try:
  tf.debugging.check_numerics(c, message='Checking c')
except Exception as e:
  assert ""Checking c : Tensor had Inf values"" in e.message
"
"tf.raw_ops.CheckNumericsV2(
    tensor, message, name=None
)
",[],[]
"tf.raw_ops.Cholesky(
    input, name=None
)
",[],[]
"tf.raw_ops.CholeskyGrad(
    l, grad, name=None
)
",[],[]
"tf.raw_ops.ChooseFastestBranchDataset(
    input_dataset,
    ratio_numerator,
    ratio_denominator,
    other_arguments,
    num_elements_per_branch,
    branches,
    other_arguments_lengths,
    output_types,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.ChooseFastestDataset(
    input_datasets, num_experiments, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.ClipByValue(
    t, clip_value_min, clip_value_max, name=None
)
",[],[]
"tf.raw_ops.CloseSummaryWriter(
    writer, name=None
)
",[],[]
"tf.raw_ops.CollectiveAllToAllV3(
    input, communicator, group_assignment, timeout_seconds=0, name=None
)
",[],[]
"tf.raw_ops.CollectiveAssignGroupV2(
    group_assignment, device_index, base_key, name=None
)
",[],[]
"tf.raw_ops.CollectiveBcastRecv(
    T,
    group_size,
    group_key,
    instance_key,
    shape,
    communication_hint='auto',
    timeout_seconds=0,
    name=None
)
",[],[]
"tf.raw_ops.CollectiveBcastRecvV2(
    group_size,
    group_key,
    instance_key,
    shape,
    T,
    communication_hint='auto',
    timeout_seconds=0,
    name=None
)
",[],[]
"tf.raw_ops.CollectiveBcastSend(
    input,
    group_size,
    group_key,
    instance_key,
    shape,
    communication_hint='auto',
    timeout_seconds=0,
    name=None
)
",[],[]
"tf.raw_ops.CollectiveBcastSendV2(
    input,
    group_size,
    group_key,
    instance_key,
    communication_hint='auto',
    timeout_seconds=0,
    name=None
)
",[],[]
"tf.raw_ops.CollectiveGather(
    input,
    group_size,
    group_key,
    instance_key,
    shape,
    communication_hint='auto',
    timeout_seconds=0,
    name=None
)
",[],[]
"tf.raw_ops.CollectiveGatherV2(
    input,
    group_size,
    group_key,
    instance_key,
    ordering_token,
    communication_hint='auto',
    timeout_seconds=0,
    name=None
)
",[],[]
"tf.raw_ops.CollectiveInitializeCommunicator(
    group_key,
    rank,
    group_size,
    communication_hint='auto',
    timeout_seconds=0,
    name=None
)
",[],[]
"tf.raw_ops.CollectivePermute(
    input, source_target_pairs, name=None
)
",[],[]
"tf.raw_ops.CollectiveReduce(
    input,
    group_size,
    group_key,
    instance_key,
    merge_op,
    final_op,
    subdiv_offsets,
    wait_for=[],
    communication_hint='auto',
    timeout_seconds=0,
    name=None
)
",[],[]
"tf.raw_ops.CollectiveReduceV2(
    input,
    group_size,
    group_key,
    instance_key,
    ordering_token,
    merge_op,
    final_op,
    communication_hint='auto',
    timeout_seconds=0,
    max_subdivs_per_device=-1,
    name=None
)
",[],[]
"tf.raw_ops.CollectiveReduceV3(
    input,
    communicator,
    group_assignment,
    reduction,
    timeout_seconds=0,
    name=None
)
",[],[]
"tf.raw_ops.CombinedNonMaxSuppression(
    boxes,
    scores,
    max_output_size_per_class,
    max_total_size,
    iou_threshold,
    score_threshold,
    pad_per_class=False,
    clip_boxes=True,
    name=None
)
",[],[]
"tf.raw_ops.Complex(
    real,
    imag,
    Tout=tf.dtypes.complex64,
    name=None
)
","[[None, '\n']]","# tensor 'real' is [2.25, 3.25]
# tensor `imag` is [4.75, 5.75]
tf.complex(real, imag) ==> [[2.25 + 4.75j], [3.25 + 5.75j]]
"
"tf.raw_ops.ComplexAbs(
    x,
    Tout=tf.dtypes.float32,
    name=None
)
","[[None, '\n']]","x = tf.complex(3.0, 4.0)
print((tf.raw_ops.ComplexAbs(x=x, Tout=tf.dtypes.float32, name=None)).numpy())
5.0"
"tf.raw_ops.CompositeTensorVariantFromComponents(
    components, metadata, name=None
)
","[['Encodes an ', 'ExtensionType', ' value into a ', 'variant', ' scalar Tensor.']]",[]
"tf.raw_ops.CompositeTensorVariantToComponents(
    encoded, metadata, Tcomponents, name=None
)
","[['Decodes a ', 'variant', ' scalar Tensor into an ', 'ExtensionType', ' value.']]",[]
"tf.raw_ops.CompressElement(
    components, name=None
)
",[],[]
"tf.raw_ops.ComputeAccidentalHits(
    true_classes, sampled_candidates, num_true, seed=0, seed2=0, name=None
)
",[],[]
"tf.raw_ops.ComputeBatchSize(
    input_dataset, name=None
)
",[],[]
"tf.raw_ops.Concat(
    concat_dim, values, name=None
)
",[],[]
"tf.raw_ops.ConcatOffset(
    concat_dim, shape, name=None
)
",[],"x = [2, 2, 7]
y = [2, 3, 7]
z = [2, 9, 7]
offsets = concat_offset(1, [x, y, z])
[list(off.numpy()) for off in offsets]
[[0, 0, 0], [0, 2, 0], [0, 5, 0]]"
"tf.raw_ops.ConcatV2(
    values, axis, name=None
)
",[],[]
"tf.raw_ops.ConcatenateDataset(
    input_dataset,
    another_dataset,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset that concatenates ', 'input_dataset', ' with ', 'another_dataset', '.']]",[]
"tf.raw_ops.ConditionalAccumulator(
    dtype,
    shape,
    container='',
    shared_name='',
    reduction_type='MEAN',
    name=None
)
",[],[]
"tf.raw_ops.ConfigureDistributedTPU(
    embedding_config='',
    tpu_embedding_config='',
    is_global_init=False,
    enable_whole_mesh_compilations=False,
    compilation_failure_closes_chips=True,
    tpu_cancellation_closes_chips=0,
    name=None
)
",[],[]
"tf.raw_ops.ConfigureTPUEmbedding(
    config, name=None
)
",[],[]
"tf.raw_ops.Conj(
    input, name=None
)
","[[None, '\n']]","# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]
tf.conj(input) ==> [-2.25 - 4.75j, 3.25 - 5.75j]
"
"tf.raw_ops.ConjugateTranspose(
    x, perm, name=None
)
",[],[]
"tf.raw_ops.Const(
    value, dtype, name=None
)
",[],[]
"tf.raw_ops.ConsumeMutexLock(
    mutex_lock, name=None
)
","[['This op consumes a lock created by ', 'MutexLock', '.']]",[]
"tf.raw_ops.ControlTrigger(
    name=None
)
",[],[]
"tf.raw_ops.Conv2D(
    input,
    filter,
    strides,
    padding,
    use_cudnn_on_gpu=True,
    explicit_paddings=[],
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
","[['Computes a 2-D convolution given 4-D ', 'input', ' and ', 'filter', ' tensors.']]","output[b, i, j, k] =
    sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q] *
                    filter[di, dj, q, k]
"
"tf.raw_ops.Conv2DBackpropFilter(
    input,
    filter_sizes,
    out_backprop,
    strides,
    padding,
    use_cudnn_on_gpu=True,
    explicit_paddings=[],
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.raw_ops.Conv2DBackpropInput(
    input_sizes,
    filter,
    out_backprop,
    strides,
    padding,
    use_cudnn_on_gpu=True,
    explicit_paddings=[],
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.raw_ops.Conv3D(
    input,
    filter,
    strides,
    padding,
    data_format='NDHWC',
    dilations=[1, 1, 1, 1, 1],
    name=None
)
","[['Computes a 3-D convolution given 5-D ', 'input', ' and ', 'filter', ' tensors.']]",[]
"tf.raw_ops.Conv3DBackpropFilter(
    input,
    filter,
    out_backprop,
    strides,
    padding,
    dilations=[1, 1, 1, 1, 1],
    name=None
)
",[],[]
"tf.raw_ops.Conv3DBackpropFilterV2(
    input,
    filter_sizes,
    out_backprop,
    strides,
    padding,
    data_format='NDHWC',
    dilations=[1, 1, 1, 1, 1],
    name=None
)
",[],[]
"tf.raw_ops.Conv3DBackpropInput(
    input,
    filter,
    out_backprop,
    strides,
    padding,
    dilations=[1, 1, 1, 1, 1],
    name=None
)
",[],[]
"tf.raw_ops.Conv3DBackpropInputV2(
    input_sizes,
    filter,
    out_backprop,
    strides,
    padding,
    data_format='NDHWC',
    dilations=[1, 1, 1, 1, 1],
    name=None
)
",[],[]
"tf.raw_ops.Copy(
    input, tensor_name='', debug_ops_spec=[], name=None
)
",[],[]
"tf.raw_ops.CopyHost(
    input, tensor_name='', debug_ops_spec=[], name=None
)
",[],[]
"tf.raw_ops.Cos(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]
"
"tf.raw_ops.Cosh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.cosh(x) ==> [inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf]
"
"tf.raw_ops.CountUpTo(
    ref, limit, name=None
)
",[],[]
"tf.raw_ops.CreateSummaryDbWriter(
    writer, db_uri, experiment_name, run_name, user_name, name=None
)
",[],[]
"tf.raw_ops.CreateSummaryFileWriter(
    writer, logdir, max_queue, flush_millis, filename_suffix, name=None
)
",[],[]
"tf.raw_ops.CropAndResize(
    image,
    boxes,
    box_ind,
    crop_size,
    method='bilinear',
    extrapolation_value=0,
    name=None
)
",[],[]
"tf.raw_ops.CropAndResizeGradBoxes(
    grads, image, boxes, box_ind, method='bilinear', name=None
)
",[],[]
"tf.raw_ops.CropAndResizeGradImage(
    grads,
    boxes,
    box_ind,
    image_size,
    T,
    method='bilinear',
    name=None
)
",[],[]
"tf.raw_ops.Cross(
    a, b, name=None
)
",[],[]
"tf.raw_ops.CrossReplicaSum(
    input, group_assignment, name=None
)
",[],[]
"tf.raw_ops.CudnnRNN(
    input,
    input_h,
    input_c,
    params,
    rnn_mode='lstm',
    input_mode='linear_input',
    direction='unidirectional',
    dropout=0,
    seed=0,
    seed2=0,
    is_training=True,
    name=None
)
",[],[]
"tf.raw_ops.CudnnRNNBackprop(
    input,
    input_h,
    input_c,
    params,
    output,
    output_h,
    output_c,
    output_backprop,
    output_h_backprop,
    output_c_backprop,
    reserve_space,
    rnn_mode='lstm',
    input_mode='linear_input',
    direction='unidirectional',
    dropout=0,
    seed=0,
    seed2=0,
    name=None
)
",[],[]
"tf.raw_ops.CudnnRNNBackpropV2(
    input,
    input_h,
    input_c,
    params,
    output,
    output_h,
    output_c,
    output_backprop,
    output_h_backprop,
    output_c_backprop,
    reserve_space,
    host_reserved,
    rnn_mode='lstm',
    input_mode='linear_input',
    direction='unidirectional',
    dropout=0,
    seed=0,
    seed2=0,
    name=None
)
",[],[]
"tf.raw_ops.CudnnRNNBackpropV3(
    input,
    input_h,
    input_c,
    params,
    sequence_lengths,
    output,
    output_h,
    output_c,
    output_backprop,
    output_h_backprop,
    output_c_backprop,
    reserve_space,
    host_reserved,
    rnn_mode='lstm',
    input_mode='linear_input',
    direction='unidirectional',
    dropout=0,
    seed=0,
    seed2=0,
    num_proj=0,
    time_major=True,
    name=None
)
",[],[]
"tf.raw_ops.CudnnRNNCanonicalToParams(
    num_layers,
    num_units,
    input_size,
    weights,
    biases,
    rnn_mode='lstm',
    input_mode='linear_input',
    direction='unidirectional',
    dropout=0,
    seed=0,
    seed2=0,
    name=None
)
",[],[]
"tf.raw_ops.CudnnRNNCanonicalToParamsV2(
    num_layers,
    num_units,
    input_size,
    weights,
    biases,
    rnn_mode='lstm',
    input_mode='linear_input',
    direction='unidirectional',
    dropout=0,
    seed=0,
    seed2=0,
    num_proj=0,
    name=None
)
",[],[]
"tf.raw_ops.CudnnRNNParamsSize(
    num_layers,
    num_units,
    input_size,
    T,
    S,
    rnn_mode='lstm',
    input_mode='linear_input',
    direction='unidirectional',
    dropout=0,
    seed=0,
    seed2=0,
    num_proj=0,
    name=None
)
",[],[]
"tf.raw_ops.CudnnRNNParamsToCanonical(
    num_layers,
    num_units,
    input_size,
    params,
    num_params,
    rnn_mode='lstm',
    input_mode='linear_input',
    direction='unidirectional',
    dropout=0,
    seed=0,
    seed2=0,
    name=None
)
",[],[]
"tf.raw_ops.CudnnRNNParamsToCanonicalV2(
    num_layers,
    num_units,
    input_size,
    params,
    num_params_weights,
    num_params_biases,
    rnn_mode='lstm',
    input_mode='linear_input',
    direction='unidirectional',
    dropout=0,
    seed=0,
    seed2=0,
    num_proj=0,
    name=None
)
",[],[]
"tf.raw_ops.CudnnRNNV2(
    input,
    input_h,
    input_c,
    params,
    rnn_mode='lstm',
    input_mode='linear_input',
    direction='unidirectional',
    dropout=0,
    seed=0,
    seed2=0,
    is_training=True,
    name=None
)
",[],[]
"tf.raw_ops.CudnnRNNV3(
    input,
    input_h,
    input_c,
    params,
    sequence_lengths,
    rnn_mode='lstm',
    input_mode='linear_input',
    direction='unidirectional',
    dropout=0,
    seed=0,
    seed2=0,
    num_proj=0,
    is_training=True,
    time_major=True,
    name=None
)
",[],[]
"tf.raw_ops.Cumprod(
    x, axis, exclusive=False, reverse=False, name=None
)
","[['Compute the cumulative product of the tensor ', 'x', ' along ', 'axis', '.']]","tf.cumprod([a, b, c])  # => [a, a * b, a * b * c]
"
"tf.raw_ops.Cumsum(
    x, axis, exclusive=False, reverse=False, name=None
)
","[['Compute the cumulative sum of the tensor ', 'x', ' along ', 'axis', '.']]","tf.cumsum([a, b, c])  # => [a, a + b, a + b + c]
"
"tf.raw_ops.CumulativeLogsumexp(
    x, axis, exclusive=False, reverse=False, name=None
)
","[['Compute the cumulative product of the tensor ', 'x', ' along ', 'axis', '.']]","tf.math.cumulative_logsumexp([a, b, c])  # => [a, log(exp(a) + exp(b)), log(exp(a) + exp(b) + exp(c))]
"
"tf.raw_ops.DataFormatDimMap(
    x, src_format='NHWC', dst_format='NCHW', name=None
)
",[],[]
"tf.raw_ops.DataFormatVecPermute(
    x, src_format='NHWC', dst_format='NCHW', name=None
)
","[['Permute input tensor from ', 'src_format', ' to ', 'dst_format', '.']]","[1, 2, 3, 4]
"
"tf.raw_ops.DataServiceDataset(
    dataset_id,
    processing_mode,
    address,
    protocol,
    job_name,
    max_outstanding_requests,
    iteration_counter,
    output_types,
    output_shapes,
    task_refresh_interval_hint_ms=-1,
    data_transfer_protocol='',
    target_workers='AUTO',
    cross_trainer_cache_options='',
    name=None
)
",[],[]
"tf.raw_ops.DataServiceDatasetV2(
    dataset_id,
    processing_mode,
    address,
    protocol,
    job_name,
    consumer_index,
    num_consumers,
    max_outstanding_requests,
    iteration_counter,
    output_types,
    output_shapes,
    task_refresh_interval_hint_ms=-1,
    data_transfer_protocol='',
    target_workers='AUTO',
    cross_trainer_cache_options='',
    name=None
)
",[],[]
"tf.raw_ops.DataServiceDatasetV3(
    dataset_id,
    processing_mode,
    address,
    protocol,
    job_name,
    consumer_index,
    num_consumers,
    max_outstanding_requests,
    iteration_counter,
    output_types,
    output_shapes,
    uncompress_fn,
    task_refresh_interval_hint_ms=-1,
    data_transfer_protocol='',
    target_workers='AUTO',
    uncompress=False,
    cross_trainer_cache_options='',
    name=None
)
",[],[]
"tf.raw_ops.DataServiceDatasetV4(
    dataset_id,
    processing_mode,
    address,
    protocol,
    job_name,
    consumer_index,
    num_consumers,
    max_outstanding_requests,
    iteration_counter,
    output_types,
    output_shapes,
    uncompress_fn,
    task_refresh_interval_hint_ms=-1,
    data_transfer_protocol='',
    target_workers='AUTO',
    uncompress=False,
    cross_trainer_cache_options='',
    name=None
)
",[],[]
"tf.raw_ops.DatasetCardinality(
    input_dataset, name=None
)
","[['Returns the cardinality of ', 'input_dataset', '.']]",[]
"tf.raw_ops.DatasetFromGraph(
    graph_def, name=None
)
","[['Creates a dataset from the given ', 'graph_def', '.']]",[]
"tf.raw_ops.DatasetToGraph(
    input_dataset,
    stateful_whitelist=[],
    allow_stateful=False,
    strip_device_assignment=False,
    name=None
)
","[['Returns a serialized GraphDef representing ', 'input_dataset', '.']]",[]
"tf.raw_ops.DatasetToGraphV2(
    input_dataset,
    external_state_policy=0,
    strip_device_assignment=False,
    name=None
)
","[['Returns a serialized GraphDef representing ', 'input_dataset', '.']]",[]
"tf.raw_ops.DatasetToSingleElement(
    dataset, output_types, output_shapes, metadata='', name=None
)
",[],[]
"tf.raw_ops.DatasetToTFRecord(
    input_dataset, filename, compression_type, name=None
)
",[],[]
"tf.raw_ops.Dawsn(
    x, name=None
)
",[],[]
"tf.raw_ops.DebugGradientIdentity(
    input, name=None
)
",[],[]
"tf.raw_ops.DebugGradientRefIdentity(
    input, name=None
)
",[],[]
"tf.raw_ops.DebugIdentity(
    input,
    device_name='',
    tensor_name='',
    debug_urls=[],
    gated_grpc=False,
    name=None
)
",[],[]
"tf.raw_ops.DebugIdentityV2(
    input,
    tfdbg_context_id='',
    op_name='',
    output_slot=-1,
    tensor_debug_mode=-1,
    debug_urls=[],
    circular_buffer_size=1000,
    tfdbg_run_id='',
    name=None
)
",[],[]
"tf.raw_ops.DebugNanCount(
    input,
    device_name='',
    tensor_name='',
    debug_urls=[],
    gated_grpc=False,
    name=None
)
",[],[]
"tf.raw_ops.DebugNumericSummary(
    input,
    device_name='',
    tensor_name='',
    debug_urls=[],
    lower_bound=float('-inf'),
    upper_bound=float('inf'),
    mute_if_healthy=False,
    gated_grpc=False,
    name=None
)
",[],[]
"tf.raw_ops.DebugNumericSummaryV2(
    input,
    output_dtype=tf.dtypes.float32,
    tensor_debug_mode=-1,
    tensor_id=-1,
    name=None
)
",[],[]
"tf.raw_ops.DecodeAndCropJpeg(
    contents,
    crop_window,
    channels=0,
    ratio=1,
    fancy_upscaling=True,
    try_recover_truncated=False,
    acceptable_fraction=1,
    dct_method='',
    name=None
)
",[],[]
"tf.raw_ops.DecodeBase64(
    input, name=None
)
",[],[]
"tf.raw_ops.DecodeBmp(
    contents, channels=0, name=None
)
",[],[]
"tf.raw_ops.DecodeCSV(
    records,
    record_defaults,
    field_delim=',',
    use_quote_delim=True,
    na_value='',
    select_cols=[],
    name=None
)
",[],[]
"tf.raw_ops.DecodeCompressed(
    bytes, compression_type='', name=None
)
",[],[]
"tf.raw_ops.DecodeGif(
    contents, name=None
)
","[[None, '\n']]","convert \\(src.gif -coalesce \\)dst.gif
"
"tf.raw_ops.DecodeImage(
    contents,
    channels=0,
    dtype=tf.dtypes.uint8,
    expand_animations=True,
    name=None
)
",[],[]
"tf.raw_ops.DecodeJSONExample(
    json_examples, name=None
)
",[],[]
"tf.raw_ops.DecodeJpeg(
    contents,
    channels=0,
    ratio=1,
    fancy_upscaling=True,
    try_recover_truncated=False,
    acceptable_fraction=1,
    dct_method='',
    name=None
)
",[],[]
"tf.raw_ops.DecodePaddedRaw(
    input_bytes, fixed_length, out_type, little_endian=True, name=None
)
",[],[]
"tf.raw_ops.DecodePng(
    contents,
    channels=0,
    dtype=tf.dtypes.uint8,
    name=None
)
",[],[]
"tf.raw_ops.DecodeProtoV2(
    bytes,
    message_type,
    field_names,
    output_types,
    descriptor_source='local://',
    message_format='binary',
    sanitize=False,
    name=None
)
",[],"from google.protobuf import text_format
# A Summary.Value contains: oneof {float simple_value; Image image}
values = [
   ""simple_value: 2.2"",
   ""simple_value: 1.2"",
   ""image { height: 128 width: 512 }"",
   ""image { height: 256 width: 256 }"",]
values = [
   text_format.Parse(v, tf.compat.v1.Summary.Value()).SerializeToString()
   for v in values]"
"tf.raw_ops.DecodeRaw(
    bytes, out_type, little_endian=True, name=None
)
",[],[]
"tf.raw_ops.DecodeWav(
    contents, desired_channels=-1, desired_samples=-1, name=None
)
",[],[]
"tf.raw_ops.DeepCopy(
    x, name=None
)
","[['Makes a copy of ', 'x', '.']]",[]
"tf.raw_ops.DeleteIterator(
    handle, deleter, name=None
)
",[],[]
"tf.raw_ops.DeleteMemoryCache(
    handle, deleter, name=None
)
",[],[]
"tf.raw_ops.DeleteMultiDeviceIterator(
    multi_device_iterator, iterators, deleter, name=None
)
",[],[]
"tf.raw_ops.DeleteRandomSeedGenerator(
    handle, deleter, name=None
)
",[],[]
"tf.raw_ops.DeleteSeedGenerator(
    handle, deleter, name=None
)
",[],[]
"tf.raw_ops.DeleteSessionTensor(
    handle, name=None
)
",[],[]
"tf.raw_ops.DenseBincount(
    input, size, weights, binary_output=False, name=None
)
",[],[]
"tf.raw_ops.DenseCountSparseOutput(
    values, weights, binary_output, minlength=-1, maxlength=-1, name=None
)
",[],[]
"tf.raw_ops.DenseToCSRSparseMatrix(
    dense_input, indices, name=None
)
",[],[]
"tf.raw_ops.DenseToDenseSetOperation(
    set1, set2, set_operation, validate_indices=True, name=None
)
","[['Applies set operation along last dimension of 2 ', 'Tensor', ' inputs.']]",[]
"tf.raw_ops.DenseToSparseBatchDataset(
    input_dataset,
    batch_size,
    row_shape,
    output_types,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.DenseToSparseSetOperation(
    set1,
    set2_indices,
    set2_values,
    set2_shape,
    set_operation,
    validate_indices=True,
    name=None
)
","[['Applies set operation along last dimension of ', 'Tensor', ' and ', 'SparseTensor', '.']]",[]
"tf.raw_ops.DepthToSpace(
    input, block_size, data_format='NHWC', name=None
)
",[],"x = [[[[1, 2, 3, 4]]]]

"
"tf.raw_ops.DepthwiseConv2dNative(
    input,
    filter,
    strides,
    padding,
    explicit_paddings=[],
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
","[['Computes a 2-D depthwise convolution given 4-D ', 'input', ' and ', 'filter', ' tensors.']]","for k in 0..in_channels-1
  for q in 0..channel_multiplier-1
    output[b, i, j, k * channel_multiplier + q] =
      sum_{di, dj} input[b, strides[1] * i + di, strides[2] * j + dj, k] *
                        filter[di, dj, k, q]
"
"tf.raw_ops.DepthwiseConv2dNativeBackpropFilter(
    input,
    filter_sizes,
    out_backprop,
    strides,
    padding,
    explicit_paddings=[],
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.raw_ops.DepthwiseConv2dNativeBackpropInput(
    input_sizes,
    filter,
    out_backprop,
    strides,
    padding,
    explicit_paddings=[],
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.raw_ops.Dequantize(
    input,
    min_range,
    max_range,
    mode='MIN_COMBINED',
    narrow_range=False,
    axis=-1,
    dtype=tf.dtypes.float32,
    name=None
)
",[],"if T == qint8: in[i] += (range(T) + 1)/ 2.0
out[i] = min_range + (in[i]* (max_range - min_range) / range(T))
"
"tf.raw_ops.DeserializeIterator(
    resource_handle, serialized, name=None
)
",[],[]
"tf.raw_ops.DeserializeManySparse(
    serialized_sparse, dtype, name=None
)
","[['Deserialize and concatenate ', 'SparseTensors', ' from a serialized minibatch.']]","index = [ 0]
        [10]
        [20]
values = [1, 2, 3]
shape = [50]
"
"tf.raw_ops.DeserializeSparse(
    serialized_sparse, dtype, name=None
)
","[['Deserialize ', 'SparseTensor', ' objects.']]","index = [ 0]
        [10]
        [20]
values = [1, 2, 3]
shape = [50]
"
"tf.raw_ops.DestroyResourceOp(
    resource, ignore_lookup_error=True, name=None
)
",[],[]
"tf.raw_ops.DestroyTemporaryVariable(
    ref, var_name, name=None
)
",[],[]
"tf.raw_ops.DeviceIndex(
    device_names, name=None
)
",[],[]
"tf.raw_ops.Diag(
    diagonal, name=None
)
",[],"# 'diagonal' is [1, 2, 3, 4]
tf.diag(diagonal) ==> [[1, 0, 0, 0]
                       [0, 2, 0, 0]
                       [0, 0, 3, 0]
                       [0, 0, 0, 4]]
"
"tf.raw_ops.DiagPart(
    input, name=None
)
",[],"# 'input' is [[1, 0, 0, 0]
              [0, 2, 0, 0]
              [0, 0, 3, 0]
              [0, 0, 0, 4]]

tf.diag_part(input) ==> [1, 2, 3, 4]
"
"tf.raw_ops.Digamma(
    x, name=None
)
",[],[]
"tf.raw_ops.Dilation2D(
    input, filter, strides, rates, padding, name=None
)
","[['Computes the grayscale dilation of 4-D ', 'input', ' and 3-D ', 'filter', ' tensors.']]","output[b, y, x, c] =
   max_{dy, dx} input[b,
                      strides[1] * y + rates[1] * dy,
                      strides[2] * x + rates[2] * dx,
                      c] +
                filter[dy, dx, c]
"
"tf.raw_ops.Dilation2DBackpropFilter(
    input, filter, out_backprop, strides, rates, padding, name=None
)
",[],[]
"tf.raw_ops.Dilation2DBackpropInput(
    input, filter, out_backprop, strides, rates, padding, name=None
)
",[],[]
"tf.raw_ops.DirectedInterleaveDataset(
    selector_input_dataset,
    data_input_datasets,
    output_types,
    output_shapes,
    stop_on_empty_dataset=False,
    name=None
)
","[['A substitute for ', 'InterleaveDataset', ' on a fixed list of ', 'N', ' datasets.']]",[]
"tf.raw_ops.DisableCopyOnRead(
    resource, name=None
)
",[],[]
"tf.raw_ops.Div(
    x, y, name=None
)
",[],[]
"tf.raw_ops.DivNoNan(
    x, y, name=None
)
",[],[]
"tf.raw_ops.DrawBoundingBoxes(
    images, boxes, name=None
)
",[],[]
"tf.raw_ops.DrawBoundingBoxesV2(
    images, boxes, colors, name=None
)
",[],[]
"tf.raw_ops.DummyIterationCounter(
    name=None
)
",[],[]
"tf.raw_ops.DummyMemoryCache(
    name=None
)
",[],[]
"tf.raw_ops.DummySeedGenerator(
    name=None
)
",[],[]
"tf.raw_ops.DynamicEnqueueTPUEmbeddingArbitraryTensorBatch(
    sample_indices_or_row_splits,
    embedding_indices,
    aggregation_weights,
    mode_override,
    device_ordinal,
    combiners=[],
    name=None
)
",[],[]
"tf.raw_ops.DynamicPartition(
    data, partitions, num_partitions, name=None
)
","[['Partitions ', 'data', ' into ', 'num_partitions', ' tensors using indices from ', 'partitions', '.']]","    outputs[i].shape = [sum(partitions == i)] + data.shape[partitions.ndim:]

    outputs[i] = pack([data[js, ...] for js if partitions[js] == i])
"
"tf.raw_ops.DynamicStitch(
    indices, data, name=None
)
","[['Interleave the values from the ', 'data', ' tensors into a single tensor.']]","    merged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]
"
"tf.raw_ops.EagerPyFunc(
    input, token, Tout, is_async=False, name=None
)
",[],[]
"tf.raw_ops.EditDistance(
    hypothesis_indices,
    hypothesis_values,
    hypothesis_shape,
    truth_indices,
    truth_values,
    truth_shape,
    normalize=True,
    name=None
)
",[],[]
"tf.raw_ops.Eig(
    input, Tout, compute_v=True, name=None
)
",[],"# a is a tensor.
# e is a tensor of eigenvalues.
# v is a tensor of eigenvectors.
e, v = eig(a)
e = eig(a, compute_v=False)
"
"tf.raw_ops.Einsum(
    inputs, equation, name=None
)
",[],[]
"tf.raw_ops.Elu(
    features, name=None
)
","[[None, '\n']]","tf.nn.elu(1.0)
<tf.Tensor: shape=(), dtype=float32, numpy=1.0>
tf.nn.elu(0.0)
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>
tf.nn.elu(-1000.0)
<tf.Tensor: shape=(), dtype=float32, numpy=-1.0>"
"tf.raw_ops.EluGrad(
    gradients, outputs, name=None
)
",[],[]
"tf.raw_ops.Empty(
    shape, dtype, init=False, name=None
)
",[],[]
"tf.raw_ops.EmptyTensorList(
    element_shape, max_num_elements, element_dtype, name=None
)
",[],[]
"tf.raw_ops.EncodeBase64(
    input, pad=False, name=None
)
",[],[]
"tf.raw_ops.EncodeJpeg(
    image,
    format='',
    quality=95,
    progressive=False,
    optimize_size=False,
    chroma_downsampling=True,
    density_unit='in',
    x_density=300,
    y_density=300,
    xmp_metadata='',
    name=None
)
",[],[]
"tf.raw_ops.EncodeJpegVariableQuality(
    images, quality, name=None
)
",[],[]
"tf.raw_ops.EncodePng(
    image, compression=-1, name=None
)
",[],[]
"tf.raw_ops.EncodeProto(
    sizes,
    values,
    field_names,
    message_type,
    descriptor_source='local://',
    name=None
)
",[],[]
"tf.raw_ops.EncodeWav(
    audio, sample_rate, name=None
)
",[],[]
"tf.raw_ops.EnqueueTPUEmbeddingArbitraryTensorBatch(
    sample_indices_or_row_splits,
    embedding_indices,
    aggregation_weights,
    mode_override,
    device_ordinal=-1,
    combiners=[],
    name=None
)
",[],[]
"tf.raw_ops.EnqueueTPUEmbeddingIntegerBatch(
    batch, mode_override, device_ordinal=-1, name=None
)
",[],[]
"tf.raw_ops.EnqueueTPUEmbeddingRaggedTensorBatch(
    sample_splits,
    embedding_indices,
    aggregation_weights,
    mode_override,
    table_ids,
    device_ordinal=-1,
    combiners=[],
    max_sequence_lengths=[],
    num_features=[],
    name=None
)
",[],[]
"tf.raw_ops.EnqueueTPUEmbeddingSparseBatch(
    sample_indices,
    embedding_indices,
    aggregation_weights,
    mode_override,
    device_ordinal=-1,
    combiners=[],
    name=None
)
",[],[]
"tf.raw_ops.EnqueueTPUEmbeddingSparseTensorBatch(
    sample_indices,
    embedding_indices,
    aggregation_weights,
    mode_override,
    table_ids,
    device_ordinal=-1,
    combiners=[],
    max_sequence_lengths=[],
    num_features=[],
    name=None
)
",[],[]
"tf.raw_ops.EnsureShape(
    input, shape, name=None
)
",[],[]
"tf.raw_ops.Enter(
    data, frame_name, is_constant=False, parallel_iterations=10, name=None
)
","[['Creates or finds a child frame, and makes ', 'data', ' available to the child frame.']]",[]
"tf.raw_ops.Equal(
    x, y, incompatible_shape_error=True, name=None
)
",[],"x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.equal(x, y) ==> array([True, False])

x = tf.constant([2, 4])
y = tf.constant([2, 4])
tf.math.equal(x, y) ==> array([True,  True])
"
"tf.raw_ops.Erf(
    x, name=None
)
","[[None, '\n'], ['Computes the ', 'Gauss error function', ' of ', 'x', ' element-wise. In statistics, for non-negative values of \\(x\\), the error function has the following interpretation: for a random variable \\(Y\\) that is normally distributed with mean 0 and variance \\(1/\\sqrt{2}\\), \\(erf(x)\\) is the probability that \\(Y\\) falls in the range \\([x, x]\\).']]","tf.math.erf([[1.0, 2.0, 3.0], [0.0, -1.0, -2.0]])
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.8427007,  0.9953223,  0.999978 ],
       [ 0.       , -0.8427007, -0.9953223]], dtype=float32)>"
"tf.raw_ops.Erfc(
    x, name=None
)
","[['Computes the complementary error function of ', 'x', ' element-wise.']]",[]
"tf.raw_ops.Erfinv(
    x, name=None
)
",[],[]
"tf.raw_ops.EuclideanNorm(
    input, axis, keep_dims=False, name=None
)
",[],[]
"tf.raw_ops.Exit(
    data, name=None
)
",[],[]
"tf.raw_ops.Exp(
    x, name=None
)
","[[None, '\n']]","  x = tf.constant(2.0)
  tf.math.exp(x) ==> 7.389056

  x = tf.constant([2.0, 8.0])
  tf.math.exp(x) ==> array([7.389056, 2980.958], dtype=float32)
"
"tf.raw_ops.ExpandDims(
    input, axis, name=None
)
",[],"# 't' is a tensor of shape [2]
shape(expand_dims(t, 0)) ==> [1, 2]
shape(expand_dims(t, 1)) ==> [2, 1]
shape(expand_dims(t, -1)) ==> [2, 1]

# 't2' is a tensor of shape [2, 3, 5]
shape(expand_dims(t2, 0)) ==> [1, 2, 3, 5]
shape(expand_dims(t2, 2)) ==> [2, 3, 1, 5]
shape(expand_dims(t2, 3)) ==> [2, 3, 5, 1]
"
"tf.raw_ops.ExperimentalAssertNextDataset(
    input_dataset, transformations, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.ExperimentalAutoShardDataset(
    input_dataset,
    num_workers,
    index,
    output_types,
    output_shapes,
    auto_shard_policy=0,
    name=None
)
",[],[]
"tf.raw_ops.ExperimentalBytesProducedStatsDataset(
    input_dataset, tag, output_types, output_shapes, name=None
)
","[['Records the bytes size of each element of ', 'input_dataset', ' in a StatsAggregator.']]",[]
"tf.raw_ops.ExperimentalCSVDataset(
    filenames,
    compression_type,
    buffer_size,
    header,
    field_delim,
    use_quote_delim,
    na_value,
    select_cols,
    record_defaults,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.ExperimentalChooseFastestDataset(
    input_datasets, num_experiments, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.ExperimentalDatasetCardinality(
    input_dataset, name=None
)
","[['Returns the cardinality of ', 'input_dataset', '.']]",[]
"tf.raw_ops.ExperimentalDatasetToTFRecord(
    input_dataset, filename, compression_type, name=None
)
",[],[]
"tf.raw_ops.ExperimentalDenseToSparseBatchDataset(
    input_dataset,
    batch_size,
    row_shape,
    output_types,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.ExperimentalDirectedInterleaveDataset(
    selector_input_dataset,
    data_input_datasets,
    output_types,
    output_shapes,
    name=None
)
","[['A substitute for ', 'InterleaveDataset', ' on a fixed list of ', 'N', ' datasets.']]",[]
"tf.raw_ops.ExperimentalGroupByReducerDataset(
    input_dataset,
    key_func_other_arguments,
    init_func_other_arguments,
    reduce_func_other_arguments,
    finalize_func_other_arguments,
    key_func,
    init_func,
    reduce_func,
    finalize_func,
    output_types,
    output_shapes,
    name=None
)
","[['Creates a dataset that computes a group-by on ', 'input_dataset', '.']]",[]
"tf.raw_ops.ExperimentalGroupByWindowDataset(
    input_dataset,
    key_func_other_arguments,
    reduce_func_other_arguments,
    window_size_func_other_arguments,
    key_func,
    reduce_func,
    window_size_func,
    output_types,
    output_shapes,
    name=None
)
","[['Creates a dataset that computes a windowed group-by on ', 'input_dataset', '.']]",[]
"tf.raw_ops.ExperimentalIgnoreErrorsDataset(
    input_dataset, output_types, output_shapes, log_warning=False, name=None
)
","[['Creates a dataset that contains the elements of ', 'input_dataset', ' ignoring errors.']]",[]
"tf.raw_ops.ExperimentalIteratorGetDevice(
    resource, name=None
)
","[['Returns the name of the device on which ', 'resource', ' has been placed.']]",[]
"tf.raw_ops.ExperimentalLMDBDataset(
    filenames, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.ExperimentalLatencyStatsDataset(
    input_dataset, tag, output_types, output_shapes, name=None
)
","[['Records the latency of producing ', 'input_dataset', ' elements in a StatsAggregator.']]",[]
"tf.raw_ops.ExperimentalMapAndBatchDataset(
    input_dataset,
    other_arguments,
    batch_size,
    num_parallel_calls,
    drop_remainder,
    f,
    output_types,
    output_shapes,
    preserve_cardinality=False,
    name=None
)
",[],[]
"tf.raw_ops.ExperimentalMapDataset(
    input_dataset,
    other_arguments,
    f,
    output_types,
    output_shapes,
    use_inter_op_parallelism=True,
    preserve_cardinality=False,
    name=None
)
","[['Creates a dataset that applies ', 'f', ' to the outputs of ', 'input_dataset', '.']]",[]
"tf.raw_ops.ExperimentalMatchingFilesDataset(
    patterns, name=None
)
",[],[]
"tf.raw_ops.ExperimentalMaxIntraOpParallelismDataset(
    input_dataset,
    max_intra_op_parallelism,
    output_types,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.ExperimentalNonSerializableDataset(
    input_dataset, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.ExperimentalParallelInterleaveDataset(
    input_dataset,
    other_arguments,
    cycle_length,
    block_length,
    sloppy,
    buffer_output_elements,
    prefetch_input_elements,
    f,
    output_types,
    output_shapes,
    name=None
)
","[['Creates a dataset that applies ', 'f', ' to the outputs of ', 'input_dataset', '.']]",[]
"tf.raw_ops.ExperimentalParseExampleDataset(
    input_dataset,
    num_parallel_calls,
    dense_defaults,
    sparse_keys,
    dense_keys,
    sparse_types,
    dense_shapes,
    output_types,
    output_shapes,
    sloppy=False,
    name=None
)
","[['Transforms ', 'input_dataset', ' containing ', 'Example', ' protos as vectors of DT_STRING into a dataset of ', 'Tensor', ' or ', 'SparseTensor', ' objects representing the parsed features.']]",[]
"tf.raw_ops.ExperimentalPrivateThreadPoolDataset(
    input_dataset, num_threads, output_types, output_shapes, name=None
)
","[['Creates a dataset that uses a custom thread pool to compute ', 'input_dataset', '.']]",[]
"tf.raw_ops.ExperimentalRandomDataset(
    seed, seed2, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.ExperimentalRebatchDataset(
    input_dataset,
    num_replicas,
    output_types,
    output_shapes,
    use_fallback=True,
    name=None
)
",[],[]
"tf.raw_ops.ExperimentalScanDataset(
    input_dataset,
    initial_state,
    other_arguments,
    f,
    output_types,
    output_shapes,
    preserve_cardinality=False,
    name=None
)
","[['Creates a dataset successively reduces ', 'f', ' over the elements of ', 'input_dataset', '.']]",[]
"tf.raw_ops.ExperimentalSetStatsAggregatorDataset(
    input_dataset,
    stats_aggregator,
    tag,
    counter_prefix,
    output_types,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.ExperimentalSleepDataset(
    input_dataset, sleep_microseconds, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.ExperimentalSlidingWindowDataset(
    input_dataset,
    window_size,
    window_shift,
    window_stride,
    output_types,
    output_shapes,
    name=None
)
","[['Creates a dataset that passes a sliding window over ', 'input_dataset', '.']]",[]
"tf.raw_ops.ExperimentalSqlDataset(
    driver_name,
    data_source_name,
    query,
    output_types,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.ExperimentalStatsAggregatorHandle(
    container='', shared_name='', name=None
)
",[],[]
"tf.raw_ops.ExperimentalStatsAggregatorSummary(
    iterator, name=None
)
",[],[]
"tf.raw_ops.ExperimentalTakeWhileDataset(
    input_dataset,
    other_arguments,
    predicate,
    output_types,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.ExperimentalThreadPoolDataset(
    input_dataset, thread_pool, output_types, output_shapes, name=None
)
","[['Creates a dataset that uses a custom thread pool to compute ', 'input_dataset', '.']]",[]
"tf.raw_ops.ExperimentalThreadPoolHandle(
    num_threads,
    display_name,
    max_intra_op_parallelism=1,
    container='',
    shared_name='',
    name=None
)
","[['Creates a dataset that uses a custom thread pool to compute ', 'input_dataset', '.']]",[]
"tf.raw_ops.ExperimentalUnbatchDataset(
    input_dataset, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.ExperimentalUniqueDataset(
    input_dataset, output_types, output_shapes, name=None
)
","[['Creates a dataset that contains the unique elements of ', 'input_dataset', '.']]",[]
"tf.raw_ops.Expint(
    x, name=None
)
",[],[]
"tf.raw_ops.Expm1(
    x, name=None
)
","[['Computes ', 'exp(x) - 1', ' element-wise.']]","  x = tf.constant(2.0)
  tf.math.expm1(x) ==> 6.389056

  x = tf.constant([2.0, 8.0])
  tf.math.expm1(x) ==> array([6.389056, 2979.958], dtype=float32)

  x = tf.constant(1 + 1j)
  tf.math.expm1(x) ==> (0.46869393991588515+2.2873552871788423j)
"
"tf.raw_ops.ExtractGlimpse(
    input,
    size,
    offsets,
    centered=True,
    normalized=True,
    uniform_noise=True,
    noise='uniform',
    name=None
)
",[],[]
"tf.raw_ops.ExtractGlimpseV2(
    input,
    size,
    offsets,
    centered=True,
    normalized=True,
    uniform_noise=True,
    noise='uniform',
    name=None
)
",[],[]
"tf.raw_ops.ExtractImagePatches(
    images, ksizes, strides, rates, padding, name=None
)
","[['Extract ', 'patches', ' from ', 'images', ' and put them in the ""depth"" output dimension.']]",[]
"tf.raw_ops.ExtractJpegShape(
    contents,
    output_type=tf.dtypes.int32,
    name=None
)
",[],[]
"tf.raw_ops.ExtractVolumePatches(
    input, ksizes, strides, padding, name=None
)
","[['Extract ', 'patches', ' from ', 'input', ' and put them in the ', '""depth""', ' output dimension. 3D extension of ', 'extract_image_patches', '.']]","ksizes = [1, ksize_planes, ksize_rows, ksize_cols, 1]
strides = [1, stride_planes, strides_rows, strides_cols, 1]
"
"tf.raw_ops.FFT(
    input, name=None
)
",[],[]
"tf.raw_ops.FFT2D(
    input, name=None
)
",[],[]
"tf.raw_ops.FFT3D(
    input, name=None
)
",[],[]
"tf.raw_ops.FIFOQueue(
    component_types,
    shapes=[],
    capacity=-1,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.FIFOQueueV2(
    component_types,
    shapes=[],
    capacity=-1,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.Fact(
    name=None
)
",[],[]
"tf.raw_ops.FakeParam(
    dtype, shape, name=None
)
",[],[]
"tf.raw_ops.FakeQuantWithMinMaxArgs(
    inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.raw_ops.FakeQuantWithMinMaxArgsGradient(
    gradients, inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.raw_ops.FakeQuantWithMinMaxVars(
    inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.raw_ops.FakeQuantWithMinMaxVarsGradient(
    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.raw_ops.FakeQuantWithMinMaxVarsPerChannel(
    inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.raw_ops.FakeQuantWithMinMaxVarsPerChannelGradient(
    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.raw_ops.FakeQueue(
    resource, name=None
)
",[],[]
"tf.raw_ops.Fill(
    dims, value, name=None
)
",[],"# Output tensor has shape [2, 3].
fill([2, 3], 9) ==> [[9, 9, 9]
                     [9, 9, 9]]
"
"tf.raw_ops.FilterByLastComponentDataset(
    input_dataset, output_types, output_shapes, name=None
)
","[['Creates a dataset containing elements of first component of ', 'input_dataset', ' having true in the last component.']]",[]
"tf.raw_ops.FilterDataset(
    input_dataset,
    other_arguments,
    predicate,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset containing elements of ', 'input_dataset', ' matching ', 'predicate', '.']]",[]
"tf.raw_ops.FinalizeDataset(
    input_dataset,
    output_types,
    output_shapes,
    has_captured_ref=False,
    name=None
)
","[['Creates a dataset by applying ', 'tf.data.Options', ' to ', 'input_dataset', '.']]",[]
"tf.raw_ops.Fingerprint(
    data, method, name=None
)
",[],"Fingerprint(data) == Fingerprint(Reshape(data, ...))
Fingerprint(data) == Fingerprint(Bitcast(data, ...))
"
"tf.raw_ops.FixedLengthRecordDataset(
    filenames,
    header_bytes,
    record_bytes,
    footer_bytes,
    buffer_size,
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.FixedLengthRecordDatasetV2(
    filenames,
    header_bytes,
    record_bytes,
    footer_bytes,
    buffer_size,
    compression_type,
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.FixedLengthRecordReader(
    record_bytes,
    header_bytes=0,
    footer_bytes=0,
    hop_bytes=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.FixedLengthRecordReaderV2(
    record_bytes,
    header_bytes=0,
    footer_bytes=0,
    hop_bytes=0,
    container='',
    shared_name='',
    encoding='',
    name=None
)
",[],[]
"tf.raw_ops.FixedUnigramCandidateSampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    vocab_file='',
    distortion=1,
    num_reserved_ids=0,
    num_shards=1,
    shard=0,
    unigrams=[],
    seed=0,
    seed2=0,
    name=None
)
",[],[]
"tf.raw_ops.FlatMapDataset(
    input_dataset,
    other_arguments,
    f,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset that applies ', 'f', ' to the outputs of ', 'input_dataset', '.']]",[]
"tf.raw_ops.Floor(
    x, name=None
)
",[],[]
"tf.raw_ops.FloorDiv(
    x, y, name=None
)
",[],[]
"tf.raw_ops.FloorMod(
    x, y, name=None
)
",[],[]
"tf.raw_ops.FlushSummaryWriter(
    writer, name=None
)
",[],[]
"tf.raw_ops.For(
    start, limit, delta, input, body, name=None
)
",[],"   output = input;
   for i in range(start, limit, delta)
     output = body(i, output);
"
"tf.raw_ops.FractionalAvgPool(
    value,
    pooling_ratio,
    pseudo_random=False,
    overlapping=False,
    deterministic=False,
    seed=0,
    seed2=0,
    name=None
)
",[],[]
"tf.raw_ops.FractionalAvgPoolGrad(
    orig_input_tensor_shape,
    out_backprop,
    row_pooling_sequence,
    col_pooling_sequence,
    overlapping=False,
    name=None
)
",[],[]
"tf.raw_ops.FractionalMaxPool(
    value,
    pooling_ratio,
    pseudo_random=False,
    overlapping=False,
    deterministic=False,
    seed=0,
    seed2=0,
    name=None
)
",[],[]
"tf.raw_ops.FractionalMaxPoolGrad(
    orig_input,
    orig_output,
    out_backprop,
    row_pooling_sequence,
    col_pooling_sequence,
    overlapping=False,
    name=None
)
",[],[]
"tf.raw_ops.FresnelCos(
    x, name=None
)
",[],[]
"tf.raw_ops.FresnelSin(
    x, name=None
)
",[],[]
"tf.raw_ops.FusedBatchNorm(
    x,
    scale,
    offset,
    mean,
    variance,
    epsilon=0.0001,
    exponential_avg_factor=1,
    data_format='NHWC',
    is_training=True,
    name=None
)
",[],[]
"tf.raw_ops.FusedBatchNormGrad(
    y_backprop,
    x,
    scale,
    reserve_space_1,
    reserve_space_2,
    epsilon=0.0001,
    data_format='NHWC',
    is_training=True,
    name=None
)
",[],[]
"tf.raw_ops.FusedBatchNormGradV2(
    y_backprop,
    x,
    scale,
    reserve_space_1,
    reserve_space_2,
    epsilon=0.0001,
    data_format='NHWC',
    is_training=True,
    name=None
)
",[],[]
"tf.raw_ops.FusedBatchNormGradV3(
    y_backprop,
    x,
    scale,
    reserve_space_1,
    reserve_space_2,
    reserve_space_3,
    epsilon=0.0001,
    data_format='NHWC',
    is_training=True,
    name=None
)
",[],[]
"tf.raw_ops.FusedBatchNormV2(
    x,
    scale,
    offset,
    mean,
    variance,
    epsilon=0.0001,
    exponential_avg_factor=1,
    data_format='NHWC',
    is_training=True,
    name=None
)
",[],[]
"tf.raw_ops.FusedBatchNormV3(
    x,
    scale,
    offset,
    mean,
    variance,
    epsilon=0.0001,
    exponential_avg_factor=1,
    data_format='NHWC',
    is_training=True,
    name=None
)
",[],[]
"tf.raw_ops.FusedPadConv2D(
    input, paddings, filter, mode, strides, padding, name=None
)
",[],[]
"tf.raw_ops.FusedResizeAndPadConv2D(
    input,
    size,
    paddings,
    filter,
    mode,
    strides,
    padding,
    resize_align_corners=False,
    name=None
)
",[],[]
"tf.raw_ops.GRUBlockCell(
    x, h_prev, w_ru, w_c, b_ru, b_c, name=None
)
",[],"x_h_prev = [x, h_prev]

[r_bar u_bar] = x_h_prev * w_ru + b_ru

r = sigmoid(r_bar)
u = sigmoid(u_bar)

h_prevr = h_prev \circ r

x_h_prevr = [x h_prevr]

c_bar = x_h_prevr * w_c + b_c
c = tanh(c_bar)

h = (1-u) \circ c + u \circ h_prev
"
"tf.raw_ops.GRUBlockCellGrad(
    x, h_prev, w_ru, w_c, b_ru, b_c, r, u, c, d_h, name=None
)
",[],"w_ru = [w_r_x w_u_x
        w_r_h_prev w_u_h_prev]
"
"tf.raw_ops.Gather(
    params, indices, validate_indices=True, name=None
)
","[['Gather slices from ', 'params', ' according to ', 'indices', '.']]","    # Scalar indices
    output[:, ..., :] = params[indices, :, ... :]

    # Vector indices
    output[i, :, ..., :] = params[indices[i], :, ... :]

    # Higher rank indices
    output[i, ..., j, :, ... :] = params[indices[i, ..., j], :, ..., :]
"
"tf.raw_ops.GatherNd(
    params, indices, name=None
)
","[[None, '\n'], ['Gather slices from ', 'params', ' into a Tensor with shape specified by ', 'indices', '.']]","output[\\(i_0, ..., i_{K-2}\\)] = params[indices[\\(i_0, ..., i_{K-2}\\)]]
"
"tf.raw_ops.GatherV2(
    params, indices, axis, batch_dims=0, name=None
)
","[['Gather slices from ', 'params', ' axis ', 'axis', ' according to ', 'indices', '.']]","    # Scalar indices (output is rank(params) - 1).
    output[a_0, ..., a_n, b_0, ..., b_n] =
      params[a_0, ..., a_n, indices, b_0, ..., b_n]

    # Vector indices (output is rank(params)).
    output[a_0, ..., a_n, i, b_0, ..., b_n] =
      params[a_0, ..., a_n, indices[i], b_0, ..., b_n]

    # Higher rank indices (output is rank(params) + rank(indices) - 1).
    output[a_0, ..., a_n, i, ..., j, b_0, ... b_n] =
      params[a_0, ..., a_n, indices[i, ..., j], b_0, ..., b_n]
"
"tf.raw_ops.GenerateBoundingBoxProposals(
    scores,
    bbox_deltas,
    image_info,
    anchors,
    nms_threshold,
    pre_nms_topn,
    min_size,
    post_nms_topn=300,
    name=None
)
",[],"  The op selects top `pre_nms_topn` scoring boxes, decodes them with respect to anchors,
  applies non-maximal suppression on overlapping boxes with higher than
  `nms_threshold` intersection-over-union (iou) value, discarding boxes where shorter
  side is less than `min_size`.
  Inputs:
  `scores`: A 4D tensor of shape [Batch, Height, Width, Num Anchors] containing the scores per anchor at given position
  `bbox_deltas`: is a tensor of shape [Batch, Height, Width, 4 x Num Anchors] boxes encoded to each anchor
  `anchors`: A 1D tensor of shape [4 x Num Anchors], representing the anchors.
  Outputs:
  `rois`: output RoIs, a 3D tensor of shape [Batch, post_nms_topn, 4], padded by 0 if less than post_nms_topn candidates found.
  `roi_probabilities`: probability scores of each roi in 'rois', a 2D tensor of shape [Batch,post_nms_topn], padded with 0 if needed, sorted by scores.
"
"tf.raw_ops.GenerateVocabRemapping(
    new_vocab_file,
    old_vocab_file,
    new_vocab_offset,
    num_new_vocab,
    old_vocab_size=-1,
    name=None
)
",[],[]
"tf.raw_ops.GeneratorDataset(
    init_func_other_args,
    next_func_other_args,
    finalize_func_other_args,
    init_func,
    next_func,
    finalize_func,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.GetElementAtIndex(
    dataset, index, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.GetOptions(
    input_dataset, name=None
)
","[['Returns the ', 'tf.data.Options', ' attached to ', 'input_dataset', '.']]",[]
"tf.raw_ops.GetSessionHandle(
    value, name=None
)
",[],[]
"tf.raw_ops.GetSessionHandleV2(
    value, name=None
)
",[],[]
"tf.raw_ops.GetSessionTensor(
    handle, dtype, name=None
)
",[],[]
"tf.raw_ops.Greater(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5, 2, 5])
tf.math.greater(x, y) ==> [False, True, True]

x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.greater(x, y) ==> [False, False, True]
"
"tf.raw_ops.GreaterEqual(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6, 7])
y = tf.constant([5, 2, 5, 10])
tf.math.greater_equal(x, y) ==> [True, True, True, False]

x = tf.constant([5, 4, 6, 7])
y = tf.constant([5])
tf.math.greater_equal(x, y) ==> [True, False, True, True]
"
"tf.raw_ops.GroupByReducerDataset(
    input_dataset,
    key_func_other_arguments,
    init_func_other_arguments,
    reduce_func_other_arguments,
    finalize_func_other_arguments,
    key_func,
    init_func,
    reduce_func,
    finalize_func,
    output_types,
    output_shapes,
    name=None
)
","[['Creates a dataset that computes a group-by on ', 'input_dataset', '.']]",[]
"tf.raw_ops.GuaranteeConst(
    input, name=None
)
",[],[]
"tf.raw_ops.HSVToRGB(
    images, name=None
)
",[],[]
"tf.raw_ops.HashTable(
    key_dtype,
    value_dtype,
    container='',
    shared_name='',
    use_node_name_sharing=False,
    name=None
)
",[],[]
"tf.raw_ops.HashTableV2(
    key_dtype,
    value_dtype,
    container='',
    shared_name='',
    use_node_name_sharing=False,
    name=None
)
",[],[]
"tf.raw_ops.HistogramFixedWidth(
    values,
    value_range,
    nbins,
    dtype=tf.dtypes.int32,
    name=None
)
",[],"# Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]

with tf.get_default_session() as sess:
  hist = tf.histogram_fixed_width(new_values, value_range, nbins=5)
  variables.global_variables_initializer().run()
  sess.run(hist) => [2, 1, 1, 0, 2]
"
"tf.raw_ops.HistogramSummary(
    tag, values, name=None
)
","[['Outputs a ', 'Summary', ' protocol buffer with a histogram.']]",[]
"tf.raw_ops.IFFT(
    input, name=None
)
",[],[]
"tf.raw_ops.IFFT2D(
    input, name=None
)
",[],[]
"tf.raw_ops.IFFT3D(
    input, name=None
)
",[],[]
"tf.raw_ops.IRFFT(
    input,
    fft_length,
    Treal=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.IRFFT2D(
    input,
    fft_length,
    Treal=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.IRFFT3D(
    input,
    fft_length,
    Treal=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.Identity(
    input, name=None
)
",[],[]
"tf.raw_ops.IdentityN(
    input, name=None
)
",[],"with tf.get_default_graph().gradient_override_map(
    {'IdentityN': 'OverrideGradientWithG'}):
  y, _ = identity_n([f(x), x])

@tf.RegisterGradient('OverrideGradientWithG')
def ApplyG(op, dy, _):
  return [None, g(dy)]  # Do not backprop to f(x).
"
"tf.raw_ops.IdentityReader(
    container='', shared_name='', name=None
)
",[],[]
"tf.raw_ops.IdentityReaderV2(
    container='', shared_name='', name=None
)
",[],[]
"tf.raw_ops.If(
    cond, input, Tout, then_branch, else_branch, output_shapes=[], name=None
)
",[],[]
"tf.raw_ops.Igamma(
    a, x, name=None
)
","[[None, '\n'], ['Compute the lower regularized incomplete Gamma function ', 'P(a, x)', '.']]",[]
"tf.raw_ops.IgammaGradA(
    a, x, name=None
)
","[['Computes the gradient of ', 'igamma(a, x)', ' wrt ', 'a', '.']]",[]
"tf.raw_ops.Igammac(
    a, x, name=None
)
","[[None, '\n'], ['Compute the upper regularized incomplete Gamma function ', 'Q(a, x)', '.']]",[]
"tf.raw_ops.IgnoreErrorsDataset(
    input_dataset, output_types, output_shapes, log_warning=False, name=None
)
","[['Creates a dataset that contains the elements of ', 'input_dataset', ' ignoring errors.']]",[]
"tf.raw_ops.Imag(
    input,
    Tout=tf.dtypes.float32,
    name=None
)
","[[None, '\n']]","# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]
tf.imag(input) ==> [4.75, 5.75]
"
"tf.raw_ops.ImageProjectiveTransformV2(
    images,
    transforms,
    output_shape,
    interpolation,
    fill_mode='CONSTANT',
    name=None
)
",[],[]
"tf.raw_ops.ImageProjectiveTransformV3(
    images,
    transforms,
    output_shape,
    fill_value,
    interpolation,
    fill_mode='CONSTANT',
    name=None
)
",[],[]
"tf.raw_ops.ImageSummary(
    tag,
    tensor,
    max_images=3,
    bad_color=_execute.make_tensor(\n    'dtype: DT_UINT8 tensor_shape { dim { size: 4 } } int_val: 255 int_val: 0 int_val: 0 int_val: 255 '\n    , 'bad_color'),
    name=None
)
","[['Outputs a ', 'Summary', ' protocol buffer with images.']]",[]
"tf.raw_ops.ImmutableConst(
    dtype, shape, memory_region_name, name=None
)
",[],[]
"tf.raw_ops.ImportEvent(
    writer, event, name=None
)
",[],[]
"tf.raw_ops.InTopK(
    predictions, targets, k, name=None
)
","[[None, '\n'], ['Says whether the targets are in the top ', 'K', ' predictions.']]",[]
"tf.raw_ops.InTopKV2(
    predictions, targets, k, name=None
)
","[[None, '\n'], ['Says whether the targets are in the top ', 'K', ' predictions.']]",[]
"tf.raw_ops.InfeedDequeue(
    dtype, shape, name=None
)
",[],[]
"tf.raw_ops.InfeedDequeueTuple(
    dtypes, shapes, name=None
)
",[],[]
"tf.raw_ops.InfeedEnqueue(
    input, shape=[], layout=[], device_ordinal=-1, name=None
)
",[],[]
"tf.raw_ops.InfeedEnqueuePrelinearizedBuffer(
    input, device_ordinal=-1, name=None
)
",[],[]
"tf.raw_ops.InfeedEnqueueTuple(
    inputs, shapes, layouts=[], device_ordinal=-1, name=None
)
",[],[]
"tf.raw_ops.InitializeTable(
    table_handle, keys, values, name=None
)
",[],[]
"tf.raw_ops.InitializeTableFromDataset(
    table_handle, dataset, name=None
)
",[],[]
"tf.raw_ops.InitializeTableFromTextFile(
    table_handle,
    filename,
    key_index,
    value_index,
    vocab_size=-1,
    delimiter='\t',
    offset=0,
    name=None
)
",[],[]
"tf.raw_ops.InitializeTableFromTextFileV2(
    table_handle,
    filename,
    key_index,
    value_index,
    vocab_size=-1,
    delimiter='\t',
    offset=0,
    name=None
)
",[],[]
"tf.raw_ops.InitializeTableV2(
    table_handle, keys, values, name=None
)
",[],[]
"tf.raw_ops.InplaceAdd(
    x, i, v, name=None
)
",[],"Computes y = x; y[i, :] += v; return y.
"
"tf.raw_ops.InplaceSub(
    x, i, v, name=None
)
","[['Subtracts ', 'v', ' into specified rows of ', 'x', '.']]",[]
"tf.raw_ops.InplaceUpdate(
    x, i, v, name=None
)
",[],[]
"tf.raw_ops.InterleaveDataset(
    input_dataset,
    other_arguments,
    cycle_length,
    block_length,
    f,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset that applies ', 'f', ' to the outputs of ', 'input_dataset', '.']]",[]
"tf.raw_ops.Inv(
    x, name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.InvGrad(
    y, dy, name=None
)
","[['Computes the gradient for the inverse of ', 'x', ' wrt its input.']]",[]
"tf.raw_ops.Invert(
    x, name=None
)
","[['Invert (flip) each bit of supported types; for example, type ', 'uint8', ' value 01010101 becomes 10101010.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops

# flip 2 (00000010) to -3 (11111101)
tf.assert_equal(-3, bitwise_ops.invert(2))

dtype_list = [dtypes.int8, dtypes.int16, dtypes.int32, dtypes.int64,
              dtypes.uint8, dtypes.uint16, dtypes.uint32, dtypes.uint64]

inputs = [0, 5, 3, 14]
for dtype in dtype_list:
  # Because of issues with negative numbers, let's test this indirectly.
  # 1. invert(a) and a = 0
  # 2. invert(a) or a = invert(0)
  input_tensor = tf.constant([0, 5, 3, 14], dtype=dtype)
  not_a_and_a, not_a_or_a, not_0 = [bitwise_ops.bitwise_and(
                                      input_tensor, bitwise_ops.invert(input_tensor)),
                                    bitwise_ops.bitwise_or(
                                      input_tensor, bitwise_ops.invert(input_tensor)),
                                    bitwise_ops.invert(
                                      tf.constant(0, dtype=dtype))]

  expected = tf.constant([0, 0, 0, 0], dtype=tf.float32)
  tf.assert_equal(tf.cast(not_a_and_a, tf.float32), expected)

  expected = tf.cast([not_0] * 4, tf.float32)
  tf.assert_equal(tf.cast(not_a_or_a, tf.float32), expected)

  # For unsigned dtypes let's also check the result directly.
  if dtype.is_unsigned:
    inverted = bitwise_ops.invert(input_tensor)
    expected = tf.constant([dtype.max - x for x in inputs], dtype=tf.float32)
    tf.assert_equal(tf.cast(inverted, tf.float32), tf.cast(expected, tf.float32))
"
"tf.raw_ops.InvertPermutation(
    x, name=None
)
",[],"# tensor `x` is [3, 4, 0, 2, 1]
invert_permutation(x) ==> [2, 4, 3, 0, 1]
"
"tf.raw_ops.IsBoostedTreesEnsembleInitialized(
    tree_ensemble_handle, name=None
)
",[],[]
"tf.raw_ops.IsBoostedTreesQuantileStreamResourceInitialized(
    quantile_stream_resource_handle, name=None
)
",[],[]
"tf.raw_ops.IsFinite(
    x, name=None
)
",[],"x = tf.constant([5.0, 4.8, 6.8, np.inf, np.nan])
tf.math.is_finite(x) ==> [True, True, True, False, False]
"
"tf.raw_ops.IsInf(
    x, name=None
)
",[],"x = tf.constant([5.0, np.inf, 6.8, np.inf])
tf.math.is_inf(x) ==> [False, True, False, True]
"
"tf.raw_ops.IsNan(
    x, name=None
)
",[],"x = tf.constant([5.0, np.nan, 6.8, np.nan, np.inf])
tf.math.is_nan(x) ==> [False, True, False, True, False]
"
"tf.raw_ops.IsTPUEmbeddingInitialized(
    config='', name=None
)
",[],[]
"tf.raw_ops.IsVariableInitialized(
    ref, name=None
)
",[],[]
"tf.raw_ops.IsotonicRegression(
    input,
    output_dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.Iterator(
    shared_name, container, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.IteratorFromStringHandle(
    string_handle, output_types=[], output_shapes=[], name=None
)
",[],[]
"tf.raw_ops.IteratorFromStringHandleV2(
    string_handle, output_types=[], output_shapes=[], name=None
)
",[],[]
"tf.raw_ops.IteratorGetDevice(
    resource, name=None
)
","[['Returns the name of the device on which ', 'resource', ' has been placed.']]",[]
"tf.raw_ops.IteratorGetNext(
    iterator, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.IteratorGetNextAsOptional(
    iterator, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.IteratorGetNextSync(
    iterator, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.IteratorToStringHandle(
    resource_handle, name=None
)
","[['Converts the given ', 'resource_handle', ' representing an iterator to a string.']]",[]
"tf.raw_ops.IteratorV2(
    shared_name, container, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.L2Loss(
    t, name=None
)
",[],"output = sum(t ** 2) / 2
"
"tf.raw_ops.LMDBDataset(
    filenames, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.LMDBReader(
    container='', shared_name='', name=None
)
",[],[]
"tf.raw_ops.LRN(
    input, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None
)
",[],"sqr_sum[a, b, c, d] =
    sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)
output = input / (bias + alpha * sqr_sum) ** beta
"
"tf.raw_ops.LRNGrad(
    input_grads,
    input_image,
    output_image,
    depth_radius=5,
    bias=1,
    alpha=1,
    beta=0.5,
    name=None
)
",[],[]
"tf.raw_ops.LSTMBlockCell(
    x,
    cs_prev,
    h_prev,
    w,
    wci,
    wcf,
    wco,
    b,
    forget_bias=1,
    cell_clip=3,
    use_peephole=False,
    name=None
)
",[],"xh = [x, h_prev]
[i, f, ci, o] = xh * w + b
f = f + forget_bias

if not use_peephole:
  wci = wcf = wco = 0

i = sigmoid(cs_prev * wci + i)
f = sigmoid(cs_prev * wcf + f)
ci = tanh(ci)

cs = ci .* i + cs_prev .* f
cs = clip(cs, cell_clip)

o = sigmoid(cs * wco + o)
co = tanh(cs)
h = co .* o
"
"tf.raw_ops.LSTMBlockCellGrad(
    x,
    cs_prev,
    h_prev,
    w,
    wci,
    wcf,
    wco,
    b,
    i,
    cs,
    f,
    o,
    ci,
    co,
    cs_grad,
    h_grad,
    use_peephole,
    name=None
)
",[],[]
"tf.raw_ops.LatencyStatsDataset(
    input_dataset, tag, output_types, output_shapes, name=None
)
","[['Records the latency of producing ', 'input_dataset', ' elements in a StatsAggregator.']]",[]
"tf.raw_ops.LeakyRelu(
    features, alpha=0.2, name=None
)
","[['Computes rectified linear: ', 'max(features, features * alpha)', '.']]",[]
"tf.raw_ops.LeakyReluGrad(
    gradients, features, alpha=0.2, name=None
)
",[],[]
"tf.raw_ops.LearnedUnigramCandidateSampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=0,
    seed2=0,
    name=None
)
",[],[]
"tf.raw_ops.LeftShift(
    x, y, name=None
)
","[['Elementwise computes the bitwise left-shift of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
import numpy as np
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]

for dtype in dtype_list:
  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)

  left_shift_result = bitwise_ops.left_shift(lhs, rhs)

  print(left_shift_result)

# This will print:
# tf.Tensor([ -32   -5 -128    0], shape=(4,), dtype=int8)
# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int16)
# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int32)
# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int64)

lhs = np.array([-2, 64, 101, 32], dtype=np.int8)
rhs = np.array([-1, -5, -3, -14], dtype=np.int8)
bitwise_ops.left_shift(lhs, rhs)
# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>
"
"tf.raw_ops.LegacyParallelInterleaveDatasetV2(
    input_dataset,
    other_arguments,
    cycle_length,
    block_length,
    buffer_output_elements,
    prefetch_input_elements,
    f,
    output_types,
    output_shapes,
    deterministic='default',
    metadata='',
    name=None
)
","[['Creates a dataset that applies ', 'f', ' to the outputs of ', 'input_dataset', '.']]",[]
"tf.raw_ops.Less(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less(x, y) ==> [False, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 7])
tf.math.less(x, y) ==> [False, True, True]
"
"tf.raw_ops.LessEqual(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less_equal(x, y) ==> [True, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 6])
tf.math.less_equal(x, y) ==> [True, True, True]
"
"tf.raw_ops.Lgamma(
    x, name=None
)
","[['Computes the log of the absolute value of ', 'Gamma(x)', ' element-wise.']]","x = tf.constant([0, 0.5, 1, 4.5, -4, -5.6])
tf.math.lgamma(x) ==> [inf, 0.5723649, 0., 2.4537368, inf, -4.6477685]
"
"tf.raw_ops.LinSpace(
    start, stop, num, name=None
)
",[],"tf.linspace(10.0, 12.0, 3, name=""linspace"") => [ 10.0  11.0  12.0]
"
"tf.raw_ops.ListDataset(
    tensors, output_types, output_shapes, metadata='', name=None
)
","[['Creates a dataset that emits each of ', 'tensors', ' once.']]",[]
"tf.raw_ops.ListDiff(
    x,
    y,
    out_idx=tf.dtypes.int32,
    name=None
)
",[],"x = [1, 2, 3, 4, 5, 6]
y = [1, 3, 5]
"
"tf.raw_ops.LoadAndRemapMatrix(
    ckpt_path,
    old_tensor_name,
    row_remapping,
    col_remapping,
    initializing_values,
    num_rows,
    num_cols,
    max_rows_in_memory=-1,
    name=None
)
","[['Loads a 2-D (matrix) ', 'Tensor', ' with name ', 'old_tensor_name', ' from the checkpoint']]",[]
"tf.raw_ops.LoadDataset(
    path,
    reader_func_other_args,
    output_types,
    output_shapes,
    reader_func,
    compression='',
    name=None
)
",[],[]
"tf.raw_ops.LoadTPUEmbeddingADAMParameters(
    parameters,
    momenta,
    velocities,
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.LoadTPUEmbeddingAdadeltaParameters(
    parameters,
    accumulators,
    updates,
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.LoadTPUEmbeddingAdagradMomentumParameters(
    parameters,
    accumulators,
    momenta,
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.LoadTPUEmbeddingAdagradParameters(
    parameters,
    accumulators,
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.LoadTPUEmbeddingCenteredRMSPropParameters(
    parameters,
    ms,
    mom,
    mg,
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.LoadTPUEmbeddingFTRLParameters(
    parameters,
    accumulators,
    linears,
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.LoadTPUEmbeddingFrequencyEstimatorParameters(
    parameters,
    last_hit_step,
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.LoadTPUEmbeddingMDLAdagradLightParameters(
    parameters,
    accumulators,
    weights,
    benefits,
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.LoadTPUEmbeddingMomentumParameters(
    parameters,
    momenta,
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.LoadTPUEmbeddingProximalAdagradParameters(
    parameters,
    accumulators,
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.LoadTPUEmbeddingProximalYogiParameters(
    parameters,
    v,
    m,
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.LoadTPUEmbeddingRMSPropParameters(
    parameters,
    ms,
    mom,
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.LoadTPUEmbeddingStochasticGradientDescentParameters(
    parameters,
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.Log(
    x, name=None
)
","[[None, '\n']]","x = tf.constant([0, 0.5, 1, 5])
tf.math.log(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([      -inf, -0.6931472,  0.       ,  1.609438 ], dtype=float32)>"
"tf.raw_ops.Log1p(
    x, name=None
)
","[[None, '\n']]","x = tf.constant([0, 0.5, 1, 5])
tf.math.log1p(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.       , 0.4054651, 0.6931472, 1.7917595], dtype=float32)>"
"tf.raw_ops.LogMatrixDeterminant(
    input, name=None
)
",[],[]
"tf.raw_ops.LogSoftmax(
    logits, name=None
)
",[],"logsoftmax[i, j] = logits[i, j] - log(sum(exp(logits[i])))
"
"tf.raw_ops.LogUniformCandidateSampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=0,
    seed2=0,
    name=None
)
",[],[]
"tf.raw_ops.LogicalAnd(
    x, y, name=None
)
",[],">>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_and(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
>>> a & b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
"
"tf.raw_ops.LogicalNot(
    x, name=None
)
","[['Returns the truth value of ', 'NOT x', ' element-wise.']]","tf.math.logical_not(tf.constant([True, False]))
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.raw_ops.LogicalOr(
    x, y, name=None
)
",[],">>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_or(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
>>> a | b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
"
"tf.raw_ops.LookupTableExport(
    table_handle, Tkeys, Tvalues, name=None
)
",[],[]
"tf.raw_ops.LookupTableExportV2(
    table_handle, Tkeys, Tvalues, name=None
)
",[],[]
"tf.raw_ops.LookupTableFind(
    table_handle, keys, default_value, name=None
)
",[],[]
"tf.raw_ops.LookupTableFindV2(
    table_handle, keys, default_value, name=None
)
",[],[]
"tf.raw_ops.LookupTableImport(
    table_handle, keys, values, name=None
)
",[],[]
"tf.raw_ops.LookupTableImportV2(
    table_handle, keys, values, name=None
)
",[],[]
"tf.raw_ops.LookupTableInsert(
    table_handle, keys, values, name=None
)
",[],[]
"tf.raw_ops.LookupTableInsertV2(
    table_handle, keys, values, name=None
)
",[],[]
"tf.raw_ops.LookupTableRemoveV2(
    table_handle, keys, name=None
)
",[],[]
"tf.raw_ops.LookupTableSize(
    table_handle, name=None
)
",[],[]
"tf.raw_ops.LookupTableSizeV2(
    table_handle, name=None
)
",[],[]
"tf.raw_ops.LoopCond(
    input, name=None
)
",[],[]
"tf.raw_ops.LowerBound(
    sorted_inputs,
    values,
    out_type=tf.dtypes.int32,
    name=None
)
",[],[]
"tf.raw_ops.Lu(
    input,
    output_idx_type=tf.dtypes.int32,
    name=None
)
",[],[]
"tf.raw_ops.MakeIterator(
    dataset, iterator, name=None
)
","[['Makes a new iterator from the given ', 'dataset', ' and stores it in ', 'iterator', '.']]",[]
"tf.raw_ops.MapAndBatchDataset(
    input_dataset,
    other_arguments,
    batch_size,
    num_parallel_calls,
    drop_remainder,
    f,
    output_types,
    output_shapes,
    preserve_cardinality=False,
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.MapClear(
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.MapDataset(
    input_dataset,
    other_arguments,
    f,
    output_types,
    output_shapes,
    use_inter_op_parallelism=True,
    preserve_cardinality=False,
    metadata='',
    name=None
)
","[['Creates a dataset that applies ', 'f', ' to the outputs of ', 'input_dataset', '.']]",[]
"tf.raw_ops.MapDefun(
    arguments,
    captured_inputs,
    output_types,
    output_shapes,
    f,
    max_intra_op_parallelism=1,
    name=None
)
",[],[]
"tf.raw_ops.MapIncompleteSize(
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.MapPeek(
    key,
    indices,
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.MapSize(
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.MapStage(
    key,
    indices,
    values,
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.MapUnstage(
    key,
    indices,
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.MapUnstageNoKey(
    indices,
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.MatMul(
    a, b, transpose_a=False, transpose_b=False, name=None
)
",[],[]
"tf.raw_ops.MatchingFiles(
    pattern, name=None
)
",[],[]
"tf.raw_ops.MatchingFilesDataset(
    patterns, name=None
)
",[],[]
"tf.raw_ops.MatrixBandPart(
    input, num_lower, num_upper, name=None
)
",[],"# if 'input' is [[ 0,  1,  2, 3]
#                [-1,  0,  1, 2]
#                [-2, -1,  0, 1]
#                [-3, -2, -1, 0]],

tf.linalg.band_part(input, 1, -1) ==> [[ 0,  1,  2, 3]
                                       [-1,  0,  1, 2]
                                       [ 0, -1,  0, 1]
                                       [ 0,  0, -1, 0]],

tf.linalg.band_part(input, 2, 1) ==> [[ 0,  1,  0, 0]
                                      [-1,  0,  1, 0]
                                      [-2, -1,  0, 1]
                                      [ 0, -2, -1, 0]]
"
"tf.raw_ops.MatrixDeterminant(
    input, name=None
)
",[],[]
"tf.raw_ops.MatrixDiag(
    diagonal, name=None
)
",[],"# 'diagonal' is [[1, 2, 3, 4], [5, 6, 7, 8]]

and diagonal.shape = (2, 4)

tf.matrix_diag(diagonal) ==> [[[1, 0, 0, 0]
                                     [0, 2, 0, 0]
                                     [0, 0, 3, 0]
                                     [0, 0, 0, 4]],
                                    [[5, 0, 0, 0]
                                     [0, 6, 0, 0]
                                     [0, 0, 7, 0]
                                     [0, 0, 0, 8]]]

which has shape (2, 4, 4)
"
"tf.raw_ops.MatrixDiagPart(
    input, name=None
)
",[],"# 'input' is [[[1, 0, 0, 0]
               [0, 2, 0, 0]
               [0, 0, 3, 0]
               [0, 0, 0, 4]],
              [[5, 0, 0, 0]
               [0, 6, 0, 0]
               [0, 0, 7, 0]
               [0, 0, 0, 8]]]

and input.shape = (2, 4, 4)

tf.matrix_diag_part(input) ==> [[1, 2, 3, 4], [5, 6, 7, 8]]

which has shape (2, 4)
"
"tf.raw_ops.MatrixDiagPartV2(
    input, k, padding_value, name=None
)
",[],"diagonal[i, j, ..., l, n]
  = input[i, j, ..., l, n+y, n+x] ; if 0 <= n+y < M and 0 <= n+x < N,
    padding_value                 ; otherwise.
"
"tf.raw_ops.MatrixDiagPartV3(
    input, k, padding_value, align='RIGHT_LEFT', name=None
)
",[],"diagonal[i, j, ..., l, n]
  = input[i, j, ..., l, n+y, n+x] ; if 0 <= n+y < M and 0 <= n+x < N,
    padding_value                 ; otherwise.
"
"tf.raw_ops.MatrixDiagV2(
    diagonal, k, num_rows, num_cols, padding_value, name=None
)
",[],"output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper
    padding_value                             ; otherwise
"
"tf.raw_ops.MatrixDiagV3(
    diagonal,
    k,
    num_rows,
    num_cols,
    padding_value,
    align='RIGHT_LEFT',
    name=None
)
",[],"output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper
    padding_value                             ; otherwise
"
"tf.raw_ops.MatrixExponential(
    input, name=None
)
",[],[]
"tf.raw_ops.MatrixInverse(
    input, adjoint=False, name=None
)
",[],[]
"tf.raw_ops.MatrixLogarithm(
    input, name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.MatrixSetDiag(
    input, diagonal, name=None
)
",[],[]
"tf.raw_ops.MatrixSetDiagV2(
    input, diagonal, k, name=None
)
",[],"output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]
    input[i, j, ..., l, m, n]              ; otherwise
"
"tf.raw_ops.MatrixSetDiagV3(
    input, diagonal, k, align='RIGHT_LEFT', name=None
)
",[],"output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]
    input[i, j, ..., l, m, n]              ; otherwise
"
"tf.raw_ops.MatrixSolve(
    matrix, rhs, adjoint=False, name=None
)
",[],[]
"tf.raw_ops.MatrixSolveLs(
    matrix, rhs, l2_regularizer, fast=True, name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.MatrixSquareRoot(
    input, name=None
)
",[],[]
"tf.raw_ops.MatrixTriangularSolve(
    matrix, rhs, lower=True, adjoint=False, name=None
)
",[],"
a = tf.constant([[3,  0,  0,  0],
                 [2,  1,  0,  0],
                 [1,  0,  1,  0],
                 [1,  1,  1,  1]], dtype=tf.float32)

b = tf.constant([[4],
                 [2],
                 [4],
                 [2]], dtype=tf.float32)

x = tf.linalg.triangular_solve(a, b, lower=True)
x
# <tf.Tensor: shape=(4, 1), dtype=float32, numpy=
# array([[ 1.3333334 ],
#        [-0.66666675],
#        [ 2.6666665 ],
#        [-1.3333331 ]], dtype=float32)>

# in python3 one can use `a@x`
tf.matmul(a, x)
# <tf.Tensor: shape=(4, 1), dtype=float32, numpy=
# array([[4.       ],
#        [2.       ],
#        [4.       ],
#        [1.9999999]], dtype=float32)>
"
"tf.raw_ops.Max(
    input, axis, keep_dims=False, name=None
)
",[],[]
"tf.raw_ops.MaxIntraOpParallelismDataset(
    input_dataset,
    max_intra_op_parallelism,
    output_types,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.MaxPool(
    input,
    ksize,
    strides,
    padding,
    explicit_paddings=[],
    data_format='NHWC',
    name=None
)
",[],[]
"tf.raw_ops.MaxPool3D(
    input, ksize, strides, padding, data_format='NDHWC', name=None
)
",[],[]
"tf.raw_ops.MaxPool3DGrad(
    orig_input,
    orig_output,
    grad,
    ksize,
    strides,
    padding,
    data_format='NDHWC',
    name=None
)
",[],[]
"tf.raw_ops.MaxPool3DGradGrad(
    orig_input,
    orig_output,
    grad,
    ksize,
    strides,
    padding,
    data_format='NDHWC',
    name=None
)
",[],[]
"tf.raw_ops.MaxPoolGrad(
    orig_input,
    orig_output,
    grad,
    ksize,
    strides,
    padding,
    explicit_paddings=[],
    data_format='NHWC',
    name=None
)
",[],[]
"tf.raw_ops.MaxPoolGradGrad(
    orig_input,
    orig_output,
    grad,
    ksize,
    strides,
    padding,
    data_format='NHWC',
    name=None
)
",[],[]
"tf.raw_ops.MaxPoolGradGradV2(
    orig_input,
    orig_output,
    grad,
    ksize,
    strides,
    padding,
    data_format='NHWC',
    name=None
)
",[],[]
"tf.raw_ops.MaxPoolGradGradWithArgmax(
    input,
    grad,
    argmax,
    ksize,
    strides,
    padding,
    include_batch_in_index=False,
    name=None
)
",[],[]
"tf.raw_ops.MaxPoolGradV2(
    orig_input,
    orig_output,
    grad,
    ksize,
    strides,
    padding,
    data_format='NHWC',
    name=None
)
",[],[]
"tf.raw_ops.MaxPoolGradWithArgmax(
    input,
    grad,
    argmax,
    ksize,
    strides,
    padding,
    include_batch_in_index=False,
    name=None
)
",[],[]
"tf.raw_ops.MaxPoolV2(
    input, ksize, strides, padding, data_format='NHWC', name=None
)
",[],[]
"tf.raw_ops.MaxPoolWithArgmax(
    input,
    ksize,
    strides,
    padding,
    Targmax=tf.dtypes.int64,
    include_batch_in_index=False,
    name=None
)
",[],[]
"tf.raw_ops.Maximum(
    x, y, name=None
)
",[],"x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-2., 0., 2., 5.])
tf.math.maximum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>"
"tf.raw_ops.Mean(
    input, axis, keep_dims=False, name=None
)
",[],[]
"tf.raw_ops.Merge(
    inputs, name=None
)
","[['Forwards the value of an available tensor from ', 'inputs', ' to ', 'output', '.']]",[]
"tf.raw_ops.MergeSummary(
    inputs, name=None
)
",[],[]
"tf.raw_ops.MergeV2Checkpoints(
    checkpoint_prefixes,
    destination_prefix,
    delete_old_dirs=True,
    allow_missing_files=False,
    name=None
)
",[],[]
"tf.raw_ops.Mfcc(
    spectrogram,
    sample_rate,
    upper_frequency_limit=4000,
    lower_frequency_limit=20,
    filterbank_channel_count=40,
    dct_coefficient_count=13,
    name=None
)
",[],[]
"tf.raw_ops.Min(
    input, axis, keep_dims=False, name=None
)
",[],[]
"tf.raw_ops.Minimum(
    x, y, name=None
)
",[],"x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-5., -2., 0., 3.])
tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>"
"tf.raw_ops.MirrorPad(
    input, paddings, mode, name=None
)
",[],"# 't' is [[1, 2, 3], [4, 5, 6]].
# 'paddings' is [[1, 1]], [2, 2]].
# 'mode' is SYMMETRIC.
# rank of 't' is 2.
pad(t, paddings) ==> [[2, 1, 1, 2, 3, 3, 2]
                      [2, 1, 1, 2, 3, 3, 2]
                      [5, 4, 4, 5, 6, 6, 5]
                      [5, 4, 4, 5, 6, 6, 5]]
"
"tf.raw_ops.MirrorPadGrad(
    input, paddings, mode, name=None
)
","[['Gradient op for ', 'MirrorPad', ' op. This op folds a mirror-padded tensor.']]","# 't' is [[1, 2, 3], [4, 5, 6], [7, 8, 9]].
# 'paddings' is [[0, 1]], [0, 1]].
# 'mode' is SYMMETRIC.
# rank of 't' is 2.
pad(t, paddings) ==> [[ 1,  5]
                      [11, 28]]
"
"tf.raw_ops.Mod(
    x, y, name=None
)
",[],[]
"tf.raw_ops.ModelDataset(
    input_dataset,
    output_types,
    output_shapes,
    algorithm=0,
    cpu_budget=0,
    ram_budget=0,
    name=None
)
",[],[]
"tf.raw_ops.Mul(
    x, y, name=None
)
",[],[]
"tf.raw_ops.MulNoNan(
    x, y, name=None
)
",[],[]
"tf.raw_ops.MultiDeviceIterator(
    devices, shared_name, container, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.MultiDeviceIteratorFromStringHandle(
    string_handle, output_types=[], output_shapes=[], name=None
)
",[],[]
"tf.raw_ops.MultiDeviceIteratorGetNextFromShard(
    multi_device_iterator,
    shard_num,
    incarnation_id,
    output_types,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.MultiDeviceIteratorInit(
    dataset, multi_device_iterator, max_buffer_size, name=None
)
",[],[]
"tf.raw_ops.MultiDeviceIteratorToStringHandle(
    multi_device_iterator, name=None
)
",[],[]
"tf.raw_ops.Multinomial(
    logits,
    num_samples,
    seed=0,
    seed2=0,
    output_dtype=tf.dtypes.int64,
    name=None
)
",[],[]
"tf.raw_ops.MutableDenseHashTable(
    empty_key,
    value_dtype,
    container='',
    shared_name='',
    use_node_name_sharing=False,
    value_shape=[],
    initial_num_buckets=131072,
    max_load_factor=0.8,
    name=None
)
",[],[]
"tf.raw_ops.MutableDenseHashTableV2(
    empty_key,
    deleted_key,
    value_dtype,
    container='',
    shared_name='',
    use_node_name_sharing=False,
    value_shape=[],
    initial_num_buckets=131072,
    max_load_factor=0.8,
    name=None
)
",[],[]
"tf.raw_ops.MutableHashTable(
    key_dtype,
    value_dtype,
    container='',
    shared_name='',
    use_node_name_sharing=False,
    name=None
)
",[],[]
"tf.raw_ops.MutableHashTableOfTensors(
    key_dtype,
    value_dtype,
    container='',
    shared_name='',
    use_node_name_sharing=False,
    value_shape=[],
    name=None
)
",[],[]
"tf.raw_ops.MutableHashTableOfTensorsV2(
    key_dtype,
    value_dtype,
    container='',
    shared_name='',
    use_node_name_sharing=False,
    value_shape=[],
    name=None
)
",[],[]
"tf.raw_ops.MutableHashTableV2(
    key_dtype,
    value_dtype,
    container='',
    shared_name='',
    use_node_name_sharing=False,
    name=None
)
",[],[]
"tf.raw_ops.MutexLock(
    mutex, name=None
)
",[],"
mutex = mutex_v2(
  shared_name=handle_name, container=container, name=name)

def execute_in_critical_section(fn, *args, **kwargs):
  lock = gen_resource_variable_ops.mutex_lock(mutex)

  with ops.control_dependencies([lock]):
    r = fn(*args, **kwargs)

  with ops.control_dependencies(nest.flatten(r)):
    with ops.colocate_with(mutex):
      ensure_lock_exists = mutex_lock_identity(lock)

    # Make sure that if any element of r is accessed, all of
    # them are executed together.
    r = nest.map_structure(tf.identity, r)

  with ops.control_dependencies([ensure_lock_exists]):
    return nest.map_structure(tf.identity, r)
"
"tf.raw_ops.MutexV2(
    container='', shared_name='', name=None
)
","[['Creates a Mutex resource that can be locked by ', 'MutexLock', '.']]",[]
"tf.raw_ops.NcclAllReduce(
    input, reduction, num_devices, shared_name, name=None
)
",[],[]
"tf.raw_ops.NcclBroadcast(
    input, shape, name=None
)
","[['Sends ', 'input', ' to all devices that are connected to the output.']]",[]
"tf.raw_ops.NcclReduce(
    input, reduction, name=None
)
","[['Reduces ', 'input', ' from ', 'num_devices', ' using ', 'reduction', ' to a single device.']]",[]
"tf.raw_ops.Ndtri(
    x, name=None
)
",[],[]
"tf.raw_ops.Neg(
    x, name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.NextAfter(
    x1, x2, name=None
)
","[['Returns the next representable value of ', 'x1', ' in the direction of ', 'x2', ', element-wise.']]",[]
"tf.raw_ops.NextIteration(
    data, name=None
)
",[],[]
"tf.raw_ops.NoOp(
    name=None
)
",[],[]
"tf.raw_ops.NonDeterministicInts(
    shape,
    dtype=tf.dtypes.int64,
    name=None
)
",[],[]
"tf.raw_ops.NonMaxSuppression(
    boxes, scores, max_output_size, iou_threshold=0.5, name=None
)
",[],[]
"tf.raw_ops.NonMaxSuppressionV2(
    boxes, scores, max_output_size, iou_threshold, name=None
)
",[],[]
"tf.raw_ops.NonMaxSuppressionV3(
    boxes, scores, max_output_size, iou_threshold, score_threshold, name=None
)
",[],[]
"tf.raw_ops.NonMaxSuppressionV4(
    boxes,
    scores,
    max_output_size,
    iou_threshold,
    score_threshold,
    pad_to_max_output_size=False,
    name=None
)
",[],[]
"tf.raw_ops.NonMaxSuppressionV5(
    boxes,
    scores,
    max_output_size,
    iou_threshold,
    score_threshold,
    soft_nms_sigma,
    pad_to_max_output_size=False,
    name=None
)
",[],[]
"tf.raw_ops.NonMaxSuppressionWithOverlaps(
    overlaps,
    scores,
    max_output_size,
    overlap_threshold,
    score_threshold,
    name=None
)
",[],[]
"tf.raw_ops.NonSerializableDataset(
    input_dataset, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.NotEqual(
    x, y, incompatible_shape_error=True, name=None
)
",[],[]
"tf.raw_ops.NthElement(
    input, n, reverse=False, name=None
)
","[['Finds values of the ', 'n', '-th order statistic for the last dimension.']]","values.shape = input.shape[:-1]
"
"tf.raw_ops.OneHot(
    indices, depth, on_value, off_value, axis=-1, name=None
)
",[],"  features x depth if axis == -1
  depth x features if axis == 0
"
"tf.raw_ops.OneShotIterator(
    dataset_factory,
    output_types,
    output_shapes,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.OnesLike(
    x, name=None
)
",[],[]
"tf.raw_ops.OptimizeDataset(
    input_dataset,
    optimizations,
    output_types,
    output_shapes,
    optimization_configs=[],
    name=None
)
","[['Creates a dataset by applying optimizations to ', 'input_dataset', '.']]",[]
"tf.raw_ops.OptimizeDatasetV2(
    input_dataset,
    optimizations_enabled,
    optimizations_disabled,
    optimizations_default,
    output_types,
    output_shapes,
    optimization_configs=[],
    name=None
)
","[['Creates a dataset by applying related optimizations to ', 'input_dataset', '.']]",[]
"tf.raw_ops.OptionalFromValue(
    components, name=None
)
",[],[]
"tf.raw_ops.OptionalGetValue(
    optional, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.OptionalHasValue(
    optional, name=None
)
",[],[]
"tf.raw_ops.OptionalNone(
    name=None
)
",[],[]
"tf.raw_ops.OptionsDataset(
    input_dataset,
    serialized_options,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset by attaching tf.data.Options to ', 'input_dataset', '.']]",[]
"tf.raw_ops.OrderedMapClear(
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.OrderedMapIncompleteSize(
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.OrderedMapPeek(
    key,
    indices,
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.OrderedMapSize(
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.OrderedMapStage(
    key,
    indices,
    values,
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.OrderedMapUnstage(
    key,
    indices,
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.OrderedMapUnstageNoKey(
    indices,
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.OutfeedDequeue(
    dtype, shape, device_ordinal=-1, name=None
)
",[],[]
"tf.raw_ops.OutfeedDequeueTuple(
    dtypes, shapes, device_ordinal=-1, name=None
)
",[],[]
"tf.raw_ops.OutfeedDequeueTupleV2(
    device_ordinal, dtypes, shapes, name=None
)
",[],[]
"tf.raw_ops.OutfeedDequeueV2(
    device_ordinal, dtype, shape, name=None
)
",[],[]
"tf.raw_ops.OutfeedEnqueue(
    input, name=None
)
",[],[]
"tf.raw_ops.OutfeedEnqueueTuple(
    inputs, name=None
)
",[],[]
"tf.raw_ops.Pack(
    values, axis=0, name=None
)
","[['Packs a list of ', 'N', ' rank-', 'R', ' tensors into one rank-', '(R+1)', ' tensor.']]","# 'x' is [1, 4]
# 'y' is [2, 5]
# 'z' is [3, 6]
pack([x, y, z]) => [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.
pack([x, y, z], axis=1) => [[1, 2, 3], [4, 5, 6]]
"
"tf.raw_ops.Pad(
    input, paddings, name=None
)
",[],"# 't' is [[1, 1], [2, 2]]
# 'paddings' is [[1, 1], [2, 2]]
# rank of 't' is 2
pad(t, paddings) ==> [[0, 0, 0, 0, 0, 0]
                      [0, 0, 1, 1, 0, 0]
                      [0, 0, 2, 2, 0, 0]
                      [0, 0, 0, 0, 0, 0]]
"
"tf.raw_ops.PadV2(
    input, paddings, constant_values, name=None
)
",[],"# 't' is [[1, 1], [2, 2]]
# 'paddings' is [[1, 1], [2, 2]]
# 'constant_values' is 0
# rank of 't' is 2
pad(t, paddings) ==> [[0, 0, 0, 0, 0, 0]
                      [0, 0, 1, 1, 0, 0]
                      [0, 0, 2, 2, 0, 0]
                      [0, 0, 0, 0, 0, 0]]
"
"tf.raw_ops.PaddedBatchDataset(
    input_dataset,
    batch_size,
    padded_shapes,
    padding_values,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset that batches and pads ', 'batch_size', ' elements from the input.']]",[]
"tf.raw_ops.PaddedBatchDatasetV2(
    input_dataset,
    batch_size,
    padded_shapes,
    padding_values,
    drop_remainder,
    output_shapes,
    parallel_copy=False,
    metadata='',
    name=None
)
","[['Creates a dataset that batches and pads ', 'batch_size', ' elements from the input.']]",[]
"tf.raw_ops.PaddingFIFOQueue(
    component_types,
    shapes=[],
    capacity=-1,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.PaddingFIFOQueueV2(
    component_types,
    shapes=[],
    capacity=-1,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.ParallelBatchDataset(
    input_dataset,
    batch_size,
    num_parallel_calls,
    drop_remainder,
    output_types,
    output_shapes,
    parallel_copy=False,
    deterministic='default',
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.ParallelConcat(
    values, shape, name=None
)
","[['Concatenates a list of ', 'N', ' tensors along the first dimension.']]","# 'x' is [[1, 4]]
# 'y' is [[2, 5]]
# 'z' is [[3, 6]]
parallel_concat([x, y, z]) => [[1, 4], [2, 5], [3, 6]]  # Pack along first dim.
"
"tf.raw_ops.ParallelDynamicStitch(
    indices, data, name=None
)
","[['Interleave the values from the ', 'data', ' tensors into a single tensor.']]","    merged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]
"
"tf.raw_ops.ParallelFilterDataset(
    input_dataset,
    other_arguments,
    num_parallel_calls,
    predicate,
    output_types,
    output_shapes,
    deterministic='default',
    metadata='',
    name=None
)
","[['Creates a dataset containing elements of ', 'input_dataset', ' matching ', 'predicate', '.']]",[]
"tf.raw_ops.ParallelInterleaveDataset(
    input_dataset,
    other_arguments,
    cycle_length,
    block_length,
    sloppy,
    buffer_output_elements,
    prefetch_input_elements,
    f,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset that applies ', 'f', ' to the outputs of ', 'input_dataset', '.']]",[]
"tf.raw_ops.ParallelInterleaveDatasetV2(
    input_dataset,
    other_arguments,
    cycle_length,
    block_length,
    num_parallel_calls,
    f,
    output_types,
    output_shapes,
    sloppy=False,
    metadata='',
    name=None
)
","[['Creates a dataset that applies ', 'f', ' to the outputs of ', 'input_dataset', '.']]",[]
"tf.raw_ops.ParallelInterleaveDatasetV3(
    input_dataset,
    other_arguments,
    cycle_length,
    block_length,
    num_parallel_calls,
    f,
    output_types,
    output_shapes,
    deterministic='default',
    metadata='',
    name=None
)
","[['Creates a dataset that applies ', 'f', ' to the outputs of ', 'input_dataset', '.']]",[]
"tf.raw_ops.ParallelInterleaveDatasetV4(
    input_dataset,
    other_arguments,
    cycle_length,
    block_length,
    buffer_output_elements,
    prefetch_input_elements,
    num_parallel_calls,
    f,
    output_types,
    output_shapes,
    deterministic='default',
    metadata='',
    name=None
)
","[['Creates a dataset that applies ', 'f', ' to the outputs of ', 'input_dataset', '.']]",[]
"tf.raw_ops.ParallelMapDataset(
    input_dataset,
    other_arguments,
    num_parallel_calls,
    f,
    output_types,
    output_shapes,
    use_inter_op_parallelism=True,
    sloppy=False,
    preserve_cardinality=False,
    metadata='',
    name=None
)
","[['Creates a dataset that applies ', 'f', ' to the outputs of ', 'input_dataset', '.']]",[]
"tf.raw_ops.ParallelMapDatasetV2(
    input_dataset,
    other_arguments,
    num_parallel_calls,
    f,
    output_types,
    output_shapes,
    use_inter_op_parallelism=True,
    deterministic='default',
    preserve_cardinality=False,
    metadata='',
    name=None
)
","[['Creates a dataset that applies ', 'f', ' to the outputs of ', 'input_dataset', '.']]",[]
"tf.raw_ops.ParameterizedTruncatedNormal(
    shape, means, stdevs, minvals, maxvals, seed=0, seed2=0, name=None
)
",[],[]
"tf.raw_ops.ParseExample(
    serialized,
    names,
    sparse_keys,
    dense_keys,
    dense_defaults,
    sparse_types,
    dense_shapes,
    name=None
)
",[],[]
"tf.raw_ops.ParseExampleDataset(
    input_dataset,
    num_parallel_calls,
    dense_defaults,
    sparse_keys,
    dense_keys,
    sparse_types,
    dense_shapes,
    output_types,
    output_shapes,
    sloppy=False,
    ragged_keys=[],
    ragged_value_types=[],
    ragged_split_types=[],
    name=None
)
","[['Transforms ', 'input_dataset', ' containing ', 'Example', ' protos as vectors of DT_STRING into a dataset of ', 'Tensor', ' or ', 'SparseTensor', ' objects representing the parsed features.']]",[]
"tf.raw_ops.ParseExampleDatasetV2(
    input_dataset,
    num_parallel_calls,
    dense_defaults,
    sparse_keys,
    dense_keys,
    sparse_types,
    dense_shapes,
    output_types,
    output_shapes,
    deterministic='default',
    ragged_keys=[],
    ragged_value_types=[],
    ragged_split_types=[],
    name=None
)
","[['Transforms ', 'input_dataset', ' containing ', 'Example', ' protos as vectors of DT_STRING into a dataset of ', 'Tensor', ' or ', 'SparseTensor', ' objects representing the parsed features.']]",[]
"tf.raw_ops.ParseExampleV2(
    serialized,
    names,
    sparse_keys,
    dense_keys,
    ragged_keys,
    dense_defaults,
    num_sparse,
    sparse_types,
    ragged_value_types,
    ragged_split_types,
    dense_shapes,
    name=None
)
",[],[]
"tf.raw_ops.ParseSequenceExample(
    serialized,
    debug_name,
    context_dense_defaults,
    feature_list_dense_missing_assumed_empty,
    context_sparse_keys,
    context_dense_keys,
    feature_list_sparse_keys,
    feature_list_dense_keys,
    Ncontext_sparse=0,
    Ncontext_dense=0,
    Nfeature_list_sparse=0,
    Nfeature_list_dense=0,
    context_sparse_types=[],
    feature_list_dense_types=[],
    context_dense_shapes=[],
    feature_list_sparse_types=[],
    feature_list_dense_shapes=[],
    name=None
)
",[],[]
"tf.raw_ops.ParseSingleExample(
    serialized,
    dense_defaults,
    num_sparse,
    sparse_keys,
    dense_keys,
    sparse_types,
    dense_shapes,
    name=None
)
",[],[]
"tf.raw_ops.ParseSingleSequenceExample(
    serialized,
    feature_list_dense_missing_assumed_empty,
    context_sparse_keys,
    context_dense_keys,
    feature_list_sparse_keys,
    feature_list_dense_keys,
    context_dense_defaults,
    debug_name,
    context_sparse_types=[],
    feature_list_dense_types=[],
    context_dense_shapes=[],
    feature_list_sparse_types=[],
    feature_list_dense_shapes=[],
    name=None
)
",[],[]
"tf.raw_ops.ParseTensor(
    serialized, out_type, name=None
)
",[],[]
"tf.raw_ops.PartitionedCall(
    args,
    Tout,
    f,
    config='',
    config_proto='',
    executor_type='',
    name=None
)
","[['returns ', 'f(inputs)', ', where ', 'f', ""'s body is placed and partitioned.""]]",[]
"tf.raw_ops.Placeholder(
    dtype, shape=None, name=None
)
",[],[]
"tf.raw_ops.PlaceholderV2(
    dtype, shape, name=None
)
",[],[]
"tf.raw_ops.PlaceholderWithDefault(
    input, shape, name=None
)
","[['A placeholder op that passes through ', 'input', ' when its output is not fed.']]",[]
"tf.raw_ops.Polygamma(
    a, x, name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.PopulationCount(
    x, name=None
)
",[],[]
"tf.raw_ops.Pow(
    x, y, name=None
)
","[[None, '\n']]","# tensor 'x' is [[2, 2]], [3, 3]]
# tensor 'y' is [[8, 16], [2, 3]]
tf.pow(x, y) ==> [[256, 65536], [9, 27]]
"
"tf.raw_ops.PrefetchDataset(
    input_dataset,
    buffer_size,
    output_types,
    output_shapes,
    slack_period=0,
    legacy_autotune=True,
    buffer_size_min=0,
    metadata='',
    name=None
)
","[['Creates a dataset that asynchronously prefetches elements from ', 'input_dataset', '.']]",[]
"tf.raw_ops.Prelinearize(
    input, shape=[], layout=[], name=None
)
",[],[]
"tf.raw_ops.PrelinearizeTuple(
    inputs, shapes, layouts=[], name=None
)
",[],[]
"tf.raw_ops.PreventGradient(
    input, message='', name=None
)
",[],[]
"tf.raw_ops.Print(
    input, data, message='', first_n=-1, summarize=3, name=None
)
",[],[]
"tf.raw_ops.PrintV2(
    input, output_stream='stderr', end='\n', name=None
)
",[],[]
"tf.raw_ops.PriorityQueue(
    shapes,
    component_types=[],
    capacity=-1,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.PriorityQueueV2(
    shapes,
    component_types=[],
    capacity=-1,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.PrivateThreadPoolDataset(
    input_dataset, num_threads, output_types, output_shapes, name=None
)
","[['Creates a dataset that uses a custom thread pool to compute ', 'input_dataset', '.']]",[]
"tf.raw_ops.Prod(
    input, axis, keep_dims=False, name=None
)
",[],[]
"tf.raw_ops.PyFunc(
    input, token, Tout, name=None
)
",[],[]
"tf.raw_ops.PyFuncStateless(
    input, token, Tout, name=None
)
",[],[]
"tf.raw_ops.Qr(
    input, full_matrices=False, name=None
)
",[],"# a is a tensor.
# q is a tensor of orthonormal matrices.
# r is a tensor of upper triangular matrices.
q, r = qr(a)
q_full, r_full = qr(a, full_matrices=True)
"
"tf.raw_ops.QuantizeAndDequantize(
    input,
    signed_input=True,
    num_bits=8,
    range_given=False,
    input_min=0,
    input_max=0,
    name=None
)
",[],[]
"tf.raw_ops.QuantizeAndDequantizeV2(
    input,
    input_min,
    input_max,
    signed_input=True,
    num_bits=8,
    range_given=False,
    round_mode='HALF_TO_EVEN',
    narrow_range=False,
    axis=-1,
    name=None
)
",[],[]
"tf.raw_ops.QuantizeAndDequantizeV3(
    input,
    input_min,
    input_max,
    num_bits,
    signed_input=True,
    range_given=True,
    narrow_range=False,
    axis=-1,
    name=None
)
",[],[]
"tf.raw_ops.QuantizeAndDequantizeV4(
    input,
    input_min,
    input_max,
    signed_input=True,
    num_bits=8,
    range_given=False,
    round_mode='HALF_TO_EVEN',
    narrow_range=False,
    axis=-1,
    name=None
)
",[],[]
"tf.raw_ops.QuantizeAndDequantizeV4Grad(
    gradients, input, input_min, input_max, axis=-1, name=None
)
","[['Returns the gradient of ', 'QuantizeAndDequantizeV4', '.']]",[]
"tf.raw_ops.QuantizeDownAndShrinkRange(
    input, input_min, input_max, out_type, name=None
)
",[],[]
"tf.raw_ops.QuantizeV2(
    input,
    min_range,
    max_range,
    T,
    mode='MIN_COMBINED',
    round_mode='HALF_AWAY_FROM_ZERO',
    narrow_range=False,
    axis=-1,
    ensure_minimum_range=0.01,
    name=None
)
",[],"out[i] = (in[i] - min_range) * range(T) / (max_range - min_range)
if T == qint8: out[i] -= (range(T) + 1) / 2.0
"
"tf.raw_ops.QuantizedAdd(
    x,
    y,
    min_x,
    max_x,
    min_y,
    max_y,
    Toutput=tf.dtypes.qint32,
    name=None
)
",[],[]
"tf.raw_ops.QuantizedAvgPool(
    input, min_input, max_input, ksize, strides, padding, name=None
)
",[],[]
"tf.raw_ops.QuantizedBatchNormWithGlobalNormalization(
    t,
    t_min,
    t_max,
    m,
    m_min,
    m_max,
    v,
    v_min,
    v_max,
    beta,
    beta_min,
    beta_max,
    gamma,
    gamma_min,
    gamma_max,
    out_type,
    variance_epsilon,
    scale_after_normalization,
    name=None
)
",[],[]
"tf.raw_ops.QuantizedBiasAdd(
    input, bias, min_input, max_input, min_bias, max_bias, out_type, name=None
)
",[],[]
"tf.raw_ops.QuantizedConcat(
    concat_dim, values, input_mins, input_maxes, name=None
)
",[],[]
"tf.raw_ops.QuantizedConv2D(
    input,
    filter,
    min_input,
    max_input,
    min_filter,
    max_filter,
    strides,
    padding,
    out_type=tf.dtypes.qint32,
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedConv2DAndRelu(
    input,
    filter,
    min_input,
    max_input,
    min_filter,
    max_filter,
    strides,
    padding,
    out_type=tf.dtypes.qint32,
    dilations=[1, 1, 1, 1],
    padding_list=[],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedConv2DAndReluAndRequantize(
    input,
    filter,
    min_input,
    max_input,
    min_filter,
    max_filter,
    min_freezed_output,
    max_freezed_output,
    strides,
    padding,
    out_type=tf.dtypes.quint8,
    dilations=[1, 1, 1, 1],
    padding_list=[],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedConv2DAndRequantize(
    input,
    filter,
    min_input,
    max_input,
    min_filter,
    max_filter,
    min_freezed_output,
    max_freezed_output,
    strides,
    padding,
    out_type=tf.dtypes.qint8,
    dilations=[1, 1, 1, 1],
    padding_list=[],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedConv2DPerChannel(
    input,
    filter,
    min_input,
    max_input,
    min_filter,
    max_filter,
    strides,
    padding,
    out_type=tf.dtypes.qint32,
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedConv2DWithBias(
    input,
    filter,
    bias,
    min_input,
    max_input,
    min_filter,
    max_filter,
    strides,
    padding,
    out_type=tf.dtypes.qint32,
    dilations=[1, 1, 1, 1],
    padding_list=[],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedConv2DWithBiasAndRelu(
    input,
    filter,
    bias,
    min_input,
    max_input,
    min_filter,
    max_filter,
    strides,
    padding,
    out_type=tf.dtypes.qint32,
    dilations=[1, 1, 1, 1],
    padding_list=[],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedConv2DWithBiasAndReluAndRequantize(
    input,
    filter,
    bias,
    min_input,
    max_input,
    min_filter,
    max_filter,
    min_freezed_output,
    max_freezed_output,
    strides,
    padding,
    out_type=tf.dtypes.quint8,
    dilations=[1, 1, 1, 1],
    padding_list=[],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedConv2DWithBiasAndRequantize(
    input,
    filter,
    bias,
    min_input,
    max_input,
    min_filter,
    max_filter,
    min_freezed_output,
    max_freezed_output,
    strides,
    padding,
    out_type=tf.dtypes.qint8,
    dilations=[1, 1, 1, 1],
    padding_list=[],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedConv2DWithBiasSignedSumAndReluAndRequantize(
    input,
    filter,
    bias,
    min_input,
    max_input,
    min_filter,
    max_filter,
    min_freezed_output,
    max_freezed_output,
    summand,
    min_summand,
    max_summand,
    strides,
    padding,
    out_type=tf.dtypes.quint8,
    dilations=[1, 1, 1, 1],
    padding_list=[],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedConv2DWithBiasSumAndRelu(
    input,
    filter,
    bias,
    min_input,
    max_input,
    min_filter,
    max_filter,
    summand,
    strides,
    padding,
    out_type=tf.dtypes.qint32,
    dilations=[1, 1, 1, 1],
    padding_list=[],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedConv2DWithBiasSumAndReluAndRequantize(
    input,
    filter,
    bias,
    min_input,
    max_input,
    min_filter,
    max_filter,
    min_freezed_output,
    max_freezed_output,
    summand,
    min_summand,
    max_summand,
    strides,
    padding,
    out_type=tf.dtypes.quint8,
    dilations=[1, 1, 1, 1],
    padding_list=[],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedDepthwiseConv2D(
    input,
    filter,
    min_input,
    max_input,
    min_filter,
    max_filter,
    strides,
    padding,
    out_type=tf.dtypes.qint32,
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedDepthwiseConv2DWithBias(
    input,
    filter,
    bias,
    min_input,
    max_input,
    min_filter,
    max_filter,
    strides,
    padding,
    out_type=tf.dtypes.qint32,
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndRelu(
    input,
    filter,
    bias,
    min_input,
    max_input,
    min_filter,
    max_filter,
    strides,
    padding,
    out_type=tf.dtypes.qint32,
    dilations=[1, 1, 1, 1],
    padding_list=[],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedDepthwiseConv2DWithBiasAndReluAndRequantize(
    input,
    filter,
    bias,
    min_input,
    max_input,
    min_filter,
    max_filter,
    min_freezed_output,
    max_freezed_output,
    strides,
    padding,
    out_type=tf.dtypes.quint8,
    dilations=[1, 1, 1, 1],
    padding_list=[],
    name=None
)
",[],[]
"tf.raw_ops.QuantizedInstanceNorm(
    x,
    x_min,
    x_max,
    output_range_given=False,
    given_y_min=0,
    given_y_max=0,
    variance_epsilon=1e-05,
    min_separation=0.001,
    name=None
)
",[],[]
"tf.raw_ops.QuantizedMatMul(
    a,
    b,
    min_a,
    max_a,
    min_b,
    max_b,
    Toutput=tf.dtypes.qint32,
    transpose_a=False,
    transpose_b=False,
    Tactivation=tf.dtypes.quint8,
    name=None
)
","[['Perform a quantized matrix multiplication of  ', 'a', ' by the matrix ', 'b', '.']]",[]
"tf.raw_ops.QuantizedMatMulWithBias(
    a,
    b,
    bias,
    min_a,
    max_a,
    min_b,
    max_b,
    Toutput=tf.dtypes.qint32,
    transpose_a=False,
    transpose_b=False,
    input_quant_mode='MIN_FIRST',
    name=None
)
","[['Performs a quantized matrix multiplication of ', 'a', ' by the matrix ', 'b', ' with bias']]","out: A `Tensor` of type `Toutput`.
min_out: A `Tensor` of type `float32`.
max_out: A `Tensor` of type `float32`.
"
"tf.raw_ops.QuantizedMatMulWithBiasAndDequantize(
    a,
    b,
    bias,
    min_a,
    max_a,
    min_b,
    max_b,
    min_freezed_output,
    max_freezed_output,
    Toutput,
    transpose_a=False,
    transpose_b=False,
    input_quant_mode='MIN_FIRST',
    name=None
)
",[],[]
"tf.raw_ops.QuantizedMatMulWithBiasAndRelu(
    a,
    b,
    bias,
    min_a,
    max_a,
    min_b,
    max_b,
    Toutput=tf.dtypes.qint32,
    transpose_a=False,
    transpose_b=False,
    input_quant_mode='MIN_FIRST',
    name=None
)
","[['Perform a quantized matrix multiplication of  ', 'a', ' by the matrix ', 'b', ' with bias']]","out: A `Tensor` of type `Toutput`.
min_out: A `Tensor` of type `float32`.
max_out: A `Tensor` of type `float32`.
"
"tf.raw_ops.QuantizedMatMulWithBiasAndReluAndRequantize(
    a,
    b,
    bias,
    min_a,
    max_a,
    min_b,
    max_b,
    min_freezed_output,
    max_freezed_output,
    Toutput=tf.dtypes.quint8,
    transpose_a=False,
    transpose_b=False,
    input_quant_mode='MIN_FIRST',
    name=None
)
","[['Perform a quantized matrix multiplication of  ', 'a', ' by the matrix ', 'b', ' with bias']]","out: A `Tensor` of type `Toutput`.
min_out: A `Tensor` of type `float32`.
max_out: A `Tensor` of type `float32`.
"
"tf.raw_ops.QuantizedMatMulWithBiasAndRequantize(
    a,
    b,
    bias,
    min_a,
    max_a,
    min_b,
    max_b,
    min_freezed_output,
    max_freezed_output,
    Toutput=tf.dtypes.quint8,
    transpose_a=False,
    transpose_b=False,
    input_quant_mode='MIN_FIRST',
    name=None
)
",[],[]
"tf.raw_ops.QuantizedMaxPool(
    input, min_input, max_input, ksize, strides, padding, name=None
)
",[],[]
"tf.raw_ops.QuantizedMul(
    x,
    y,
    min_x,
    max_x,
    min_y,
    max_y,
    Toutput=tf.dtypes.qint32,
    name=None
)
",[],[]
"tf.raw_ops.QuantizedRelu(
    features,
    min_features,
    max_features,
    out_type=tf.dtypes.quint8,
    name=None
)
","[['Computes Quantized Rectified Linear: ', 'max(features, 0)']]",[]
"tf.raw_ops.QuantizedRelu6(
    features,
    min_features,
    max_features,
    out_type=tf.dtypes.quint8,
    name=None
)
","[['Computes Quantized Rectified Linear 6: ', 'min(max(features, 0), 6)']]",[]
"tf.raw_ops.QuantizedReluX(
    features,
    max_value,
    min_features,
    max_features,
    out_type=tf.dtypes.quint8,
    name=None
)
","[['Computes Quantized Rectified Linear X: ', 'min(max(features, 0), max_value)']]",[]
"tf.raw_ops.QuantizedReshape(
    tensor, shape, input_min, input_max, name=None
)
",[],"
<!-- Tabular view -->
 <table class=""responsive fixed orange"">
<colgroup><col width=""214px""><col></colgroup>
<tr><th colspan=""2""><h2 class=""add-link"">Args</h2></th></tr>

<tr>
<td>
`tensor`<a id=""tensor""></a>
</td>
<td>
A `Tensor`.
</td>
</tr><tr>
<td>
`shape`<a id=""shape""></a>
</td>
<td>
A `Tensor`. Must be one of the following types: `int32`, `int64`.
Defines the shape of the output tensor.
</td>
</tr><tr>
<td>
`input_min`<a id=""input_min""></a>
</td>
<td>
A `Tensor` of type `float32`. The minimum value of the input.
</td>
</tr><tr>
<td>
`input_max`<a id=""input_max""></a>
</td>
<td>
A `Tensor` of type `float32`. The maximum value of the input.
</td>
</tr><tr>
<td>
`name`<a id=""name""></a>
</td>
<td>
A name for the operation (optional).
</td>
</tr>
</table>



<!-- Tabular view -->
 <table class=""responsive fixed orange"">
<colgroup><col width=""214px""><col></colgroup>
<tr><th colspan=""2""><h2 class=""add-link"">Returns</h2></th></tr>
<tr class=""alt"">
<td colspan=""2"">
A tuple of `Tensor` objects (output, output_min, output_max).
</td>
</tr>
<tr>
<td>
`output`<a id=""output""></a>
</td>
<td>
A `Tensor`. Has the same type as `tensor`.
</td>
</tr><tr>
<td>
`output_min`<a id=""output_min""></a>
</td>
<td>
A `Tensor` of type `float32`.
</td>
</tr><tr>
<td>
`output_max`<a id=""output_max""></a>
</td>
<td>
A `Tensor` of type `float32`.
</td>
</tr>
</table>
"
"tf.raw_ops.QuantizedResizeBilinear(
    images,
    size,
    min,
    max,
    align_corners=False,
    half_pixel_centers=False,
    name=None
)
","[['Resize quantized ', 'images', ' to ', 'size', ' using quantized bilinear interpolation.']]",[]
"tf.raw_ops.QueueClose(
    handle, cancel_pending_enqueues=False, name=None
)
",[],[]
"tf.raw_ops.QueueCloseV2(
    handle, cancel_pending_enqueues=False, name=None
)
",[],[]
"tf.raw_ops.QueueDequeue(
    handle, component_types, timeout_ms=-1, name=None
)
",[],[]
"tf.raw_ops.QueueDequeueMany(
    handle, n, component_types, timeout_ms=-1, name=None
)
","[['Dequeues ', 'n', ' tuples of one or more tensors from the given queue.']]",[]
"tf.raw_ops.QueueDequeueManyV2(
    handle, n, component_types, timeout_ms=-1, name=None
)
","[['Dequeues ', 'n', ' tuples of one or more tensors from the given queue.']]",[]
"tf.raw_ops.QueueDequeueUpTo(
    handle, n, component_types, timeout_ms=-1, name=None
)
","[['Dequeues ', 'n', ' tuples of one or more tensors from the given queue.']]",[]
"tf.raw_ops.QueueDequeueUpToV2(
    handle, n, component_types, timeout_ms=-1, name=None
)
","[['Dequeues ', 'n', ' tuples of one or more tensors from the given queue.']]",[]
"tf.raw_ops.QueueDequeueV2(
    handle, component_types, timeout_ms=-1, name=None
)
",[],[]
"tf.raw_ops.QueueEnqueue(
    handle, components, timeout_ms=-1, name=None
)
",[],[]
"tf.raw_ops.QueueEnqueueMany(
    handle, components, timeout_ms=-1, name=None
)
",[],[]
"tf.raw_ops.QueueEnqueueManyV2(
    handle, components, timeout_ms=-1, name=None
)
",[],[]
"tf.raw_ops.QueueEnqueueV2(
    handle, components, timeout_ms=-1, name=None
)
",[],[]
"tf.raw_ops.QueueIsClosed(
    handle, name=None
)
",[],[]
"tf.raw_ops.QueueIsClosedV2(
    handle, name=None
)
",[],[]
"tf.raw_ops.QueueSize(
    handle, name=None
)
",[],[]
"tf.raw_ops.QueueSizeV2(
    handle, name=None
)
",[],[]
"tf.raw_ops.RFFT(
    input,
    fft_length,
    Tcomplex=tf.dtypes.complex64,
    name=None
)
",[],[]
"tf.raw_ops.RFFT2D(
    input,
    fft_length,
    Tcomplex=tf.dtypes.complex64,
    name=None
)
",[],[]
"tf.raw_ops.RFFT3D(
    input,
    fft_length,
    Tcomplex=tf.dtypes.complex64,
    name=None
)
",[],[]
"tf.raw_ops.RGBToHSV(
    images, name=None
)
",[],"blue_image = tf.stack([
   tf.zeros([5,5]),
   tf.zeros([5,5]),
   tf.ones([5,5])],
   axis=-1)
blue_hsv_image = tf.image.rgb_to_hsv(blue_image)
blue_hsv_image[0,0].numpy()
array([0.6666667, 1. , 1. ], dtype=float32)"
"tf.raw_ops.RaggedBincount(
    splits, values, size, weights, binary_output=False, name=None
)
",[],[]
"tf.raw_ops.RaggedCountSparseOutput(
    splits,
    values,
    weights,
    binary_output,
    minlength=-1,
    maxlength=-1,
    name=None
)
",[],[]
"tf.raw_ops.RaggedCross(
    ragged_values,
    ragged_row_splits,
    sparse_indices,
    sparse_values,
    sparse_shape,
    dense_inputs,
    input_order,
    hashed_output,
    num_buckets,
    hash_key,
    out_values_type,
    out_row_splits_type,
    name=None
)
",[],"output_values: A `Tensor` of type `out_values_type`.
output_row_splits: A `Tensor` of type `out_row_splits_type`.
"
"tf.raw_ops.RaggedGather(
    params_nested_splits,
    params_dense_values,
    indices,
    OUTPUT_RAGGED_RANK,
    name=None
)
","[['Gather ragged slices from ', 'params', ' axis ', '0', ' according to ', 'indices', '.']]","output.shape = indices.shape + params.shape[1:]
output.ragged_rank = indices.shape.ndims + params.ragged_rank
output[i...j, d0...dn] = params[indices[i...j], d0...dn]
"
"tf.raw_ops.RaggedRange(
    starts,
    limits,
    deltas,
    Tsplits=tf.dtypes.int64,
    name=None
)
","[['Returns a ', 'RaggedTensor', ' containing the specified sequences of numbers.']]","(rt_nested_splits, rt_dense_values) = ragged_range(
      starts=[2, 5, 8], limits=[3, 5, 12], deltas=1)
result = tf.ragged.from_row_splits(rt_dense_values, rt_nested_splits)
print(result)
<tf.RaggedTensor [[2], [], [8, 9, 10, 11]] >
"
"tf.raw_ops.RaggedTensorFromVariant(
    encoded_ragged,
    input_ragged_rank,
    output_ragged_rank,
    Tvalues,
    Tsplits=tf.dtypes.int64,
    name=None
)
","[['Decodes a ', 'variant', ' Tensor into a ', 'RaggedTensor', '.']]",[]
"tf.raw_ops.RaggedTensorToSparse(
    rt_nested_splits, rt_dense_values, name=None
)
","[['Converts a ', 'RaggedTensor', ' into a ', 'SparseTensor', ' with the same values.']]",[]
"tf.raw_ops.RaggedTensorToTensor(
    shape,
    values,
    default_value,
    row_partition_tensors,
    row_partition_types,
    name=None
)
",[],[]
"tf.raw_ops.RaggedTensorToVariant(
    rt_nested_splits, rt_dense_values, batched_input, name=None
)
","[['Encodes a ', 'RaggedTensor', ' into a ', 'variant', ' Tensor.']]",[]
"tf.raw_ops.RaggedTensorToVariantGradient(
    encoded_ragged_grad, row_splits, dense_values_shape, Tvalues, name=None
)
","[['Helper used to compute the gradient for ', 'RaggedTensorToVariant', '.']]",[]
"tf.raw_ops.RandomCrop(
    image, size, seed=0, seed2=0, name=None
)
","[['Randomly crop ', 'image', '.']]",[]
"tf.raw_ops.RandomDataset(
    seed, seed2, output_types, output_shapes, metadata='', name=None
)
",[],[]
"tf.raw_ops.RandomGamma(
    shape, alpha, seed=0, seed2=0, name=None
)
",[],[]
"tf.raw_ops.RandomGammaGrad(
    alpha, sample, name=None
)
","[['Computes the derivative of a Gamma random sample w.r.t. ', 'alpha', '.']]",[]
"tf.raw_ops.RandomIndexShuffle(
    index, seed, max_index, rounds=4, name=None
)
","[['Outputs the position of ', 'value', ' in a permutation of [0, ..., max_index].']]",[]
"tf.raw_ops.RandomPoisson(
    shape, rate, seed=0, seed2=0, name=None
)
",[],[]
"tf.raw_ops.RandomPoissonV2(
    shape,
    rate,
    seed=0,
    seed2=0,
    dtype=tf.dtypes.int64,
    name=None
)
",[],[]
"tf.raw_ops.RandomShuffle(
    value, seed=0, seed2=0, name=None
)
",[],"[[1, 2],       [[5, 6],
 [3, 4],  ==>   [1, 2],
 [5, 6]]        [3, 4]]
"
"tf.raw_ops.RandomShuffleQueue(
    component_types,
    shapes=[],
    capacity=-1,
    min_after_dequeue=0,
    seed=0,
    seed2=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.RandomShuffleQueueV2(
    component_types,
    shapes=[],
    capacity=-1,
    min_after_dequeue=0,
    seed=0,
    seed2=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.RandomStandardNormal(
    shape, dtype, seed=0, seed2=0, name=None
)
",[],[]
"tf.raw_ops.RandomUniform(
    shape, dtype, seed=0, seed2=0, name=None
)
",[],[]
"tf.raw_ops.RandomUniformInt(
    shape, minval, maxval, seed=0, seed2=0, name=None
)
",[],[]
"tf.raw_ops.Range(
    start, limit, delta, name=None
)
",[],"# 'start' is 3
# 'limit' is 18
# 'delta' is 3
tf.range(start, limit, delta) ==> [3, 6, 9, 12, 15]
"
"tf.raw_ops.RangeDataset(
    start,
    stop,
    step,
    output_types,
    output_shapes,
    metadata='',
    replicate_on_split=False,
    name=None
)
",[],[]
"tf.raw_ops.Rank(
    input, name=None
)
",[],"# 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]
# shape of tensor 't' is [2, 2, 3]
rank(t) ==> 3
"
"tf.raw_ops.ReadFile(
    filename, name=None
)
",[],[]
"tf.raw_ops.ReadVariableOp(
    resource, dtype, name=None
)
",[],[]
"tf.raw_ops.ReadVariableXlaSplitND(
    resource, T, N, num_splits, paddings=[], name=None
)
",[],"[[0, 1, 2],
 [3, 4, 5],
 [6, 7, 8]]
"
"tf.raw_ops.ReaderNumRecordsProduced(
    reader_handle, name=None
)
",[],[]
"tf.raw_ops.ReaderNumRecordsProducedV2(
    reader_handle, name=None
)
",[],[]
"tf.raw_ops.ReaderNumWorkUnitsCompleted(
    reader_handle, name=None
)
",[],[]
"tf.raw_ops.ReaderNumWorkUnitsCompletedV2(
    reader_handle, name=None
)
",[],[]
"tf.raw_ops.ReaderRead(
    reader_handle, queue_handle, name=None
)
",[],[]
"tf.raw_ops.ReaderReadUpTo(
    reader_handle, queue_handle, num_records, name=None
)
","[['Returns up to ', 'num_records', ' (key, value) pairs produced by a Reader.']]",[]
"tf.raw_ops.ReaderReadUpToV2(
    reader_handle, queue_handle, num_records, name=None
)
","[['Returns up to ', 'num_records', ' (key, value) pairs produced by a Reader.']]",[]
"tf.raw_ops.ReaderReadV2(
    reader_handle, queue_handle, name=None
)
",[],[]
"tf.raw_ops.ReaderReset(
    reader_handle, name=None
)
",[],[]
"tf.raw_ops.ReaderResetV2(
    reader_handle, name=None
)
",[],[]
"tf.raw_ops.ReaderRestoreState(
    reader_handle, state, name=None
)
",[],[]
"tf.raw_ops.ReaderRestoreStateV2(
    reader_handle, state, name=None
)
",[],[]
"tf.raw_ops.ReaderSerializeState(
    reader_handle, name=None
)
",[],[]
"tf.raw_ops.ReaderSerializeStateV2(
    reader_handle, name=None
)
",[],[]
"tf.raw_ops.Real(
    input,
    Tout=tf.dtypes.float32,
    name=None
)
","[[None, '\n']]","# tensor 'input' is [-2.25 + 4.75j, 3.25 + 5.75j]
tf.real(input) ==> [-2.25, 3.25]
"
"tf.raw_ops.RealDiv(
    x, y, name=None
)
",[],[]
"tf.raw_ops.RebatchDataset(
    input_dataset,
    num_replicas,
    output_types,
    output_shapes,
    use_fallback=True,
    name=None
)
",[],[]
"tf.raw_ops.RebatchDatasetV2(
    input_dataset,
    batch_sizes,
    drop_remainder,
    output_types,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.Reciprocal(
    x, name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.ReciprocalGrad(
    y, dy, name=None
)
","[['Computes the gradient for the inverse of ', 'x', ' wrt its input.']]",[]
"tf.raw_ops.RecordInput(
    file_pattern,
    file_random_seed=301,
    file_shuffle_shift_ratio=0,
    file_buffer_size=10000,
    file_parallelism=16,
    batch_size=32,
    compression_type='',
    name=None
)
",[],[]
"tf.raw_ops.Recv(
    tensor_type,
    tensor_name,
    send_device,
    send_device_incarnation,
    recv_device,
    client_terminated=False,
    name=None
)
",[],[]
"tf.raw_ops.RecvTPUEmbeddingActivations(
    num_outputs, config, name=None
)
",[],[]
"tf.raw_ops.ReduceDataset(
    input_dataset,
    initial_state,
    other_arguments,
    f,
    output_types,
    output_shapes,
    use_inter_op_parallelism=True,
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.ReduceJoin(
    inputs,
    reduction_indices,
    keep_dims=False,
    separator='',
    name=None
)
","[[None, '\n']]","# tensor `a` is [[""a"", ""b""], [""c"", ""d""]]
tf.reduce_join(a, 0) ==> [""ac"", ""bd""]
tf.reduce_join(a, 1) ==> [""ab"", ""cd""]
tf.reduce_join(a, -2) = tf.reduce_join(a, 0) ==> [""ac"", ""bd""]
tf.reduce_join(a, -1) = tf.reduce_join(a, 1) ==> [""ab"", ""cd""]
tf.reduce_join(a, 0, keep_dims=True) ==> [[""ac"", ""bd""]]
tf.reduce_join(a, 1, keep_dims=True) ==> [[""ab""], [""cd""]]
tf.reduce_join(a, 0, separator=""."") ==> [""a.c"", ""b.d""]
tf.reduce_join(a, [0, 1]) ==> ""acbd""
tf.reduce_join(a, [1, 0]) ==> ""abcd""
tf.reduce_join(a, []) ==> [[""a"", ""b""], [""c"", ""d""]]
tf.reduce_join(a) = tf.reduce_join(a, [1, 0]) ==> ""abcd""
"
"tf.raw_ops.RefEnter(
    data, frame_name, is_constant=False, parallel_iterations=10, name=None
)
","[['Creates or finds a child frame, and makes ', 'data', ' available to the child frame.']]",[]
"tf.raw_ops.RefExit(
    data, name=None
)
",[],[]
"tf.raw_ops.RefIdentity(
    input, name=None
)
",[],[]
"tf.raw_ops.RefMerge(
    inputs, name=None
)
","[['Forwards the value of an available tensor from ', 'inputs', ' to ', 'output', '.']]",[]
"tf.raw_ops.RefNextIteration(
    data, name=None
)
",[],[]
"tf.raw_ops.RefSelect(
    index, inputs, name=None
)
","[['Forwards the ', 'index', 'th element of ', 'inputs', ' to ', 'output', '.']]",[]
"tf.raw_ops.RefSwitch(
    data, pred, name=None
)
","[['Forwards the ref tensor ', 'data', ' to the output port determined by ', 'pred', '.']]",[]
"tf.raw_ops.RegexFullMatch(
    input, pattern, name=None
)
","[[None, '\n']]","tf.strings.regex_full_match([""TF lib"", ""lib TF""], "".*lib$"")
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
tf.strings.regex_full_match([""TF lib"", ""lib TF""], "".*TF$"")
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.raw_ops.RegexReplace(
    input, pattern, rewrite, replace_global=True, name=None
)
","[['Replaces matches of the ', 'pattern', ' regular expression in ', 'input', ' with the']]",[]
"tf.raw_ops.RegisterDataset(
    dataset,
    address,
    protocol,
    external_state_policy,
    element_spec='',
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.RegisterDatasetV2(
    dataset,
    address,
    protocol,
    external_state_policy,
    element_spec='',
    requested_dataset_id='',
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.Relu(
    features, name=None
)
","[['Computes rectified linear: ', 'max(features, 0)', '.']]",">>> tf.nn.relu([-2., 0., 3.]).numpy()
array([0., 0., 3.], dtype=float32)
"
"tf.raw_ops.Relu6(
    features, name=None
)
","[['Computes rectified linear 6: ', 'min(max(features, 0), 6)', '.']]",[]
"tf.raw_ops.Relu6Grad(
    gradients, features, name=None
)
",[],[]
"tf.raw_ops.ReluGrad(
    gradients, features, name=None
)
",[],[]
"tf.raw_ops.RemoteCall(
    target, args, Tout, f, name=None
)
","[['Runs function ', 'f', ' on a remote device indicated by ', 'target', '.']]",[]
"tf.raw_ops.RepeatDataset(
    input_dataset,
    count,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset that emits the outputs of ', 'input_dataset', ' ', 'count', ' times.']]",[]
"tf.raw_ops.RequantizationRange(
    input, input_min, input_max, name=None
)
",[],[]
"tf.raw_ops.RequantizationRangePerChannel(
    input, input_min, input_max, clip_value_max, name=None
)
",[],[]
"tf.raw_ops.Requantize(
    input,
    input_min,
    input_max,
    requested_output_min,
    requested_output_max,
    out_type,
    name=None
)
","[['Converts the quantized ', 'input', ' tensor into a lower-precision ', 'output', '.']]",[]
"tf.raw_ops.RequantizePerChannel(
    input,
    input_min,
    input_max,
    requested_output_min,
    requested_output_max,
    out_type=tf.dtypes.quint8,
    name=None
)
",[],[]
"tf.raw_ops.Reshape(
    tensor, shape, name=None
)
",[],"# tensor 't' is [1, 2, 3, 4, 5, 6, 7, 8, 9]
# tensor 't' has shape [9]
reshape(t, [3, 3]) ==> [[1, 2, 3],
                        [4, 5, 6],
                        [7, 8, 9]]

# tensor 't' is [[[1, 1], [2, 2]],
#                [[3, 3], [4, 4]]]
# tensor 't' has shape [2, 2, 2]
reshape(t, [2, 4]) ==> [[1, 1, 2, 2],
                        [3, 3, 4, 4]]

# tensor 't' is [[[1, 1, 1],
#                 [2, 2, 2]],
#                [[3, 3, 3],
#                 [4, 4, 4]],
#                [[5, 5, 5],
#                 [6, 6, 6]]]
# tensor 't' has shape [3, 2, 3]
# pass '[-1]' to flatten 't'
reshape(t, [-1]) ==> [1, 1, 1, 2, 2, 2, 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6]

# -1 can also be used to infer the shape

# -1 is inferred to be 9:
reshape(t, [2, -1]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],
                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]
# -1 is inferred to be 2:
reshape(t, [-1, 9]) ==> [[1, 1, 1, 2, 2, 2, 3, 3, 3],
                         [4, 4, 4, 5, 5, 5, 6, 6, 6]]
# -1 is inferred to be 3:
reshape(t, [ 2, -1, 3]) ==> [[[1, 1, 1],
                              [2, 2, 2],
                              [3, 3, 3]],
                             [[4, 4, 4],
                              [5, 5, 5],
                              [6, 6, 6]]]

# tensor 't' is [7]
# shape `[]` reshapes to a scalar
reshape(t, []) ==> 7
"
"tf.raw_ops.ResizeArea(
    images, size, align_corners=False, name=None
)
","[['Resize ', 'images', ' to ', 'size', ' using area interpolation.']]",[]
"tf.raw_ops.ResizeBicubic(
    images, size, align_corners=False, half_pixel_centers=False, name=None
)
","[['Resize ', 'images', ' to ', 'size', ' using bicubic interpolation.']]",[]
"tf.raw_ops.ResizeBicubicGrad(
    grads,
    original_image,
    align_corners=False,
    half_pixel_centers=False,
    name=None
)
",[],[]
"tf.raw_ops.ResizeBilinear(
    images, size, align_corners=False, half_pixel_centers=False, name=None
)
","[['Resize ', 'images', ' to ', 'size', ' using bilinear interpolation.']]",[]
"tf.raw_ops.ResizeBilinearGrad(
    grads,
    original_image,
    align_corners=False,
    half_pixel_centers=False,
    name=None
)
",[],[]
"tf.raw_ops.ResizeNearestNeighbor(
    images, size, align_corners=False, half_pixel_centers=False, name=None
)
","[['Resize ', 'images', ' to ', 'size', ' using nearest neighbor interpolation.']]",[]
"tf.raw_ops.ResizeNearestNeighborGrad(
    grads, size, align_corners=False, half_pixel_centers=False, name=None
)
",[],[]
"tf.raw_ops.ResourceAccumulatorApplyGradient(
    handle, local_step, gradient, name=None
)
",[],[]
"tf.raw_ops.ResourceAccumulatorNumAccumulated(
    handle, name=None
)
",[],[]
"tf.raw_ops.ResourceAccumulatorSetGlobalStep(
    handle, new_global_step, name=None
)
",[],[]
"tf.raw_ops.ResourceAccumulatorTakeGradient(
    handle, num_required, dtype, name=None
)
",[],[]
"tf.raw_ops.ResourceApplyAdaMax(
    var,
    m,
    v,
    beta1_power,
    lr,
    beta1,
    beta2,
    epsilon,
    grad,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceApplyAdadelta(
    var,
    accum,
    accum_update,
    lr,
    rho,
    epsilon,
    grad,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceApplyAdagrad(
    var, accum, lr, grad, use_locking=False, update_slots=True, name=None
)
",[],[]
"tf.raw_ops.ResourceApplyAdagradDA(
    var,
    gradient_accumulator,
    gradient_squared_accumulator,
    grad,
    lr,
    l1,
    l2,
    global_step,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceApplyAdagradV2(
    var,
    accum,
    lr,
    epsilon,
    grad,
    use_locking=False,
    update_slots=True,
    name=None
)
",[],[]
"tf.raw_ops.ResourceApplyAdam(
    var,
    m,
    v,
    beta1_power,
    beta2_power,
    lr,
    beta1,
    beta2,
    epsilon,
    grad,
    use_locking=False,
    use_nesterov=False,
    name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.ResourceApplyAdamWithAmsgrad(
    var,
    m,
    v,
    vhat,
    beta1_power,
    beta2_power,
    lr,
    beta1,
    beta2,
    epsilon,
    grad,
    use_locking=False,
    name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.ResourceApplyAddSign(
    var, m, lr, alpha, sign_decay, beta, grad, use_locking=False, name=None
)
",[],[]
"tf.raw_ops.ResourceApplyCenteredRMSProp(
    var,
    mg,
    ms,
    mom,
    lr,
    rho,
    momentum,
    epsilon,
    grad,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceApplyFtrl(
    var,
    accum,
    linear,
    grad,
    lr,
    l1,
    l2,
    lr_power,
    use_locking=False,
    multiply_linear_by_lr=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceApplyFtrlV2(
    var,
    accum,
    linear,
    grad,
    lr,
    l1,
    l2,
    l2_shrinkage,
    lr_power,
    use_locking=False,
    multiply_linear_by_lr=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceApplyGradientDescent(
    var, alpha, delta, use_locking=False, name=None
)
",[],[]
"tf.raw_ops.ResourceApplyKerasMomentum(
    var,
    accum,
    lr,
    grad,
    momentum,
    use_locking=False,
    use_nesterov=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceApplyMomentum(
    var,
    accum,
    lr,
    grad,
    momentum,
    use_locking=False,
    use_nesterov=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceApplyPowerSign(
    var, m, lr, logbase, sign_decay, beta, grad, use_locking=False, name=None
)
",[],[]
"tf.raw_ops.ResourceApplyProximalAdagrad(
    var, accum, lr, l1, l2, grad, use_locking=False, name=None
)
","[[""Update '"", ""var' and '"", ""accum' according to FOBOS with Adagrad learning rate.""]]",[]
"tf.raw_ops.ResourceApplyProximalGradientDescent(
    var, alpha, l1, l2, delta, use_locking=False, name=None
)
",[],[]
"tf.raw_ops.ResourceApplyRMSProp(
    var,
    ms,
    mom,
    lr,
    rho,
    momentum,
    epsilon,
    grad,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceConditionalAccumulator(
    dtype,
    shape,
    container='',
    shared_name='',
    reduction_type='MEAN',
    name=None
)
",[],[]
"tf.raw_ops.ResourceCountUpTo(
    resource, limit, T, name=None
)
",[],[]
"tf.raw_ops.ResourceGather(
    resource, indices, dtype, batch_dims=0, validate_indices=True, name=None
)
","[['Gather slices from the variable pointed to by ', 'resource', ' according to ', 'indices', '.']]","    # Scalar indices
    output[:, ..., :] = params[indices, :, ... :]

    # Vector indices
    output[i, :, ..., :] = params[indices[i], :, ... :]

    # Higher rank indices
    output[i, ..., j, :, ... :] = params[indices[i, ..., j], :, ..., :]
"
"tf.raw_ops.ResourceGatherNd(
    resource, indices, dtype, name=None
)
",[],[]
"tf.raw_ops.ResourceScatterAdd(
    resource, indices, updates, name=None
)
","[['Adds sparse updates to the variable referenced by ', 'resource', '.']]","# Scalar indices
ref[indices, ...] += updates[...]

# Vector indices (for each i)
ref[indices[i], ...] += updates[i, ...]

# High rank indices (for each i, ..., j)
ref[indices[i, ..., j], ...] += updates[i, ..., j, ...]
"
"tf.raw_ops.ResourceScatterDiv(
    resource, indices, updates, name=None
)
","[['Divides sparse updates into the variable referenced by ', 'resource', '.']]","# Scalar indices
ref[indices, ...] /= updates[...]

# Vector indices (for each i)
ref[indices[i], ...] /= updates[i, ...]

# High rank indices (for each i, ..., j)
ref[indices[i, ..., j], ...] /= updates[i, ..., j, ...]
"
"tf.raw_ops.ResourceScatterMax(
    resource, indices, updates, name=None
)
","[['Reduces sparse updates into the variable referenced by ', 'resource', ' using the ', 'max', ' operation.']]","# Scalar indices
ref[indices, ...] = max(ref[indices, ...], updates[...])

# Vector indices (for each i)
ref[indices[i], ...] = max(ref[indices[i], ...], updates[i, ...])

# High rank indices (for each i, ..., j)
ref[indices[i, ..., j], ...] = max(ref[indices[i, ..., j], ...], updates[i, ..., j, ...])
"
"tf.raw_ops.ResourceScatterMin(
    resource, indices, updates, name=None
)
","[['Reduces sparse updates into the variable referenced by ', 'resource', ' using the ', 'min', ' operation.']]","# Scalar indices
ref[indices, ...] = min(ref[indices, ...], updates[...])

# Vector indices (for each i)
ref[indices[i], ...] = min(ref[indices[i], ...], updates[i, ...])

# High rank indices (for each i, ..., j)
ref[indices[i, ..., j], ...] = min(ref[indices[i, ..., j], ...], updates[i, ..., j, ...])
"
"tf.raw_ops.ResourceScatterMul(
    resource, indices, updates, name=None
)
","[['Multiplies sparse updates into the variable referenced by ', 'resource', '.']]","# Scalar indices
ref[indices, ...] *= updates[...]

# Vector indices (for each i)
ref[indices[i], ...] *= updates[i, ...]

# High rank indices (for each i, ..., j)
ref[indices[i, ..., j], ...] *= updates[i, ..., j, ...]
"
"tf.raw_ops.ResourceScatterNdAdd(
    ref, indices, updates, use_locking=True, name=None
)
",[],"[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]]
"
"tf.raw_ops.ResourceScatterNdMax(
    ref, indices, updates, use_locking=True, name=None
)
",[],[]
"tf.raw_ops.ResourceScatterNdMin(
    ref, indices, updates, use_locking=True, name=None
)
",[],[]
"tf.raw_ops.ResourceScatterNdSub(
    ref, indices, updates, use_locking=True, name=None
)
",[],"[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]]
"
"tf.raw_ops.ResourceScatterNdUpdate(
    ref, indices, updates, use_locking=True, name=None
)
","[['Applies sparse ', 'updates', ' to individual values or slices within a given']]","[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].
"
"tf.raw_ops.ResourceScatterSub(
    resource, indices, updates, name=None
)
","[['Subtracts sparse updates from the variable referenced by ', 'resource', '.']]","# Scalar indices
ref[indices, ...] -= updates[...]

# Vector indices (for each i)
ref[indices[i], ...] -= updates[i, ...]

# High rank indices (for each i, ..., j)
ref[indices[i, ..., j], ...] -= updates[i, ..., j, ...]
"
"tf.raw_ops.ResourceScatterUpdate(
    resource, indices, updates, name=None
)
","[['Assigns sparse updates to the variable referenced by ', 'resource', '.']]","# Scalar indices
ref[indices, ...] = updates[...]

# Vector indices (for each i)
ref[indices[i], ...] = updates[i, ...]

# High rank indices (for each i, ..., j)
ref[indices[i, ..., j], ...] = updates[i, ..., j, ...]
"
"tf.raw_ops.ResourceSparseApplyAdadelta(
    var,
    accum,
    accum_update,
    lr,
    rho,
    epsilon,
    grad,
    indices,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceSparseApplyAdagrad(
    var,
    accum,
    lr,
    grad,
    indices,
    use_locking=False,
    update_slots=True,
    name=None
)
","[[""Update relevant entries in '"", ""var' and '"", ""accum' according to the adagrad scheme.""]]",[]
"tf.raw_ops.ResourceSparseApplyAdagradDA(
    var,
    gradient_accumulator,
    gradient_squared_accumulator,
    grad,
    indices,
    lr,
    l1,
    l2,
    global_step,
    use_locking=False,
    name=None
)
","[[""Update entries in '"", ""var' and '"", ""accum' according to the proximal adagrad scheme.""]]",[]
"tf.raw_ops.ResourceSparseApplyAdagradV2(
    var,
    accum,
    lr,
    epsilon,
    grad,
    indices,
    use_locking=False,
    update_slots=True,
    name=None
)
","[[""Update relevant entries in '"", ""var' and '"", ""accum' according to the adagrad scheme.""]]",[]
"tf.raw_ops.ResourceSparseApplyCenteredRMSProp(
    var,
    mg,
    ms,
    mom,
    lr,
    rho,
    momentum,
    epsilon,
    grad,
    indices,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceSparseApplyFtrl(
    var,
    accum,
    linear,
    grad,
    indices,
    lr,
    l1,
    l2,
    lr_power,
    use_locking=False,
    multiply_linear_by_lr=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceSparseApplyFtrlV2(
    var,
    accum,
    linear,
    grad,
    indices,
    lr,
    l1,
    l2,
    l2_shrinkage,
    lr_power,
    use_locking=False,
    multiply_linear_by_lr=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceSparseApplyKerasMomentum(
    var,
    accum,
    lr,
    grad,
    indices,
    momentum,
    use_locking=False,
    use_nesterov=False,
    name=None
)
","[[""Update relevant entries in '"", ""var' and '"", ""accum' according to the momentum scheme.""]]",[]
"tf.raw_ops.ResourceSparseApplyMomentum(
    var,
    accum,
    lr,
    grad,
    indices,
    momentum,
    use_locking=False,
    use_nesterov=False,
    name=None
)
","[[""Update relevant entries in '"", ""var' and '"", ""accum' according to the momentum scheme.""]]",[]
"tf.raw_ops.ResourceSparseApplyProximalAdagrad(
    var, accum, lr, l1, l2, grad, indices, use_locking=False, name=None
)
","[[""Sparse update entries in '"", ""var' and '"", ""accum' according to FOBOS algorithm.""]]",[]
"tf.raw_ops.ResourceSparseApplyProximalGradientDescent(
    var, alpha, l1, l2, grad, indices, use_locking=False, name=None
)
",[],[]
"tf.raw_ops.ResourceSparseApplyRMSProp(
    var,
    ms,
    mom,
    lr,
    rho,
    momentum,
    epsilon,
    grad,
    indices,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.ResourceStridedSliceAssign(
    ref,
    begin,
    end,
    strides,
    value,
    begin_mask=0,
    end_mask=0,
    ellipsis_mask=0,
    new_axis_mask=0,
    shrink_axis_mask=0,
    name=None
)
","[['Assign ', 'value', ' to the sliced l-value reference of ', 'ref', '.']]",[]
"tf.raw_ops.Restore(
    file_pattern, tensor_name, dt, preferred_shard=-1, name=None
)
",[],[]
"tf.raw_ops.RestoreSlice(
    file_pattern,
    tensor_name,
    shape_and_slice,
    dt,
    preferred_shard=-1,
    name=None
)
",[],[]
"tf.raw_ops.RestoreV2(
    prefix, tensor_names, shape_and_slices, dtypes, name=None
)
",[],[]
"tf.raw_ops.RetrieveTPUEmbeddingADAMParameters(
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.RetrieveTPUEmbeddingAdadeltaParameters(
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.RetrieveTPUEmbeddingAdagradMomentumParameters(
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.RetrieveTPUEmbeddingAdagradParameters(
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.RetrieveTPUEmbeddingCenteredRMSPropParameters(
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.RetrieveTPUEmbeddingFTRLParameters(
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.RetrieveTPUEmbeddingFrequencyEstimatorParameters(
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.RetrieveTPUEmbeddingMDLAdagradLightParameters(
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.RetrieveTPUEmbeddingMomentumParameters(
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.RetrieveTPUEmbeddingProximalAdagradParameters(
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.RetrieveTPUEmbeddingProximalYogiParameters(
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.RetrieveTPUEmbeddingRMSPropParameters(
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.RetrieveTPUEmbeddingStochasticGradientDescentParameters(
    num_shards,
    shard_id,
    table_id=-1,
    table_name='',
    config='',
    name=None
)
",[],[]
"tf.raw_ops.Reverse(
    tensor, dims, name=None
)
",[],"# tensor 't' is [[[[ 0,  1,  2,  3],
#                  [ 4,  5,  6,  7],
#                  [ 8,  9, 10, 11]],
#                 [[12, 13, 14, 15],
#                  [16, 17, 18, 19],
#                  [20, 21, 22, 23]]]]
# tensor 't' shape is [1, 2, 3, 4]

# 'dims' is [False, False, False, True]
reverse(t, dims) ==> [[[[ 3,  2,  1,  0],
                        [ 7,  6,  5,  4],
                        [ 11, 10, 9, 8]],
                       [[15, 14, 13, 12],
                        [19, 18, 17, 16],
                        [23, 22, 21, 20]]]]

# 'dims' is [False, True, False, False]
reverse(t, dims) ==> [[[[12, 13, 14, 15],
                        [16, 17, 18, 19],
                        [20, 21, 22, 23]
                       [[ 0,  1,  2,  3],
                        [ 4,  5,  6,  7],
                        [ 8,  9, 10, 11]]]]

# 'dims' is [False, False, True, False]
reverse(t, dims) ==> [[[[8, 9, 10, 11],
                        [4, 5, 6, 7],
                        [0, 1, 2, 3]]
                       [[20, 21, 22, 23],
                        [16, 17, 18, 19],
                        [12, 13, 14, 15]]]]
"
"tf.raw_ops.ReverseSequence(
    input, seq_lengths, seq_dim, batch_dim=0, name=None
)
",[],"# Given this:
batch_dim = 0
seq_dim = 1
input.dims = (4, 8, ...)
seq_lengths = [7, 2, 3, 5]

# then slices of input are reversed on seq_dim, but only up to seq_lengths:
output[0, 0:7, :, ...] = input[0, 7:0:-1, :, ...]
output[1, 0:2, :, ...] = input[1, 2:0:-1, :, ...]
output[2, 0:3, :, ...] = input[2, 3:0:-1, :, ...]
output[3, 0:5, :, ...] = input[3, 5:0:-1, :, ...]

# while entries past seq_lens are copied through:
output[0, 7:, :, ...] = input[0, 7:, :, ...]
output[1, 2:, :, ...] = input[1, 2:, :, ...]
output[2, 3:, :, ...] = input[2, 3:, :, ...]
output[3, 2:, :, ...] = input[3, 2:, :, ...]
"
"tf.raw_ops.ReverseV2(
    tensor, axis, name=None
)
",[],"# tensor 't' is [[[[ 0,  1,  2,  3],
#                  [ 4,  5,  6,  7],
#                  [ 8,  9, 10, 11]],
#                 [[12, 13, 14, 15],
#                  [16, 17, 18, 19],
#                  [20, 21, 22, 23]]]]
# tensor 't' shape is [1, 2, 3, 4]

# 'dims' is [3] or 'dims' is [-1]
reverse(t, dims) ==> [[[[ 3,  2,  1,  0],
                        [ 7,  6,  5,  4],
                        [ 11, 10, 9, 8]],
                       [[15, 14, 13, 12],
                        [19, 18, 17, 16],
                        [23, 22, 21, 20]]]]

# 'dims' is '[1]' (or 'dims' is '[-3]')
reverse(t, dims) ==> [[[[12, 13, 14, 15],
                        [16, 17, 18, 19],
                        [20, 21, 22, 23]
                       [[ 0,  1,  2,  3],
                        [ 4,  5,  6,  7],
                        [ 8,  9, 10, 11]]]]

# 'dims' is '[2]' (or 'dims' is '[-2]')
reverse(t, dims) ==> [[[[8, 9, 10, 11],
                        [4, 5, 6, 7],
                        [0, 1, 2, 3]]
                       [[20, 21, 22, 23],
                        [16, 17, 18, 19],
                        [12, 13, 14, 15]]]]
"
"tf.raw_ops.RewriteDataset(
    input_dataset, rewrite_name, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.RightShift(
    x, y, name=None
)
","[['Elementwise computes the bitwise right-shift of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
import numpy as np
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]

for dtype in dtype_list:
  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)

  right_shift_result = bitwise_ops.right_shift(lhs, rhs)

  print(right_shift_result)

# This will print:
# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int8)
# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int16)
# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int32)
# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int64)

lhs = np.array([-2, 64, 101, 32], dtype=np.int8)
rhs = np.array([-1, -5, -3, -14], dtype=np.int8)
bitwise_ops.right_shift(lhs, rhs)
# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>
"
"tf.raw_ops.Rint(
    x, name=None
)
",[],"rint(-1.5) ==> -2.0
rint(0.5000001) ==> 1.0
rint([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0]) ==> [-2., -2., -0., 0., 2., 2., 2.]
"
"tf.raw_ops.RngReadAndSkip(
    resource, alg, delta, name=None
)
",[],[]
"tf.raw_ops.RngSkip(
    resource, algorithm, delta, name=None
)
",[],[]
"tf.raw_ops.Roll(
    input, shift, axis, name=None
)
",[],"# 't' is [0, 1, 2, 3, 4]
roll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]

# shifting along multiple dimensions
# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
roll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]

# shifting along the same axis multiple times
# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
roll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]
"
"tf.raw_ops.Round(
    x, name=None
)
",[],[]
"tf.raw_ops.Rsqrt(
    x, name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.RsqrtGrad(
    y, dy, name=None
)
","[['Computes the gradient for the rsqrt of ', 'x', ' wrt its input.']]",[]
"tf.raw_ops.SampleDistortedBoundingBox(
    image_size,
    bounding_boxes,
    seed=0,
    seed2=0,
    min_object_covered=0.1,
    aspect_ratio_range=[0.75, 1.33],
    area_range=[0.05, 1],
    max_attempts=100,
    use_image_if_no_bounding_boxes=False,
    name=None
)
",[],"    # Generate a single distorted bounding box.
    begin, size, bbox_for_draw = tf.image.sample_distorted_bounding_box(
        tf.shape(image),
        bounding_boxes=bounding_boxes)

    # Draw the bounding box in an image summary.
    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),
                                                  bbox_for_draw)
    tf.summary.image('images_with_box', image_with_box)

    # Employ the bounding box to distort the image.
    distorted_image = tf.slice(image, begin, size)
"
"tf.raw_ops.SampleDistortedBoundingBoxV2(
    image_size,
    bounding_boxes,
    min_object_covered,
    seed=0,
    seed2=0,
    aspect_ratio_range=[0.75, 1.33],
    area_range=[0.05, 1],
    max_attempts=100,
    use_image_if_no_bounding_boxes=False,
    name=None
)
",[],"    # Generate a single distorted bounding box.
    begin, size, bbox_for_draw = tf.image.sample_distorted_bounding_box(
        tf.shape(image),
        bounding_boxes=bounding_boxes)

    # Draw the bounding box in an image summary.
    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),
                                                  bbox_for_draw)
    tf.summary.image('images_with_box', image_with_box)

    # Employ the bounding box to distort the image.
    distorted_image = tf.slice(image, begin, size)
"
"tf.raw_ops.SamplingDataset(
    input_dataset, rate, seed, seed2, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.Save(
    filename, tensor_names, data, name=None
)
",[],[]
"tf.raw_ops.SaveDataset(
    input_dataset,
    path,
    shard_func_other_args,
    shard_func,
    compression='',
    use_shard_func=True,
    name=None
)
",[],[]
"tf.raw_ops.SaveDatasetV2(
    input_dataset,
    path,
    shard_func_other_args,
    shard_func,
    output_types,
    output_shapes,
    compression='',
    use_shard_func=True,
    name=None
)
",[],[]
"tf.raw_ops.SaveSlices(
    filename, tensor_names, shapes_and_slices, data, name=None
)
",[],[]
"tf.raw_ops.SaveV2(
    prefix, tensor_names, shape_and_slices, tensors, name=None
)
",[],[]
"tf.raw_ops.ScalarSummary(
    tags, values, name=None
)
","[['Outputs a ', 'Summary', ' protocol buffer with scalar values.']]",[]
"tf.raw_ops.ScaleAndTranslate(
    images,
    size,
    scale,
    translation,
    kernel_type='lanczos3',
    antialias=True,
    name=None
)
",[],[]
"tf.raw_ops.ScaleAndTranslateGrad(
    grads,
    original_image,
    scale,
    translation,
    kernel_type='lanczos3',
    antialias=True,
    name=None
)
",[],[]
"tf.raw_ops.ScanDataset(
    input_dataset,
    initial_state,
    other_arguments,
    f,
    output_types,
    output_shapes,
    preserve_cardinality=False,
    use_default_device=True,
    metadata='',
    name=None
)
","[['Creates a dataset successively reduces ', 'f', ' over the elements of ', 'input_dataset', '.']]",[]
"tf.raw_ops.ScatterAdd(
    ref, indices, updates, use_locking=False, name=None
)
",[],"# Scalar indices
ref[indices, ...] += updates[...]

# Vector indices (for each i)
ref[indices[i], ...] += updates[i, ...]

# High rank indices (for each i, ..., j)
ref[indices[i, ..., j], ...] += updates[i, ..., j, ...]
"
"tf.raw_ops.ScatterDiv(
    ref, indices, updates, use_locking=False, name=None
)
",[],"    # Scalar indices
    ref[indices, ...] /= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] /= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] /= updates[i, ..., j, ...]
"
"tf.raw_ops.ScatterMax(
    ref, indices, updates, use_locking=False, name=None
)
","[['Reduces sparse updates into a variable reference using the ', 'max', ' operation.']]","# Scalar indices
ref[indices, ...] = max(ref[indices, ...], updates[...])

# Vector indices (for each i)
ref[indices[i], ...] = max(ref[indices[i], ...], updates[i, ...])

# High rank indices (for each i, ..., j)
ref[indices[i, ..., j], ...] = max(ref[indices[i, ..., j], ...], updates[i, ..., j, ...])
"
"tf.raw_ops.ScatterMin(
    ref, indices, updates, use_locking=False, name=None
)
","[['Reduces sparse updates into a variable reference using the ', 'min', ' operation.']]","# Scalar indices
ref[indices, ...] = min(ref[indices, ...], updates[...])

# Vector indices (for each i)
ref[indices[i], ...] = min(ref[indices[i], ...], updates[i, ...])

# High rank indices (for each i, ..., j)
ref[indices[i, ..., j], ...] = min(ref[indices[i, ..., j], ...], updates[i, ..., j, ...])
"
"tf.raw_ops.ScatterMul(
    ref, indices, updates, use_locking=False, name=None
)
",[],"    # Scalar indices
    ref[indices, ...] *= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] *= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] *= updates[i, ..., j, ...]
"
"tf.raw_ops.ScatterNd(
    indices, updates, shape, name=None
)
","[['Scatters ', 'updates', ' into a tensor of shape ', 'shape', ' according to ', 'indices', '.']]","indices.shape[-1] <= shape.rank
"
"tf.raw_ops.ScatterNdAdd(
    ref, indices, updates, use_locking=False, name=None
)
",[],"[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]]
"
"tf.raw_ops.ScatterNdMax(
    ref, indices, updates, use_locking=False, name=None
)
",[],[]
"tf.raw_ops.ScatterNdMin(
    ref, indices, updates, use_locking=False, name=None
)
",[],[]
"tf.raw_ops.ScatterNdNonAliasingAdd(
    input, indices, updates, name=None
)
","[[None, '\n'], ['Applies sparse addition to ', 'input', ' using individual values or slices']]","input = tf.constant([1, 2, 3, 4, 5, 6, 7, 8])
indices = tf.constant([[4], [3], [1], [7]])
updates = tf.constant([9, 10, 11, 12])
output = tf.scatter_nd_non_aliasing_add(input, indices, updates)
with tf.Session() as sess:
  print(sess.run(output))
"
"tf.raw_ops.ScatterNdSub(
    ref, indices, updates, use_locking=False, name=None
)
",[],"[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]]
"
"tf.raw_ops.ScatterNdUpdate(
    ref, indices, updates, use_locking=True, name=None
)
","[[None, '\n'], ['Applies sparse ', 'updates', ' to individual values or slices within a given']]","    ref = tf.Variable([1, 2, 3, 4, 5, 6, 7, 8])
    indices = tf.constant([[4], [3], [1] ,[7]])
    updates = tf.constant([9, 10, 11, 12])
    update = tf.scatter_nd_update(ref, indices, updates)
    with tf.Session() as sess:
      print sess.run(update)
"
"tf.raw_ops.ScatterSub(
    ref, indices, updates, use_locking=False, name=None
)
",[],"    # Scalar indices
    ref[indices, ...] -= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] -= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] -= updates[i, ..., j, ...]
"
"tf.raw_ops.ScatterUpdate(
    ref, indices, updates, use_locking=True, name=None
)
",[],"    # Scalar indices
    ref[indices, ...] = updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] = updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] = updates[i, ..., j, ...]
"
"tf.raw_ops.SdcaFprint(
    input, name=None
)
",[],[]
"tf.raw_ops.SdcaOptimizer(
    sparse_example_indices,
    sparse_feature_indices,
    sparse_feature_values,
    dense_features,
    example_weights,
    example_labels,
    sparse_indices,
    sparse_weights,
    dense_weights,
    example_state_data,
    loss_type,
    l1,
    l2,
    num_loss_partitions,
    num_inner_iterations,
    adaptative=True,
    name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.SdcaOptimizerV2(
    sparse_example_indices,
    sparse_feature_indices,
    sparse_feature_values,
    dense_features,
    example_weights,
    example_labels,
    sparse_indices,
    sparse_weights,
    dense_weights,
    example_state_data,
    loss_type,
    l1,
    l2,
    num_loss_partitions,
    num_inner_iterations,
    adaptive=True,
    name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.SdcaShrinkL1(
    weights, l1, l2, name=None
)
",[],[]
"tf.raw_ops.SegmentMax(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_max(c, tf.constant([0, 0, 1])).numpy()
array([[4, 3, 3, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.raw_ops.SegmentMean(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1.0,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_mean(c, tf.constant([0, 0, 1])).numpy()
array([[2.5, 2.5, 2.5, 2.5],
       [5., 6., 7., 8.]], dtype=float32)"
"tf.raw_ops.SegmentMin(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_min(c, tf.constant([0, 0, 1])).numpy()
array([[1, 2, 2, 1],
       [5, 6, 7, 8]], dtype=int32)"
"tf.raw_ops.SegmentProd(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_prod(c, tf.constant([0, 0, 1])).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.raw_ops.SegmentSum(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_sum(c, tf.constant([0, 0, 1])).numpy()
array([[5, 5, 5, 5],
       [5, 6, 7, 8]], dtype=int32)"
"tf.raw_ops.Select(
    condition, x, y, name=None
)
","[['Selects elements from ', 'x', ' or ', 'y', ', depending on ', 'condition', '.']]","# 'condition' tensor is [[True,  False]
#                        [False, True]]
# 't' is [[1, 2],
#         [3, 4]]
# 'e' is [[5, 6],
#         [7, 8]]
select(condition, t, e)  # => [[1, 6], [7, 4]]


# 'condition' tensor is [True, False]
# 't' is [[1, 2],
#         [3, 4]]
# 'e' is [[5, 6],
#         [7, 8]]
select(condition, t, e) ==> [[1, 2],
                             [7, 8]]

"
"tf.raw_ops.SelectV2(
    condition, t, e, name=None
)
",[],[]
"tf.raw_ops.SelfAdjointEig(
    input, name=None
)
",[],[]
"tf.raw_ops.SelfAdjointEigV2(
    input, compute_v=True, name=None
)
",[],"# a is a tensor.
# e is a tensor of eigenvalues.
# v is a tensor of eigenvectors.
e, v = self_adjoint_eig(a)
e = self_adjoint_eig(a, compute_v=False)
"
"tf.raw_ops.Selu(
    features, name=None
)
","[['Computes scaled exponential linear: ', 'scale * alpha * (exp(features) - 1)']]",[]
"tf.raw_ops.SeluGrad(
    gradients, outputs, name=None
)
",[],[]
"tf.raw_ops.Send(
    tensor,
    tensor_name,
    send_device,
    send_device_incarnation,
    recv_device,
    client_terminated=False,
    name=None
)
",[],[]
"tf.raw_ops.SendTPUEmbeddingGradients(
    inputs, learning_rates, config, name=None
)
",[],[]
"tf.raw_ops.SerializeIterator(
    resource_handle, external_state_policy=0, name=None
)
","[['Converts the given ', 'resource_handle', ' representing an iterator to a variant tensor.']]",[]
"tf.raw_ops.SerializeManySparse(
    sparse_indices,
    sparse_values,
    sparse_shape,
    out_type=tf.dtypes.string,
    name=None
)
","[['Serialize an ', 'N', '-minibatch ', 'SparseTensor', ' into an ', '[N, 3]', ' ', 'Tensor', ' object.']]",[]
"tf.raw_ops.SerializeSparse(
    sparse_indices,
    sparse_values,
    sparse_shape,
    out_type=tf.dtypes.string,
    name=None
)
","[['Serialize a ', 'SparseTensor', ' into a ', '[3]', ' ', 'Tensor', ' object.']]",[]
"tf.raw_ops.SerializeTensor(
    tensor, name=None
)
",[],[]
"tf.raw_ops.SetSize(
    set_indices, set_values, set_shape, validate_indices=True, name=None
)
","[['Number of unique elements along last dimension of input ', 'set', '.']]",[]
"tf.raw_ops.SetStatsAggregatorDataset(
    input_dataset,
    stats_aggregator,
    tag,
    counter_prefix,
    output_types,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.Shape(
    input,
    out_type=tf.dtypes.int32,
    name=None
)
",[],"# 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]
shape(t) ==> [2, 2, 3]
"
"tf.raw_ops.ShapeN(
    input,
    out_type=tf.dtypes.int32,
    name=None
)
",[],[]
"tf.raw_ops.ShardDataset(
    input_dataset,
    num_shards,
    index,
    output_types,
    output_shapes,
    require_non_empty=False,
    metadata='',
    name=None
)
","[['Creates a ', 'Dataset', ' that includes only 1/', 'num_shards', ' of this dataset.']]",[]
"tf.raw_ops.ShardedFilename(
    basename, shard, num_shards, name=None
)
",[],[]
"tf.raw_ops.ShardedFilespec(
    basename, num_shards, name=None
)
",[],[]
"tf.raw_ops.ShuffleAndRepeatDataset(
    input_dataset,
    buffer_size,
    seed,
    seed2,
    count,
    output_types,
    output_shapes,
    reshuffle_each_iteration=True,
    metadata='',
    name=None
)
","[['Creates a dataset that shuffles and repeats elements from ', 'input_dataset']]",[]
"tf.raw_ops.ShuffleAndRepeatDatasetV2(
    input_dataset,
    buffer_size,
    seed,
    seed2,
    count,
    seed_generator,
    output_types,
    output_shapes,
    reshuffle_each_iteration=True,
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.ShuffleDataset(
    input_dataset,
    buffer_size,
    seed,
    seed2,
    output_types,
    output_shapes,
    reshuffle_each_iteration=True,
    metadata='',
    name=None
)
","[['Creates a dataset that shuffles elements from ', 'input_dataset', ' pseudorandomly.']]",[]
"tf.raw_ops.ShuffleDatasetV2(
    input_dataset,
    buffer_size,
    seed_generator,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.ShuffleDatasetV3(
    input_dataset,
    buffer_size,
    seed,
    seed2,
    seed_generator,
    output_types,
    output_shapes,
    reshuffle_each_iteration=True,
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.ShutdownDistributedTPU(
    name=None
)
",[],[]
"tf.raw_ops.Sigmoid(
    x, name=None
)
","[['Computes sigmoid of ', 'x', ' element-wise.']]",[]
"tf.raw_ops.SigmoidGrad(
    y, dy, name=None
)
","[['Computes the gradient of the sigmoid of ', 'x', ' wrt its input.']]",[]
"tf.raw_ops.Sign(
    x, name=None
)
",[],"tf.math.sign([0., 2., -3.])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([ 0.,  1., -1.], dtype=float32)>"
"tf.raw_ops.Sin(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10, float(""inf"")])
  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]
"
"tf.raw_ops.Sinh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.sinh(x) ==> [-inf -4.0515420e+03 -5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf]
"
"tf.raw_ops.Size(
    input,
    out_type=tf.dtypes.int32,
    name=None
)
",[],"# 't' is [[[1, 1,, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]]
size(t) ==> 12
"
"tf.raw_ops.SkipDataset(
    input_dataset,
    count,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset that skips ', 'count', ' elements from the ', 'input_dataset', '.']]",[]
"tf.raw_ops.SleepDataset(
    input_dataset, sleep_microseconds, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.Slice(
    input, begin, size, name=None
)
",[],[]
"tf.raw_ops.SlidingWindowDataset(
    input_dataset,
    window_size,
    window_shift,
    window_stride,
    output_types,
    output_shapes,
    drop_remainder=True,
    name=None
)
","[['Creates a dataset that passes a sliding window over ', 'input_dataset', '.']]",[]
"tf.raw_ops.Snapshot(
    input, name=None
)
",[],[]
"tf.raw_ops.SnapshotDataset(
    input_dataset,
    path,
    output_types,
    output_shapes,
    compression='',
    reader_path_prefix='',
    writer_path_prefix='',
    shard_size_bytes=10737418240,
    pending_snapshot_expiry_seconds=86400,
    num_reader_threads=1,
    reader_buffer_size=1,
    num_writer_threads=1,
    writer_buffer_size=1,
    shuffle_on_read=False,
    seed=0,
    seed2=0,
    mode='auto',
    snapshot_name='',
    name=None
)
",[],[]
"tf.raw_ops.SnapshotDatasetReader(
    shard_dir,
    start_index,
    output_types,
    output_shapes,
    version,
    compression='',
    name=None
)
",[],[]
"tf.raw_ops.SnapshotDatasetV2(
    input_dataset,
    path,
    reader_func_other_args,
    shard_func_other_args,
    output_types,
    output_shapes,
    reader_func,
    shard_func,
    compression='',
    reader_prefix='',
    writer_prefix='',
    hash_valid=False,
    hash=0,
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.SnapshotNestedDatasetReader(
    inputs, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.SobolSample(
    dim,
    num_results,
    skip,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.Softmax(
    logits, name=None
)
","[[None, '\n']]","$$softmax[i, j] = exp(logits[i, j]) / sum_j(exp(logits[i, j]))$$
"
"tf.raw_ops.SoftmaxCrossEntropyWithLogits(
    features, labels, name=None
)
",[],[]
"tf.raw_ops.Softplus(
    features, name=None
)
",[],[]
"tf.raw_ops.SoftplusGrad(
    gradients, features, name=None
)
",[],[]
"tf.raw_ops.Softsign(
    features, name=None
)
","[['Computes softsign: ', 'features / (abs(features) + 1)', '.']]",[]
"tf.raw_ops.SoftsignGrad(
    gradients, features, name=None
)
",[],[]
"tf.raw_ops.SpaceToBatch(
    input, paddings, block_size, name=None
)
",[],"[batch*block_size*block_size, height_pad/block_size, width_pad/block_size,
 depth]
"
"tf.raw_ops.SpaceToBatchND(
    input, block_shape, paddings, name=None
)
",[],"x = [[[[1], [2]], [[3], [4]]]]
"
"tf.raw_ops.SpaceToDepth(
    input, block_size, data_format='NHWC', name=None
)
",[],"x = [[[[1], [2]],
      [[3], [4]]]]
"
"tf.raw_ops.SparseAccumulatorApplyGradient(
    handle,
    local_step,
    gradient_indices,
    gradient_values,
    gradient_shape,
    has_known_shape,
    name=None
)
",[],[]
"tf.raw_ops.SparseAccumulatorTakeGradient(
    handle, num_required, dtype, name=None
)
",[],[]
"tf.raw_ops.SparseAdd(
    a_indices,
    a_values,
    a_shape,
    b_indices,
    b_values,
    b_shape,
    thresh,
    name=None
)
","[['Adds two ', 'SparseTensor', ' objects to produce another ', 'SparseTensor', '.']]",[]
"tf.raw_ops.SparseAddGrad(
    backprop_val_grad, a_indices, b_indices, sum_indices, name=None
)
",[],[]
"tf.raw_ops.SparseApplyAdadelta(
    var,
    accum,
    accum_update,
    lr,
    rho,
    epsilon,
    grad,
    indices,
    use_locking=False,
    name=None
)
",[],[]
"tf.raw_ops.SparseApplyAdagrad(
    var,
    accum,
    lr,
    grad,
    indices,
    use_locking=False,
    update_slots=True,
    name=None
)
","[[None, '\n'], [""Update relevant entries in '"", ""var' and '"", ""accum' according to the adagrad scheme.""]]",[]
"tf.raw_ops.SparseApplyAdagradDA(
    var,
    gradient_accumulator,
    gradient_squared_accumulator,
    grad,
    indices,
    lr,
    l1,
    l2,
    global_step,
    use_locking=False,
    name=None
)
","[[""Update entries in '"", ""var' and '"", ""accum' according to the proximal adagrad scheme.""]]",[]
"tf.raw_ops.SparseApplyAdagradV2(
    var,
    accum,
    lr,
    epsilon,
    grad,
    indices,
    use_locking=False,
    update_slots=True,
    name=None
)
","[[None, '\n'], [""Update relevant entries in '"", ""var' and '"", ""accum' according to the adagrad scheme.""]]",[]
"tf.raw_ops.SparseApplyCenteredRMSProp(
    var,
    mg,
    ms,
    mom,
    lr,
    rho,
    momentum,
    epsilon,
    grad,
    indices,
    use_locking=False,
    name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.SparseApplyFtrl(
    var,
    accum,
    linear,
    grad,
    indices,
    lr,
    l1,
    l2,
    lr_power,
    use_locking=False,
    multiply_linear_by_lr=False,
    name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.SparseApplyFtrlV2(
    var,
    accum,
    linear,
    grad,
    indices,
    lr,
    l1,
    l2,
    l2_shrinkage,
    lr_power,
    use_locking=False,
    multiply_linear_by_lr=False,
    name=None
)
",[],[]
"tf.raw_ops.SparseApplyMomentum(
    var,
    accum,
    lr,
    grad,
    indices,
    momentum,
    use_locking=False,
    use_nesterov=False,
    name=None
)
","[[None, '\n'], [""Update relevant entries in '"", ""var' and '"", ""accum' according to the momentum scheme.""]]",[]
"tf.raw_ops.SparseApplyProximalAdagrad(
    var, accum, lr, l1, l2, grad, indices, use_locking=False, name=None
)
","[[None, '\n'], [""Sparse update entries in '"", ""var' and '"", ""accum' according to FOBOS algorithm.""]]",[]
"tf.raw_ops.SparseApplyProximalGradientDescent(
    var, alpha, l1, l2, grad, indices, use_locking=False, name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.SparseApplyRMSProp(
    var,
    ms,
    mom,
    lr,
    rho,
    momentum,
    epsilon,
    grad,
    indices,
    use_locking=False,
    name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.SparseBincount(
    indices, values, dense_shape, size, weights, binary_output=False, name=None
)
",[],[]
"tf.raw_ops.SparseConcat(
    indices, values, shapes, concat_dim, name=None
)
","[['Concatenates a list of ', 'SparseTensor', ' along the specified dimension.']]","sp_inputs[0]: shape = [2, 3]
[0, 2]: ""a""
[1, 0]: ""b""
[1, 1]: ""c""

sp_inputs[1]: shape = [2, 4]
[0, 1]: ""d""
[0, 2]: ""e""
"
"tf.raw_ops.SparseConditionalAccumulator(
    dtype,
    shape,
    container='',
    shared_name='',
    reduction_type='MEAN',
    name=None
)
",[],[]
"tf.raw_ops.SparseCountSparseOutput(
    indices,
    values,
    dense_shape,
    weights,
    binary_output,
    minlength=-1,
    maxlength=-1,
    name=None
)
",[],[]
"tf.raw_ops.SparseCross(
    indices,
    values,
    shapes,
    dense_inputs,
    hashed_output,
    num_buckets,
    hash_key,
    out_type,
    internal_type,
    name=None
)
",[],"inputs[0]: SparseTensor with shape = [2, 2]
[0, 0]: ""a""
[1, 0]: ""b""
[1, 1]: ""c""

inputs[1]: SparseTensor with shape = [2, 1]
[0, 0]: ""d""
[1, 0]: ""e""

inputs[2]: Tensor [[""f""], [""g""]]
"
"tf.raw_ops.SparseCrossHashed(
    indices,
    values,
    shapes,
    dense_inputs,
    num_buckets,
    strong_hash,
    salt,
    name=None
)
",[],"inputs[0]: SparseTensor with shape = [2, 2]
[0, 0]: ""a""
[1, 0]: ""b""
[1, 1]: ""c""

inputs[1]: SparseTensor with shape = [2, 1]
[0, 0]: ""d""
[1, 0]: ""e""

inputs[2]: Tensor [[""f""], [""g""]]
"
"tf.raw_ops.SparseCrossV2(
    indices, values, shapes, dense_inputs, sep, name=None
)
",[],"inputs[0]: SparseTensor with shape = [2, 2]
[0, 0]: ""a""
[1, 0]: ""b""
[1, 1]: ""c""

inputs[1]: SparseTensor with shape = [2, 1]
[0, 0]: ""d""
[1, 0]: ""e""

inputs[2]: Tensor [[""f""], [""g""]]
"
"tf.raw_ops.SparseDenseCwiseAdd(
    sp_indices, sp_values, sp_shape, dense, name=None
)
",[],[]
"tf.raw_ops.SparseDenseCwiseDiv(
    sp_indices, sp_values, sp_shape, dense, name=None
)
",[],[]
"tf.raw_ops.SparseDenseCwiseMul(
    sp_indices, sp_values, sp_shape, dense, name=None
)
",[],[]
"tf.raw_ops.SparseFillEmptyRows(
    indices, values, dense_shape, default_value, name=None
)
","[['Fills empty rows in the input 2-D ', 'SparseTensor', ' with a default value.']]","[0, 1]: a
[0, 3]: b
[2, 0]: c
[3, 1]: d
"
"tf.raw_ops.SparseFillEmptyRowsGrad(
    reverse_index_map, grad_values, name=None
)
",[],[]
"tf.raw_ops.SparseMatMul(
    a,
    b,
    transpose_a=False,
    transpose_b=False,
    a_is_sparse=False,
    b_is_sparse=False,
    name=None
)
",[],[]
"tf.raw_ops.SparseMatrixAdd(
    a, b, alpha, beta, name=None
)
",[],[]
"tf.raw_ops.SparseMatrixMatMul(
    a,
    b,
    transpose_a=False,
    transpose_b=False,
    adjoint_a=False,
    adjoint_b=False,
    transpose_output=False,
    conjugate_output=False,
    name=None
)
",[],"  C = A . B
"
"tf.raw_ops.SparseMatrixMul(
    a, b, name=None
)
",[],[]
"tf.raw_ops.SparseMatrixNNZ(
    sparse_matrix, name=None
)
","[['Returns the number of nonzeroes of ', 'sparse_matrix', '.']]",[]
"tf.raw_ops.SparseMatrixOrderingAMD(
    input, name=None
)
","[['Computes the Approximate Minimum Degree (AMD) ordering of ', 'input', '.']]","    from tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops

    a_indices = np.array([[0, 0], [1, 1], [2, 1], [2, 2], [3, 3]])
    a_values = np.array([1.0, 2.0, 1.0, 3.0, 4.0], np.float32)
    a_dense_shape = [4, 4]

    with tf.Session() as sess:
      # Define (COO format) SparseTensor over Numpy array.
      a_st = tf.sparse.SparseTensor(a_indices, a_values, a_dense_shape)

      # Convert SparseTensors to CSR SparseMatrix.
      a_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(
          a_st.indices, a_st.values, a_st.dense_shape)

      # Obtain the AMD Ordering for the CSR SparseMatrix.
      ordering_amd = sparse_csr_matrix_ops.sparse_matrix_ordering_amd(sparse_matrix)

      ordering_amd_value = sess.run(ordering_amd)
"
"tf.raw_ops.SparseMatrixSoftmax(
    logits, type, name=None
)
",[],[]
"tf.raw_ops.SparseMatrixSoftmaxGrad(
    softmax, grad_softmax, type, name=None
)
",[],[]
"tf.raw_ops.SparseMatrixSparseCholesky(
    input, permutation, type, name=None
)
","[['Computes the sparse Cholesky decomposition of ', 'input', '.']]","  A = L * Lt
"
"tf.raw_ops.SparseMatrixSparseMatMul(
    a,
    b,
    type,
    transpose_a=False,
    transpose_b=False,
    adjoint_a=False,
    adjoint_b=False,
    name=None
)
","[['Sparse-matrix-multiplies two CSR matrices ', 'a', ' and ', 'b', '.']]","    from tensorflow.python.ops.linalg.sparse import sparse_csr_matrix_ops

    a_indices = np.array([[0, 0], [2, 3], [2, 4], [3, 0]])
    a_values = np.array([1.0, 5.0, -1.0, -2.0], np.float32)
    a_dense_shape = [4, 5]

    b_indices = np.array([[0, 0], [3, 0], [3, 1]])
    b_values = np.array([2.0, 7.0, 8.0], np.float32)
    b_dense_shape = [5, 3]

    with tf.Session() as sess:
      # Define (COO format) Sparse Tensors over Numpy arrays
      a_st = tf.sparse.SparseTensor(a_indices, a_values, a_dense_shape)
      b_st = tf.sparse.SparseTensor(b_indices, b_values, b_dense_shape)

      # Convert SparseTensors to CSR SparseMatrix
      a_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(
          a_st.indices, a_st.values, a_st.dense_shape)
      b_sm = sparse_csr_matrix_ops.sparse_tensor_to_csr_sparse_matrix(
          b_st.indices, b_st.values, b_st.dense_shape)

      # Compute the CSR SparseMatrix matrix multiplication
      c_sm = sparse_csr_matrix_ops.sparse_matrix_sparse_mat_mul(
          a=a_sm, b=b_sm, type=tf.float32)

      # Convert the CSR SparseMatrix product to a dense Tensor
      c_sm_dense = sparse_csr_matrix_ops.csr_sparse_matrix_to_dense(
          c_sm, tf.float32)
      # Evaluate the dense Tensor value
      c_sm_dense_value = sess.run(c_sm_dense)
"
"tf.raw_ops.SparseMatrixTranspose(
    input, type, conjugate=False, name=None
)
",[],[]
"tf.raw_ops.SparseMatrixZeros(
    dense_shape, type, name=None
)
","[['Creates an all-zeros CSRSparseMatrix with shape ', 'dense_shape', '.']]",[]
"tf.raw_ops.SparseReduceMax(
    input_indices,
    input_values,
    input_shape,
    reduction_axes,
    keep_dims=False,
    name=None
)
",[],[]
"tf.raw_ops.SparseReduceMaxSparse(
    input_indices,
    input_values,
    input_shape,
    reduction_axes,
    keep_dims=False,
    name=None
)
",[],[]
"tf.raw_ops.SparseReduceSum(
    input_indices,
    input_values,
    input_shape,
    reduction_axes,
    keep_dims=False,
    name=None
)
",[],[]
"tf.raw_ops.SparseReduceSumSparse(
    input_indices,
    input_values,
    input_shape,
    reduction_axes,
    keep_dims=False,
    name=None
)
",[],[]
"tf.raw_ops.SparseReorder(
    input_indices, input_values, input_shape, name=None
)
",[],[]
"tf.raw_ops.SparseReshape(
    input_indices, input_shape, new_shape, name=None
)
",[],[]
"tf.raw_ops.SparseSegmentMean(
    data, indices, segment_ids, name=None
)
",[],[]
"tf.raw_ops.SparseSegmentMeanGrad(
    grad, indices, segment_ids, output_dim0, name=None
)
",[],[]
"tf.raw_ops.SparseSegmentMeanWithNumSegments(
    data, indices, segment_ids, num_segments, name=None
)
",[],[]
"tf.raw_ops.SparseSegmentSqrtN(
    data, indices, segment_ids, name=None
)
",[],[]
"tf.raw_ops.SparseSegmentSqrtNGrad(
    grad, indices, segment_ids, output_dim0, name=None
)
",[],[]
"tf.raw_ops.SparseSegmentSqrtNWithNumSegments(
    data, indices, segment_ids, num_segments, name=None
)
",[],[]
"tf.raw_ops.SparseSegmentSum(
    data, indices, segment_ids, name=None
)
",[],"c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])

# Select two rows, one segment.
tf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))
# => [[0 0 0 0]]

# Select two rows, two segment.
tf.sparse_segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))
# => [[ 1  2  3  4]
#     [-1 -2 -3 -4]]

# Select all rows, two segments.
tf.sparse_segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))
# => [[0 0 0 0]
#     [5 6 7 8]]

# Which is equivalent to:
tf.segment_sum(c, tf.constant([0, 0, 1]))
"
"tf.raw_ops.SparseSegmentSumGrad(
    grad, indices, segment_ids, output_dim0, name=None
)
",[],[]
"tf.raw_ops.SparseSegmentSumWithNumSegments(
    data, indices, segment_ids, num_segments, name=None
)
",[],"c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])

tf.sparse_segment_sum_with_num_segments(
    c, tf.constant([0, 1]), tf.constant([0, 0]), num_segments=3)
# => [[0 0 0 0]
#     [0 0 0 0]
#     [0 0 0 0]]

tf.sparse_segment_sum_with_num_segments(c,
                                        tf.constant([0, 1]),
                                        tf.constant([0, 2],
                                        num_segments=4))
# => [[ 1  2  3  4]
#     [ 0  0  0  0]
#     [-1 -2 -3 -4]
#     [ 0  0  0  0]]
"
"tf.raw_ops.SparseSlice(
    indices, values, shape, start, size, name=None
)
","[['Slice a ', 'SparseTensor', ' based on the ', 'start', ' and ', 'size', '.']]","input_tensor = shape = [2, 7]
[    a   d e  ]
[b c          ]
"
"tf.raw_ops.SparseSliceGrad(
    backprop_val_grad, input_indices, input_start, output_indices, name=None
)
",[],[]
"tf.raw_ops.SparseSoftmax(
    sp_indices, sp_values, sp_shape, name=None
)
","[['Applies softmax to a batched N-D ', 'SparseTensor', '.']]",[]
"tf.raw_ops.SparseSoftmaxCrossEntropyWithLogits(
    features, labels, name=None
)
",[],[]
"tf.raw_ops.SparseSparseMaximum(
    a_indices, a_values, a_shape, b_indices, b_values, b_shape, name=None
)
",[],[]
"tf.raw_ops.SparseSparseMinimum(
    a_indices, a_values, a_shape, b_indices, b_values, b_shape, name=None
)
",[],[]
"tf.raw_ops.SparseSplit(
    split_dim, indices, values, shape, num_split, name=None
)
","[['Split a ', 'SparseTensor', ' into ', 'num_split', ' tensors along one dimension.']]","input_tensor = shape = [2, 7]
[    a   d e  ]
[b c          ]
"
"tf.raw_ops.SparseTensorDenseAdd(
    a_indices, a_values, a_shape, b, name=None
)
","[['Adds up a ', 'SparseTensor', ' and a dense ', 'Tensor', ', producing a dense ', 'Tensor', '.']]",[]
"tf.raw_ops.SparseTensorDenseMatMul(
    a_indices,
    a_values,
    a_shape,
    b,
    adjoint_a=False,
    adjoint_b=False,
    name=None
)
",[],[]
"tf.raw_ops.SparseTensorSliceDataset(
    indices, values, dense_shape, name=None
)
",[],[]
"tf.raw_ops.SparseTensorToCSRSparseMatrix(
    indices, values, dense_shape, name=None
)
",[],[]
"tf.raw_ops.SparseToDense(
    sparse_indices,
    output_shape,
    sparse_values,
    default_value,
    validate_indices=True,
    name=None
)
",[],"# If sparse_indices is scalar
dense[i] = (i == sparse_indices ? sparse_values : default_value)

# If sparse_indices is a vector, then for each i
dense[sparse_indices[i]] = sparse_values[i]

# If sparse_indices is an n by d matrix, then for each i in [0, n)
dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]
"
"tf.raw_ops.SparseToSparseSetOperation(
    set1_indices,
    set1_values,
    set1_shape,
    set2_indices,
    set2_values,
    set2_shape,
    set_operation,
    validate_indices=True,
    name=None
)
","[['Applies set operation along last dimension of 2 ', 'SparseTensor', ' inputs.']]",[]
"tf.raw_ops.Spence(
    x, name=None
)
",[],[]
"tf.raw_ops.Split(
    axis, value, num_split, name=None
)
","[['Splits a tensor into ', 'num_split', ' tensors along one dimension.']]",[]
"tf.raw_ops.SqlDataset(
    driver_name,
    data_source_name,
    query,
    output_types,
    output_shapes,
    name=None
)
",[],[]
"tf.raw_ops.Sqrt(
    x, name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.SqrtGrad(
    y, dy, name=None
)
","[['Computes the gradient for the sqrt of ', 'x', ' wrt its input.']]",[]
"tf.raw_ops.Square(
    x, name=None
)
","[[None, '\n']]","tf.math.square([-2., 0., 3.])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>"
"tf.raw_ops.SquaredDifference(
    x, y, name=None
)
",[],[]
"tf.raw_ops.Squeeze(
    input, axis=[], name=None
)
",[],"# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]
shape(squeeze(t)) ==> [2, 3]
"
"tf.raw_ops.Stack(
    elem_type, stack_name='', name=None
)
",[],[]
"tf.raw_ops.StackClose(
    handle, name=None
)
",[],[]
"tf.raw_ops.StackCloseV2(
    handle, name=None
)
",[],[]
"tf.raw_ops.StackPop(
    handle, elem_type, name=None
)
",[],[]
"tf.raw_ops.StackPopV2(
    handle, elem_type, name=None
)
",[],[]
"tf.raw_ops.StackPush(
    handle, elem, swap_memory=False, name=None
)
",[],[]
"tf.raw_ops.StackPushV2(
    handle, elem, swap_memory=False, name=None
)
",[],[]
"tf.raw_ops.StackV2(
    max_size, elem_type, stack_name='', name=None
)
",[],[]
"tf.raw_ops.Stage(
    values,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.StageClear(
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.StagePeek(
    index,
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.StageSize(
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.StatefulPartitionedCall(
    args,
    Tout,
    f,
    config='',
    config_proto='',
    executor_type='',
    name=None
)
","[['returns ', 'f(inputs)', ', where ', 'f', ""'s body is placed and partitioned.""]]",[]
"tf.raw_ops.StatefulRandomBinomial(
    resource,
    algorithm,
    shape,
    counts,
    probs,
    dtype=tf.dtypes.int64,
    name=None
)
",[],[]
"tf.raw_ops.StatefulStandardNormal(
    resource,
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.StatefulStandardNormalV2(
    resource,
    algorithm,
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.StatefulTruncatedNormal(
    resource,
    algorithm,
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.StatefulUniform(
    resource,
    algorithm,
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.StatefulUniformFullInt(
    resource,
    algorithm,
    shape,
    dtype=tf.dtypes.uint64,
    name=None
)
",[],[]
"tf.raw_ops.StatefulUniformInt(
    resource, algorithm, shape, minval, maxval, name=None
)
",[],[]
"tf.raw_ops.StatelessCase(
    branch_index, input, Tout, branches, output_shapes=[], name=None
)
",[],"An n-way switch statement, implementing the following:

```
switch (branch_index) {
  case 0:
    output = branches[0](input);
    break;
  case 1:
    output = branches[1](input);
    break;
  ...
  case [[nbranches-1]]:
  default:
    output = branches[nbranches-1](input);
    break;
}
```

This should only be used when the none of branches has stateful ops.
"
"tf.raw_ops.StatelessIf(
    cond, input, Tout, then_branch, else_branch, output_shapes=[], name=None
)
",[],[]
"tf.raw_ops.StatelessMultinomial(
    logits,
    num_samples,
    seed,
    output_dtype=tf.dtypes.int64,
    name=None
)
",[],[]
"tf.raw_ops.StatelessParameterizedTruncatedNormal(
    shape, seed, means, stddevs, minvals, maxvals, name=None
)
",[],[]
"tf.raw_ops.StatelessRandomBinomial(
    shape,
    seed,
    counts,
    probs,
    dtype=tf.dtypes.int64,
    name=None
)
",[],[]
"tf.raw_ops.StatelessRandomGammaV2(
    shape, seed, alpha, name=None
)
",[],[]
"tf.raw_ops.StatelessRandomGetAlg(
    name=None
)
",[],[]
"tf.raw_ops.StatelessRandomGetKeyCounter(
    seed, name=None
)
",[],[]
"tf.raw_ops.StatelessRandomGetKeyCounterAlg(
    seed, name=None
)
",[],[]
"tf.raw_ops.StatelessRandomNormal(
    shape,
    seed,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.StatelessRandomNormalV2(
    shape,
    key,
    counter,
    alg,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.StatelessRandomPoisson(
    shape, seed, lam, dtype, name=None
)
",[],[]
"tf.raw_ops.StatelessRandomUniform(
    shape,
    seed,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.StatelessRandomUniformFullInt(
    shape,
    seed,
    dtype=tf.dtypes.uint64,
    name=None
)
",[],[]
"tf.raw_ops.StatelessRandomUniformFullIntV2(
    shape,
    key,
    counter,
    alg,
    dtype=tf.dtypes.uint64,
    name=None
)
",[],[]
"tf.raw_ops.StatelessRandomUniformInt(
    shape, seed, minval, maxval, name=None
)
",[],[]
"tf.raw_ops.StatelessRandomUniformIntV2(
    shape, key, counter, alg, minval, maxval, name=None
)
",[],[]
"tf.raw_ops.StatelessRandomUniformV2(
    shape,
    key,
    counter,
    alg,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.StatelessSampleDistortedBoundingBox(
    image_size,
    bounding_boxes,
    min_object_covered,
    seed,
    aspect_ratio_range=[0.75, 1.33],
    area_range=[0.05, 1],
    max_attempts=100,
    use_image_if_no_bounding_boxes=False,
    name=None
)
",[],"image = np.array([[[1], [2], [3]], [[4], [5], [6]], [[7], [8], [9]]])
bbox = tf.constant(
  [0.0, 0.0, 1.0, 1.0], dtype=tf.float32, shape=[1, 1, 4])
seed = (1, 2)
# Generate a single distorted bounding box.
bbox_begin, bbox_size, bbox_draw = (
  tf.image.stateless_sample_distorted_bounding_box(
    tf.shape(image), bounding_boxes=bbox, seed=seed))
# Employ the bounding box to distort the image.
tf.slice(image, bbox_begin, bbox_size)
<tf.Tensor: shape=(2, 2, 1), dtype=int64, numpy=
array([[[1],
        [2]],
       [[4],
        [5]]])>
# Draw the bounding box in an image summary.
colors = np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]])
tf.image.draw_bounding_boxes(
  tf.expand_dims(tf.cast(image, tf.float32),0), bbox_draw, colors)
<tf.Tensor: shape=(1, 3, 3, 1), dtype=float32, numpy=
array([[[[1.],
         [1.],
         [3.]],
        [[1.],
         [1.],
         [6.]],
        [[7.],
         [8.],
         [9.]]]], dtype=float32)>"
"tf.raw_ops.StatelessShuffle(
    value, key, counter, alg, name=None
)
",[],"[[1, 2],       [[5, 6],
 [3, 4],  ==>   [1, 2],
 [5, 6]]        [3, 4]]
"
"tf.raw_ops.StatelessTruncatedNormal(
    shape,
    seed,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.StatelessTruncatedNormalV2(
    shape,
    key,
    counter,
    alg,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.raw_ops.StatelessWhile(
    input, cond, body, output_shapes=[], parallel_iterations=10, name=None
)
",[],[]
"tf.raw_ops.StaticRegexFullMatch(
    input, pattern, name=None
)
",[],[]
"tf.raw_ops.StaticRegexReplace(
    input, pattern, rewrite, replace_global=True, name=None
)
",[],[]
"tf.raw_ops.StatsAggregatorHandle(
    container='', shared_name='', name=None
)
",[],[]
"tf.raw_ops.StatsAggregatorHandleV2(
    container='', shared_name='', name=None
)
",[],[]
"tf.raw_ops.StatsAggregatorSetSummaryWriter(
    stats_aggregator, summary, name=None
)
",[],[]
"tf.raw_ops.StatsAggregatorSummary(
    iterator, name=None
)
",[],[]
"tf.raw_ops.StopGradient(
    input, name=None
)
",[],"
  def softmax(x):
    numerator = tf.exp(x)
    denominator = tf.reduce_sum(numerator)
    return numerator / denominator
"
"tf.raw_ops.StridedSlice(
    input,
    begin,
    end,
    strides,
    begin_mask=0,
    end_mask=0,
    ellipsis_mask=0,
    new_axis_mask=0,
    shrink_axis_mask=0,
    name=None
)
","[['Return a strided slice from ', 'input', '.']]","begin = [1, 2, x, x, 0, x] # x denotes don't care (usually 0)
end = [2, 4, x, x, -3, x]
strides = [1, 1, x, x, -1, 1]
begin_mask = 1<<4 | 1<<5 = 48
end_mask = 1<<5 = 32
ellipsis_mask = 1<<3 = 8
new_axis_mask = 1<<2 = 4
shrink_axis_mask = 1<<0 = 1
"
"tf.raw_ops.StridedSliceAssign(
    ref,
    begin,
    end,
    strides,
    value,
    begin_mask=0,
    end_mask=0,
    ellipsis_mask=0,
    new_axis_mask=0,
    shrink_axis_mask=0,
    name=None
)
","[['Assign ', 'value', ' to the sliced l-value reference of ', 'ref', '.']]",[]
"tf.raw_ops.StridedSliceGrad(
    shape,
    begin,
    end,
    strides,
    dy,
    begin_mask=0,
    end_mask=0,
    ellipsis_mask=0,
    new_axis_mask=0,
    shrink_axis_mask=0,
    name=None
)
","[['Returns the gradient of ', 'StridedSlice', '.']]",[]
"tf.raw_ops.StringFormat(
    inputs,
    template='%s',
    placeholder='%s',
    summarize=3,
    name=None
)
",[],[]
"tf.raw_ops.StringJoin(
    inputs, separator='', name=None
)
",[],"s = [""hello"", ""world"", ""tensorflow""]
tf.strings.join(s, "" "")
<tf.Tensor: shape=(), dtype=string, numpy=b'hello world tensorflow'>"
"tf.raw_ops.StringLength(
    input, unit='BYTE', name=None
)
","[['String lengths of ', 'input', '.']]","strings = tf.constant(['Hello','TensorFlow', '\U0001F642'])
tf.strings.length(strings).numpy() # default counts bytes
array([ 5, 10, 4], dtype=int32)
tf.strings.length(strings, unit=""UTF8_CHAR"").numpy()
array([ 5, 10, 1], dtype=int32)"
"tf.raw_ops.StringLower(
    input, encoding='', name=None
)
",[],"tf.strings.lower(""CamelCase string and ALL CAPS"")
<tf.Tensor: shape=(), dtype=string, numpy=b'camelcase string and all caps'>"
"tf.raw_ops.StringNGrams(
    data,
    data_splits,
    separator,
    ngram_widths,
    left_pad,
    right_pad,
    pad_width,
    preserve_short_sequences,
    name=None
)
",[],[]
"tf.raw_ops.StringSplit(
    input, delimiter, skip_empty=True, name=None
)
","[['Split elements of ', 'input', ' based on ', 'delimiter', ' into a ', 'SparseTensor', '.']]",[]
"tf.raw_ops.StringSplitV2(
    input, sep, maxsplit=-1, name=None
)
","[['Split elements of ', 'source', ' based on ', 'sep', ' into a ', 'SparseTensor', '.']]","st.indices = [0, 0;
              0, 1;
              1, 0;
              1, 1;
              1, 2]
st.shape = [2, 3]
st.values = ['hello', 'world', 'a', 'b', 'c']
"
"tf.raw_ops.StringToHashBucket(
    string_tensor, num_buckets, name=None
)
",[],[]
"tf.raw_ops.StringToHashBucketFast(
    input, num_buckets, name=None
)
",[],"tf.strings.to_hash_bucket_fast([""Hello"", ""TensorFlow"", ""2.x""], 3).numpy()
array([0, 2, 2])"
"tf.raw_ops.StringToHashBucketStrong(
    input, num_buckets, key, name=None
)
",[],"tf.strings.to_hash_bucket_strong([""Hello"", ""TF""], 3, [1, 2]).numpy()
array([2, 0])"
"tf.raw_ops.StringToNumber(
    string_tensor,
    out_type=tf.dtypes.float32,
    name=None
)
",[],"strings = [""5.0"", ""3.0"", ""7.0""]
tf.strings.to_number(strings)
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([5., 3., 7.], dtype=float32)>"
"tf.raw_ops.StringUpper(
    input, encoding='', name=None
)
",[],"tf.strings.upper(""CamelCase string and ALL CAPS"")
<tf.Tensor: shape=(), dtype=string, numpy=b'CAMELCASE STRING AND ALL CAPS'>"
"tf.raw_ops.Sub(
    x, y, name=None
)
",[],"x = [1, 2, 3, 4, 5]
y = 1
tf.subtract(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>
tf.subtract(y, x)
<tf.Tensor: shape=(5,), dtype=int32,
numpy=array([ 0, -1, -2, -3, -4], dtype=int32)>"
"tf.raw_ops.Substr(
    input, pos, len, unit='BYTE', name=None
)
","[['Return substrings from ', 'Tensor', ' of strings.']]","input = [b'Hello', b'World']
position = 1
length = 3

output = [b'ell', b'orl']
"
"tf.raw_ops.Sum(
    input, axis, keep_dims=False, name=None
)
",[],[]
"tf.raw_ops.SummaryWriter(
    shared_name='', container='', name=None
)
",[],[]
"tf.raw_ops.Svd(
    input, compute_uv=True, full_matrices=False, name=None
)
",[],"# a is a tensor containing a batch of matrices.
# s is a tensor of singular values for each matrix.
# u is the tensor containing the left singular vectors for each matrix.
# v is the tensor containing the right singular vectors for each matrix.
s, u, v = svd(a)
s, _, _ = svd(a, compute_uv=False)
"
"tf.raw_ops.Switch(
    data, pred, name=None
)
","[['Forwards ', 'data', ' to the output port determined by ', 'pred', '.']]",[]
"tf.raw_ops.SymbolicGradient(
    input, Tout, f, name=None
)
",[],[]
"tf.raw_ops.TFRecordDataset(
    filenames, compression_type, buffer_size, metadata='', name=None
)
",[],[]
"tf.raw_ops.TFRecordReader(
    container='',
    shared_name='',
    compression_type='',
    name=None
)
",[],[]
"tf.raw_ops.TFRecordReaderV2(
    container='',
    shared_name='',
    compression_type='',
    name=None
)
",[],[]
"tf.raw_ops.TPUCompilationResult(
    name=None
)
",[],[]
"tf.raw_ops.TPUEmbeddingActivations(
    embedding_variable, sliced_activations, table_id, lookup_id, name=None
)
",[],[]
"tf.raw_ops.TPUOrdinalSelector(
    name=None
)
",[],[]
"tf.raw_ops.TPUPartitionedCall(
    args, device_ordinal, Tout, f, autotuner_thresh=0, name=None
)
",[],[]
"tf.raw_ops.TPUPartitionedInput(
    inputs, partition_dim=0, name=None
)
",[],[]
"tf.raw_ops.TPUPartitionedOutput(
    inputs, num_splits, partition_dim=0, name=None
)
",[],[]
"tf.raw_ops.TPUReplicateMetadata(
    num_replicas,
    num_cores_per_replica=1,
    topology='',
    use_tpu=True,
    device_assignment=[],
    computation_shape=[],
    host_compute_core=[],
    padding_map=[],
    step_marker_location='STEP_MARK_AT_ENTRY',
    allow_soft_placement=False,
    use_spmd_for_xla_partitioning=False,
    tpu_compile_options_proto='',
    name=None
)
",[],[]
"tf.raw_ops.TPUReplicatedInput(
    inputs, is_mirrored_variable=False, index=-1, is_packed=False, name=None
)
",[],"%a = ""tf.opA""()
%b = ""tf.opB""()
%replicated_input = ""tf.TPUReplicatedInput""(%a, %b)
%computation = ""tf.Computation""(%replicated_input)
"
"tf.raw_ops.TPUReplicatedOutput(
    input, num_replicas, name=None
)
",[],"%computation = ""tf.Computation""()
%replicated_output:2 = ""tf.TPUReplicatedOutput""(%computation)
"
"tf.raw_ops.TakeDataset(
    input_dataset,
    count,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset that contains ', 'count', ' elements from the ', 'input_dataset', '.']]",[]
"tf.raw_ops.TakeManySparseFromTensorsMap(
    sparse_handles,
    dtype,
    container='',
    shared_name='',
    name=None
)
","[['Read ', 'SparseTensors', ' from a ', 'SparseTensorsMap', ' and concatenate them.']]","    index = [ 0]
            [10]
            [20]
    values = [1, 2, 3]
    shape = [50]
"
"tf.raw_ops.TakeWhileDataset(
    input_dataset,
    other_arguments,
    predicate,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.Tan(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.tan(x) ==> [nan 0.45231566 -0.5463025 1.5574077 2.572152 -1.7925274 0.32097113 nan]
"
"tf.raw_ops.Tanh(
    x, name=None
)
","[['Computes hyperbolic tangent of ', 'x', ' element-wise.']]",[]
"tf.raw_ops.TanhGrad(
    y, dy, name=None
)
","[['Computes the gradient for the tanh of ', 'x', ' wrt its input.']]",[]
"tf.raw_ops.TemporaryVariable(
    shape, dtype, var_name='', name=None
)
",[],[]
"tf.raw_ops.TensorArray(
    size,
    dtype,
    dynamic_size=False,
    clear_after_read=True,
    tensor_array_name='',
    element_shape=None,
    name=None
)
",[],[]
"tf.raw_ops.TensorArrayClose(
    handle, name=None
)
",[],[]
"tf.raw_ops.TensorArrayCloseV2(
    handle, name=None
)
",[],[]
"tf.raw_ops.TensorArrayCloseV3(
    handle, name=None
)
",[],[]
"tf.raw_ops.TensorArrayConcat(
    handle, flow_in, dtype, element_shape_except0=None, name=None
)
",[],[]
"tf.raw_ops.TensorArrayConcatV2(
    handle, flow_in, dtype, element_shape_except0=None, name=None
)
",[],[]
"tf.raw_ops.TensorArrayConcatV3(
    handle, flow_in, dtype, element_shape_except0=None, name=None
)
","[['Concat the elements from the TensorArray into value ', 'value', '.']]","  (n0 x d0 x d1 x ...), (n1 x d0 x d1 x ...), ..., (n(T-1) x d0 x d1 x ...)
"
"tf.raw_ops.TensorArrayGather(
    handle, indices, flow_in, dtype, element_shape=None, name=None
)
",[],[]
"tf.raw_ops.TensorArrayGatherV2(
    handle, indices, flow_in, dtype, element_shape=None, name=None
)
",[],[]
"tf.raw_ops.TensorArrayGatherV3(
    handle, indices, flow_in, dtype, element_shape=None, name=None
)
","[['Gather specific elements from the TensorArray into output ', 'value', '.']]",[]
"tf.raw_ops.TensorArrayGrad(
    handle, flow_in, source, name=None
)
",[],[]
"tf.raw_ops.TensorArrayGradV2(
    handle, flow_in, source, name=None
)
",[],[]
"tf.raw_ops.TensorArrayGradV3(
    handle, flow_in, source, name=None
)
",[],[]
"tf.raw_ops.TensorArrayGradWithShape(
    handle, flow_in, shape_to_prepend, source, name=None
)
",[],[]
"tf.raw_ops.TensorArrayPack(
    handle, flow_in, dtype, element_shape=None, name=None
)
",[],[]
"tf.raw_ops.TensorArrayRead(
    handle, index, flow_in, dtype, name=None
)
",[],[]
"tf.raw_ops.TensorArrayReadV2(
    handle, index, flow_in, dtype, name=None
)
",[],[]
"tf.raw_ops.TensorArrayReadV3(
    handle, index, flow_in, dtype, name=None
)
","[['Read an element from the TensorArray into output ', 'value', '.']]",[]
"tf.raw_ops.TensorArrayScatter(
    handle, indices, value, flow_in, name=None
)
",[],[]
"tf.raw_ops.TensorArrayScatterV2(
    handle, indices, value, flow_in, name=None
)
",[],[]
"tf.raw_ops.TensorArrayScatterV3(
    handle, indices, value, flow_in, name=None
)
",[],[]
"tf.raw_ops.TensorArraySize(
    handle, flow_in, name=None
)
",[],[]
"tf.raw_ops.TensorArraySizeV2(
    handle, flow_in, name=None
)
",[],[]
"tf.raw_ops.TensorArraySizeV3(
    handle, flow_in, name=None
)
",[],[]
"tf.raw_ops.TensorArraySplit(
    handle, value, lengths, flow_in, name=None
)
",[],[]
"tf.raw_ops.TensorArraySplitV2(
    handle, value, lengths, flow_in, name=None
)
",[],[]
"tf.raw_ops.TensorArraySplitV3(
    handle, value, lengths, flow_in, name=None
)
",[],"  (n0, n1, ..., n(T-1))
"
"tf.raw_ops.TensorArrayUnpack(
    handle, value, flow_in, name=None
)
",[],[]
"tf.raw_ops.TensorArrayV2(
    size,
    dtype,
    element_shape=None,
    dynamic_size=False,
    clear_after_read=True,
    tensor_array_name='',
    name=None
)
",[],[]
"tf.raw_ops.TensorArrayV3(
    size,
    dtype,
    element_shape=None,
    dynamic_size=False,
    clear_after_read=True,
    identical_element_shapes=False,
    tensor_array_name='',
    name=None
)
",[],[]
"tf.raw_ops.TensorArrayWrite(
    handle, index, value, flow_in, name=None
)
",[],[]
"tf.raw_ops.TensorArrayWriteV2(
    handle, index, value, flow_in, name=None
)
",[],[]
"tf.raw_ops.TensorArrayWriteV3(
    handle, index, value, flow_in, name=None
)
",[],[]
"tf.raw_ops.TensorDataset(
    components, output_shapes, metadata='', name=None
)
","[['Creates a dataset that emits ', 'components', ' as a tuple of tensors once.']]",[]
"tf.raw_ops.TensorListConcat(
    input_handle, element_dtype, element_shape=None, name=None
)
",[],[]
"tf.raw_ops.TensorListConcatLists(
    input_a, input_b, element_dtype, name=None
)
",[],[]
"tf.raw_ops.TensorListConcatV2(
    input_handle, element_shape, leading_dims, element_dtype, name=None
)
",[],[]
"tf.raw_ops.TensorListElementShape(
    input_handle, shape_type, name=None
)
",[],[]
"tf.raw_ops.TensorListFromTensor(
    tensor, element_shape, name=None
)
","[['Creates a TensorList which, when stacked, has the value of ', 'tensor', '.']]",[]
"tf.raw_ops.TensorListGather(
    input_handle, indices, element_shape, element_dtype, name=None
)
",[],[]
"tf.raw_ops.TensorListGetItem(
    input_handle, index, element_shape, element_dtype, name=None
)
",[],[]
"tf.raw_ops.TensorListLength(
    input_handle, name=None
)
",[],[]
"tf.raw_ops.TensorListPopBack(
    input_handle, element_shape, element_dtype, name=None
)
",[],[]
"tf.raw_ops.TensorListPushBack(
    input_handle, tensor, name=None
)
","[['Returns a list which has the passed-in ', 'Tensor', ' as last element and the other elements of the given list in ', 'input_handle', '.']]",[]
"tf.raw_ops.TensorListPushBackBatch(
    input_handles, tensor, name=None
)
",[],[]
"tf.raw_ops.TensorListReserve(
    element_shape, num_elements, element_dtype, name=None
)
",[],[]
"tf.raw_ops.TensorListResize(
    input_handle, size, name=None
)
",[],[]
"tf.raw_ops.TensorListScatter(
    tensor, indices, element_shape, name=None
)
",[],[]
"tf.raw_ops.TensorListScatterIntoExistingList(
    input_handle, tensor, indices, name=None
)
",[],[]
"tf.raw_ops.TensorListScatterV2(
    tensor, indices, element_shape, num_elements, name=None
)
",[],[]
"tf.raw_ops.TensorListSetItem(
    input_handle, index, item, name=None
)
",[],[]
"tf.raw_ops.TensorListSplit(
    tensor, element_shape, lengths, name=None
)
",[],[]
"tf.raw_ops.TensorListStack(
    input_handle, element_shape, element_dtype, num_elements=-1, name=None
)
",[],[]
"tf.raw_ops.TensorScatterAdd(
    tensor, indices, updates, name=None
)
","[['Adds sparse ', 'updates', ' to an existing tensor according to ', 'indices', '.']]","indices.shape[-1] <= tensor.shape.rank
"
"tf.raw_ops.TensorScatterMax(
    tensor, indices, updates, name=None
)
",[],"tensor = [0, 0, 0, 0, 0, 0, 0, 0]
indices = [[1], [4], [5]]
updates = [1, -1, 1]
tf.tensor_scatter_nd_max(tensor, indices, updates).numpy()
array([0, 1, 0, 0, 0, 1, 0, 0], dtype=int32)"
"tf.raw_ops.TensorScatterMin(
    tensor, indices, updates, name=None
)
",[],[]
"tf.raw_ops.TensorScatterSub(
    tensor, indices, updates, name=None
)
","[['Subtracts sparse ', 'updates', ' from an existing tensor according to ', 'indices', '.']]","indices.shape[-1] <= shape.rank
"
"tf.raw_ops.TensorScatterUpdate(
    tensor, indices, updates, name=None
)
","[['Scatter ', 'updates', ' into an existing tensor according to ', 'indices', '.']]","indices.shape[:-1] + tensor.shape[indices.shape[-1]:]
"
"tf.raw_ops.TensorSliceDataset(
    components,
    output_shapes,
    is_files=False,
    metadata='',
    replicate_on_split=False,
    name=None
)
","[['Creates a dataset that emits each dim-0 slice of ', 'components', ' once.']]",[]
"tf.raw_ops.TensorStridedSliceUpdate(
    input,
    begin,
    end,
    strides,
    value,
    begin_mask=0,
    end_mask=0,
    ellipsis_mask=0,
    new_axis_mask=0,
    shrink_axis_mask=0,
    name=None
)
","[['Assign ', 'value', ' to the sliced l-value reference of ', 'input', '.']]",[]
"tf.raw_ops.TensorSummary(
    tensor,
    description='',
    labels=[],
    display_name='',
    name=None
)
","[['Outputs a ', 'Summary', ' protocol buffer with a tensor.']]",[]
"tf.raw_ops.TensorSummaryV2(
    tag, tensor, serialized_summary_metadata, name=None
)
","[['Outputs a ', 'Summary', ' protocol buffer with a tensor and per-plugin data.']]",[]
"tf.raw_ops.TextLineDataset(
    filenames, compression_type, buffer_size, metadata='', name=None
)
",[],[]
"tf.raw_ops.TextLineReader(
    skip_header_lines=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.TextLineReaderV2(
    skip_header_lines=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.ThreadPoolDataset(
    input_dataset, thread_pool, output_types, output_shapes, name=None
)
","[['Creates a dataset that uses a custom thread pool to compute ', 'input_dataset', '.']]",[]
"tf.raw_ops.ThreadPoolHandle(
    num_threads,
    display_name,
    max_intra_op_parallelism=1,
    container='',
    shared_name='',
    name=None
)
","[['Creates a dataset that uses a custom thread pool to compute ', 'input_dataset', '.']]",[]
"tf.raw_ops.ThreadUnsafeUnigramCandidateSampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=0,
    seed2=0,
    name=None
)
",[],[]
"tf.raw_ops.Tile(
    input, multiples, name=None
)
",[],"a = tf.constant([[1,2,3],[4,5,6]], tf.int32)
b = tf.constant([1,2], tf.int32)
tf.tile(a, b)
<tf.Tensor: shape=(2, 6), dtype=int32, numpy=
array([[1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6]], dtype=int32)>
c = tf.constant([2,1], tf.int32)
tf.tile(a, c)
<tf.Tensor: shape=(4, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6],
       [1, 2, 3],
       [4, 5, 6]], dtype=int32)>
d = tf.constant([2,2], tf.int32)
tf.tile(a, d)
<tf.Tensor: shape=(4, 6), dtype=int32, numpy=
array([[1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6],
       [1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6]], dtype=int32)>"
"tf.raw_ops.TileGrad(
    input, multiples, name=None
)
","[['Returns the gradient of ', 'Tile', '.']]",[]
"tf.raw_ops.Timestamp(
    name=None
)
",[],[]
"tf.raw_ops.ToBool(
    input, name=None
)
",[],[]
"tf.raw_ops.TopK(
    input, k, sorted=True, name=None
)
","[['Finds values and indices of the ', 'k', ' largest elements for the last dimension.']]","values.shape = indices.shape = input.shape[:-1] + [k]
"
"tf.raw_ops.TopKV2(
    input, k, sorted=True, name=None
)
","[['Finds values and indices of the ', 'k', ' largest elements for the last dimension.']]","values.shape = indices.shape = input.shape[:-1] + [k]
"
"tf.raw_ops.Transpose(
    x, perm, name=None
)
",[],[]
"tf.raw_ops.TridiagonalMatMul(
    superdiag, maindiag, subdiag, rhs, name=None
)
",[],[]
"tf.raw_ops.TridiagonalSolve(
    diagonals, rhs, partial_pivoting=True, perturb_singular=False, name=None
)
",[],[]
"tf.raw_ops.TruncateDiv(
    x, y, name=None
)
",[],[]
"tf.raw_ops.TruncateMod(
    x, y, name=None
)
",[],[]
"tf.raw_ops.TruncatedNormal(
    shape, dtype, seed=0, seed2=0, name=None
)
",[],[]
"tf.raw_ops.Unbatch(
    batched_tensor,
    batch_index,
    id,
    timeout_micros,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.UnbatchDataset(
    input_dataset,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
",[],[]
"tf.raw_ops.UnbatchGrad(
    original_input,
    batch_index,
    grad,
    id,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.UncompressElement(
    compressed, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.UnicodeDecode(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    Tsplits=tf.dtypes.int64,
    name=None
)
","[['Decodes each string in ', 'input', ' into a sequence of Unicode code points.']]",[]
"tf.raw_ops.UnicodeDecodeWithOffsets(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    Tsplits=tf.dtypes.int64,
    name=None
)
","[['Decodes each string in ', 'input', ' into a sequence of Unicode code points.']]",[]
"tf.raw_ops.UnicodeEncode(
    input_values,
    input_splits,
    output_encoding,
    errors='replace',
    replacement_char=65533,
    name=None
)
",[],"input_values = [72, 101, 108, 108, 111, 87, 111, 114, 108, 100]
input_splits = [0, 5, 10]
output_encoding = 'UTF-8'

output = ['Hello', 'World']
"
"tf.raw_ops.UnicodeScript(
    input, name=None
)
",[],"tf.strings.unicode_script([1, 31, 38])
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 0], dtype=int32)>"
"tf.raw_ops.UnicodeTranscode(
    input,
    input_encoding,
    output_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    name=None
)
",[],"tf.strings.unicode_transcode([""Hello"", ""TensorFlow"", ""2.x""], ""UTF-8"", ""UTF-16-BE"")
<tf.Tensor: shape=(3,), dtype=string, numpy=
array([b'\x00H\x00e\x00l\x00l\x00o',
       b'\x00T\x00e\x00n\x00s\x00o\x00r\x00F\x00l\x00o\x00w',
       b'\x002\x00.\x00x'], dtype=object)>
tf.strings.unicode_transcode([""A"", ""B"", ""C""], ""US ASCII"", ""UTF-8"").numpy()
array([b'A', b'B', b'C'], dtype=object)"
"tf.raw_ops.UniformCandidateSampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=0,
    seed2=0,
    name=None
)
",[],[]
"tf.raw_ops.UniformDequantize(
    input,
    scales,
    zero_points,
    Tout,
    quantization_min_val,
    quantization_max_val,
    quantization_axis=-1,
    name=None
)
","[['Perform dequantization on the quantized Tensor ', 'input', '.']]",[]
"tf.raw_ops.UniformQuantize(
    input,
    scales,
    zero_points,
    Tout,
    quantization_min_val,
    quantization_max_val,
    quantization_axis=-1,
    name=None
)
","[['Perform quantization on Tensor ', 'input', '.']]",[]
"tf.raw_ops.UniformQuantizedClipByValue(
    operand,
    min,
    max,
    scales,
    zero_points,
    quantization_min_val,
    quantization_max_val,
    quantization_axis=-1,
    name=None
)
","[['Perform clip by value on the quantized Tensor ', 'operand', '.']]",[]
"tf.raw_ops.UniformQuantizedDot(
    lhs,
    rhs,
    lhs_scales,
    lhs_zero_points,
    rhs_scales,
    rhs_zero_points,
    output_scales,
    output_zero_points,
    Tout,
    lhs_quantization_min_val,
    lhs_quantization_max_val,
    rhs_quantization_min_val,
    rhs_quantization_max_val,
    output_quantization_min_val,
    output_quantization_max_val,
    lhs_quantization_axis=-1,
    rhs_quantization_axis=-1,
    output_quantization_axis=-1,
    name=None
)
","[['Perform quantized dot of quantized Tensor ', 'lhs', ' and quantized Tensor ', 'rhs', ' to make quantized ', 'output', '.']]",[]
"tf.raw_ops.UniformQuantizedDotHybrid(
    lhs,
    rhs,
    rhs_scales,
    rhs_zero_points,
    Tout,
    rhs_quantization_min_val,
    rhs_quantization_max_val,
    rhs_quantization_axis=-1,
    name=None
)
","[['Perform hybrid quantized dot of float Tensor ', 'lhs', ' and quantized Tensor ', 'rhs', '.']]",[]
"tf.raw_ops.UniformRequantize(
    input,
    input_scales,
    input_zero_points,
    output_scales,
    output_zero_points,
    Tout,
    input_quantization_min_val,
    input_quantization_max_val,
    output_quantization_min_val,
    output_quantization_max_val,
    input_quantization_axis=-1,
    output_quantization_axis=-1,
    name=None
)
","[['Given quantized tensor ', 'input', ', requantize it with new quantization parameters.']]",[]
"tf.raw_ops.Unique(
    x,
    out_idx=tf.dtypes.int32,
    name=None
)
",[],"# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]
y, idx = unique(x)
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
"
"tf.raw_ops.UniqueDataset(
    input_dataset,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset that contains the unique elements of ', 'input_dataset', '.']]",[]
"tf.raw_ops.UniqueV2(
    x,
    axis,
    out_idx=tf.dtypes.int32,
    name=None
)
",[],"# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]
y, idx = unique(x)
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
"
"tf.raw_ops.UniqueWithCounts(
    x,
    out_idx=tf.dtypes.int32,
    name=None
)
",[],"# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]
y, idx, count = unique_with_counts(x)
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
count ==> [2, 1, 3, 1, 2]
"
"tf.raw_ops.UniqueWithCountsV2(
    x,
    axis,
    out_idx=tf.dtypes.int32,
    name=None
)
",[],"x = tf.constant([1, 1, 2, 4, 4, 4, 7, 8, 8])
y, idx, count = UniqueWithCountsV2(x, axis = [0])
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
count ==> [2, 1, 3, 1, 2]
"
"tf.raw_ops.Unpack(
    value, num, axis=0, name=None
)
","[['Unpacks a given dimension of a rank-', 'R', ' tensor into ', 'num', ' rank-', '(R-1)', ' tensors.']]",[]
"tf.raw_ops.UnravelIndex(
    indices, dims, name=None
)
",[],"y = tf.unravel_index(indices=[2, 5, 7], dims=[3, 3])
# 'dims' represent a hypothetical (3, 3) tensor of indices:
# [[0, 1, *2*],
#  [3, 4, *5*],
#  [6, *7*, 8]]
# For each entry from 'indices', this operation returns
# its coordinates (marked with '*'), such as
# 2 ==> (0, 2)
# 5 ==> (1, 2)
# 7 ==> (2, 1)
y ==> [[0, 1, 2], [2, 2, 1]]
"
"tf.raw_ops.UnsortedSegmentJoin(
    inputs, segment_ids, num_segments, separator='', name=None
)
",[],[]
"tf.raw_ops.UnsortedSegmentMax(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_max(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[4, 3, 3, 4],
       [5,  6, 7, 8]], dtype=int32)"
"tf.raw_ops.UnsortedSegmentMin(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_min(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[1, 2, 2, 1],
       [5, 6, 7, 8]], dtype=int32)"
"tf.raw_ops.UnsortedSegmentProd(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_prod(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.raw_ops.UnsortedSegmentSum(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = [[1,2,3,4], [5,6,7,8], [4,3,2,1]]
tf.math.unsorted_segment_sum(c, [0, 1, 0], num_segments=2).numpy()
array([[5, 5, 5, 5],
       [5, 6, 7, 8]], dtype=int32)"
"tf.raw_ops.Unstage(
    dtypes,
    capacity=0,
    memory_limit=0,
    container='',
    shared_name='',
    name=None
)
",[],[]
"tf.raw_ops.UnwrapDatasetVariant(
    input_handle, name=None
)
",[],[]
"tf.raw_ops.UpperBound(
    sorted_inputs,
    values,
    out_type=tf.dtypes.int32,
    name=None
)
",[],[]
"tf.raw_ops.VarHandleOp(
    dtype,
    shape,
    container='',
    shared_name='',
    allowed_devices=[],
    name=None
)
",[],[]
"tf.raw_ops.VarIsInitializedOp(
    resource, name=None
)
",[],[]
"tf.raw_ops.Variable(
    shape, dtype, container='', shared_name='', name=None
)
",[],[]
"tf.raw_ops.VariableShape(
    input,
    out_type=tf.dtypes.int32,
    name=None
)
","[['Returns the shape of the variable pointed to by ', 'resource', '.']]","# 't' is [[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]]
shape(t) ==> [2, 2, 3]
"
"tf.raw_ops.VariableV2(
    shape, dtype, container='', shared_name='', name=None
)
",[],[]
"tf.raw_ops.Where(
    condition, name=None
)
",[],"# 'input' tensor is [[True, False]
#                    [True, False]]
# 'input' has two true values, so output has two coordinates.
# 'input' has rank of 2, so coordinates have two indices.
where(input) ==> [[0, 0],
                  [1, 0]]

# `condition` tensor is [[[True, False]
#                     [True, False]]
#                    [[False, True]
#                     [False, True]]
#                    [[False, False]
#                     [False, True]]]
# 'input' has 5 true values, so output has 5 coordinates.
# 'input' has rank of 3, so coordinates have three indices.
where(input) ==> [[0, 0, 0],
                  [0, 1, 0],
                  [1, 0, 1],
                  [1, 1, 1],
                  [2, 1, 1]]

# `condition` tensor is [[[1.5,  0.0]
#                     [-0.5, 0.0]]
#                    [[0.0,  0.25]
#                     [0.0,  0.75]]
#                    [[0.0,  0.0]
#                     [0.0,  0.01]]]
# 'input' has 5 nonzero values, so output has 5 coordinates.
# 'input' has rank of 3, so coordinates have three indices.
where(input) ==> [[0, 0, 0],
                  [0, 1, 0],
                  [1, 0, 1],
                  [1, 1, 1],
                  [2, 1, 1]]

# `condition` tensor is [[[1.5 + 0.0j, 0.0  + 0.0j]
#                     [0.0 + 0.5j, 0.0  + 0.0j]]
#                    [[0.0 + 0.0j, 0.25 + 1.5j]
#                     [0.0 + 0.0j, 0.75 + 0.0j]]
#                    [[0.0 + 0.0j, 0.0  + 0.0j]
#                     [0.0 + 0.0j, 0.01 + 0.0j]]]
# 'input' has 5 nonzero magnitude values, so output has 5 coordinates.
# 'input' has rank of 3, so coordinates have three indices.
where(input) ==> [[0, 0, 0],
                  [0, 1, 0],
                  [1, 0, 1],
                  [1, 1, 1],
                  [2, 1, 1]]
"
"tf.raw_ops.While(
    input, cond, body, output_shapes=[], parallel_iterations=10, name=None
)
",[],[]
"tf.raw_ops.WholeFileReader(
    container='', shared_name='', name=None
)
",[],[]
"tf.raw_ops.WholeFileReaderV2(
    container='', shared_name='', name=None
)
",[],[]
"tf.raw_ops.WindowDataset(
    input_dataset,
    size,
    shift,
    stride,
    drop_remainder,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
",[],"1 + (k-1) * shift
"
"tf.raw_ops.WindowOp(
    inputs, output_types, output_shapes, name=None
)
",[],[]
"tf.raw_ops.WorkerHeartbeat(
    request, name=None
)
",[],[]
"tf.raw_ops.WrapDatasetVariant(
    input_handle, name=None
)
",[],[]
"tf.raw_ops.WriteAudioSummary(
    writer, step, tag, tensor, sample_rate, max_outputs=3, name=None
)
",[],[]
"tf.raw_ops.WriteFile(
    filename, contents, name=None
)
","[['Writes ', 'contents', ' to the file at input ', 'filename', '.']]",[]
"tf.raw_ops.WriteGraphSummary(
    writer, step, tensor, name=None
)
",[],[]
"tf.raw_ops.WriteHistogramSummary(
    writer, step, tag, values, name=None
)
",[],[]
"tf.raw_ops.WriteImageSummary(
    writer, step, tag, tensor, bad_color, max_images=3, name=None
)
",[],[]
"tf.raw_ops.WriteRawProtoSummary(
    writer, step, tensor, name=None
)
",[],[]
"tf.raw_ops.WriteScalarSummary(
    writer, step, tag, value, name=None
)
",[],[]
"tf.raw_ops.WriteSummary(
    writer, step, tensor, tag, summary_metadata, name=None
)
",[],[]
"tf.raw_ops.Xdivy(
    x, y, name=None
)
",[],[]
"tf.raw_ops.XlaConcatND(
    inputs, num_concats, paddings=[], name=None
)
",[],"[[0, 1],
 [4, 5]]
[[2, 3],
 [6, 7]]
[[8, 9],
 [12, 13]]
[[10, 11],
 [14, 15]]
"
"tf.raw_ops.XlaSplitND(
    input, N, num_splits, paddings=[], name=None
)
",[],"[[0, 1, 2],
 [3, 4, 5],
 [6, 7, 8]]
"
"tf.raw_ops.Xlog1py(
    x, y, name=None
)
",[],[]
"tf.raw_ops.Xlogy(
    x, y, name=None
)
",[],[]
"tf.raw_ops.ZerosLike(
    x, name=None
)
",[],[]
"tf.raw_ops.Zeta(
    x, q, name=None
)
","[[None, '\n']]",[]
"tf.raw_ops.ZipDataset(
    input_datasets,
    output_types,
    output_shapes,
    metadata='',
    name=None
)
","[['Creates a dataset that zips together ', 'input_datasets', '.']]",[]
"tf.realdiv(
    x, y, name=None
)
",[],[]
"tf.recompute_grad(
    f
)
",[],y = tf.Variable(1.0)
"tf.math.reduce_all(
    input_tensor, axis=None, keepdims=False, name=None
)
","[['Computes ', 'tf.math.logical_and', ' of elements across dimensions of a tensor.']]",">>> x = tf.constant([[True,  True], [False, False]])
>>> tf.math.reduce_all(x)
<tf.Tensor: shape=(), dtype=bool, numpy=False>
>>> tf.math.reduce_all(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False, False])>
>>> tf.math.reduce_all(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.math.reduce_any(
    input_tensor, axis=None, keepdims=False, name=None
)
","[['Computes ', 'tf.math.logical_or', ' of elements across dimensions of a tensor.']]",">>> x = tf.constant([[True,  True], [False, False]])
>>> tf.reduce_any(x)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
>>> tf.reduce_any(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>
>>> tf.reduce_any(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.math.reduce_logsumexp(
    input_tensor, axis=None, keepdims=False, name=None
)
",[],"x = tf.constant([[0., 0., 0.], [0., 0., 0.]])
tf.reduce_logsumexp(x)  # log(6)
tf.reduce_logsumexp(x, 0)  # [log(2), log(2), log(2)]
tf.reduce_logsumexp(x, 1)  # [log(3), log(3)]
tf.reduce_logsumexp(x, 1, keepdims=True)  # [[log(3)], [log(3)]]
tf.reduce_logsumexp(x, [0, 1])  # log(6)
"
"tf.math.reduce_max(
    input_tensor, axis=None, keepdims=False, name=None
)
","[['Computes ', 'tf.math.maximum', ' of elements across dimensions of a tensor.']]",">>> x = tf.constant([5, 1, 2, 4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=5>
>>> x = tf.constant([-5, -1, -2, -4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=-1>
>>> x = tf.constant([4, float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('nan'), float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('-inf'), float('inf')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
"
"tf.math.reduce_mean(
    input_tensor, axis=None, keepdims=False, name=None
)
",[],"x = tf.constant([[1., 1.], [2., 2.]])
tf.reduce_mean(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.5>
tf.reduce_mean(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>
tf.reduce_mean(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
"tf.math.reduce_min(
    input_tensor, axis=None, keepdims=False, name=None
)
","[['Computes the ', 'tf.math.minimum', ' of elements across dimensions of a tensor.']]","a = tf.constant([
  [[1, 2], [3, 4]],
  [[1, 2], [3, 4]]
])
tf.reduce_min(a)
<tf.Tensor: shape=(), dtype=int32, numpy=1>"
"tf.math.reduce_prod(
    input_tensor, axis=None, keepdims=False, name=None
)
","[['Computes ', 'tf.math.multiply', ' of elements across dimensions of a tensor.']]",">>> x = tf.constant([[1., 2.], [3., 4.]])
>>> tf.math.reduce_prod(x)
<tf.Tensor: shape=(), dtype=float32, numpy=24.>
>>> tf.math.reduce_prod(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 8.], dtype=float32)>
>>> tf.math.reduce_prod(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 12.],
dtype=float32)>
"
"tf.math.reduce_sum(
    input_tensor, axis=None, keepdims=False, name=None
)
",[],">>> # x has a shape of (2, 3) (two rows and three columns):
>>> x = tf.constant([[1, 1, 1], [1, 1, 1]])
>>> x.numpy()
array([[1, 1, 1],
       [1, 1, 1]], dtype=int32)
>>> # sum all the elements
>>> # 1 + 1 + 1 + 1 + 1+ 1 = 6
>>> tf.reduce_sum(x).numpy()
6
>>> # reduce along the first dimension
>>> # the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]
>>> tf.reduce_sum(x, 0).numpy()
array([2, 2, 2], dtype=int32)
>>> # reduce along the second dimension
>>> # the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]
>>> tf.reduce_sum(x, 1).numpy()
array([3, 3], dtype=int32)
>>> # keep the original dimensions
>>> tf.reduce_sum(x, 1, keepdims=True).numpy()
array([[3],
       [3]], dtype=int32)
>>> # reduce along both dimensions
>>> # the result is 1 + 1 + 1 + 1 + 1 + 1 = 6
>>> # or, equivalently, reduce along rows, then reduce the resultant array
>>> # [1, 1, 1] + [1, 1, 1] = [2, 2, 2]
>>> # 2 + 2 + 2 = 6
>>> tf.reduce_sum(x, [0, 1]).numpy()
6
"
"tf.register_tensor_conversion_function(
    base_type, conversion_func, priority=100
)
","[['Registers a function for converting objects of ', 'base_type', ' to ', 'Tensor', '.']]","    def conversion_func(value, dtype=None, name=None, as_ref=False):
      # ...
"
"tf.repeat(
    input, repeats, axis=None, name=None
)
","[['Repeat elements of ', 'input', '.']]","repeat(['a', 'b', 'c'], repeats=[3, 0, 2], axis=0)
<tf.Tensor: shape=(5,), dtype=string,
numpy=array([b'a', b'a', b'a', b'c', b'c'], dtype=object)>"
"tf.required_space_to_batch_paddings(
    input_shape, block_shape, base_paddings=None, name=None
)
",[],[]
"tf.reshape(
    tensor, shape, name=None
)
",[],"t1 = [[1, 2, 3],
      [4, 5, 6]]
print(tf.shape(t1).numpy())
[2 3]
t2 = tf.reshape(t1, [6])
t2
<tf.Tensor: shape=(6,), dtype=int32,
  numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>
tf.reshape(t2, [3, 2])
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
  array([[1, 2],
         [3, 4],
         [5, 6]], dtype=int32)>"
"tf.reverse(
    tensor, axis, name=None
)
",[],"# tensor 't' is [[[[ 0,  1,  2,  3],
#                  [ 4,  5,  6,  7],
#                  [ 8,  9, 10, 11]],
#                 [[12, 13, 14, 15],
#                  [16, 17, 18, 19],
#                  [20, 21, 22, 23]]]]
# tensor 't' shape is [1, 2, 3, 4]

# 'dims' is [3] or 'dims' is [-1]
reverse(t, dims) ==> [[[[ 3,  2,  1,  0],
                        [ 7,  6,  5,  4],
                        [ 11, 10, 9, 8]],
                       [[15, 14, 13, 12],
                        [19, 18, 17, 16],
                        [23, 22, 21, 20]]]]

# 'dims' is '[1]' (or 'dims' is '[-3]')
reverse(t, dims) ==> [[[[12, 13, 14, 15],
                        [16, 17, 18, 19],
                        [20, 21, 22, 23]
                       [[ 0,  1,  2,  3],
                        [ 4,  5,  6,  7],
                        [ 8,  9, 10, 11]]]]

# 'dims' is '[2]' (or 'dims' is '[-2]')
reverse(t, dims) ==> [[[[8, 9, 10, 11],
                        [4, 5, 6, 7],
                        [0, 1, 2, 3]]
                       [[20, 21, 22, 23],
                        [16, 17, 18, 19],
                        [12, 13, 14, 15]]]]
"
"tf.reverse_sequence(
    input, seq_lengths, seq_axis=None, batch_axis=None, name=None
)
",[],"seq_lengths = [7, 2, 3, 5]
input = [[1, 2, 3, 4, 5, 0, 0, 0], [1, 2, 0, 0, 0, 0, 0, 0],
         [1, 2, 3, 4, 0, 0, 0, 0], [1, 2, 3, 4, 5, 6, 7, 8]]
output = tf.reverse_sequence(input, seq_lengths, seq_axis=1, batch_axis=0)
output
<tf.Tensor: shape=(4, 8), dtype=int32, numpy=
array([[0, 0, 5, 4, 3, 2, 1, 0],
       [2, 1, 0, 0, 0, 0, 0, 0],
       [3, 2, 1, 4, 0, 0, 0, 0],
       [5, 4, 3, 2, 1, 6, 7, 8]], dtype=int32)>"
"tf.roll(
    input, shift, axis, name=None
)
",[],"# 't' is [0, 1, 2, 3, 4]
roll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]

# shifting along multiple dimensions
# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
roll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]

# shifting along the same axis multiple times
# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
roll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]
"
"tf.math.round(
    x, name=None
)
",[],"x = tf.constant([0.9, 2.5, 2.3, 1.5, -4.5])
tf.round(x)  # [ 1.0, 2.0, 2.0, 2.0, -4.0 ]
"
"tf.dtypes.saturate_cast(
    value, dtype, name=None
)
","[['Performs a safe saturating cast of ', 'value', ' to ', 'dtype', '.']]",[]
"tf.saved_model.Asset(
    path
)
",[],"filename = tf.saved_model.Asset(""file.txt"")

@tf.function(input_signature=[])
def func():
  return tf.io.read_file(filename)

trackable_obj = tf.train.Checkpoint()
trackable_obj.func = func
trackable_obj.filename = filename
tf.saved_model.save(trackable_obj, ""/tmp/saved_model"")

# The created SavedModel is hermetic, it does not depend on
# the original file and can be moved to another path.
tf.io.gfile.remove(""file.txt"")
tf.io.gfile.rename(""/tmp/saved_model"", ""/tmp/new_location"")

reloaded_obj = tf.saved_model.load(""/tmp/new_location"")
print(reloaded_obj.func())
"
"tf.saved_model.LoadOptions(
    allow_partial_checkpoint=False,
    experimental_io_device=None,
    experimental_skip_checkpoint=False,
    experimental_variable_policy=None
)
",[],[]
"tf.saved_model.SaveOptions(
    namespace_whitelist=None,
    save_debug_info=False,
    function_aliases=None,
    experimental_io_device=None,
    experimental_variable_policy=None,
    experimental_custom_gradients=True
)
",[],"class Adder(tf.Module):
  @tf.function
  def double(self, x):
    return x + x"
"tf.saved_model.contains_saved_model(
    export_dir
)
",[],[]
"tf.saved_model.experimental.TrackableResource(
    device=''
)
",[],"class DemoResource(tf.saved_model.experimental.TrackableResource):
  def __init__(self):
    super().__init__()
    self._initialize()
  def _create_resource(self):
    return tf.raw_ops.VarHandleOp(dtype=tf.float32, shape=[2])
  def _initialize(self):
    tf.raw_ops.AssignVariableOp(
        resource=self.resource_handle, value=tf.ones([2]))
  def _destroy_resource(self):
    tf.raw_ops.DestroyResourceOp(resource=self.resource_handle)
class DemoModule(tf.Module):
  def __init__(self):
    self.resource = DemoResource()
  def increment(self, tensor):
    return tensor + tf.raw_ops.ReadVariableOp(
        resource=self.resource.resource_handle, dtype=tf.float32)
demo = DemoModule()
demo.increment([5, 1])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 2.], dtype=float32)>"
"tf.saved_model.load(
    export_dir, tags=None, options=None
)
","[['Load a SavedModel from ', 'export_dir', '.']]","imported = tf.saved_model.load(path)
f = imported.signatures[""serving_default""]
print(f(x=tf.constant([[1.]])))
"
"tf.saved_model.save(
    obj, export_dir, signatures=None, options=None
)
","[['Exports a ', 'tf.Module', ' (and subclasses) ', 'obj', ' to ', 'SavedModel format', '.']]","class Adder(tf.Module):
  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32)])
  def add(self, x):
    return x + x"
"tf.math.scalar_mul(
    scalar, x, name=None
)
","[['Multiplies a scalar times a ', 'Tensor', ' or ', 'IndexedSlices', ' object.']]","x = tf.reshape(tf.range(30, dtype=tf.float32), [10, 3])
with tf.GradientTape() as g:
  g.watch(x)
  y = tf.gather(x, [1, 2])  # IndexedSlices
  z = tf.math.scalar_mul(10.0, y)"
"tf.scan(
    fn,
    elems,
    initializer=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    infer_shape=True,
    reverse=False,
    name=None
)
","[['scan on the list of tensors unpacked from ', 'elems', ' on dimension 0. (deprecated argument values)']]","elems = np.array([1, 2, 3, 4, 5, 6])
sum = scan(lambda a, x: a + x, elems)
# sum == [1, 3, 6, 10, 15, 21]
sum = scan(lambda a, x: a + x, elems, reverse=True)
# sum == [21, 20, 18, 15, 11, 6]
"
"tf.scatter_nd(
    indices, updates, shape, name=None
)
","[['Scatters ', 'updates', ' into a tensor of shape ', 'shape', ' according to ', 'indices', '.']]","indices.shape[-1] <= shape.rank
"
"tf.searchsorted(
    sorted_sequence,
    values,
    side='left',
    out_type=tf.dtypes.int32,
    name=None
)
",[],"edges = [-1, 3.3, 9.1, 10.0]
values = [0.0, 4.1, 12.0]
tf.searchsorted(edges, values).numpy()
array([1, 2, 4], dtype=int32)"
"tf.sequence_mask(
    lengths,
    maxlen=None,
    dtype=tf.dtypes.bool,
    name=None
)
",[],"mask[i_1, i_2, ..., i_n, j] = (j < lengths[i_1, i_2, ..., i_n])
"
"tf.sets.difference(
    a, b, aminusb=True, validate_indices=True
)
","[['Compute set difference of elements in last dimension of ', 'a', ' and ', 'b', '.']]","  import tensorflow as tf
  import collections

  # Represent the following array of sets as a sparse tensor:
  # a = np.array([[{1, 2}, {3}], [{4}, {5, 6}]])
  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2, 2, 2])

  # np.array([[{1, 3}, {2}], [{4, 5}, {5, 6, 7, 8}]])
  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 3),
      ((0, 1, 0), 2),
      ((1, 0, 0), 4),
      ((1, 0, 1), 5),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  # `set_difference` is applied to each aligned pair of sets.
  tf.sets.difference(a, b)

  # The result will be equivalent to either of:
  #
  # np.array([[{2}, {3}], [{}, {}]])
  #
  # collections.OrderedDict([
  #     ((0, 0, 0), 2),
  #     ((0, 1, 0), 3),
  # ])
"
"tf.sets.intersection(
    a, b, validate_indices=True
)
","[['Compute set intersection of elements in last dimension of ', 'a', ' and ', 'b', '.']]","  import tensorflow as tf
  import collections

  # Represent the following array of sets as a sparse tensor:
  # a = np.array([[{1, 2}, {3}], [{4}, {5, 6}]])
  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2,2,2])

  # b = np.array([[{1}, {}], [{4}, {5, 6, 7, 8}]])
  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  # `tf.sets.intersection` is applied to each aligned pair of sets.
  tf.sets.intersection(a, b)

  # The result will be equivalent to either of:
  #
  # np.array([[{1}, {}], [{4}, {5, 6}]])
  #
  # collections.OrderedDict([
  #     ((0, 0, 0), 1),
  #     ((1, 0, 0), 4),
  #     ((1, 1, 0), 5),
  #     ((1, 1, 1), 6),
  # ])
"
"tf.sets.size(
    a, validate_indices=True
)
","[['Compute number of unique elements along last dimension of ', 'a', '.']]",[]
"tf.sets.union(
    a, b, validate_indices=True
)
","[['Compute set union of elements in last dimension of ', 'a', ' and ', 'b', '.']]","  import tensorflow as tf
  import collections

  # [[{1, 2}, {3}], [{4}, {5, 6}]]
  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2, 2, 2])

  # [[{1, 3}, {2}], [{4, 5}, {5, 6, 7, 8}]]
  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 3),
      ((0, 1, 0), 2),
      ((1, 0, 0), 4),
      ((1, 0, 1), 5),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  # `set_union` is applied to each aligned pair of sets.
  tf.sets.union(a, b)

  # The result will be a equivalent to either of:
  #
  # np.array([[{1, 2, 3}, {2, 3}], [{4, 5}, {5, 6, 7, 8}]])
  #
  # collections.OrderedDict([
  #     ((0, 0, 0), 1),
  #     ((0, 0, 1), 2),
  #     ((0, 0, 2), 3),
  #     ((0, 1, 0), 2),
  #     ((0, 1, 1), 3),
  #     ((1, 0, 0), 4),
  #     ((1, 0, 1), 5),
  #     ((1, 1, 0), 5),
  #     ((1, 1, 1), 6),
  #     ((1, 1, 2), 7),
  #     ((1, 1, 3), 8),
  # ])
"
"tf.shape(
    input,
    out_type=tf.dtypes.int32,
    name=None
)
",[],"tf.shape(1.)
<tf.Tensor: shape=(0,), dtype=int32, numpy=array([], dtype=int32)>"
"tf.shape_n(
    input,
    out_type=tf.dtypes.int32,
    name=None
)
",[],[]
"tf.math.sigmoid(
    x, name=None
)
","[[None, '\n'], ['Computes sigmoid of ', 'x', ' element-wise.']]","x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32,
numpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>"
"tf.math.sign(
    x, name=None
)
",[],"# real number
tf.math.sign([0., 2., -3.])
<tf.Tensor: shape=(3,), dtype=float32,
numpy=array([ 0.,  1., -1.], dtype=float32)>"
"tf.signal.dct(
    input, type=2, n=None, axis=-1, norm=None, name=None
)
","[['Computes the 1D ', 'Discrete Cosine Transform (DCT)', ' of ', 'input', '.']]",[]
"tf.signal.fft(
    input, name=None
)
",[],[]
"tf.signal.fft2d(
    input, name=None
)
",[],[]
"tf.signal.fft3d(
    input, name=None
)
",[],[]
"tf.signal.fftshift(
    x, axes=None, name=None
)
",[],"x = tf.signal.fftshift([ 0.,  1.,  2.,  3.,  4., -5., -4., -3., -2., -1.])
x.numpy() # array([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])
"
"tf.signal.frame(
    signal,
    frame_length,
    frame_step,
    pad_end=False,
    pad_value=0,
    axis=-1,
    name=None
)
","[['Expands ', 'signal', ""'s "", 'axis', ' dimension into frames of ', 'frame_length', '.']]","# A batch size 3 tensor of 9152 audio samples.
audio = tf.random.normal([3, 9152])
# Compute overlapping frames of length 512 with a step of 180 (frames overlap
# by 332 samples). By default, only 49 frames are generated since a frame
# with start position j*180 for j > 48 would overhang the end.
frames = tf.signal.frame(audio, 512, 180)
frames.shape.assert_is_compatible_with([3, 49, 512])
# When pad_end is enabled, the final two frames are kept (padded with zeros).
frames = tf.signal.frame(audio, 512, 180, pad_end=True)
frames.shape.assert_is_compatible_with([3, 51, 512])"
"tf.signal.hamming_window(
    window_length,
    periodic=True,
    dtype=tf.dtypes.float32,
    name=None
)
","[['Generate a ', 'Hamming', ' window.']]",[]
"tf.signal.hann_window(
    window_length,
    periodic=True,
    dtype=tf.dtypes.float32,
    name=None
)
","[['Generate a ', 'Hann window', '.']]",[]
"tf.signal.idct(
    input, type=2, n=None, axis=-1, norm=None, name=None
)
","[['Computes the 1D ', 'Inverse Discrete Cosine Transform (DCT)', ' of ', 'input', '.']]",[]
"tf.signal.ifft(
    input, name=None
)
",[],[]
"tf.signal.ifft2d(
    input, name=None
)
",[],[]
"tf.signal.ifft3d(
    input, name=None
)
",[],[]
"tf.signal.ifftshift(
    x, axes=None, name=None
)
",[],"x = tf.signal.ifftshift([[ 0.,  1.,  2.],[ 3.,  4., -4.],[-3., -2., -1.]])
x.numpy() # array([[ 4., -4.,  3.],[-2., -1., -3.],[ 1.,  2.,  0.]])
"
"tf.signal.inverse_mdct(
    mdcts,
    window_fn=tf.signal.vorbis_window,
    norm=None,
    name=None
)
","[['Computes the inverse modified DCT of ', 'mdcts', '.']]","@tf.function
def compare_round_trip():
  samples = 1000
  frame_length = 400
  halflen = frame_length // 2
  waveform = tf.random.normal(dtype=tf.float32, shape=[samples])
  waveform_pad = tf.pad(waveform, [[halflen, 0],])
  mdct = tf.signal.mdct(waveform_pad, frame_length, pad_end=True,
                        window_fn=tf.signal.vorbis_window)
  inverse_mdct = tf.signal.inverse_mdct(mdct,
                                        window_fn=tf.signal.vorbis_window)
  inverse_mdct = inverse_mdct[halflen: halflen + samples]
  return waveform, inverse_mdct
waveform, inverse_mdct = compare_round_trip()
np.allclose(waveform.numpy(), inverse_mdct.numpy(), rtol=1e-3, atol=1e-4)
True"
"tf.signal.inverse_stft(
    stfts,
    frame_length,
    frame_step,
    fft_length=None,
    window_fn=tf.signal.hann_window,
    name=None
)
","[['Computes the inverse ', 'Short-time Fourier Transform', ' of ', 'stfts', '.']]","frame_length = 400
frame_step = 160
waveform = tf.random.normal(dtype=tf.float32, shape=[1000])
stft = tf.signal.stft(waveform, frame_length, frame_step)
inverse_stft = tf.signal.inverse_stft(
    stft, frame_length, frame_step,
    window_fn=tf.signal.inverse_stft_window_fn(frame_step))
"
"tf.signal.inverse_stft_window_fn(
    frame_step,
    forward_window_fn=tf.signal.hann_window,
    name=None
)
","[['Generates a window function that can be used in ', 'inverse_stft', '.']]",[]
"tf.signal.irfft(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.irfft2d(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.irfft3d(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.kaiser_bessel_derived_window(
    window_length,
    beta=12.0,
    dtype=tf.dtypes.float32,
    name=None
)
","[['Generate a ', 'Kaiser Bessel derived window', '.']]",[]
"tf.signal.kaiser_window(
    window_length,
    beta=12.0,
    dtype=tf.dtypes.float32,
    name=None
)
","[['Generate a ', 'Kaiser window', '.']]",[]
"tf.signal.linear_to_mel_weight_matrix(
    num_mel_bins=20,
    num_spectrogram_bins=129,
    sample_rate=8000,
    lower_edge_hertz=125.0,
    upper_edge_hertz=3800.0,
    dtype=tf.dtypes.float32,
    name=None
)
","[[None, '\n'], ['Returns a matrix to warp linear scale spectrograms to the ', 'mel scale', '.']]","$$\textrm{mel}(f) = 2595 * \textrm{log}_{10}(1 + \frac{f}{700})$$
"
"tf.signal.mdct(
    signals,
    frame_length,
    window_fn=tf.signal.vorbis_window,
    pad_end=False,
    norm=None,
    name=None
)
","[['Computes the ', 'Modified Discrete Cosine Transform', ' of ', 'signals', '.']]",[]
"tf.signal.mfccs_from_log_mel_spectrograms(
    log_mel_spectrograms, name=None
)
","[['Computes ', 'MFCCs', ' of ', 'log_mel_spectrograms', '.']]","batch_size, num_samples, sample_rate = 32, 32000, 16000.0
# A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1].
pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)

# A 1024-point STFT with frames of 64 ms and 75% overlap.
stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256,
                       fft_length=1024)
spectrograms = tf.abs(stfts)

# Warp the linear scale spectrograms into the mel-scale.
num_spectrogram_bins = stfts.shape[-1].value
lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80
linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(
  num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,
  upper_edge_hertz)
mel_spectrograms = tf.tensordot(
  spectrograms, linear_to_mel_weight_matrix, 1)
mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(
  linear_to_mel_weight_matrix.shape[-1:]))

# Compute a stabilized log to get log-magnitude mel-scale spectrograms.
log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)

# Compute MFCCs from log_mel_spectrograms and take the first 13.
mfccs = tf.signal.mfccs_from_log_mel_spectrograms(
  log_mel_spectrograms)[..., :13]
"
"tf.signal.overlap_and_add(
    signal, frame_step, name=None
)
",[],"output_size = (frames - 1) * frame_step + frame_length
"
"tf.signal.rfft(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.rfft2d(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.rfft3d(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.stft(
    signals,
    frame_length,
    frame_step,
    fft_length=None,
    window_fn=tf.signal.hann_window,
    pad_end=False,
    name=None
)
","[['Computes the ', 'Short-time Fourier Transform', ' of ', 'signals', '.']]",[]
"tf.signal.vorbis_window(
    window_length,
    dtype=tf.dtypes.float32,
    name=None
)
","[['Generate a ', 'Vorbis power complementary window', '.']]",[]
"tf.math.sin(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10, float(""inf"")])
  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]
"
"tf.math.sinh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.sinh(x) ==> [-inf -4.0515420e+03 -5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf]
"
"tf.size(
    input,
    out_type=tf.dtypes.int32,
    name=None
)
",[],"t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])
tf.size(t)
<tf.Tensor: shape=(), dtype=int32, numpy=12>"
"tf.slice(
    input_, begin, size, name=None
)
",[],"t = tf.constant([[[1, 1, 1], [2, 2, 2]],
                 [[3, 3, 3], [4, 4, 4]],
                 [[5, 5, 5], [6, 6, 6]]])
tf.slice(t, [1, 0, 0], [1, 1, 3])  # [[[3, 3, 3]]]
tf.slice(t, [1, 0, 0], [1, 2, 3])  # [[[3, 3, 3],
                                   #   [4, 4, 4]]]
tf.slice(t, [1, 0, 0], [2, 1, 3])  # [[[3, 3, 3]],
                                   #  [[5, 5, 5]]]
"
"tf.sort(
    values, axis=-1, direction='ASCENDING', name=None
)
",[],"a = [1, 10, 26.9, 2.8, 166.32, 62.3]
tf.sort(a).numpy()
array([  1.  ,   2.8 ,  10.  ,  26.9 ,  62.3 , 166.32], dtype=float32)"
"tf.space_to_batch(
    input, block_shape, paddings, name=None
)
",[],"x = [[[[1], [2]], [[3], [4]]]]
"
"tf.space_to_batch_nd(
    input, block_shape, paddings, name=None
)
",[],"x = [[[[1], [2]], [[3], [4]]]]
"
"tf.sparse.SparseTensor(
    indices, values, dense_shape
)
",[],"dense.shape = dense_shape
dense[tuple(indices[i])] = values[i]
"
"tf.sparse.add(
    a, b, threshold=0
)
","[['Adds two tensors, at least one of each is a ', 'SparseTensor', '.']]","[       2]
[.1     0]
[ 6   -.2]
"
"tf.sparse.bincount(
    values,
    weights=None,
    axis=0,
    minlength=None,
    maxlength=None,
    binary_output=False,
    name=None
)
",[],"data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)
output = tf.sparse.bincount(data, axis=-1)
print(output)
SparseTensor(indices=tf.Tensor(
[[    0    10]
 [    0    20]
 [    0    30]
 [    1    11]
 [    1   101]
 [    1 10001]], shape=(6, 2), dtype=int64),
 values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int64),
 dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))"
"tf.sparse.concat(
    axis, sp_inputs, expand_nonconcat_dims=False, name=None
)
","[['Concatenates a list of ', 'SparseTensor', ' along the specified dimension. (deprecated arguments)']]","sp_inputs[0]: shape = [2, 3]
[0, 2]: ""a""
[1, 0]: ""b""
[1, 1]: ""c""

sp_inputs[1]: shape = [2, 4]
[0, 1]: ""d""
[0, 2]: ""e""
"
"tf.sparse.cross(
    inputs, name=None, separator=None
)
",[],"* inputs[0]: SparseTensor with shape = [2, 2]
  [0, 0]: ""a""
  [1, 0]: ""b""
  [1, 1]: ""c""
* inputs[1]: SparseTensor with shape = [2, 1]
  [0, 0]: ""d""
  [1, 0]: ""e""
* inputs[2]: Tensor [[""f""], [""g""]]
"
"tf.sparse.cross_hashed(
    inputs, num_buckets=0, hash_key=None, name=None
)
",[],"* inputs[0]: SparseTensor with shape = [2, 2]
  [0, 0]: ""a""
  [1, 0]: ""b""
  [1, 1]: ""c""
* inputs[1]: SparseTensor with shape = [2, 1]
  [0, 0]: ""d""
  [1, 0]: ""e""
* inputs[2]: Tensor [[""f""], [""g""]]
"
"tf.sparse.expand_dims(
    sp_input, axis=None, name=None
)
","[['Returns a tensor with an length 1 axis inserted at index ', 'axis', '.']]","sp = tf.sparse.SparseTensor(indices=[[3,4,1]], values=[7,],
                            dense_shape=[10,10,3])"
"tf.sparse.eye(
    num_rows,
    num_columns=None,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.sparse.fill_empty_rows(
    sp_input, default_value, name=None
)
","[['Fills empty rows in the input 2-D ', 'SparseTensor', ' with a default value.']]","[0, 1]: a
[0, 3]: b
[2, 0]: c
[3, 1]: d
"
"tf.sparse.from_dense(
    tensor, name=None
)
",[],"sp = tf.sparse.from_dense([0, 0, 3, 0, 1])
sp.shape.as_list()
[5]
sp.values.numpy()
array([3, 1], dtype=int32)
sp.indices.numpy()
array([[2],
       [4]])"
"tf.sparse.map_values(
    op, *args, **kwargs
)
","[['Applies ', 'op', ' to the ', '.values', ' tensor of one or more ', 'SparseTensor', 's.']]","s = tf.sparse.from_dense([[1, 2, 0],
                          [0, 4, 0],
                          [1, 0, 0]])
tf.sparse.to_dense(tf.sparse.map_values(tf.ones_like, s)).numpy()
array([[1, 1, 0],
       [0, 1, 0],
       [1, 0, 0]], dtype=int32)"
"tf.sparse.mask(
    a, mask_indices, name=None
)
","[['Masks elements of ', 'IndexedSlices', '.']]","# `a` contains slices at indices [12, 26, 37, 45] from a large tensor
# with shape [1000, 10]
a.indices  # [12, 26, 37, 45]
tf.shape(a.values)  # [4, 10]

# `b` will be the subset of `a` slices at its second and third indices, so
# we want to mask its first and last indices (which are at absolute
# indices 12, 45)
b = tf.sparse.mask(a, [12, 45])

b.indices  # [26, 37]
tf.shape(b.values)  # [2, 10]
"
"tf.sparse.maximum(
    sp_a, sp_b, name=None
)
",[],">>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])
>>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])
>>> res = tf.sparse.maximum(sp_zero, sp_one)
>>> res.indices
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
array([[0],
       [1]])>
>>> res.values
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>
>>> res.dense_shape
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
"
"tf.sparse.minimum(
    sp_a, sp_b, name=None
)
",[],">>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])
>>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])
>>> res = tf.sparse.minimum(sp_zero, sp_one)
>>> res.indices
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
array([[0],
       [1]])>
>>> res.values
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>
>>> res.dense_shape
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
"
"tf.sparse.reduce_max(
    sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None
)
","[['Computes ', 'tf.sparse.maximum', ' of elements across dimensions of a SparseTensor.']]","x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])
tf.sparse.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=3>
tf.sparse.reduce_max(x, 0)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>
tf.sparse.reduce_max(x, 1)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>
tf.sparse.reduce_max(x, 1, keepdims=True)
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
array([[2],
       [3]], dtype=int32)>
tf.sparse.reduce_max(x, [0, 1])
<tf.Tensor: shape=(), dtype=int32, numpy=3>"
"tf.sparse.reduce_sum(
    sp_input, axis=None, keepdims=None, output_is_sparse=False, name=None
)
","[['Computes ', 'tf.sparse.add', ' of elements across dimensions of a SparseTensor.']]","x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])
tf.sparse.reduce_sum(x)
<tf.Tensor: shape=(), dtype=int32, numpy=3>
tf.sparse.reduce_sum(x, 0)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>
tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>
tf.sparse.reduce_sum(x, 1, keepdims=True)
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
array([[2],
       [1]], dtype=int32)>
tf.sparse.reduce_sum(x, [0, 1])
<tf.Tensor: shape=(), dtype=int32, numpy=3>"
"tf.sparse.reorder(
    sp_input, name=None
)
","[['Reorders a ', 'SparseTensor', ' into the canonical, row-major ordering.']]","[0, 3]: b
[0, 1]: a
[3, 1]: d
[2, 0]: c
"
"tf.sparse.reset_shape(
    sp_input, new_shape=None
)
","[['Resets the shape of a ', 'SparseTensor', ' with indices and values unchanged.']]",[]
"tf.sparse.reshape(
    sp_input, shape, name=None
)
","[['Reshapes a ', 'SparseTensor', ' to represent values in a new dense shape.']]","[0, 0, 0]: a
[0, 0, 1]: b
[0, 1, 0]: c
[1, 0, 0]: d
[1, 2, 3]: e
"
"tf.sparse.retain(
    sp_input, to_retain
)
","[['Retains specified non-empty values within a ', 'SparseTensor', '.']]","[0, 1]: a
[0, 3]: b
[2, 0]: c
[3, 1]: d
"
"tf.sparse.segment_mean(
    data, indices, segment_ids, num_segments=None, name=None
)
",[],[]
"tf.sparse.segment_sqrt_n(
    data, indices, segment_ids, num_segments=None, name=None
)
",[],[]
"tf.sparse.segment_sum(
    data, indices, segment_ids, num_segments=None, name=None
)
",[],"c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])

# Select two rows, one segment.
tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))
# => [[0 0 0 0]]

# Select two rows, two segment.
tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))
# => [[ 1  2  3  4]
#     [-1 -2 -3 -4]]

# With missing segment ids.
tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 2]),
                      num_segments=4)
# => [[ 1  2  3  4]
#     [ 0  0  0  0]
#     [-1 -2 -3 -4]
#     [ 0  0  0  0]]

# Select all rows, two segments.
tf.sparse.segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))
# => [[0 0 0 0]
#     [5 6 7 8]]

# Which is equivalent to:
tf.math.segment_sum(c, tf.constant([0, 0, 1]))
"
"tf.sparse.slice(
    sp_input, start, size, name=None
)
","[['Slice a ', 'SparseTensor', ' based on the ', 'start', ' and ', 'size', '.']]","input_tensor = shape = [2, 7]
[    a   d e  ]
[b c          ]
"
"tf.sparse.softmax(
    sp_input, name=None
)
","[['Applies softmax to a batched N-D ', 'SparseTensor', '.']]","# First batch:
# [?   e.]
# [1.  ? ]
# Second batch:
# [e   ? ]
# [e   e ]
shape = [2, 2, 2]  # 3-D SparseTensor
values = np.asarray([[[0., np.e], [1., 0.]], [[np.e, 0.], [np.e, np.e]]])
indices = np.vstack(np.where(values)).astype(np.int64).T

result = tf.sparse.softmax(tf.sparse.SparseTensor(indices, values, shape))
# ...returning a 3-D SparseTensor, equivalent to:
# [?   1.]     [1    ?]
# [1.  ? ] and [.5  .5]
# where ? means implicitly zero.
"
"tf.sparse.sparse_dense_matmul(
    sp_a, b, adjoint_a=False, adjoint_b=False, name=None
)
",[],"[[  a      ]
 [b       c]
 [    d    ]]
"
"tf.sparse.split(
    sp_input=None, num_split=None, axis=None, name=None
)
","[['Split a ', 'SparseTensor', ' into ', 'num_split', ' tensors along ', 'axis', '.']]","indices = [[0, 2], [0, 4], [0, 5], [1, 0], [1, 1]]
values = [1, 2, 3, 4, 5]
t = tf.sparse.SparseTensor(indices=indices, values=values, dense_shape=[2, 7])
tf.sparse.to_dense(t)
<tf.Tensor: shape=(2, 7), dtype=int32, numpy=
array([[0, 0, 1, 0, 2, 3, 0],
       [4, 5, 0, 0, 0, 0, 0]], dtype=int32)>"
"tf.sparse.to_dense(
    sp_input, default_value=None, validate_indices=True, name=None
)
","[['Converts a ', 'SparseTensor', ' into a dense tensor.']]","sp_input = tf.sparse.SparseTensor(
  dense_shape=[3, 5],
  values=[7, 8, 9],
  indices =[[0, 1],
            [0, 3],
            [2, 0]])"
"tf.sparse.to_indicator(
    sp_input, vocab_size, name=None
)
","[['Converts a ', 'SparseTensor', ' of ids into a dense bool indicator tensor.']]","output[d_0, d_1, ..., d_n, sp_input[d_0, d_1, ..., d_n, k]] = True
"
"tf.sparse.transpose(
    sp_input, perm=None, name=None
)
","[['Transposes a ', 'SparseTensor']]","[0, 3]: b
[0, 1]: a
[3, 1]: d
[2, 0]: c
"
"tf.split(
    value, num_or_size_splits, axis=0, num=None, name='split'
)
","[['Splits a tensor ', 'value', ' into a list of sub tensors.']]","x = tf.Variable(tf.random.uniform([5, 30], -1, 1))
# Split `x` into 3 tensors along dimension 1
s0, s1, s2 = tf.split(x, num_or_size_splits=3, axis=1)
tf.shape(s0).numpy()
array([ 5, 10], dtype=int32)
# Split `x` into 3 tensors with sizes [4, 15, 11] along dimension 1
split0, split1, split2 = tf.split(x, [4, 15, 11], 1)
tf.shape(split0).numpy()
array([5, 4], dtype=int32)
tf.shape(split1).numpy()
array([ 5, 15], dtype=int32)
tf.shape(split2).numpy()
array([ 5, 11], dtype=int32)"
"tf.math.sqrt(
    x, name=None
)
",[],"x = tf.constant([[4.0], [16.0]])
tf.sqrt(x)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[2.],
         [4.]], dtype=float32)>
y = tf.constant([[-4.0], [16.0]])
tf.sqrt(y)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[nan],
         [ 4.]], dtype=float32)>
z = tf.constant([[-1.0], [16.0]], dtype=tf.complex128)
tf.sqrt(z)
<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=
  array([[0.0+1.j],
         [4.0+0.j]])>"
"tf.math.square(
    x, name=None
)
","[[None, '\n']]","tf.math.square([-2., 0., 3.])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>"
"tf.squeeze(
    input, axis=None, name=None
)
",[],"# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]
tf.shape(tf.squeeze(t))  # [2, 3]
"
"tf.stack(
    values, axis=0, name='stack'
)
","[['Stacks a list of rank-', 'R', ' tensors into one rank-', '(R+1)', ' tensor.']]","x = tf.constant([1, 4])
y = tf.constant([2, 5])
z = tf.constant([3, 6])
tf.stack([x, y, z])
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[1, 4],
       [2, 5],
       [3, 6]], dtype=int32)>
tf.stack([x, y, z], axis=1)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)>"
"tf.stop_gradient(
    input, name=None
)
",[],"
  def softmax(x):
    numerator = tf.exp(x)
    denominator = tf.reduce_sum(numerator)
    return numerator / denominator
"
"tf.strided_slice(
    input_,
    begin,
    end,
    strides=None,
    begin_mask=0,
    end_mask=0,
    ellipsis_mask=0,
    new_axis_mask=0,
    shrink_axis_mask=0,
    var=None,
    name=None
)
",[],"t = tf.constant([[[1, 1, 1], [2, 2, 2]],
                 [[3, 3, 3], [4, 4, 4]],
                 [[5, 5, 5], [6, 6, 6]]])
tf.strided_slice(t, [1, 0, 0], [2, 1, 3], [1, 1, 1])  # [[[3, 3, 3]]]
tf.strided_slice(t, [1, 0, 0], [2, 2, 3], [1, 1, 1])  # [[[3, 3, 3],
                                                      #   [4, 4, 4]]]
tf.strided_slice(t, [1, -1, 0], [2, -3, 3], [1, -1, 1])  # [[[4, 4, 4],
                                                         #   [3, 3, 3]]]
"
"tf.strings.as_string(
    input,
    precision=-1,
    scientific=False,
    shortest=False,
    width=-1,
    fill='',
    name=None
)
",[],"tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)"
"tf.strings.bytes_split(
    input, name=None
)
","[['Split string elements of ', 'input', ' into bytes.']]","tf.strings.bytes_split('hello').numpy()
array([b'h', b'e', b'l', b'l', b'o'], dtype=object)
tf.strings.bytes_split(['hello', '123'])
<tf.RaggedTensor [[b'h', b'e', b'l', b'l', b'o'], [b'1', b'2', b'3']]>"
"tf.strings.format(
    template, inputs, placeholder='{}', summarize=3, name=None
)
",[],"tensor = tf.range(5)
tf.strings.format(""tensor: {}, suffix"", tensor)
<tf.Tensor: shape=(), dtype=string, numpy=b'tensor: [0 1 2 3 4], suffix'>"
"tf.strings.join(
    inputs, separator='', name=None
)
",[],"tf.strings.join(['abc','def']).numpy()
b'abcdef'
tf.strings.join([['abc','123'],
                 ['def','456'],
                 ['ghi','789']]).numpy()
array([b'abcdefghi', b'123456789'], dtype=object)
tf.strings.join([['abc','123'],
                 ['def','456']],
                 separator="" "").numpy()
array([b'abc def', b'123 456'], dtype=object)"
"tf.strings.length(
    input, unit='BYTE', name=None
)
","[['String lengths of ', 'input', '.']]","strings = tf.constant(['Hello','TensorFlow', '\U0001F642'])
tf.strings.length(strings).numpy() # default counts bytes
array([ 5, 10, 4], dtype=int32)
tf.strings.length(strings, unit=""UTF8_CHAR"").numpy()
array([ 5, 10, 1], dtype=int32)"
"tf.strings.lower(
    input, encoding='', name=None
)
",[],"tf.strings.lower(""CamelCase string and ALL CAPS"")
<tf.Tensor: shape=(), dtype=string, numpy=b'camelcase string and all caps'>"
"tf.strings.ngrams(
    data,
    ngram_width,
    separator=' ',
    pad_values=None,
    padding_width=None,
    preserve_short_sequences=False,
    name=None
)
","[['Create a tensor of n-grams based on ', 'data', '.']]","tf.strings.ngrams([""A"", ""B"", ""C"", ""D""], 2).numpy()
array([b'A B', b'B C', b'C D'], dtype=object)
tf.strings.ngrams([""TF"", ""and"", ""keras""], 1).numpy()
array([b'TF', b'and', b'keras'], dtype=object)"
"tf.strings.reduce_join(
    inputs, axis=None, keepdims=False, separator='', name=None
)
",[],"tf.strings.reduce_join([['abc','123'],
                        ['def','456']]).numpy()
b'abc123def456'
tf.strings.reduce_join([['abc','123'],
                        ['def','456']], axis=-1).numpy()
array([b'abc123', b'def456'], dtype=object)
tf.strings.reduce_join([['abc','123'],
                        ['def','456']],
                       axis=-1,
                       separator="" "").numpy()
array([b'abc 123', b'def 456'], dtype=object)"
"tf.strings.regex_full_match(
    input, pattern, name=None
)
","[[None, '\n']]","tf.strings.regex_full_match([""TF lib"", ""lib TF""], "".*lib$"")
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
tf.strings.regex_full_match([""TF lib"", ""lib TF""], "".*TF$"")
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.strings.regex_replace(
    input, pattern, rewrite, replace_global=True, name=None
)
","[['Replace elements of ', 'input', ' matching regex ', 'pattern', ' with ', 'rewrite', '.']]","tf.strings.regex_replace(""Text with tags.<br /><b>contains html</b>"",
                         ""<[^>]+>"", "" "")
<tf.Tensor: shape=(), dtype=string, numpy=b'Text with tags.  contains html '>"
"tf.strings.split(
    input, sep=None, maxsplit=-1, name=None
)
","[['Split elements of ', 'input', ' based on ', 'sep', ' into a ', 'RaggedTensor', '.']]","tf.strings.split('hello world').numpy()
 array([b'hello', b'world'], dtype=object)
tf.strings.split(['hello world', 'a b c'])
<tf.RaggedTensor [[b'hello', b'world'], [b'a', b'b', b'c']]>"
"tf.strings.strip(
    input, name=None
)
",[],"tf.strings.strip([""\nTensorFlow"", ""     The python library    ""]).numpy()
array([b'TensorFlow', b'The python library'], dtype=object)"
"tf.strings.substr(
    input, pos, len, unit='BYTE', name=None
)
","[['Return substrings from ', 'Tensor', ' of strings.']]","input = [b'Hello', b'World']
position = 1
length = 3

output = [b'ell', b'orl']
"
"tf.strings.to_hash_bucket(
    input, num_buckets, name=None
)
",[],"tf.strings.to_hash_bucket([""Hello"", ""TensorFlow"", ""2.x""], 3)
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([2, 0, 1])>"
"tf.strings.to_hash_bucket_fast(
    input, num_buckets, name=None
)
",[],"tf.strings.to_hash_bucket_fast([""Hello"", ""TensorFlow"", ""2.x""], 3).numpy()
array([0, 2, 2])"
"tf.strings.to_hash_bucket_strong(
    input, num_buckets, key, name=None
)
",[],"tf.strings.to_hash_bucket_strong([""Hello"", ""TF""], 3, [1, 2]).numpy()
array([2, 0])"
"tf.strings.to_number(
    input,
    out_type=tf.dtypes.float32,
    name=None
)
",[],"tf.strings.to_number(""1.55"")
<tf.Tensor: shape=(), dtype=float32, numpy=1.55>
tf.strings.to_number(""3"", tf.int32)
<tf.Tensor: shape=(), dtype=int32, numpy=3>"
"tf.strings.unicode_decode(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    name=None
)
","[['Decodes each string in ', 'input', ' into a sequence of Unicode code points.']]","input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
tf.strings.unicode_decode(input, 'UTF-8').to_list()
[[71, 246, 246, 100, 110, 105, 103, 104, 116], [128522]]"
"tf.strings.unicode_decode_with_offsets(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    name=None
)
",[],"input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
result = tf.strings.unicode_decode_with_offsets(input, 'UTF-8')
result[0].to_list()  # codepoints
[[71, 246, 246, 100, 110, 105, 103, 104, 116], [128522]]
result[1].to_list()  # offsets
[[0, 1, 3, 5, 6, 7, 8, 9, 10], [0]]"
"tf.strings.unicode_encode(
    input,
    output_encoding,
    errors='replace',
    replacement_char=65533,
    name=None
)
","[['Encodes each sequence of Unicode code points in ', 'input', ' into a string.']]","  * `'replace'`: Replace invalid codepoint with the
    `replacement_char`. (default)
  * `'ignore'`: Skip invalid codepoints.
  * `'strict'`: Raise an exception for any invalid codepoint.
"
"tf.strings.unicode_script(
    input, name=None
)
",[],"tf.strings.unicode_script([1, 31, 38])
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 0], dtype=int32)>"
"tf.strings.unicode_split(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    name=None
)
","[['Splits each string in ', 'input', ' into a sequence of Unicode code points.']]","input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
tf.strings.unicode_split(input, 'UTF-8').to_list()
[[b'G', b'\xc3\xb6', b'\xc3\xb6', b'd', b'n', b'i', b'g', b'h', b't'],
 [b'\xf0\x9f\x98\x8a']]"
"tf.strings.unicode_split_with_offsets(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    name=None
)
",[],"input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
result = tf.strings.unicode_split_with_offsets(input, 'UTF-8')
result[0].to_list()  # character substrings
[[b'G', b'\xc3\xb6', b'\xc3\xb6', b'd', b'n', b'i', b'g', b'h', b't'],
 [b'\xf0\x9f\x98\x8a']]
result[1].to_list()  # offsets
[[0, 1, 3, 5, 6, 7, 8, 9, 10], [0]]"
"tf.strings.unicode_transcode(
    input,
    input_encoding,
    output_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    name=None
)
",[],"tf.strings.unicode_transcode([""Hello"", ""TensorFlow"", ""2.x""], ""UTF-8"", ""UTF-16-BE"")
<tf.Tensor: shape=(3,), dtype=string, numpy=
array([b'\x00H\x00e\x00l\x00l\x00o',
       b'\x00T\x00e\x00n\x00s\x00o\x00r\x00F\x00l\x00o\x00w',
       b'\x002\x00.\x00x'], dtype=object)>
tf.strings.unicode_transcode([""A"", ""B"", ""C""], ""US ASCII"", ""UTF-8"").numpy()
array([b'A', b'B', b'C'], dtype=object)"
"tf.strings.unsorted_segment_join(
    inputs, segment_ids, num_segments, separator='', name=None
)
","[['Joins the elements of ', 'inputs', ' based on ', 'segment_ids', '.']]","output[i, k1...kM] = strings.join([data[j1...jN, k1...kM])
"
"tf.strings.upper(
    input, encoding='', name=None
)
",[],"tf.strings.upper(""CamelCase string and ALL CAPS"")
<tf.Tensor: shape=(), dtype=string, numpy=b'CAMELCASE STRING AND ALL CAPS'>"
"tf.math.subtract(
    x, y, name=None
)
",[],"x = [1, 2, 3, 4, 5]
y = 1
tf.subtract(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>
tf.subtract(y, x)
<tf.Tensor: shape=(5,), dtype=int32,
numpy=array([ 0, -1, -2, -3, -4], dtype=int32)>"
"tf.summary.audio(
    name,
    data,
    sample_rate,
    step=None,
    max_outputs=3,
    encoding=None,
    description=None
)
",[],[]
"tf.summary.create_file_writer(
    logdir,
    max_queue=None,
    flush_millis=None,
    filename_suffix=None,
    name=None,
    experimental_trackable=False
)
",[],[]
"tf.summary.flush(
    writer=None, name=None
)
",[],[]
"tf.summary.graph(
    graph_data
)
",[],"writer = tf.summary.create_file_writer(""/tmp/mylogs"")

@tf.function
def f():
  x = constant_op.constant(2)
  y = constant_op.constant(3)
  return x**y

with writer.as_default():
  tf.summary.graph(f.get_concrete_function().graph)

# Another example: in a very rare use case, when you are dealing with a TF v1
# graph.
graph = tf.Graph()
with graph.as_default():
  c = tf.constant(30.0)
with writer.as_default():
  tf.summary.graph(graph)
"
"tf.summary.histogram(
    name, data, step=None, buckets=None, description=None
)
",[],"w = tf.summary.create_file_writer('test/logs')
with w.as_default():
    tf.summary.histogram(""activations"", tf.random.uniform([100, 50]), step=0)
    tf.summary.histogram(""initial_weights"", tf.random.normal([1000]), step=0)
"
"tf.summary.image(
    name, data, step=None, max_outputs=3, description=None
)
",[],"w = tf.summary.create_file_writer('test/logs')
with w.as_default():
  image1 = tf.random.uniform(shape=[8, 8, 1])
  image2 = tf.random.uniform(shape=[8, 8, 1])
  tf.summary.image(""grayscale_noise"", [image1, image2], step=0)
"
"tf.summary.scalar(
    name, data, step=None, description=None
)
",[],"test_summary_writer = tf.summary.create_file_writer('test/logdir')
with test_summary_writer.as_default():
    tf.summary.scalar('loss', 0.345, step=1)
    tf.summary.scalar('loss', 0.234, step=2)
    tf.summary.scalar('loss', 0.123, step=3)
"
"tf.summary.text(
    name, data, step=None, description=None
)
",[],"test_summary_writer = tf.summary.create_file_writer('test/logdir')
with test_summary_writer.as_default():
    tf.summary.text('first_text', 'hello world!', step=0)
    tf.summary.text('first_text', 'nice to meet you!', step=1)
"
"tf.summary.trace_export(
    name, step=None, profiler_outdir=None
)
",[],[]
"tf.summary.trace_on(
    graph=True, profiler=False
)
",[],[]
"tf.summary.write(
    tag, tensor, step=None, metadata=None, name=None
)
",[],[]
"tf.switch_case(
    branch_index, branch_fns, default=None, name='switch_case'
)
",[],"switch (branch_index) {  // c-style switch
  case 0: return 17;
  case 1: return 31;
  default: return -1;
}
"
"tf.math.tan(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.tan(x) ==> [nan 0.45231566 -0.5463025 1.5574077 2.572152 -1.7925274 0.32097113 nan]
"
"tf.math.tanh(
    x, name=None
)
","[['Computes hyperbolic tangent of ', 'x', ' element-wise.']]",[]
"tf.tensor_scatter_nd_add(
    tensor, indices, updates, name=None
)
","[['Adds sparse ', 'updates', ' to an existing tensor according to ', 'indices', '.']]","indices.shape[-1] <= tensor.shape.rank
"
"tf.tensor_scatter_nd_max(
    tensor, indices, updates, name=None
)
",[],"tensor = [0, 0, 0, 0, 0, 0, 0, 0]
indices = [[1], [4], [5]]
updates = [1, -1, 1]
tf.tensor_scatter_nd_max(tensor, indices, updates).numpy()
array([0, 1, 0, 0, 0, 1, 0, 0], dtype=int32)"
"tf.tensor_scatter_nd_min(
    tensor, indices, updates, name=None
)
",[],[]
"tf.tensor_scatter_nd_sub(
    tensor, indices, updates, name=None
)
","[['Subtracts sparse ', 'updates', ' from an existing tensor according to ', 'indices', '.']]","indices.shape[-1] <= shape.rank
"
"tf.tensor_scatter_nd_update(
    tensor, indices, updates, name=None
)
","[['Scatter ', 'updates', ' into an existing tensor according to ', 'indices', '.']]","# Not implemented: tensors cannot be updated inplace.
tensor[indices] = updates
"
"tf.tensordot(
    a, b, axes, name=None
)
","[[None, '\n']]",[]
"tf.tile(
    input, multiples, name=None
)
",[],"a = tf.constant([[1,2,3],[4,5,6]], tf.int32)
b = tf.constant([1,2], tf.int32)
tf.tile(a, b)
<tf.Tensor: shape=(2, 6), dtype=int32, numpy=
array([[1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6]], dtype=int32)>
c = tf.constant([2,1], tf.int32)
tf.tile(a, c)
<tf.Tensor: shape=(4, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6],
       [1, 2, 3],
       [4, 5, 6]], dtype=int32)>
d = tf.constant([2,2], tf.int32)
tf.tile(a, d)
<tf.Tensor: shape=(4, 6), dtype=int32, numpy=
array([[1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6],
       [1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6]], dtype=int32)>"
"tf.timestamp(
    name=None
)
",[],[]
"tf.tpu.XLAOptions(
    use_spmd_for_xla_partitioning=True, enable_xla_dynamic_padder=True
)
",[],[]
"tf.tpu.experimental.DeviceAssignment(
    topology: tf.tpu.experimental.Topology,
    core_assignment: np.ndarray
)
",[],"@staticmethod
build(
    topology: "
"tf.tpu.experimental.HardwareFeature(
    tpu_hardware_feature_proto
)
",[],[]
"tf.tpu.experimental.TPUSystemMetadata(
    num_cores, num_hosts, num_of_cores_per_host, topology, devices
)
",[],[]
"tf.tpu.experimental.Topology(
    serialized=None, mesh_shape=None, device_coordinates=None
)
",[],"cpu_device_name_at_coordinates(
    device_coordinates, job=None
)
"
"tf.tpu.experimental.embedding.Adagrad(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    initial_accumulator_value: float = 0.1,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None
)
",[],"embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.Adagrad(0.1))
"
"tf.tpu.experimental.embedding.AdagradMomentum(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    momentum: float = 0.0,
    use_nesterov: bool = False,
    exponent: float = 2,
    beta2: float = 1,
    epsilon: float = 1e-10,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None
)
",[],"embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.AdagradMomentum(0.1))
"
"tf.tpu.experimental.embedding.Adam(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    beta_1: float = 0.9,
    beta_2: float = 0.999,
    epsilon: float = 1e-07,
    lazy_adam: bool = True,
    sum_inside_sqrt: bool = True,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None
)
",[],"embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.Adam(0.1))
"
"tf.tpu.experimental.embedding.FTRL(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    learning_rate_power: float = -0.5,
    l1_regularization_strength: float = 0.0,
    l2_regularization_strength: float = 0.0,
    beta: float = 0.0,
    initial_accumulator_value: float = 0.1,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None,
    multiply_linear_by_learning_rate: bool = False,
    allow_zero_accumulator: bool = False
)
",[],"embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.FTRL(0.1))
"
"tf.tpu.experimental.embedding.FeatureConfig(
    table: tf.tpu.experimental.embedding.TableConfig,
    max_sequence_length: int = 0,
    validate_weights_and_indices: bool = True,
    output_shape: Optional[Union[List[int], tf.TensorShape]] = None,
    name: Optional[Text] = None
)
",[],"table_config_one = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
table_config_two = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
feature_config = {
    'feature_one': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_two': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_three': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_two)}
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config=feature_config,
    batch_size=...
    optimizer=tf.tpu.experimental.embedding.Adam(0.1))
"
"tf.tpu.experimental.embedding.QuantizationConfig(
    num_buckets: int, lower: float, upper: float
)
",[],"if input < lower
  input = lower
if input > upper
  input = upper
quantum = (upper - lower) / (num_buckets - 1)
input = math.floor((input - lower) / quantum + 0.5) * quantium + lower
"
"tf.tpu.experimental.embedding.SGD(
    learning_rate: Union[float, Callable[[], float]] = 0.01,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    clipvalue: Optional[ClipValueType] = None
)
",[],"embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.SGD(0.1))
"
"tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],
    optimizer: Optional[tpu_embedding_v2_utils._Optimizer],
    pipeline_execution_with_tensor_core: bool = False
)
",[],"table_config_one = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
table_config_two = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
feature_config = {
    'feature_one': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_two': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_three': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_two)}
"
"tf.tpu.experimental.embedding.TPUEmbeddingForServing(
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],
    optimizer: Optional[tpu_embedding_v2_utils._Optimizer]
)
",[],"model = model_fn(...)
strategy = tf.distribute.TPUStrategy(...)
with strategy.scope():
  embedding = tf.tpu.experimental.embedding.TPUEmbedding(
      feature_config=feature_config,
      optimizer=tf.tpu.experimental.embedding.SGD(0.1))

# Your custom training code.

checkpoint = tf.train.Checkpoint(model=model, embedding=embedding)
checkpoint.save(...)

"
"tf.tpu.experimental.embedding.TPUEmbeddingV0(
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],
    optimizer: Optional[tpu_embedding_v2_utils._Optimizer]
)
",[],"strategy = tf.distribute.TPUStrategy(...)
with strategy.scope():
  embedding = tf.tpu.experimental.embedding.TPUEmbeddingV0(
      feature_config=feature_config,
      optimizer=tf.tpu.experimental.embedding.SGD(0.1))
"
"tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size: int,
    dim: int,
    initializer: Optional[Callable[[Any], None]] = None,
    optimizer: Optional[_Optimizer] = None,
    combiner: Text = 'mean',
    name: Optional[Text] = None,
    quantization_config: tf.tpu.experimental.embedding.QuantizationConfig = None
)
",[],"table_config_one = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
table_config_two = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
feature_config = {
    'feature_one': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_two': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_three': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_two)}
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config=feature_config,
    batch_size=...
    optimizer=tf.tpu.experimental.embedding.Adam(0.1))
"
"tf.tpu.experimental.embedding.serving_embedding_lookup(
    inputs: Any,
    weights: Optional[Any],
    tables: Dict[tf.tpu.experimental.embedding.TableConfig, tf.Variable],
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable]
) -> Any
","[['Apply standard lookup ops with ', 'tf.tpu.experimental.embedding', ' configs.']]","model = model_fn(...)
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config=feature_config,
    batch_size=1024,
    optimizer=tf.tpu.experimental.embedding.SGD(0.1))
checkpoint = tf.train.Checkpoint(model=model, embedding=embedding)
checkpoint.restore(...)

@tf.function(input_signature=[{'feature_one': tf.TensorSpec(...),
                               'feature_two': tf.TensorSpec(...),
                               'feature_three': tf.TensorSpec(...)}])
def serve_tensors(embedding_features):
  embedded_features = tf.tpu.experimental.embedding.serving_embedding_lookup(
      embedding_features, None, embedding.embedding_tables,
      feature_config)
  return model(embedded_features)

model.embedding_api = embedding
tf.saved_model.save(model,
                    export_dir=...,
                    signatures={'serving_default': serve_tensors})

"
"tf.tpu.experimental.initialize_tpu_system(
    cluster_resolver=None
)
",[],[]
"tf.tpu.experimental.shutdown_tpu_system(
    cluster_resolver=None
)
",[],[]
tf.io.parse_example(,"[['Used in ', 'tf.train.Example', ' protos. Holds a list of byte-strings.'], ['An ', 'Example', ' proto is a representation of the following python type:'], ['This proto implements the ', 'List[bytes]', ' portion.'], ['\n', 'from google.protobuf import text_format', '\n', ""example = text_format.Parse('''"", '\n', '  features {', '\n', '    feature {key: ""my_feature""', '\n', ""             value {bytes_list {value: ['abc', '12345' ]} } }"", '\n', ""  }''',"", '\n', '  tf.train.Example())', '\n', None, '\n', ""example.features.feature['my_feature'].bytes_list.value"", '\n', '[""abc"", ""12345""]', '\n'], ['Use ', 'tf.io.parse_example', ' to extract tensors from a serialized ', 'Example', ' proto:']]","from google.protobuf import text_format
example = text_format.Parse('''
  features {
    feature {key: ""my_feature""
             value {bytes_list {value: ['abc', '12345' ]} } }
  }''',
  tf.train.Example())
example.features.feature['my_feature'].bytes_list.value
[""abc"", ""12345""]"
"tf.train.Checkpoint(
    root=None, **kwargs
)
",[],"model = tf.keras.Model(...)
checkpoint = tf.train.Checkpoint(model)

# Save a checkpoint to /tmp/training_checkpoints-{save_counter}. Every time
# checkpoint.save is called, the save counter is increased.
save_path = checkpoint.save('/tmp/training_checkpoints')

# Restore the checkpointed values to the `model` object.
checkpoint.restore(save_path)
"
"tf.train.CheckpointManager(
    checkpoint,
    directory,
    max_to_keep,
    keep_checkpoint_every_n_hours=None,
    checkpoint_name='ckpt',
    step_counter=None,
    checkpoint_interval=None,
    init_fn=None
)
",[],"import tensorflow as tf
checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
manager = tf.train.CheckpointManager(
    checkpoint, directory=""/tmp/model"", max_to_keep=5)
status = checkpoint.restore(manager.latest_checkpoint)
while True:
  # train
  manager.save()
"
"tf.train.CheckpointOptions(
    experimental_io_device=None, experimental_enable_async_checkpoint=False
)
",[],"step = tf.Variable(0, name=""step"")
checkpoint = tf.train.Checkpoint(step=step)
options = tf.train.CheckpointOptions(experimental_io_device=""/job:localhost"")
checkpoint.save(""/tmp/ckpt"", options=options)
"
"tf.train.CheckpointView(
    save_path
)
",[],"class SimpleModule(tf.Module):
  def __init__(self, name=None):
    super().__init__(name=name)
    self.a_var = tf.Variable(5.0)
    self.b_var = tf.Variable(4.0)
    self.vars = [tf.Variable(1.0), tf.Variable(2.0)]"
"tf.train.ClusterSpec(
    cluster
)
",[],"cluster = tf.train.ClusterSpec({""worker"": [""worker0.example.com:2222"",
                                           ""worker1.example.com:2222"",
                                           ""worker2.example.com:2222""],
                                ""ps"": [""ps0.example.com:2222"",
                                       ""ps1.example.com:2222""]})
"
"tf.train.Coordinator(
    clean_stop_exception_types=None
)
",[],"# Create a coordinator.
coord = Coordinator()
# Start a number of threads, passing the coordinator to each of them.
...start thread 1...(coord, ...)
...start thread N...(coord, ...)
# Wait for all the threads to terminate.
coord.join(threads)
"
tf.io.parse_example(,"[['An ', 'Example', ' is a standard proto storing data for training and inference.'], ['An ', 'Example', ' proto is a representation of the following python type:'], ['It contains a key-value store ', 'Example.features', ' where each key (string) maps\nto a ', 'tf.train.Feature', ' message which contains a fixed-type list. This flexible\nand compact format allows the storage of large amounts of typed data, but\nrequires that the data shape and use be determined by the configuration files\nand parsers that are used to read and write this format (refer to\n', 'tf.io.parse_example', ' for details).'], ['\n', 'from google.protobuf import text_format', '\n', ""example = text_format.Parse('''"", '\n', '  features {', '\n', '    feature {key: ""my_feature""', '\n', '             value {int64_list {value: [1, 2, 3, 4]} } }', '\n', ""  }''',"", '\n', '  tf.train.Example())', '\n'], ['Use ', 'tf.io.parse_example', ' to extract tensors from a serialized ', 'Example', ' proto:']]","from google.protobuf import text_format
example = text_format.Parse('''
  features {
    feature {key: ""my_feature""
             value {int64_list {value: [1, 2, 3, 4]} } }
  }''',
  tf.train.Example())"
"tf.train.ExponentialMovingAverage(
    decay,
    num_updates=None,
    zero_debias=False,
    name='ExponentialMovingAverage'
)
",[],"# Create variables.
var0 = tf.Variable(...)
var1 = tf.Variable(...)
# ... use the variables to build a training model...

# Create an ExponentialMovingAverage object
ema = tf.train.ExponentialMovingAverage(decay=0.9999)

# The first `apply` creates the shadow variables that hold the moving averages
ema.apply([var0, var1])

# grab the moving averages for checkpointing purposes or to be able to
# load the moving averages into the model weights
averages = [ema.average(var0), ema.average(var1)]

...
def train_step(...):
...
  # Apply the optimizer.
  opt.minimize(my_loss, [var0, var1])

  # Update the moving averages
  # of var0 and var1 with additional calls to `apply`
  ema.apply([var0, var1])

...train the model by running train_step multiple times...
"
tf.io.parse_example(,"[['Used in ', 'tf.train.Example', ' protos. Contains a list of values.'], ['An ', 'Example', ' proto is a representation of the following python type:'], ['This proto implements the ', 'Union', '.'], ['\n', 'tf.train.BytesList', '\n', 'tf.train.FloatList', '\n', 'tf.train.Int64List', '\n'], ['\n', 'int_feature = tf.train.Feature(', '\n', '    int64_list=tf.train.Int64List(value=[1, 2, 3, 4]))', '\n', 'float_feature = tf.train.Feature(', '\n', '    float_list=tf.train.FloatList(value=[1., 2., 3., 4.]))', '\n', 'bytes_feature = tf.train.Feature(', '\n', '    bytes_list=tf.train.BytesList(value=[b""abc"", b""1234""]))', '\n', None, '\n', 'example = tf.train.Example(', '\n', '    features=tf.train.Features(feature={', '\n', ""        'my_ints': int_feature,"", '\n', ""        'my_floats': float_feature,"", '\n', ""        'my_bytes': bytes_feature,"", '\n', '    }))', '\n'], ['Use ', 'tf.io.parse_example', ' to extract tensors from a serialized ', 'Example', ' proto:']]","int_feature = tf.train.Feature(
    int64_list=tf.train.Int64List(value=[1, 2, 3, 4]))
float_feature = tf.train.Feature(
    float_list=tf.train.FloatList(value=[1., 2., 3., 4.]))
bytes_feature = tf.train.Feature(
    bytes_list=tf.train.BytesList(value=[b""abc"", b""1234""]))
example = tf.train.Example(
    features=tf.train.Features(feature={
        'my_ints': int_feature,
        'my_floats': float_feature,
        'my_bytes': bytes_feature,
    }))"
tf.io.parse_example(,"[['Used in ', 'tf.train.Example', ' protos. Contains the mapping from keys to ', 'Feature', '.'], ['An ', 'Example', ' proto is a representation of the following python type:'], ['This proto implements the ', 'Dict', '.'], ['\n', 'int_feature = tf.train.Feature(', '\n', '    int64_list=tf.train.Int64List(value=[1, 2, 3, 4]))', '\n', 'float_feature = tf.train.Feature(', '\n', '    float_list=tf.train.FloatList(value=[1., 2., 3., 4.]))', '\n', 'bytes_feature = tf.train.Feature(', '\n', '    bytes_list=tf.train.BytesList(value=[b""abc"", b""1234""]))', '\n', None, '\n', 'example = tf.train.Example(', '\n', '    features=tf.train.Features(feature={', '\n', ""        'my_ints': int_feature,"", '\n', ""        'my_floats': float_feature,"", '\n', ""        'my_bytes': bytes_feature,"", '\n', '    }))', '\n'], ['Use ', 'tf.io.parse_example', ' to extract tensors from a serialized ', 'Example', ' proto:']]","int_feature = tf.train.Feature(
    int64_list=tf.train.Int64List(value=[1, 2, 3, 4]))
float_feature = tf.train.Feature(
    float_list=tf.train.FloatList(value=[1., 2., 3., 4.]))
bytes_feature = tf.train.Feature(
    bytes_list=tf.train.BytesList(value=[b""abc"", b""1234""]))
example = tf.train.Example(
    features=tf.train.Features(feature={
        'my_ints': int_feature,
        'my_floats': float_feature,
        'my_bytes': bytes_feature,
    }))"
tf.io.parse_example(,"[['Used in ', 'tf.train.Example', ' protos. Holds a list of floats.'], ['An ', 'Example', ' proto is a representation of the following python type:'], ['This proto implements the ', 'List[float]', ' portion.'], ['\n', 'from google.protobuf import text_format', '\n', ""example = text_format.Parse('''"", '\n', '  features {', '\n', '    feature {key: ""my_feature""', '\n', '             value {float_list {value: [1., 2., 3., 4. ]} } }', '\n', ""  }''',"", '\n', '  tf.train.Example())', '\n', None, '\n', ""example.features.feature['my_feature'].float_list.value"", '\n', '[1.0, 2.0, 3.0, 4.0]', '\n'], ['Use ', 'tf.io.parse_example', ' to extract tensors from a serialized ', 'Example', ' proto:']]","from google.protobuf import text_format
example = text_format.Parse('''
  features {
    feature {key: ""my_feature""
             value {float_list {value: [1., 2., 3., 4. ]} } }
  }''',
  tf.train.Example())
example.features.feature['my_feature'].float_list.value
[1.0, 2.0, 3.0, 4.0]"
tf.io.parse_example(,"[['Used in ', 'tf.train.Example', ' protos. Holds a list of Int64s.'], ['An ', 'Example', ' proto is a representation of the following python type:'], ['This proto implements the ', 'List[int64]', ' portion.'], ['\n', 'from google.protobuf import text_format', '\n', ""example = text_format.Parse('''"", '\n', '  features {', '\n', '    feature {key: ""my_feature""', '\n', '             value {int64_list {value: [1, 2, 3, 4]} } }', '\n', ""  }''',"", '\n', '  tf.train.Example())', '\n', None, '\n', ""example.features.feature['my_feature'].int64_list.value"", '\n', '[1, 2, 3, 4]', '\n'], ['Use ', 'tf.io.parse_example', ' to extract tensors from a serialized ', 'Example', ' proto:']]","from google.protobuf import text_format
example = text_format.Parse('''
  features {
    feature {key: ""my_feature""
             value {int64_list {value: [1, 2, 3, 4]} } }
  }''',
  tf.train.Example())
example.features.feature['my_feature'].int64_list.value
[1, 2, 3, 4]"
"tf.train.TrackableView(
    root
)
",[],"class SimpleModule(tf.Module):
  def __init__(self, name=None):
    super().__init__(name=name)
    self.a_var = tf.Variable(5.0)
    self.b_var = tf.Variable(4.0)
    self.vars = [tf.Variable(1.0), tf.Variable(2.0)]"
"tf.train.checkpoints_iterator(
    checkpoint_dir, min_interval_secs=0, timeout=None, timeout_fn=None
)
",[],[]
"tf.train.get_checkpoint_state(
    checkpoint_dir, latest_filename=None
)
",[],[]
"tf.train.latest_checkpoint(
    checkpoint_dir, latest_filename=None
)
",[],[]
"tf.train.list_variables(
    ckpt_dir_or_file
)
",[],"</td>
</tr>

</table>


import tensorflow as tf
import os
ckpt_directory = ""/tmp/training_checkpoints/ckpt""
ckpt = tf.train.Checkpoint(optimizer=optimizer, model=model)
manager = tf.train.CheckpointManager(ckpt, ckpt_directory, max_to_keep=3)
train_and_checkpoint(model, manager)
tf.train.list_variables(manager.latest_checkpoint)
"
"tf.train.load_checkpoint(
    ckpt_dir_or_file
)
","[['Returns ', 'CheckpointReader', ' for checkpoint found in ', 'ckpt_dir_or_file', '.']]",[]
"tf.train.load_variable(
    ckpt_dir_or_file, name
)
",[],[]
"tf.transpose(
    a, perm=None, conjugate=False, name='transpose'
)
","[['Transposes ', 'a', ', where ', 'a', ' is a Tensor.']]","x = tf.constant([[1, 2, 3], [4, 5, 6]])
tf.transpose(x)
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[1, 4],
       [2, 5],
       [3, 6]], dtype=int32)>"
"tf.math.truediv(
    x, y, name=None
)
",[],[]
"tf.truncatediv(
    x, y, name=None
)
",[],[]
"tf.truncatemod(
    x, y, name=None
)
",[],[]
"tf.tuple(
    tensors, control_inputs=None, name=None
)
",[],[]
"tf.type_spec_from_value(
    value
) -> tf.TypeSpec
","[['Returns a ', 'tf.TypeSpec', ' that represents the given ', 'value', '.']]",">>> tf.type_spec_from_value(tf.constant([1, 2, 3]))
TensorSpec(shape=(3,), dtype=tf.int32, name=None)
>>> tf.type_spec_from_value(np.array([4.0, 5.0], np.float64))
TensorSpec(shape=(2,), dtype=tf.float64, name=None)
>>> tf.type_spec_from_value(tf.ragged.constant([[1, 2], [3, 4, 5]]))
RaggedTensorSpec(TensorShape([2, None]), tf.int32, 1, tf.int64)
"
"tf.unique(
    x,
    out_idx=tf.dtypes.int32,
    name=None
)
",[],"# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]
y, idx = unique(x)
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
"
"tf.unique_with_counts(
    x,
    out_idx=tf.dtypes.int32,
    name=None
)
",[],"# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]
y, idx, count = unique_with_counts(x)
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
count ==> [2, 1, 3, 1, 2]
"
"tf.unravel_index(
    indices, dims, name=None
)
",[],"y = tf.unravel_index(indices=[2, 5, 7], dims=[3, 3])
# 'dims' represent a hypothetical (3, 3) tensor of indices:
# [[0, 1, *2*],
#  [3, 4, *5*],
#  [6, *7*, 8]]
# For each entry from 'indices', this operation returns
# its coordinates (marked with '*'), such as
# 2 ==> (0, 2)
# 5 ==> (1, 2)
# 7 ==> (2, 1)
y ==> [[0, 1, 2], [2, 2, 1]]
"
"tf.unstack(
    value, num=None, axis=0, name='unstack'
)
","[['Unpacks the given dimension of a rank-', 'R', ' tensor into rank-', '(R-1)', ' tensors.']]","x = tf.reshape(tf.range(12), (3,4))
p, q, r = tf.unstack(x)
p.shape.as_list()
[4]"
"tf.vectorized_map(
    fn, elems, fallback_to_while_loop=True, warn=True
)
","[['Parallel map on the list of tensors unpacked from ', 'elems', ' on dimension 0.']]","def outer_product(a):
  return tf.tensordot(a, a, 0)

batch_size = 100
a = tf.ones((batch_size, 32, 32))
c = tf.vectorized_map(outer_product, a)
assert c.shape == (batch_size, 32, 32, 32, 32)
"
"tf.where(
    condition, x=None, y=None, name=None
)
","[['Returns the indices of non-zero elements, or multiplexes ', 'x', ' and ', 'y', '.']]","tf.where([True, False, False, True]).numpy()
array([[0],
       [3]])"
"tf.while_loop(
    cond,
    body,
    loop_vars,
    shape_invariants=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    maximum_iterations=None,
    name=None
)
","[['Repeat ', 'body', ' while the condition ', 'cond', ' is true. (deprecated argument values)']]","i = tf.constant(0)
c = lambda i: tf.less(i, 10)
b = lambda i: (tf.add(i, 1), )
r = tf.while_loop(c, b, [i])
"
"tf.zeros(
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
",[],"tf.zeros([3, 4], tf.int32)
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
array([[0, 0, 0, 0],
       [0, 0, 0, 0],
       [0, 0, 0, 0]], dtype=int32)>"
"tf.zeros_like(
    input, dtype=None, name=None
)
",[],">>> tensor = tf.constant([[1, 2, 3], [4, 5, 6]])
>>> tf.zeros_like(tensor)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[0, 0, 0],
       [0, 0, 0]], dtype=int32)>
"
"tf.debugging.Assert(
    condition, data, summarize=None, name=None
)
",[],"# Ensure maximum element of x is smaller or equal to 1
assert_op = tf.Assert(tf.less_equal(tf.reduce_max(x), 1.), [x])
with tf.control_dependencies([assert_op]):
  ... code using x ...
"
"tf.compat.v1.ConditionalAccumulator(
    dtype,
    shape=None,
    shared_name=None,
    name='conditional_accumulator',
    reduction_type='MEAN'
)
","[['Inherits From: ', 'ConditionalAccumulatorBase']]","apply_grad(
    grad, local_step=0, name=None
)
"
"tf.compat.v1.ConditionalAccumulatorBase(
    dtype, shape, accumulator_ref
)
",[],"num_accumulated(
    name=None
)
"
"tf.CriticalSection(
    name=None, shared_name=None, critical_section_def=None, import_scope=None
)
",[],"v = resource_variable_ops.ResourceVariable(0.0, name=""v"")

def count():
  value = v.read_value()
  with tf.control_dependencies([value]):
    with tf.control_dependencies([v.assign_add(1)]):
      return tf.identity(value)
"
"tf.compat.v1.DeviceSpec(
    job=None, replica=None, task=None, device_type=None, device_index=None
)
","[['Inherits From: ', 'DeviceSpec']]","# Place the operations on device ""GPU:0"" in the ""ps"" job.
device_spec = DeviceSpec(job=""ps"", device_type=""GPU"", device_index=0)
with tf.device(device_spec.to_string()):
  # Both my_var and squared_var will be placed on /job:ps/device:GPU:0.
  my_var = tf.Variable(..., name=""my_variable"")
  squared_var = tf.square(my_var)
"
"tf.compat.v1.Dimension(
    value
)
",[],[]
"tf.queue.FIFOQueue(
    capacity,
    dtypes,
    shapes=None,
    names=None,
    shared_name=None,
    name='fifo_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.io.FixedLenFeature(
    shape, dtype, default_value=None
)
",[],[]
"tf.io.FixedLenSequenceFeature(
    shape, dtype, allow_missing=False, default_value=None
)
","[['Configuration for parsing a variable-length input feature into a ', 'Tensor', '.']]",[]
"tf.compat.v1.FixedLengthRecordReader(
    record_bytes,
    header_bytes=None,
    footer_bytes=None,
    hop_bytes=None,
    name=None,
    encoding=None
)
","[['Inherits From: ', 'ReaderBase']]","num_records_produced(
    name=None
)
"
"tf.GradientTape(
    persistent=False, watch_accessed_variables=True
)
",[],"x = tf.constant(3.0)
with tf.GradientTape() as g:
  g.watch(x)
  y = x * x
dy_dx = g.gradient(y, x)
print(dy_dx)
tf.Tensor(6.0, shape=(), dtype=float32)"
"tf.compat.v1.IdentityReader(
    name=None
)
","[['Inherits From: ', 'ReaderBase']]","num_records_produced(
    name=None
)
"
"tf.IndexedSlices(
    values, indices, dense_shape=None
)
",[],"dense[slices.indices[i], :, :, :, ...] = slices.values[i, :, :, :, ...]
"
"tf.IndexedSlicesSpec(
    shape=None,
    dtype=tf.dtypes.float32,
    indices_dtype=tf.dtypes.int64,
    dense_shape_dtype=None,
    indices_shape=None
)
","[['Type specification for a ', 'tf.IndexedSlices', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.compat.v1.InteractiveSession(
    target='', graph=None, config=None
)
","[['A TensorFlow ', 'Session', ' for use in interactive contexts, such as a shell.']]","sess = tf.compat.v1.InteractiveSession()
a = tf.constant(5.0)
b = tf.constant(6.0)
c = a * b
# We can just use 'c.eval()' without passing 'sess'
print(c.eval())
sess.close()
"
"tf.compat.v1.LMDBReader(
    name=None, options=None
)
","[['Inherits From: ', 'ReaderBase']]","num_records_produced(
    name=None
)
"
"tf.Module(
    name=None
)
",[],"class Dense(tf.Module):
  def __init__(self, input_dim, output_size, name=None):
    super().__init__(name=name)
    self.w = tf.Variable(
      tf.random.normal([input_dim, output_size]), name='w')
    self.b = tf.Variable(tf.zeros([output_size]), name='b')
  def __call__(self, x):
    y = tf.matmul(x, self.w) + self.b
    return tf.nn.relu(y)"
"tf.no_gradient(
    op_type
)
","[['Specifies that ops of type ', 'op_type', ' is not differentiable.']]","tf.no_gradient(""Size"")
"
"tf.no_gradient(
    op_type
)
","[['Specifies that ops of type ', 'op_type', ' is not differentiable.']]","tf.no_gradient(""Size"")
"
"tf.errors.OpError(
    node_def, op, message, error_code, *args
)
",[],[]
"tf.Operation(
    node_def,
    g,
    inputs=None,
    output_types=None,
    control_inputs=None,
    input_types=None,
    original_op=None,
    op_def=None
)
",[],"colocation_groups()
"
"tf.OptionalSpec(
    element_spec
)
","[['Type specification for ', 'tf.experimental.Optional', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","@tf.function(input_signature=[tf.OptionalSpec(
  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))])
def maybe_square(optional):
  if optional.has_value():
    x = optional.get_value()
    return x * x
  return -1
optional = tf.experimental.Optional.from_value(5)
print(maybe_square(optional))
tf.Tensor(25, shape=(), dtype=int32)"
"tf.queue.PaddingFIFOQueue(
    capacity,
    dtypes,
    shapes,
    names=None,
    shared_name=None,
    name='padding_fifo_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.compat.v1.Print(
    input_, data, message=None, first_n=None, summarize=None, name=None
)
",[],[]
"tf.queue.PriorityQueue(
    capacity,
    types,
    shapes=None,
    names=None,
    shared_name=None,
    name='priority_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.queue.QueueBase(
    dtypes, shapes, names, queue_ref
)
",[],"close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.ragged.constant([[0], [1, 2]]).shape","[[None, '\n'], ['A ', 'RaggedTensor', ' is a tensor with one or more ', 'ragged dimensions', ', which are\ndimensions whose slices may have different lengths.  For example, the inner\n(column) dimension of ', 'rt=[[3, 1, 4, 1], [], [5, 9, 2], [6], []]', ' is ragged,\nsince the column slices (', 'rt[0, :]', ', ..., ', 'rt[4, :]', ') have different lengths.\nDimensions whose slices all have the same length are called ', 'uniform\ndimensions', '.  The outermost dimension of a ', 'RaggedTensor', ' is always uniform,\nsince it consists of a single slice (and so there is no possibility for\ndiffering slice lengths).'], ['The total number of dimensions in a ', 'RaggedTensor', ' is called its ', 'rank', ',\nand the number of ragged dimensions in a ', 'RaggedTensor', ' is called its\n', 'ragged-rank', '.  A ', 'RaggedTensor', ""'s ragged-rank is fixed at graph creation\ntime: it can't depend on the runtime values of "", 'Tensor', ""s, and can't vary\ndynamically for different session runs.""], ['Note that the ', '__init__', ' constructor is private. Please use one of the\nfollowing methods to construct a ', 'RaggedTensor', ':'], ['\n', 'tf.RaggedTensor.from_row_lengths', '\n', 'tf.RaggedTensor.from_value_rowids', '\n', 'tf.RaggedTensor.from_row_splits', '\n', 'tf.RaggedTensor.from_row_starts', '\n', 'tf.RaggedTensor.from_row_limits', '\n', 'tf.RaggedTensor.from_nested_row_splits', '\n', 'tf.RaggedTensor.from_nested_row_lengths', '\n', 'tf.RaggedTensor.from_nested_value_rowids', '\n'], ['Many ops support both ', 'Tensor', 's and ', 'RaggedTensor', 's\n(see ', 'tf.ragged', ' for a\nfull listing). The term ""potentially ragged tensor"" may be used to refer to a\ntensor that might be either a ', 'Tensor', ' or a ', 'RaggedTensor', '.  The ragged-rank\nof a ', 'Tensor', ' is zero.'], ['When documenting the shape of a RaggedTensor, ragged dimensions can be\nindicated by enclosing them in parentheses.  For example, the shape of\na 3-D ', 'RaggedTensor', ' that stores the fixed-size word embedding for each\nword in a sentence, for each sentence in a batch, could be written as\n', '[num_sentences, (num_words), embedding_size]', '.  The parentheses around\n', '(num_words)', ' indicate that dimension is ragged, and that the length\nof each element list in that dimension may vary for each item.'], ['Internally, a ', 'RaggedTensor', ' consists of a concatenated list of values that\nare partitioned into variable-length rows.  In particular, each ', 'RaggedTensor', '\nconsists of:'], ['\n', 'A ', '\n', 'A ', '\n'], ['\n', 'print(tf.RaggedTensor.from_row_splits(', '\n', '      values=[3, 1, 4, 1, 5, 9, 2, 6],', '\n', '      row_splits=[0, 4, 4, 7, 8, 8]))', '\n', '<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>', '\n'], ['In addition to ', 'row_splits', ', ragged tensors provide support for five other\nrow-partitioning schemes:'], ['\n', 'row_lengths', '\n', 'value_rowids', '\n', 'row_starts', '\n', 'row_limits', '\n', 'uniform_row_length', '\n'], ['Example: The following ragged tensors are equivalent, and all represent the\nnested list ', '[[3, 1, 4, 1], [], [5, 9, 2], [6], []]', '.'], ['\n', 'values = [3, 1, 4, 1, 5, 9, 2, 6]', '\n', 'RaggedTensor.from_row_splits(values, row_splits=[0, 4, 4, 7, 8, 8])', '\n', '<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>', '\n', 'RaggedTensor.from_row_lengths(values, row_lengths=[4, 0, 3, 1, 0])', '\n', '<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>', '\n', 'RaggedTensor.from_value_rowids(', '\n', '    values, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)', '\n', '<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>', '\n', 'RaggedTensor.from_row_starts(values, row_starts=[0, 4, 4, 7, 8])', '\n', '<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>', '\n', 'RaggedTensor.from_row_limits(values, row_limits=[4, 4, 7, 8, 8])', '\n', '<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>', '\n', 'RaggedTensor.from_uniform_row_length(values, uniform_row_length=2)', '\n', '<tf.RaggedTensor [[3, 1], [4, 1], [5, 9], [2, 6]]>', '\n'], ['RaggedTensor', 's with multiple ragged dimensions can be defined by using\na nested ', 'RaggedTensor', ' for the ', 'values', ' tensor.  Each nested ', 'RaggedTensor', '\nadds a single ragged dimension.'], ['\n', 'inner_rt = RaggedTensor.from_row_splits(  # =rt1 from above', '\n', '    values=[3, 1, 4, 1, 5, 9, 2, 6], row_splits=[0, 4, 4, 7, 8, 8])', '\n', 'outer_rt = RaggedTensor.from_row_splits(', '\n', '    values=inner_rt, row_splits=[0, 3, 3, 5])', '\n', 'print(outer_rt.to_list())', '\n', '[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]', '\n', 'print(outer_rt.ragged_rank)', '\n', '2', '\n'], ['The factory function ', 'RaggedTensor.from_nested_row_splits', ' may be used to\nconstruct a ', 'RaggedTensor', ' with multiple ragged dimensions directly, by\nproviding a list of ', 'row_splits', ' tensors:'], ['\n', 'RaggedTensor.from_nested_row_splits(', '\n', '    flat_values=[3, 1, 4, 1, 5, 9, 2, 6],', '\n', '    nested_row_splits=([0, 3, 3, 5], [0, 4, 4, 7, 8, 8])).to_list()', '\n', '[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]', '\n'], ['RaggedTensor', 's with uniform inner dimensions can be defined\nby using a multidimensional ', 'Tensor', ' for ', 'values', '.'], ['\n', 'rt = RaggedTensor.from_row_splits(values=tf.ones([5, 3], tf.int32),', '\n', '                                  row_splits=[0, 2, 5])', '\n', 'print(rt.to_list())', '\n', '[[[1, 1, 1], [1, 1, 1]],', '\n', ' [[1, 1, 1], [1, 1, 1], [1, 1, 1]]]', '\n', 'print(rt.shape)', '\n', '(2, None, 3)', '\n'], ['RaggedTensor', 's with uniform outer dimensions can be defined by using\none or more ', 'RaggedTensor', ' with a ', 'uniform_row_length', ' row-partitioning\ntensor.  For example, a ', 'RaggedTensor', ' with shape ', '[2, 2, None]', ' can be\nconstructed with this method from a ', 'RaggedTensor', ' values with shape\n', '[4, None]', ':'], ['\n', 'values = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])', '\n', 'print(values.shape)', '\n', '(4, None)', '\n', 'rt6 = tf.RaggedTensor.from_uniform_row_length(values, 2)', '\n', 'print(rt6)', '\n', '<tf.RaggedTensor [[[1, 2, 3], [4]], [[5, 6], [7, 8, 9, 10]]]>', '\n', 'print(rt6.shape)', '\n', '(2, 2, None)', '\n'], ['Note that ', 'rt6', ' only contains one ragged dimension (the innermost\ndimension). In contrast, if ', 'from_row_splits', ' is used to construct a similar\n', 'RaggedTensor', ', then that ', 'RaggedTensor', ' will have two ragged dimensions:'], ['\n', 'rt7 = tf.RaggedTensor.from_row_splits(values, [0, 2, 4])', '\n', 'print(rt7.shape)', '\n', '(2, None, None)', '\n'], ['Uniform and ragged outer dimensions may be interleaved, meaning that a\ntensor with any combination of ragged and uniform dimensions may be created.\nFor example, a RaggedTensor ', 't4', ' with shape ', '[3, None, 4, 8, None, 2]', ' could\nbe constructed as follows:'], ['Concretely, if ', 'rt.values', ' is a ', 'Tensor', ', then ', 'rt.flat_values', ' is\n', 'rt.values', '; otherwise, ', 'rt.flat_values', ' is ', 'rt.values.flat_values', '.'], ['Conceptually, ', 'flat_values', ' is the tensor formed by flattening the\noutermost dimension and all of the ragged dimensions into a single\ndimension.'], ['rt.flat_values.shape = [nvals] + rt.shape[rt.ragged_rank + 1:]', '\n(where ', 'nvals', ' is the number of items in the flattened dimensions).'], ['\n', 'rt = tf.ragged.constant([[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]])', '\n', 'print(rt.flat_values)', '\n', 'tf.Tensor([3 1 4 1 5 9 2 6], shape=(8,), dtype=int32)', '\n'], ['rt.nested_row_splits', ' is a tuple containing the ', 'row_splits', ' tensors for\nall ragged dimensions in ', 'rt', ', ordered from outermost to innermost.  In\nparticular, ', 'rt.nested_row_splits = (rt.row_splits,) + value_splits', ' where:'], ['\n', 'rt = tf.ragged.constant(', '\n', '    [[[[3, 1, 4, 1], [], [5, 9, 2]], [], [[6], []]]])', '\n', 'for i, splits in enumerate(rt.nested_row_splits):', '\n', ""  print('Splits for dimension %d: %s' % (i+1, splits.numpy()))"", '\n', 'Splits for dimension 1: [0 3]', '\n', 'Splits for dimension 2: [0 3 3 5]', '\n', 'Splits for dimension 3: [0 4 4 7 8 8]', '\n'], ['\n', 'values = tf.ragged.constant([[1, 2, 3], [4], [5, 6], [7, 8, 9, 10]])', '\n', 'values.ragged_rank', '\n', '1', '\n'], ['\n', 'rt = tf.RaggedTensor.from_uniform_row_length(values, 2)', '\n', 'rt.ragged_rank', '\n', '2', '\n'], ['rt.row_splits', ' specifies where the values for each row begin and end in\n', 'rt.values', '.  In particular, the values for row ', 'rt[i]', ' are stored in\nthe slice ', 'rt.values[rt.row_splits[i]:rt.row_splits[i+1]]', '.'], ['\n', 'rt = tf.ragged.constant([[3, 1, 4, 1], [], [5, 9, 2], [6], []])', '\n', 'print(rt.row_splits)  # indices of row splits in rt.values', '\n', 'tf.Tensor([0 4 4 7 8 8], shape=(6,), dtype=int64)', '\n']]","values = [3, 1, 4, 1, 5, 9, 2, 6]
RaggedTensor.from_row_splits(values, row_splits=[0, 4, 4, 7, 8, 8])
<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>
RaggedTensor.from_row_lengths(values, row_lengths=[4, 0, 3, 1, 0])
<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>
RaggedTensor.from_value_rowids(
    values, value_rowids=[0, 0, 0, 0, 2, 2, 2, 3], nrows=5)
<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>
RaggedTensor.from_row_starts(values, row_starts=[0, 4, 4, 7, 8])
<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>
RaggedTensor.from_row_limits(values, row_limits=[4, 4, 7, 8, 8])
<tf.RaggedTensor [[3, 1, 4, 1], [], [5, 9, 2], [6], []]>
RaggedTensor.from_uniform_row_length(values, uniform_row_length=2)
<tf.RaggedTensor [[3, 1], [4, 1], [5, 9], [2, 6]]>"
"tf.RaggedTensorSpec(
    shape=None,
    dtype=tf.dtypes.float32,
    ragged_rank=None,
    row_splits_dtype=tf.dtypes.int64,
    flat_values_spec=None
)
","[['Type specification for a ', 'tf.RaggedTensor', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","rt = tf.ragged.constant([[""a""], [""b"", ""c""]], dtype=tf.string)
tf.type_spec_from_value(rt).dtype
tf.string"
"tf.queue.RandomShuffleQueue(
    capacity,
    min_after_dequeue,
    dtypes,
    shapes=None,
    names=None,
    seed=None,
    shared_name=None,
    name='random_shuffle_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.compat.v1.ReaderBase(
    reader_ref, supports_serialize=False
)
",[],"num_records_produced(
    name=None
)
"
"tf.RegisterGradient(
    op_type
)
",[],"@tf.RegisterGradient(""Sub"")
def _sub_grad(unused_op, grad):
  return grad, tf.negative(grad)
"
"tf.compat.v1.Session(
    target='', graph=None, config=None
)
",[],[]
"tf.compat.v1.SparseConditionalAccumulator(
    dtype,
    shape=None,
    shared_name=None,
    name='sparse_conditional_accumulator',
    reduction_type='MEAN'
)
","[['Inherits From: ', 'ConditionalAccumulatorBase']]","apply_grad(
    grad_indices, grad_values, grad_shape=None, local_step=0, name=None
)
"
"tf.io.SparseFeature(
    index_key, value_key, dtype, size, already_sorted=False
)
","[['Configuration for parsing a sparse input feature from an ', 'Example', '.']]","SparseTensor(indices=[[3, 1], [20, 0]],
             values=[0.5, -1.0]
             dense_shape=[100, 3])
"
"tf.sparse.SparseTensor(
    indices, values, dense_shape
)
",[],"dense.shape = dense_shape
dense[tuple(indices[i])] = values[i]
"
"tf.SparseTensorSpec(
    shape=None,
    dtype=tf.dtypes.float32
)
","[['Type specification for a ', 'tf.sparse.SparseTensor', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.compat.v1.SparseTensorValue(
    indices, values, dense_shape
)
",[],[]
"tf.compat.v1.TFRecordReader(
    name=None, options=None
)
","[['Inherits From: ', 'ReaderBase']]","num_records_produced(
    name=None
)
"
"tf.Tensor(
    op, value_index, dtype
)
","[[None, '\n'], ['A ', 'tf.Tensor', ' represents a multidimensional array of elements.']]","# Compute some values using a Tensor
c = tf.constant([[1.0, 2.0], [3.0, 4.0]])
d = tf.constant([[1.0, 1.0], [0.0, 1.0]])
e = tf.matmul(c, d)
print(e)
tf.Tensor(
[[1. 3.]
 [3. 7.]], shape=(2, 2), dtype=float32)"
"tf.TensorArray(
    dtype,
    size=None,
    dynamic_size=None,
    clear_after_read=None,
    tensor_array_name=None,
    handle=None,
    flow=None,
    infer_shape=True,
    element_shape=None,
    colocate_with_first_write_call=True,
    name=None
)
",[],"ta = tf.TensorArray(tf.float32, size=0, dynamic_size=True, clear_after_read=False)
ta = ta.write(0, 10)
ta = ta.write(1, 20)
ta = ta.write(2, 30)
ta.read(0)
<tf.Tensor: shape=(), dtype=float32, numpy=10.0>
ta.read(1)
<tf.Tensor: shape=(), dtype=float32, numpy=20.0>
ta.read(2)
<tf.Tensor: shape=(), dtype=float32, numpy=30.0>
ta.stack()
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([10., 20., 30.],
dtype=float32)>"
"tf.TensorArraySpec(
    element_shape=None,
    dtype=tf.dtypes.float32,
    dynamic_size=False,
    infer_shape=True
)
","[['Type specification for a ', 'tf.TensorArray', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.TensorShape(
    dims
)
","[['Represents the shape of a ', 'Tensor', '.'], ['Inherits From: ', 'TraceType']]","as_list()
"
"tf.TensorSpec(
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
","[['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","experimental_as_proto() -> struct_pb2.TensorSpecProto
"
"tf.compat.v1.TextLineReader(
    skip_header_lines=None, name=None
)
","[['Inherits From: ', 'ReaderBase']]","num_records_produced(
    name=None
)
"
"tf.io.VarLenFeature(
    dtype
)
",[],[]
"tf.compat.v1.Variable(
    initial_value=None,
    trainable=None,
    collections=None,
    validate_shape=True,
    caching_device=None,
    name=None,
    variable_def=None,
    dtype=None,
    expected_shape=None,
    import_scope=None,
    constraint=None,
    use_resource=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE,
    shape=None
)
","[[None, '\n'], ['See the ', 'Variables Guide', '.'], ['Inherits From: ', 'Variable']]","import tensorflow as tf

# Create a variable.
w = tf.Variable(<initial-value>, name=<optional-name>)

# Use the variable in the graph like any Tensor.
y = tf.matmul(w, ...another variable or tensor...)

# The overloaded operators are available too.
z = tf.sigmoid(w + y)

# Assign a new value to the variable with `assign()` or a related method.
w.assign(w + 1.0)
w.assign_add(1.0)
"
"tf.Variable.SaveSliceInfo(
    full_name=None,
    full_shape=None,
    var_offset=None,
    var_shape=None,
    save_slice_info_def=None,
    import_scope=None
)
",[],"to_proto(
    export_scope=None
)
"
"tf.compat.v1.VariableScope(
    reuse,
    name='',
    initializer=None,
    regularizer=None,
    caching_device=None,
    partitioner=None,
    custom_getter=None,
    name_scope='',
    dtype=tf.dtypes.float32,
    use_resource=None,
    constraint=None
)
","[['Variable scope object to carry defaults to provide to ', 'get_variable', '.']]","get_collection(
    name
)
"
"tf.compat.v1.WholeFileReader(
    name=None
)
","[['Inherits From: ', 'ReaderBase']]","num_records_produced(
    name=None
)
"
"tf.math.abs(
    x, name=None
)
","[[None, '\n']]","# real number
x = tf.constant([-2.25, 3.25])
tf.abs(x)
<tf.Tensor: shape=(2,), dtype=float32,
numpy=array([2.25, 3.25], dtype=float32)>"
"tf.math.accumulate_n(
    inputs, shape=None, tensor_dtype=None, name=None
)
",[],"a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[5, 0], [0, 6]])
tf.math.accumulate_n([a, b, a]).numpy()
array([[ 7, 4],
       [ 6, 14]], dtype=int32)"
"tf.math.acos(
    x, name=None
)
",[],"x = tf.constant([1.0, -0.5, 3.4, 0.2, 0.0, -2], dtype = tf.float32)
tf.math.acos(x)
<tf.Tensor: shape=(6,), dtype=float32,
numpy= array([0. , 2.0943952, nan, 1.3694383, 1.5707964, nan],
dtype=float32)>"
"tf.math.acosh(
    x, name=None
)
",[],"x = tf.constant([-2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
tf.math.acosh(x) ==> [nan nan 0. 0.62236255 5.9914584 9.903487 inf]
"
"tf.math.add(
    x, y, name=None
)
",[],"x = [1, 2, 3, 4, 5]
y = 1
tf.add(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],
dtype=int32)>"
"tf.math.add_n(
    inputs, name=None
)
",[],"a = tf.constant([[3, 5], [4, 8]])
b = tf.constant([[1, 6], [2, 9]])
tf.math.add_n([a, b, a]).numpy()
array([[ 7, 16],
       [10, 25]], dtype=int32)"
"tf.compat.v1.add_to_collection(
    name, value
)
","[['Wrapper for ', 'Graph.add_to_collection()', ' using the default graph.']]",[]
"tf.compat.v1.add_to_collections(
    names, value
)
","[['Wrapper for ', 'Graph.add_to_collections()', ' using the default graph.']]",[]
"tf.math.angle(
    input, name=None
)
","[[None, '\n']]","input = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j], dtype=tf.complex64)
tf.math.angle(input).numpy()
# ==> array([2.0131705, 1.056345 ], dtype=float32)
"
"tf.compat.v1.app.run(
    main=None, argv=None
)
",[],[]
"tf.approx_top_k(
    input,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    is_max_k=True,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
",[],[]
"tf.compat.v1.arg_max(
    input,
    dimension,
    output_type=tf.dtypes.int64,
    name=None
)
",[],"import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmax(input = a)
c = tf.keras.backend.eval(b)
# c = 4
# here a[4] = 166.32 which is the largest element of a across axis 0
"
"tf.compat.v1.arg_min(
    input,
    dimension,
    output_type=tf.dtypes.int64,
    name=None
)
",[],"import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmin(input = a)
c = tf.keras.backend.eval(b)
# c = 0
# here a[0] = 1 which is the smallest element of a across axis 0
"
"tf.compat.v1.argmax(
    input,
    axis=None,
    name=None,
    dimension=None,
    output_type=tf.dtypes.int64
)
",[],"import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmax(input = a)
c = tf.keras.backend.eval(b)
# c = 4
# here a[4] = 166.32 which is the largest element of a across axis 0
"
"tf.compat.v1.argmin(
    input,
    axis=None,
    name=None,
    dimension=None,
    output_type=tf.dtypes.int64
)
",[],"import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmin(input = a)
c = tf.keras.backend.eval(b)
# c = 0
# here a[0] = 1 which is the smallest element of a across axis 0
"
"tf.argsort(
    values, axis=-1, direction='ASCENDING', stable=False, name=None
)
",[],"values = [1, 10, 26.9, 2.8, 166.32, 62.3]
sort_order = tf.argsort(values)
sort_order.numpy()
array([0, 3, 1, 2, 5, 4], dtype=int32)"
"tf.dtypes.as_dtype(
    type_value
)
","[['Converts the given ', 'type_value', ' to a ', 'DType', '.']]",[]
"tf.strings.as_string(
    input,
    precision=-1,
    scientific=False,
    shortest=False,
    width=-1,
    fill='',
    name=None
)
",[],"tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)"
"tf.math.asin(
    x, name=None
)
",[],"# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]
x = tf.constant([1.047, 0.785])
y = tf.math.sin(x) # [0.8659266, 0.7068252]

tf.math.asin(y) # [1.047, 0.785] = x
"
"tf.math.asinh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.asinh(x) ==> [-inf -1.4436355 -0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf]
"
"tf.compat.v1.assert_equal(
    x, y, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x == y', ' holds element-wise.']]","tf.compat.v1.assert_equal(
  x=x, y=y, data=data, summarize=summarize,
  message=message, name=name)
"
"tf.compat.v1.assert_greater(
    x, y, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x > y', ' holds element-wise.']]","tf.compat.v1.assert_greater(
  x=x, y=y, data=data, summarize=summarize,
  message=message, name=name)
"
"tf.compat.v1.assert_greater_equal(
    x, y, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x >= y', ' holds element-wise.']]","tf.compat.v1.assert_greater_equal(
  x=x, y=y, data=data, summarize=summarize,
  message=message, name=name)
"
"tf.compat.v1.assert_integer(
    x, message=None, name=None
)
","[['Assert that ', 'x', ' is of integer dtype.']]","with tf.control_dependencies([tf.compat.v1.assert_integer(x)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_less(
    x, y, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x < y', ' holds element-wise.']]","tf.compat.v1.assert_less(
  x=x, y=y, data=data, summarize=summarize,
  message=message, name=name)
"
"tf.compat.v1.assert_less_equal(
    x, y, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x <= y', ' holds element-wise.']]","tf.compat.v1.assert_less_equal(
  x=x, y=y, data=data, summarize=summarize,
  message=message, name=name)
"
"tf.compat.v1.assert_near(
    x,
    y,
    rtol=None,
    atol=None,
    data=None,
    summarize=None,
    message=None,
    name=None
)
","[['Assert the condition ', 'x', ' and ', 'y', ' are close element-wise.']]","with tf.control_dependencies([tf.compat.v1.assert_near(x, y)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_negative(
    x, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x < 0', ' holds element-wise.']]","with tf.control_dependencies([tf.debugging.assert_negative(x, y)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_non_negative(
    x, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x >= 0', ' holds element-wise.']]","with tf.control_dependencies([tf.debugging.assert_non_negative(x, y)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_non_positive(
    x, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x <= 0', ' holds element-wise.']]","with tf.control_dependencies([tf.debugging.assert_non_positive(x, y)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_none_equal(
    x, y, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x != y', ' holds element-wise.']]","tf.compat.v1.assert_none_equal(
  x=x, y=y, data=data, summarize=summarize,
  message=message, name=name)
"
"tf.compat.v1.assert_positive(
    x, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x > 0', ' holds element-wise.']]","with tf.control_dependencies([tf.debugging.assert_positive(x, y)]):
  output = tf.reduce_sum(x)
"
"tf.debugging.assert_proper_iterable(
    values
)
",[],[]
"tf.compat.v1.assert_rank(
    x, rank, data=None, summarize=None, message=None, name=None
)
","[['Assert ', 'x', ' has rank equal to ', 'rank', '.']]","with tf.control_dependencies([tf.compat.v1.assert_rank(x, 2)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_rank_at_least(
    x, rank, data=None, summarize=None, message=None, name=None
)
","[['Assert ', 'x', ' has rank equal to ', 'rank', ' or higher.']]","with tf.control_dependencies([tf.compat.v1.assert_rank_at_least(x, 2)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_rank_in(
    x, ranks, data=None, summarize=None, message=None, name=None
)
","[['Assert ', 'x', ' has rank in ', 'ranks', '.']]","with tf.control_dependencies([tf.compat.v1.assert_rank_in(x, (2, 4))]):
  output = tf.reduce_sum(x)
"
"tf.debugging.assert_same_float_dtype(
    tensors=None, dtype=None
)
","[['Validate and return float type based on ', 'tensors', ' and ', 'dtype', '.']]",[]
"tf.compat.v1.assert_scalar(
    tensor, name=None, message=None
)
","[['Asserts that the given ', 'tensor', ' is a scalar (i.e. zero-dimensional).']]",[]
"tf.compat.v1.assert_type(
    tensor, tf_type, message=None, name=None
)
","[['Statically asserts that the given ', 'Tensor', ' is of the specified type.']]",[]
"tf.compat.v1.assert_variables_initialized(
    var_list=None
)
",[],[]
"tf.compat.v1.assign(
    ref, value, validate_shape=None, use_locking=None, name=None
)
","[['Update ', 'ref', ' by assigning ', 'value', ' to it.']]","import tensorflow as tf
a = tf.Variable([1], shape=tf.TensorShape(None))
tf.compat.v1.assign(a, [2,3])"
"tf.compat.v1.assign_add(
    ref, value, use_locking=None, name=None
)
","[['Update ', 'ref', ' by adding ', 'value', ' to it.']]","with tf.Graph().as_default():
  with tf.compat.v1.Session() as sess:
    a = tf.compat.v1.Variable(0, dtype=tf.int64)
    sess.run(a.initializer)
    update_op = tf.compat.v1.assign_add(a, 1)
    res_a = sess.run(update_op)
    res_a
1"
"tf.compat.v1.assign_sub(
    ref, value, use_locking=None, name=None
)
","[['Update ', 'ref', ' by subtracting ', 'value', ' from it.']]","with tf.Graph().as_default():
  with tf.compat.v1.Session() as sess:
    a = tf.compat.v1.Variable(1, dtype=tf.int64)
    sess.run(a.initializer)
    update_op = tf.compat.v1.assign_sub(a, 1)
    res_a = sess.run(update_op)
    res_a
0"
"tf.math.atan(
    x, name=None
)
",[],"# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]
x = tf.constant([1.047, 0.785])
y = tf.math.tan(x) # [1.731261, 0.99920404]

tf.math.atan(y) # [1.047, 0.785] = x
"
"tf.math.atan2(
    y, x, name=None
)
","[[None, '\n'], ['Computes arctangent of ', 'y/x', ' element-wise, respecting signs of the arguments.']]","x = [1., 1.]
y = [1., -1.]
print((tf.math.atan2(y,x) * (180 / np.pi)).numpy())
[ 45. -45.]"
"tf.math.atanh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -1, -0.5, 1, 0, 0.5, 10, float(""inf"")])
  tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]
"
"tf.audio.decode_wav(
    contents, desired_channels=-1, desired_samples=-1, name=None
)
",[],[]
"tf.audio.encode_wav(
    audio, sample_rate, name=None
)
",[],[]
"tf.autograph.experimental.do_not_convert(
    func=None
)
",[],[]
"tf.autograph.experimental.set_loop_options(
    parallel_iterations=UNSPECIFIED,
    swap_memory=UNSPECIFIED,
    maximum_iterations=UNSPECIFIED,
    shape_invariants=UNSPECIFIED
)
",[],">>> @tf.function(autograph=True)
... def f():
...   n = 0
...   for i in tf.range(10):
...     tf.autograph.experimental.set_loop_options(maximum_iterations=3)
...     n += 1
...   return n
"
"tf.autograph.set_verbosity(
    level, alsologtostdout=False
)
",[],"import os
import tensorflow as tf

os.environ['AUTOGRAPH_VERBOSITY'] = '5'
# Verbosity is now 5

tf.autograph.set_verbosity(0)
# Verbosity is now 0

os.environ['AUTOGRAPH_VERBOSITY'] = '1'
# No effect, because set_verbosity was already called.
"
"tf.compat.v1.autograph.to_code(
    entity,
    recursive=True,
    arg_values=None,
    arg_types=None,
    indentation='  ',
    experimental_optional_features=None
)
",[],"def f(x):
  if x < 0:
    x = -x
  return x
tf.autograph.to_code(f)
""...def tf__f(x):..."""
"tf.compat.v1.autograph.to_graph(
    entity,
    recursive=True,
    arg_values=None,
    arg_types=None,
    experimental_optional_features=None
)
",[],"  def foo(x):
    if x > 0:
      y = x * x
    else:
      y = -x
    return y

  converted_foo = to_graph(foo)

  x = tf.constant(1)
  y = converted_foo(x)  # converted_foo is a TensorFlow Op-like.
  assert is_tensor(y)
"
"tf.autograph.trace(
    *args
)
",[],"import tensorflow as tf

for i in tf.range(10):
  tf.autograph.trace(i)
# Output: <Tensor ...>
"
"tf.compat.v1.batch_gather(
    params, indices, name=None
)
",[],[]
"tf.compat.v1.batch_scatter_update(
    ref, indices, updates, use_locking=True, name=None
)
","[['Generalization of ', 'tf.compat.v1.scatter_update', ' to axis different than 0. (deprecated)']]",[]
"tf.compat.v1.batch_to_space(
    input, crops, block_size, name=None, block_shape=None
)
",[],"crops = [[crop_top, crop_bottom], [crop_left, crop_right]]
"
"tf.compat.v1.batch_to_space_nd(
    input, block_shape, crops, name=None
)
",[],"[[[[1]]], [[[2]]], [[[3]]], [[[4]]]]
"
"tf.math.betainc(
    a, b, x, name=None
)
","[[None, '\n']]",[]
"tf.compat.v1.bincount(
    arr,
    weights=None,
    minlength=None,
    maxlength=None,
    dtype=tf.dtypes.int32
)
",[],[]
"tf.bitcast(
    input, type, name=None
)
",[],"a = [1., 2., 3.]
equality_bitcast = tf.bitcast(a, tf.complex128)
Traceback (most recent call last):
InvalidArgumentError: Cannot bitcast from 1 to 18 [Op:Bitcast]
equality_cast = tf.cast(a, tf.complex128)
print(equality_cast)
tf.Tensor([1.+0.j 2.+0.j 3.+0.j], shape=(3,), dtype=complex128)"
"tf.bitwise.bitwise_and(
    x, y, name=None
)
","[['Elementwise computes the bitwise AND of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([0, 0, 3, 10], dtype=tf.float32)

  res = bitwise_ops.bitwise_and(lhs, rhs)
  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE
"
"tf.bitwise.bitwise_or(
    x, y, name=None
)
","[['Elementwise computes the bitwise OR of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([5, 5, 7, 15], dtype=tf.float32)

  res = bitwise_ops.bitwise_or(lhs, rhs)
  tf.assert_equal(tf.cast(res,  tf.float32), exp)  # TRUE
"
"tf.bitwise.bitwise_xor(
    x, y, name=None
)
","[['Elementwise computes the bitwise XOR of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64,
              tf.uint8, tf.uint16, tf.uint32, tf.uint64]

for dtype in dtype_list:
  lhs = tf.constant([0, 5, 3, 14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)
  exp = tf.constant([5, 5, 4, 5],  dtype=tf.float32)

  res = bitwise_ops.bitwise_xor(lhs, rhs)
  tf.assert_equal(tf.cast(res, tf.float32), exp) # TRUE
"
"tf.bitwise.invert(
    x, name=None
)
","[['Invert (flip) each bit of supported types; for example, type ', 'uint8', ' value 01010101 becomes 10101010.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops

# flip 2 (00000010) to -3 (11111101)
tf.assert_equal(-3, bitwise_ops.invert(2))

dtype_list = [dtypes.int8, dtypes.int16, dtypes.int32, dtypes.int64,
              dtypes.uint8, dtypes.uint16, dtypes.uint32, dtypes.uint64]

inputs = [0, 5, 3, 14]
for dtype in dtype_list:
  # Because of issues with negative numbers, let's test this indirectly.
  # 1. invert(a) and a = 0
  # 2. invert(a) or a = invert(0)
  input_tensor = tf.constant([0, 5, 3, 14], dtype=dtype)
  not_a_and_a, not_a_or_a, not_0 = [bitwise_ops.bitwise_and(
                                      input_tensor, bitwise_ops.invert(input_tensor)),
                                    bitwise_ops.bitwise_or(
                                      input_tensor, bitwise_ops.invert(input_tensor)),
                                    bitwise_ops.invert(
                                      tf.constant(0, dtype=dtype))]

  expected = tf.constant([0, 0, 0, 0], dtype=tf.float32)
  tf.assert_equal(tf.cast(not_a_and_a, tf.float32), expected)

  expected = tf.cast([not_0] * 4, tf.float32)
  tf.assert_equal(tf.cast(not_a_or_a, tf.float32), expected)

  # For unsigned dtypes let's also check the result directly.
  if dtype.is_unsigned:
    inverted = bitwise_ops.invert(input_tensor)
    expected = tf.constant([dtype.max - x for x in inputs], dtype=tf.float32)
    tf.assert_equal(tf.cast(inverted, tf.float32), tf.cast(expected, tf.float32))
"
"tf.bitwise.left_shift(
    x, y, name=None
)
","[['Elementwise computes the bitwise left-shift of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
import numpy as np
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]

for dtype in dtype_list:
  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)

  left_shift_result = bitwise_ops.left_shift(lhs, rhs)

  print(left_shift_result)

# This will print:
# tf.Tensor([ -32   -5 -128    0], shape=(4,), dtype=int8)
# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int16)
# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int32)
# tf.Tensor([   -32     -5   -384 -28672], shape=(4,), dtype=int64)

lhs = np.array([-2, 64, 101, 32], dtype=np.int8)
rhs = np.array([-1, -5, -3, -14], dtype=np.int8)
bitwise_ops.left_shift(lhs, rhs)
# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>
"
"tf.bitwise.right_shift(
    x, y, name=None
)
","[['Elementwise computes the bitwise right-shift of ', 'x', ' and ', 'y', '.']]","import tensorflow as tf
from tensorflow.python.ops import bitwise_ops
import numpy as np
dtype_list = [tf.int8, tf.int16, tf.int32, tf.int64]

for dtype in dtype_list:
  lhs = tf.constant([-1, -5, -3, -14], dtype=dtype)
  rhs = tf.constant([5, 0, 7, 11], dtype=dtype)

  right_shift_result = bitwise_ops.right_shift(lhs, rhs)

  print(right_shift_result)

# This will print:
# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int8)
# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int16)
# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int32)
# tf.Tensor([-1 -5 -1 -1], shape=(4,), dtype=int64)

lhs = np.array([-2, 64, 101, 32], dtype=np.int8)
rhs = np.array([-1, -5, -3, -14], dtype=np.int8)
bitwise_ops.right_shift(lhs, rhs)
# <tf.Tensor: shape=(4,), dtype=int8, numpy=array([ -2,  64, 101,  32], dtype=int8)>
"
"tf.compat.v1.boolean_mask(
    tensor, mask, name='boolean_mask', axis=None
)
",[],"# 1-D example
tensor = [0, 1, 2, 3]
mask = np.array([True, False, True, False])
tf.boolean_mask(tensor, mask)  # [0, 2]

# 2-D example
tensor = [[1, 2], [3, 4], [5, 6]]
mask = np.array([True, False, True])
tf.boolean_mask(tensor, mask)  # [[1, 2], [5, 6]]
"
"tf.broadcast_dynamic_shape(
    shape_x, shape_y
)
",[],"shape_x = (1, 2, 3)
shape_y = (5, 1, 3)
tf.broadcast_dynamic_shape(shape_x, shape_y)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([5, 2, 3], ...>"
"tf.broadcast_static_shape(
    shape_x, shape_y
)
",[],"shape_x = tf.TensorShape([1, 2, 3])
shape_y = tf.TensorShape([5, 1 ,3])
tf.broadcast_static_shape(shape_x, shape_y)
TensorShape([5, 2, 3])"
"tf.broadcast_to(
    input, shape, name=None
)
",[],"x = tf.constant([[1, 2, 3]])   # Shape (1, 3,)
y = tf.broadcast_to(x, [2, 3])
print(y)
tf.Tensor(
    [[1 2 3]
     [1 2 3]], shape=(2, 3), dtype=int32)"
"tf.compat.v1.case(
    pred_fn_pairs,
    default=None,
    exclusive=False,
    strict=False,
    name='case'
)
",[],"if (x < y) return 17;
else return 23;
"
"tf.cast(
    x, dtype, name=None
)
",[],"x = tf.constant([1.8, 2.2], dtype=tf.float32)
tf.cast(x, tf.int32)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>"
"tf.math.ceil(
    x, name=None
)
",[],"tf.math.ceil([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])
<tf.Tensor: shape=(7,), dtype=float32,
numpy=array([-1., -1., -0.,  1.,  2.,  2.,  2.], dtype=float32)>"
"tf.debugging.check_numerics(
    tensor, message, name=None
)
",[],"a = tf.Variable(1.0)
tf.debugging.check_numerics(a, message='')

b = tf.Variable(np.nan)
try:
  tf.debugging.check_numerics(b, message='Checking b')
except Exception as e:
  assert ""Checking b : Tensor had NaN values"" in e.message

c = tf.Variable(np.inf)
try:
  tf.debugging.check_numerics(c, message='Checking c')
except Exception as e:
  assert ""Checking c : Tensor had Inf values"" in e.message
"
"tf.linalg.cholesky(
    input, name=None
)
",[],[]
"tf.linalg.cholesky_solve(
    chol, rhs, name=None
)
","[['Solves systems of linear eqns ', 'A X = RHS', ', given Cholesky factorizations.']]","# Solve 10 separate 2x2 linear systems:
A = ... # shape 10 x 2 x 2
RHS = ... # shape 10 x 2 x 1
chol = tf.linalg.cholesky(A)  # shape 10 x 2 x 2
X = tf.linalg.cholesky_solve(chol, RHS)  # shape 10 x 2 x 1
# tf.matmul(A, X) ~ RHS
X[3, :, 0]  # Solution to the linear system A[3, :, :] x = RHS[3, :, 0]

# Solve five linear systems (K = 5) for every member of the length 10 batch.
A = ... # shape 10 x 2 x 2
RHS = ... # shape 10 x 2 x 5
...
X[3, :, 2]  # Solution to the linear system A[3, :, :] x = RHS[3, :, 2]
"
"tf.compat.v1.clip_by_average_norm(
    t, clip_norm, name=None
)
",[],[]
"tf.clip_by_global_norm(
    t_list, clip_norm, use_norm=None, name=None
)
",[],"t_list[i] * clip_norm / max(global_norm, clip_norm)
"
"tf.clip_by_norm(
    t, clip_norm, axes=None, name=None
)
",[],"some_nums = tf.constant([[1, 2, 3, 4, 5]], dtype=tf.float32)
tf.clip_by_norm(some_nums, 2.0).numpy()
array([[0.26967996, 0.5393599 , 0.80903983, 1.0787199 , 1.3483998 ]],
      dtype=float32)"
"tf.clip_by_value(
    t, clip_value_min, clip_value_max, name=None
)
",[],"t = tf.constant([[-10., -1., 0.], [0., 2., 10.]])
t2 = tf.clip_by_value(t, clip_value_min=-1, clip_value_max=1)
t2.numpy()
array([[-1., -1.,  0.],
       [ 0.,  1.,  1.]], dtype=float32)"
"tf.compat.v1.colocate_with(
    op, ignore_existing=False
)
",[],[]
"tf.compat.as_bytes(
    bytes_or_text, encoding='utf-8'
)
","[['Converts ', 'bytearray', ', ', 'bytes', ', or unicode python input types to ', 'bytes', '.']]",[]
"tf.compat.as_str(
    bytes_or_text, encoding='utf-8'
)
",[],[]
"tf.compat.as_str_any(
    value, encoding='utf-8'
)
","[['Converts input to ', 'str', ' type.']]",[]
"tf.compat.as_text(
    bytes_or_text, encoding='utf-8'
)
",[],[]
"tf.compat.dimension_at_index(
    shape, index
)
",[],"# If you had this in your V1 code:
dim = tensor_shape[i]

# Use `dimension_at_index` as direct replacement compatible with both V1 & V2:
dim = dimension_at_index(tensor_shape, i)

# Another possibility would be this, but WARNING: it only works if the
# tensor_shape instance has a defined rank.
dim = tensor_shape.dims[i]  # `dims` may be None if the rank is undefined!

# In native V2 code, we recommend instead being more explicit:
if tensor_shape.rank is None:
  dim = Dimension(None)
else:
  dim = tensor_shape.dims[i]

# Being more explicit will save you from the following trap (present in V1):
# you might do in-place modifications to `dim` and expect them to be reflected
# in `tensor_shape[i]`, but they would not be (as the Dimension object was
# instantiated on the fly.
"
"tf.compat.dimension_value(
    dimension
)
",[],"# If you had this in your V1 code:
value = tensor_shape[i].value

# Use `dimension_value` as direct replacement compatible with both V1 & V2:
value = dimension_value(tensor_shape[i])

# This would be the V2 equivalent:
value = tensor_shape[i]  # Warning: this will return the dim value in V2!
"
"tf.compat.forward_compatible(
    year, month, day
)
",[],"def add(inputs, name=None):
  return gen_math_ops.add(inputs, name)
"
"tf.compat.path_to_str(
    path
)
","[[None, '\n'], ['Converts input which is a ', 'PathLike', ' object to ', 'str', ' type.']]","$ tf.compat.path_to_str('C:\XYZ\tensorflow\./.././tensorflow')
'C:\XYZ\tensorflow\./.././tensorflow' # Windows OS
$ tf.compat.path_to_str(Path('C:\XYZ\tensorflow\./.././tensorflow'))
'C:\XYZ\tensorflow\..\tensorflow' # Windows OS
$ tf.compat.path_to_str(Path('./corpus'))
'corpus' # Linux OS
$ tf.compat.path_to_str('./.././Corpus')
'./.././Corpus' # Linux OS
$ tf.compat.path_to_str(Path('./.././Corpus'))
'../Corpus' # Linux OS
$ tf.compat.path_to_str(Path('./..////../'))
'../..' # Linux OS

"
"tf.dtypes.complex(
    real, imag, name=None
)
","[[None, '\n']]","real = tf.constant([2.25, 3.25])
imag = tf.constant([4.75, 5.75])
tf.complex(real, imag)  # [[2.25 + 4.75j], [3.25 + 5.75j]]
"
"tf.concat(
    values, axis, name='concat'
)
",[],"[D0, D1, ... Raxis, ...Dn]
"
"tf.compat.v1.cond(
    pred,
    true_fn=None,
    false_fn=None,
    strict=False,
    name=None,
    fn1=None,
    fn2=None
)
","[['Return ', 'true_fn()', ' if the predicate ', 'pred', ' is true else ', 'false_fn()', '. (deprecated arguments)']]","z = tf.multiply(a, b)
result = tf.cond(x < y, lambda: tf.add(x, z), lambda: tf.square(y))
"
"tf.config.LogicalDevice(
    name, device_type
)
",[],[]
"tf.config.LogicalDeviceConfiguration(
    memory_limit=None,
    experimental_priority=None,
    experimental_device_ordinal=None
)
",[],[]
"tf.config.PhysicalDevice(
    name, device_type
)
",[],[]
"tf.config.LogicalDeviceConfiguration(
    memory_limit=None,
    experimental_priority=None,
    experimental_device_ordinal=None
)
",[],[]
"tf.config.experimental.enable_tensor_float_32_execution(
    enabled
)
",[],"x = tf.fill((2, 2), 1.0001)
y = tf.fill((2, 2), 1.)
# TensorFloat-32 is enabled, so matmul is run with reduced precision
print(tf.linalg.matmul(x, y))  # [[2., 2.], [2., 2.]]
tf.config.experimental.enable_tensor_float_32_execution(False)
# Matmul is run with full precision
print(tf.linalg.matmul(x, y))  # [[2.0002, 2.0002], [2.0002, 2.0002]]
"
"tf.config.experimental.get_device_details(
    device
)
",[],"gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
  details = tf.config.experimental.get_device_details(gpu_devices[0])
  details.get('device_name', 'Unknown GPU')"
"tf.config.experimental.get_memory_growth(
    device
)
","[['Get if memory growth is enabled for a ', 'PhysicalDevice', '.']]","physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.experimental.set_memory_growth(physical_devices[0], True)
  assert tf.config.experimental.get_memory_growth(physical_devices[0])
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass"
"tf.config.experimental.get_memory_info(
    device
)
",[],"if tf.config.list_physical_devices('GPU'):
  # Returns a dict in the form {'current': <current mem usage>,
  #                             'peak': <peak mem usage>}
  tf.config.experimental.get_memory_info('GPU:0')"
"tf.config.experimental.get_memory_usage(
    device
)
",[],"gpu_devices = tf.config.list_physical_devices('GPU')
if gpu_devices:
  tf.config.experimental.get_memory_usage('GPU:0')"
"tf.config.get_logical_device_configuration(
    device
)
","[['Get the virtual device configuration for a ', 'tf.config.PhysicalDevice', '.']]","physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
configs = tf.config.get_logical_device_configuration(
  physical_devices[0])
try:
  assert configs is None
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  configs = tf.config.get_logical_device_configuration(
    physical_devices[0])
  assert len(configs) == 2
except:
  # Cannot modify virtual devices once initialized.
  pass"
"tf.config.get_visible_devices(
    device_type=None
)
",[],"physical_devices = tf.config.list_physical_devices('GPU')
try:
  # Disable all GPUS
  tf.config.set_visible_devices([], 'GPU')
  visible_devices = tf.config.get_visible_devices()
  for device in visible_devices:
    assert device.device_type != 'GPU'
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass"
"tf.config.list_logical_devices(
    device_type=None
)
",[],"logical_devices = tf.config.list_logical_devices('GPU')
if len(logical_devices) > 0:
  # Allocate on GPU:0
  with tf.device(logical_devices[0].name):
    one = tf.constant(1)
  # Allocate on GPU:1
  with tf.device(logical_devices[1].name):
    two = tf.constant(2)"
"tf.config.list_physical_devices(
    device_type=None
)
",[],"physical_devices = tf.config.list_physical_devices('GPU')
print(""Num GPUs:"", len(physical_devices))
Num GPUs: ..."
"tf.config.experimental.reset_memory_stats(
    device
)
",[],"if tf.config.list_physical_devices('GPU'):
  # Sets the peak memory to the current memory.
  tf.config.experimental.reset_memory_stats('GPU:0')
  # Creates the first peak memory usage.
  x1 = tf.ones(1000 * 1000, dtype=tf.float64)
  del x1 # Frees the memory referenced by `x1`.
  peak1 = tf.config.experimental.get_memory_info('GPU:0')['peak']
  # Sets the peak memory to the current memory again.
  tf.config.experimental.reset_memory_stats('GPU:0')
  # Creates the second peak memory usage.
  x2 = tf.ones(1000 * 1000, dtype=tf.float32)
  del x2
  peak2 = tf.config.experimental.get_memory_info('GPU:0')['peak']
  assert peak2 < peak1  # tf.float32 consumes less memory than tf.float64."
"tf.config.experimental.set_device_policy(
    device_policy
)
",[],[]
"tf.config.experimental.set_memory_growth(
    device, enable
)
","[['Set if memory growth should be enabled for a ', 'PhysicalDevice', '.']]","physical_devices = tf.config.list_physical_devices('GPU')
try:
  tf.config.experimental.set_memory_growth(physical_devices[0], True)
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass"
"tf.config.experimental.set_synchronous_execution(
    enable
)
",[],[]
"tf.config.set_logical_device_configuration(
    device, logical_devices
)
","[['Set the logical device configuration for a ', 'tf.config.PhysicalDevice', '.']]","physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
# Specify 2 virtual CPUs. Note currently memory limit is not supported.
try:
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  logical_devices = tf.config.list_logical_devices('CPU')
  assert len(logical_devices) == 2
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
except:
  # Cannot modify logical devices once initialized.
  pass"
"tf.config.set_visible_devices(
    devices, device_type=None
)
",[],"physical_devices = tf.config.list_physical_devices('GPU')
try:
  # Disable first GPU
  tf.config.set_visible_devices(physical_devices[1:], 'GPU')
  logical_devices = tf.config.list_logical_devices('GPU')
  # Logical device was not created for first GPU
  assert len(logical_devices) == len(physical_devices) - 1
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass"
"tf.config.experimental_connect_to_cluster(
    cluster_spec_or_resolver,
    job_name='localhost',
    task_index=0,
    protocol=None,
    make_master_device_default=True,
    cluster_device_filters=None
)
",[],"cdf = tf.config.experimental.ClusterDeviceFilters()
# For any worker, only the devices on PS nodes and itself are visible
for i in range(num_workers):
  cdf.set_device_filters('worker', i, ['/job:ps'])
# Similarly for any ps, only the devices on workers and itself are visible
for i in range(num_ps):
  cdf.set_device_filters('ps', i, ['/job:worker'])

tf.config.experimental_connect_to_cluster(cluster_def,
                                          cluster_device_filters=cdf)
"
"tf.config.experimental_connect_to_host(
    remote_host=None, job_name='worker'
)
",[],"# When eager execution is enabled, connect to the remote host.
tf.config.experimental_connect_to_host(""exampleaddr.com:9876"")

with ops.device(""job:worker/replica:0/task:1/device:CPU:0""):
  # The following tensors should be resident on the remote device, and the op
  # will also execute remotely.
  x1 = array_ops.ones([2, 2])
  x2 = array_ops.ones([2, 2])
  y = math_ops.matmul(x1, x2)
"
"tf.config.experimental_run_functions_eagerly(
    run_eagerly
)
","[['Enables / disables eager execution of ', 'tf.function', 's. (deprecated)']]",[]
"tf.config.get_logical_device_configuration(
    device
)
","[['Get the virtual device configuration for a ', 'tf.config.PhysicalDevice', '.']]","physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
configs = tf.config.get_logical_device_configuration(
  physical_devices[0])
try:
  assert configs is None
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  configs = tf.config.get_logical_device_configuration(
    physical_devices[0])
  assert len(configs) == 2
except:
  # Cannot modify virtual devices once initialized.
  pass"
"tf.config.get_visible_devices(
    device_type=None
)
",[],"physical_devices = tf.config.list_physical_devices('GPU')
try:
  # Disable all GPUS
  tf.config.set_visible_devices([], 'GPU')
  visible_devices = tf.config.get_visible_devices()
  for device in visible_devices:
    assert device.device_type != 'GPU'
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass"
"tf.config.list_logical_devices(
    device_type=None
)
",[],"logical_devices = tf.config.list_logical_devices('GPU')
if len(logical_devices) > 0:
  # Allocate on GPU:0
  with tf.device(logical_devices[0].name):
    one = tf.constant(1)
  # Allocate on GPU:1
  with tf.device(logical_devices[1].name):
    two = tf.constant(2)"
"tf.config.list_physical_devices(
    device_type=None
)
",[],"physical_devices = tf.config.list_physical_devices('GPU')
print(""Num GPUs:"", len(physical_devices))
Num GPUs: ..."
"tf.config.optimizer.get_jit() -> str
","[['Returns JIT compilation configuration for code inside ', 'tf.function', '.']]",[]
"tf.config.optimizer.set_experimental_options(
    options
)
",[],[]
"tf.config.optimizer.set_jit(
    enabled: Union[bool, str]
)
",[],[]
"tf.config.run_functions_eagerly(
    run_eagerly
)
","[['Enables / disables eager execution of ', 'tf.function', 's.']]","def my_func(a):
 print(""Python side effect"")
 return a + a
a_fn = tf.function(my_func)"
"tf.config.set_logical_device_configuration(
    device, logical_devices
)
","[['Set the logical device configuration for a ', 'tf.config.PhysicalDevice', '.']]","physical_devices = tf.config.list_physical_devices('CPU')
assert len(physical_devices) == 1, ""No CPUs found""
# Specify 2 virtual CPUs. Note currently memory limit is not supported.
try:
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
  logical_devices = tf.config.list_logical_devices('CPU')
  assert len(logical_devices) == 2
  tf.config.set_logical_device_configuration(
    physical_devices[0],
    [tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration(),
     tf.config.LogicalDeviceConfiguration()])
except:
  # Cannot modify logical devices once initialized.
  pass"
"tf.config.set_soft_device_placement(
    enabled
)
",[],[]
"tf.config.set_visible_devices(
    devices, device_type=None
)
",[],"physical_devices = tf.config.list_physical_devices('GPU')
try:
  # Disable first GPU
  tf.config.set_visible_devices(physical_devices[1:], 'GPU')
  logical_devices = tf.config.list_logical_devices('GPU')
  # Logical device was not created for first GPU
  assert len(logical_devices) == len(physical_devices) - 1
except:
  # Invalid device or cannot modify virtual devices once initialized.
  pass"
"tf.config.threading.set_inter_op_parallelism_threads(
    num_threads
)
",[],[]
"tf.config.threading.set_intra_op_parallelism_threads(
    num_threads
)
",[],[]
"tf.compat.v1.confusion_matrix(
    labels,
    predictions,
    num_classes=None,
    dtype=tf.dtypes.int32,
    name=None,
    weights=None
)
",[],"  tf.math.confusion_matrix([1, 2, 4], [2, 2, 4]) ==>
      [[0 0 0 0 0]
       [0 0 1 0 0]
       [0 0 1 0 0]
       [0 0 0 0 0]
       [0 0 0 0 1]]
"
"tf.math.conj(
    x, name=None
)
","[[None, '\n']]","x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
tf.math.conj(x)
<tf.Tensor: shape=(2,), dtype=complex128,
numpy=array([-2.25-4.75j,  3.25-5.75j])>"
"tf.compat.v1.constant(
    value, dtype=None, shape=None, name='Const', verify_shape=False
)
",[],"# Constant 1-D Tensor populated with value list.
tensor = tf.constant([1, 2, 3, 4, 5, 6, 7]) => [1 2 3 4 5 6 7]

# Constant 2-D tensor populated with scalar value -1.
tensor = tf.constant(-1.0, shape=[2, 3]) => [[-1. -1. -1.]
                                             [-1. -1. -1.]]
"
"tf.compat.v1.keras.initializers.Constant(
    value=0,
    dtype=tf.dtypes.float32,
    verify_shape=False
)
",[],"value = [0, 1, 2, 3, 4, 5, 6, 7]
initializer = tf.compat.v1.constant_initializer(
    value=value,
    dtype=tf.float32,
    verify_shape=False)
variable = tf.Variable(initializer(shape=[2, 4]))
"
"tf.compat.v1.container(
    container_name
)
","[['Wrapper for ', 'Graph.container()', ' using the default graph.']]",[]
"tf.control_dependencies(
    control_inputs
)
","[['Wrapper for ', 'Graph.control_dependencies()', ' using the default graph.']]",[]
"tf.compat.v1.convert_to_tensor(
    value, dtype=None, name=None, preferred_dtype=None, dtype_hint=None
)
","[['Converts the given ', 'value', ' to a ', 'Tensor', '.']]","import numpy as np

def my_func(arg):
  arg = tf.convert_to_tensor(arg, dtype=tf.float32)
  return tf.matmul(arg, arg) + arg

# The following calls are equivalent.
value_1 = my_func(tf.constant([[1.0, 2.0], [3.0, 4.0]]))
value_2 = my_func([[1.0, 2.0], [3.0, 4.0]])
value_3 = my_func(np.array([[1.0, 2.0], [3.0, 4.0]], dtype=np.float32))
"
"tf.compat.v1.convert_to_tensor_or_indexed_slices(
    value, dtype=None, name=None
)
","[['Converts the given object to a ', 'Tensor', ' or an ', 'IndexedSlices', '.']]",[]
"tf.compat.v1.convert_to_tensor_or_sparse_tensor(
    value, dtype=None, name=None
)
","[['Converts value to a ', 'SparseTensor', ' or ', 'Tensor', '.']]",[]
"tf.math.cos(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]
"
"tf.math.cosh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.cosh(x) ==> [inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf]
"
"tf.compat.v1.count_nonzero(
    input_tensor=None,
    axis=None,
    keepdims=None,
    dtype=tf.dtypes.int64,
    name=None,
    reduction_indices=None,
    keep_dims=None,
    input=None
)
",[],"x = tf.constant([[0, 1, 0], [1, 1, 0]])
tf.math.count_nonzero(x)  # 3
tf.math.count_nonzero(x, 0)  # [1, 2, 0]
tf.math.count_nonzero(x, 1)  # [1, 2]
tf.math.count_nonzero(x, 1, keepdims=True)  # [[1], [2]]
tf.math.count_nonzero(x, [0, 1])  # 3
"
"tf.compat.v1.count_up_to(
    ref, limit, name=None
)
",[],[]
"tf.compat.v1.create_partitioned_variables(
    shape,
    slicing,
    initializer,
    dtype=tf.dtypes.float32,
    trainable=True,
    collections=None,
    name=None,
    reuse=None
)
","[['Create a list of partitioned variables according to the given ', 'slicing', '. (deprecated)']]",[]
"tf.linalg.cross(
    a, b, name=None
)
",[],[]
"tf.math.cumprod(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","[['Compute the cumulative product of the tensor ', 'x', ' along ', 'axis', '.']]","tf.math.cumprod([a, b, c])  # [a, a * b, a * b * c]
"
"tf.math.cumsum(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","[['Compute the cumulative sum of the tensor ', 'x', ' along ', 'axis', '.']]","# tf.cumsum([a, b, c])   # [a, a + b, a + b + c]
x = tf.constant([2, 4, 6, 8])
tf.cumsum(x)
<tf.Tensor: shape=(4,), dtype=int32,
numpy=array([ 2,  6, 12, 20], dtype=int32)>"
"tf.custom_gradient(
    f=None
)
",[],"def log1pexp(x):
  return tf.math.log(1 + tf.exp(x))
"
"tf.data.DatasetSpec(
    element_spec, dataset_shape=()
)
","[['Type specification for ', 'tf.data.Dataset', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","dataset = tf.data.Dataset.range(3)
tf.data.DatasetSpec.from_value(dataset)
DatasetSpec(TensorSpec(shape=(), dtype=tf.int64, name=None), TensorShape([]))"
"tf.compat.v1.data.FixedLengthRecordDataset(
    filenames,
    record_bytes,
    header_bytes=None,
    footer_bytes=None,
    buffer_size=None,
    compression_type=None,
    num_parallel_reads=None,
    name=None
)
","[['A ', 'Dataset', ' of fixed-length records from one or more binary files.'], ['Inherits From: ', 'Dataset', ', ', 'Dataset']]","dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int32, name=None)"
"tf.compat.v1.data.Iterator(
    iterator_resource, initializer, output_types, output_shapes, output_classes
)
","[['Represents the state of iterating through a ', 'Dataset', '.']]","@staticmethod
from_string_handle(
    string_handle, output_types, output_shapes=None, output_classes=None
)
"
"tf.compat.v1.data.TFRecordDataset(
    filenames,
    compression_type=None,
    buffer_size=None,
    num_parallel_reads=None,
    name=None
)
","[['A ', 'Dataset', ' comprising records from one or more TFRecord files.'], ['Inherits From: ', 'Dataset', ', ', 'Dataset']]","dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int32, name=None)"
"tf.compat.v1.data.TextLineDataset(
    filenames,
    compression_type=None,
    buffer_size=None,
    num_parallel_reads=None,
    name=None
)
","[['A ', 'Dataset', ' comprising lines from one or more text files.'], ['Inherits From: ', 'Dataset', ', ', 'Dataset']]","dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int32, name=None)"
"tf.data.experimental.CheckpointInputPipelineHook(
    estimator, external_state_policy=None
)
","[['Inherits From: ', 'SessionRunHook']]","est = tf.estimator.Estimator(model_fn)
while True:
  est.train(
      train_input_fn,
      hooks=[tf.data.experimental.CheckpointInputPipelineHook(est)],
      steps=train_steps_per_eval)
  # Note: We do not pass the hook here.
  metrics = est.evaluate(eval_input_fn)
  if should_stop_the_training(metrics):
    break
"
"tf.compat.v1.data.experimental.Counter(
    start=0,
    step=1,
    dtype=tf.dtypes.int64
)
","[['Creates a ', 'Dataset', ' that counts from ', 'start', ' in steps of size ', 'step', '. (deprecated)']]","dataset = tf.data.experimental.Counter().take(5)
list(dataset.as_numpy_iterator())
[0, 1, 2, 3, 4]
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int64, name=None)
dataset = tf.data.experimental.Counter(dtype=tf.int32)
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int32, name=None)
dataset = tf.data.experimental.Counter(start=2).take(5)
list(dataset.as_numpy_iterator())
[2, 3, 4, 5, 6]
dataset = tf.data.experimental.Counter(start=2, step=5).take(5)
list(dataset.as_numpy_iterator())
[2, 7, 12, 17, 22]
dataset = tf.data.experimental.Counter(start=10, step=-1).take(5)
list(dataset.as_numpy_iterator())
[10, 9, 8, 7, 6]"
"tf.compat.v1.data.experimental.CsvDataset(
    filenames,
    record_defaults,
    compression_type=None,
    buffer_size=None,
    header=False,
    field_delim=',',
    use_quote_delim=True,
    na_value='',
    select_cols=None,
    exclude_cols=None
)
","[['Inherits From: ', 'Dataset', ', ', 'Dataset']]","dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int32, name=None)"
"tf.data.experimental.DatasetInitializer(
    dataset
)
","[['Creates a table initializer from a ', 'tf.data.Dataset', '.']]","keys = tf.data.Dataset.range(100)
values = tf.data.Dataset.range(100).map(
    lambda x: tf.strings.as_string(x * 2))
ds = tf.data.Dataset.zip((keys, values))
init = tf.data.experimental.DatasetInitializer(ds)
table = tf.lookup.StaticHashTable(init, """")
table.lookup(tf.constant([0, 1, 2], dtype=tf.int64)).numpy()
array([b'0', b'2', b'4'], dtype=object)"
"tf.data.DatasetSpec(
    element_spec, dataset_shape=()
)
","[['Type specification for ', 'tf.data.Dataset', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","dataset = tf.data.Dataset.range(3)
tf.data.DatasetSpec.from_value(dataset)
DatasetSpec(TensorSpec(shape=(), dtype=tf.int64, name=None), TensorShape([]))"
"tf.OptionalSpec(
    element_spec
)
","[['Type specification for ', 'tf.experimental.Optional', '.'], ['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","@tf.function(input_signature=[tf.OptionalSpec(
  tf.TensorSpec(shape=(), dtype=tf.int32, name=None))])
def maybe_square(optional):
  if optional.has_value():
    x = optional.get_value()
    return x * x
  return -1
optional = tf.experimental.Optional.from_value(5)
print(maybe_square(optional))
tf.Tensor(25, shape=(), dtype=int32)"
"tf.compat.v1.data.experimental.RaggedTensorStructure(
    dtype, shape, ragged_rank
)
",[],[]
"tf.compat.v1.data.experimental.RandomDataset(
    seed=None, name=None
)
","[['A ', 'Dataset', ' of pseudorandom values. (deprecated)'], ['Inherits From: ', 'Dataset', ', ', 'Dataset']]","dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int32, name=None)"
"tf.data.experimental.Reducer(
    init_func, reduce_func, finalize_func
)
",[],"def init_func(_):
  return (0.0, 0.0)

def reduce_func(state, value):
  return (state[0] + value['features'], state[1] + 1)

def finalize_func(s, n):
  return s / n

reducer = tf.data.experimental.Reducer(init_func, reduce_func, finalize_func)
"
"tf.compat.v1.data.experimental.SparseTensorStructure(
    dtype, shape
)
",[],[]
"tf.compat.v1.data.experimental.SqlDataset(
    driver_name, data_source_name, query, output_types
)
","[['A ', 'Dataset', ' consisting of the results from a SQL query.'], ['Inherits From: ', 'Dataset', ', ', 'Dataset']]","dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
dataset.element_spec
TensorSpec(shape=(), dtype=tf.int32, name=None)"
"tf.data.experimental.TFRecordWriter(
    filename, compression_type=None
)
",[],"dataset = tf.data.Dataset.range(3)
dataset = dataset.map(tf.io.serialize_tensor)
writer = tf.data.experimental.TFRecordWriter(""/path/to/file.tfrecord"")
writer.write(dataset)
"
"tf.compat.v1.data.experimental.TensorArrayStructure(
    dtype, element_shape, dynamic_size, infer_shape
)
",[],[]
"tf.compat.v1.data.experimental.TensorStructure(
    dtype, shape
)
",[],[]
"tf.data.experimental.assert_cardinality(
    expected_cardinality
)
",[],"dataset = tf.data.TFRecordDataset(""examples.tfrecord"")
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.UNKNOWN_CARDINALITY).numpy())
True
dataset = dataset.apply(tf.data.experimental.assert_cardinality(42))
print(tf.data.experimental.cardinality(dataset).numpy())
42"
"tf.data.experimental.bucket_by_sequence_length(
    element_length_func,
    bucket_boundaries,
    bucket_batch_sizes,
    padded_shapes=None,
    padding_values=None,
    pad_to_bucket_boundary=False,
    no_padding=False,
    drop_remainder=False
)
","[['A transformation that buckets elements in a ', 'Dataset', ' by length. (deprecated)']]","elements = [
  [0], [1, 2, 3, 4], [5, 6, 7],
  [7, 8, 9, 10, 11], [13, 14, 15, 16, 19, 20], [21, 22]]"
"tf.data.experimental.cardinality(
    dataset
)
","[['Returns the cardinality of ', 'dataset', ', if known.']]","dataset = tf.data.Dataset.range(42)
print(tf.data.experimental.cardinality(dataset).numpy())
42
dataset = dataset.repeat()
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.INFINITE_CARDINALITY).numpy())
True
dataset = dataset.filter(lambda x: True)
cardinality = tf.data.experimental.cardinality(dataset)
print((cardinality == tf.data.experimental.UNKNOWN_CARDINALITY).numpy())
True"
"tf.compat.v1.data.experimental.choose_from_datasets(
    datasets, choice_dataset, stop_on_empty_dataset=False
)
","[['Creates a dataset that deterministically chooses elements from ', 'datasets', '. (deprecated)']]","datasets = [tf.data.Dataset.from_tensors(""foo"").repeat(),
            tf.data.Dataset.from_tensors(""bar"").repeat(),
            tf.data.Dataset.from_tensors(""baz"").repeat()]

# Define a dataset containing `[0, 1, 2, 0, 1, 2, 0, 1, 2]`.
choice_dataset = tf.data.Dataset.range(3).repeat(3)

result = tf.data.experimental.choose_from_datasets(datasets, choice_dataset)
"
"tf.data.experimental.copy_to_device(
    target_device, source_device='/cpu:0'
)
","[['A transformation that copies dataset elements to the given ', 'target_device', '.']]",[]
"tf.data.experimental.dense_to_ragged_batch(
    batch_size,
    drop_remainder=False,
    row_splits_dtype=tf.dtypes.int64
)
","[['A transformation that batches ragged elements into ', 'tf.RaggedTensor', 's.']]","dataset = tf.data.Dataset.from_tensor_slices(np.arange(6))
dataset = dataset.map(lambda x: tf.range(x))
dataset.element_spec.shape
TensorShape([None])
dataset = dataset.apply(
    tf.data.experimental.dense_to_ragged_batch(batch_size=2))
for batch in dataset:
  print(batch)
<tf.RaggedTensor [[], [0]]>
<tf.RaggedTensor [[0, 1], [0, 1, 2]]>
<tf.RaggedTensor [[0, 1, 2, 3], [0, 1, 2, 3, 4]]>"
"tf.data.experimental.dense_to_sparse_batch(
    batch_size, row_shape
)
","[['A transformation that batches ragged elements into ', 'tf.sparse.SparseTensor', 's.']]","# NOTE: The following examples use `{ ... }` to represent the
# contents of a dataset.
a = { ['a', 'b', 'c'], ['a', 'b'], ['a', 'b', 'c', 'd'] }

a.apply(tf.data.experimental.dense_to_sparse_batch(
    batch_size=2, row_shape=[6])) ==
{
    ([[0, 0], [0, 1], [0, 2], [1, 0], [1, 1]],  # indices
     ['a', 'b', 'c', 'a', 'b'],                 # values
     [2, 6]),                                   # dense_shape
    ([[0, 0], [0, 1], [0, 2], [0, 3]],
     ['a', 'b', 'c', 'd'],
     [1, 6])
}
"
"tf.data.experimental.enumerate_dataset(
    start=0
)
",[],"# NOTE: The following examples use `{ ... }` to represent the
# contents of a dataset.
a = { 1, 2, 3 }
b = { (7, 8), (9, 10) }

# The nested structure of the `datasets` argument determines the
# structure of elements in the resulting dataset.
a.apply(tf.data.experimental.enumerate_dataset(start=5))
=> { (5, 1), (6, 2), (7, 3) }
b.apply(tf.data.experimental.enumerate_dataset())
=> { (0, (7, 8)), (1, (9, 10)) }
"
"tf.data.experimental.from_list(
    elements, name=None
)
","[['Creates a ', 'Dataset', ' comprising the given list of elements.']]","dataset = tf.data.experimental.from_list([(1, 'a'), (2, 'b'), (3, 'c')])
list(dataset.as_numpy_iterator())
[(1, b'a'), (2, b'b'), (3, b'c')]"
"tf.data.experimental.from_variant(
    variant, structure
)
",[],[]
"tf.data.experimental.get_next_as_optional(
    iterator
)
","[['Returns a ', 'tf.experimental.Optional', ' with the next element of the iterator. (deprecated)']]",[]
"tf.data.experimental.get_single_element(
    dataset
)
","[['Returns the single element of the ', 'dataset', ' as a nested structure of tensors. (deprecated)']]","def preprocessing_fn(raw_feature):
  # ... the raw_feature is preprocessed as per the use-case
  return feature

raw_features = ...  # input batch of BATCH_SIZE elements.
dataset = (tf.data.Dataset.from_tensor_slices(raw_features)
           .map(preprocessing_fn, num_parallel_calls=BATCH_SIZE)
           .batch(BATCH_SIZE))

processed_features = tf.data.experimental.get_single_element(dataset)
"
"tf.data.experimental.get_structure(
    dataset_or_iterator
)
",[],"dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
tf.data.experimental.get_structure(dataset)
TensorSpec(shape=(), dtype=tf.int32, name=None)"
"tf.data.experimental.group_by_reducer(
    key_func, reducer
)
",[],[]
"tf.data.experimental.group_by_window(
    key_func, reduce_func, window_size=None, window_size_func=None
)
",[],[]
"tf.data.experimental.ignore_errors(
    log_warning=False
)
","[['Creates a ', 'Dataset', ' from another ', 'Dataset', ' and silently ignores any errors. (deprecated)']]","dataset = tf.data.Dataset.from_tensor_slices([1., 2., 0., 4.])

# Computing `tf.debugging.check_numerics(1. / 0.)` will raise an
InvalidArgumentError.
dataset = dataset.map(lambda x: tf.debugging.check_numerics(1. / x, ""error""))

# Using `ignore_errors()` will drop the element that causes an error.
dataset =
    dataset.apply(tf.data.experimental.ignore_errors())  # ==> {1., 0.5, 0.2}
"
"tf.data.experimental.index_table_from_dataset(
    dataset=None,
    num_oov_buckets=0,
    vocab_size=None,
    default_value=-1,
    hasher_spec=lookup_ops.FastHashSpec,
    key_dtype=tf.dtypes.string,
    name=None
)
",[],"ds = tf.data.Dataset.range(100).map(lambda x: tf.strings.as_string(x * 2))
table = tf.data.experimental.index_table_from_dataset(
                                    ds, key_dtype=dtypes.int64)
table.lookup(tf.constant(['0', '2', '4'], dtype=tf.string)).numpy()
array([0, 1, 2])"
"tf.compat.v1.data.experimental.make_batched_features_dataset(
    file_pattern,
    batch_size,
    features,
    reader=None,
    label_key=None,
    reader_args=None,
    num_epochs=None,
    shuffle=True,
    shuffle_buffer_size=10000,
    shuffle_seed=None,
    prefetch_buffer_size=None,
    reader_num_threads=None,
    parser_num_threads=None,
    sloppy_ordering=False,
    drop_final_batch=False
)
","[['Returns a ', 'Dataset', ' of feature dictionaries from ', 'Example', ' protos.']]","serialized_examples = [
  features {
    feature { key: ""age"" value { int64_list { value: [ 0 ] } } }
    feature { key: ""gender"" value { bytes_list { value: [ ""f"" ] } } }
    feature { key: ""kws"" value { bytes_list { value: [ ""code"", ""art"" ] } } }
  },
  features {
    feature { key: ""age"" value { int64_list { value: [] } } }
    feature { key: ""gender"" value { bytes_list { value: [ ""f"" ] } } }
    feature { key: ""kws"" value { bytes_list { value: [ ""sports"" ] } } }
  }
]
"
"tf.compat.v1.data.experimental.make_csv_dataset(
    file_pattern,
    batch_size,
    column_names=None,
    column_defaults=None,
    label_name=None,
    select_columns=None,
    field_delim=',',
    use_quote_delim=True,
    na_value='',
    header=True,
    num_epochs=None,
    shuffle=True,
    shuffle_buffer_size=10000,
    shuffle_seed=None,
    prefetch_buffer_size=None,
    num_parallel_reads=None,
    sloppy=False,
    num_rows_for_inference=100,
    compression_type=None,
    ignore_errors=False,
    encoding='utf-8'
)
",[],"# No label column specified
dataset = tf.data.experimental.make_csv_dataset(filename, batch_size=2)
iterator = dataset.as_numpy_iterator()
print(dict(next(iterator)))
# prints a dictionary of batched features:
# OrderedDict([('Feature_A', array([1, 4], dtype=int32)),
#              ('Feature_B', array([b'a', b'd'], dtype=object))])
"
"tf.data.experimental.make_saveable_from_iterator(
    iterator, external_state_policy=None
)
",[],"with tf.Graph().as_default():
  ds = tf.data.Dataset.range(10)
  iterator = ds.make_initializable_iterator()
  # Build the iterator SaveableObject.
  saveable_obj = tf.data.experimental.make_saveable_from_iterator(iterator)
  # Add the SaveableObject to the SAVEABLE_OBJECTS collection so
  # it can be automatically saved using Saver.
  tf.compat.v1.add_to_collection(tf.GraphKeys.SAVEABLE_OBJECTS, saveable_obj)
  saver = tf.compat.v1.train.Saver()

  while continue_training:
    ... Perform training ...
    if should_save_checkpoint:
      saver.save()
"
"tf.data.experimental.map_and_batch(
    map_func,
    batch_size,
    num_parallel_batches=None,
    drop_remainder=False,
    num_parallel_calls=None
)
","[['Fused implementation of ', 'map', ' and ', 'batch', '. (deprecated)']]",[]
"tf.compat.v1.data.experimental.map_and_batch_with_legacy_function(
    map_func,
    batch_size,
    num_parallel_batches=None,
    drop_remainder=False,
    num_parallel_calls=None
)
","[['Fused implementation of ', 'map', ' and ', 'batch', '. (deprecated)']]",[]
"tf.data.experimental.parallel_interleave(
    map_func,
    cycle_length,
    block_length=1,
    sloppy=False,
    buffer_output_elements=None,
    prefetch_input_elements=None
)
","[['A parallel version of the ', 'Dataset.interleave()', ' transformation. (deprecated)']]","# Preprocess 4 files concurrently.
filenames = tf.data.Dataset.list_files(""/path/to/data/train*.tfrecords"")
dataset = filenames.apply(
    tf.data.experimental.parallel_interleave(
        lambda filename: tf.data.TFRecordDataset(filename),
        cycle_length=4))
"
"tf.data.experimental.parse_example_dataset(
    features, num_parallel_calls=1, deterministic=None
)
","[['A transformation that parses ', 'Example', ' protos into a ', 'dict', ' of tensors.']]",[]
"tf.data.experimental.prefetch_to_device(
    device, buffer_size=None
)
","[['A transformation that prefetches dataset values to the given ', 'device', '.']]",">>> dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])
>>> dataset = dataset.apply(tf.data.experimental.prefetch_to_device(""/cpu:0""))
>>> for element in dataset:
...   print(f'Tensor {element} is on device {element.device}')
Tensor 1 is on device /job:localhost/replica:0/task:0/device:CPU:0
Tensor 2 is on device /job:localhost/replica:0/task:0/device:CPU:0
Tensor 3 is on device /job:localhost/replica:0/task:0/device:CPU:0
"
"tf.data.experimental.rejection_resample(
    class_func, target_dist, initial_dist=None, seed=None
)
",[],[]
"tf.compat.v1.data.experimental.sample_from_datasets(
    datasets, weights=None, seed=None, stop_on_empty_dataset=False
)
","[['Samples elements at random from the datasets in ', 'datasets', '. (deprecated)']]","dataset1 = tf.data.Dataset.range(0, 3)
dataset2 = tf.data.Dataset.range(100, 103)
"
"tf.data.experimental.scan(
    initial_state, scan_func
)
",[],[]
"tf.data.experimental.service.CrossTrainerCache(
    trainer_id
)
",[],"dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=tf.data.experimental.service.ShardingPolicy.OFF,
    service=FLAGS.tf_data_service_address,
    job_name=""job"",
    cross_trainer_cache=data_service_ops.CrossTrainerCache(
        trainer_id=trainer_id())))
"
"tf.data.experimental.service.DispatcherConfig(
    port=0,
    protocol=None,
    work_dir=None,
    fault_tolerant_mode=False,
    worker_addresses=None,
    job_gc_check_interval_ms=None,
    job_gc_timeout_ms=None
)
",[],[]
"tf.data.experimental.service.WorkerConfig(
    dispatcher_address,
    worker_address=None,
    port=0,
    protocol=None,
    heartbeat_interval_ms=None,
    dispatcher_timeout_ms=None,
    data_transfer_protocol=None,
    data_transfer_address=None
)
",[],[]
"tf.data.experimental.service.distribute(
    processing_mode,
    service,
    job_name=None,
    consumer_index=None,
    num_consumers=None,
    max_outstanding_requests=None,
    data_transfer_protocol=None,
    compression='AUTO',
    cross_trainer_cache=None,
    target_workers='AUTO'
)
",[],"dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
# Start two workers
workers = [
    tf.data.experimental.service.WorkerServer(
        tf.data.experimental.service.WorkerConfig(
            dispatcher_address=dispatcher_address)) for _ in range(2)
]
dataset = tf.data.Dataset.range(10)
dataset = dataset.apply(tf.data.experimental.service.distribute(
    processing_mode=""parallel_epochs"", service=dispatcher.target))
print(sorted(list(dataset.as_numpy_iterator())))
[0, 0, 1, 1, 2, 2, 3, 3, 4, 4, 5, 5, 6, 6, 7, 7, 8, 8, 9, 9]"
"tf.data.experimental.service.from_dataset_id(
    processing_mode,
    service,
    dataset_id,
    element_spec=None,
    job_name=None,
    consumer_index=None,
    num_consumers=None,
    max_outstanding_requests=None,
    data_transfer_protocol=None,
    cross_trainer_cache=None,
    target_workers='AUTO'
)
",[],"dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset_id = tf.data.experimental.service.register_dataset(
    dispatcher.target, dataset)
dataset = tf.data.experimental.service.from_dataset_id(
    processing_mode=""parallel_epochs"",
    service=dispatcher.target,
    dataset_id=dataset_id,
    element_spec=dataset.element_spec)
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.service.register_dataset(
    service, dataset, compression='AUTO', dataset_id=None
)
",[],"dispatcher = tf.data.experimental.service.DispatchServer()
dispatcher_address = dispatcher.target.split(""://"")[1]
worker = tf.data.experimental.service.WorkerServer(
    tf.data.experimental.service.WorkerConfig(
        dispatcher_address=dispatcher_address))
dataset = tf.data.Dataset.range(10)
dataset_id = tf.data.experimental.service.register_dataset(
    dispatcher.target, dataset)
dataset = tf.data.experimental.service.from_dataset_id(
    processing_mode=""parallel_epochs"",
    service=dispatcher.target,
    dataset_id=dataset_id,
    element_spec=dataset.element_spec)
print(list(dataset.as_numpy_iterator()))
[0, 1, 2, 3, 4, 5, 6, 7, 8, 9]"
"tf.data.experimental.shuffle_and_repeat(
    buffer_size, count=None, seed=None
)
",[],"d = tf.data.Dataset.from_tensor_slices([1, 2, 3])
d = d.apply(tf.data.experimental.shuffle_and_repeat(2, count=2))
[elem.numpy() for elem in d] # doctest: +SKIP
[2, 3, 1, 1, 3, 2]"
"tf.data.experimental.snapshot(
    path, compression='AUTO', reader_func=None, shard_func=None
)
",[],"dataset = ...
dataset = dataset.enumerate()
dataset = dataset.apply(tf.data.experimental.snapshot(""/path/to/snapshot/dir"",
    shard_func=lambda x, y: x % NUM_SHARDS, ...))
dataset = dataset.map(lambda x, y: y)
"
"tf.data.experimental.table_from_dataset(
    dataset=None,
    num_oov_buckets=0,
    vocab_size=None,
    default_value=None,
    hasher_spec=lookup_ops.FastHashSpec,
    key_dtype=tf.dtypes.string,
    name=None
)
",[],"keys = tf.data.Dataset.range(100)
values = tf.data.Dataset.range(100).map(
    lambda x: tf.strings.as_string(x * 2))
ds = tf.data.Dataset.zip((keys, values))
table = tf.data.experimental.table_from_dataset(
                              ds, default_value='n/a', key_dtype=tf.int64)
table.lookup(tf.constant([0, 1, 2], dtype=tf.int64)).numpy()
array([b'0', b'2', b'4'], dtype=object)"
"tf.data.experimental.take_while(
    predicate
)
","[['A transformation that stops dataset iteration based on a ', 'predicate', '. (deprecated)']]",[]
"tf.data.experimental.to_variant(
    dataset
)
",[],[]
"tf.compat.v1.data.get_output_classes(
    dataset_or_iterator
)
",[],[]
"tf.compat.v1.data.get_output_shapes(
    dataset_or_iterator
)
",[],[]
"tf.compat.v1.data.get_output_types(
    dataset_or_iterator
)
",[],[]
"tf.compat.v1.data.make_initializable_iterator(
    dataset, shared_name=None
)
","[['Creates an iterator for elements of ', 'dataset', '.']]",[]
"tf.compat.v1.data.make_one_shot_iterator(
    dataset
)
","[['Creates an iterator for elements of ', 'dataset', '.']]",[]
"tf.debugging.Assert(
    condition, data, summarize=None, name=None
)
",[],"# Ensure maximum element of x is smaller or equal to 1
assert_op = tf.Assert(tf.less_equal(tf.reduce_max(x), 1.), [x])
with tf.control_dependencies([assert_op]):
  ... code using x ...
"
"tf.compat.v1.verify_tensor_all_finite(
    t=None, msg=None, name=None, x=None, message=None
)
",[],[]
"tf.compat.v1.assert_equal(
    x, y, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x == y', ' holds element-wise.']]","tf.compat.v1.assert_equal(
  x=x, y=y, data=data, summarize=summarize,
  message=message, name=name)
"
"tf.compat.v1.assert_greater(
    x, y, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x > y', ' holds element-wise.']]","tf.compat.v1.assert_greater(
  x=x, y=y, data=data, summarize=summarize,
  message=message, name=name)
"
"tf.compat.v1.assert_greater_equal(
    x, y, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x >= y', ' holds element-wise.']]","tf.compat.v1.assert_greater_equal(
  x=x, y=y, data=data, summarize=summarize,
  message=message, name=name)
"
"tf.compat.v1.assert_integer(
    x, message=None, name=None
)
","[['Assert that ', 'x', ' is of integer dtype.']]","with tf.control_dependencies([tf.compat.v1.assert_integer(x)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_less(
    x, y, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x < y', ' holds element-wise.']]","tf.compat.v1.assert_less(
  x=x, y=y, data=data, summarize=summarize,
  message=message, name=name)
"
"tf.compat.v1.assert_less_equal(
    x, y, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x <= y', ' holds element-wise.']]","tf.compat.v1.assert_less_equal(
  x=x, y=y, data=data, summarize=summarize,
  message=message, name=name)
"
"tf.compat.v1.assert_near(
    x,
    y,
    rtol=None,
    atol=None,
    data=None,
    summarize=None,
    message=None,
    name=None
)
","[['Assert the condition ', 'x', ' and ', 'y', ' are close element-wise.']]","with tf.control_dependencies([tf.compat.v1.assert_near(x, y)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_negative(
    x, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x < 0', ' holds element-wise.']]","with tf.control_dependencies([tf.debugging.assert_negative(x, y)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_non_negative(
    x, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x >= 0', ' holds element-wise.']]","with tf.control_dependencies([tf.debugging.assert_non_negative(x, y)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_non_positive(
    x, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x <= 0', ' holds element-wise.']]","with tf.control_dependencies([tf.debugging.assert_non_positive(x, y)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_none_equal(
    x, y, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x != y', ' holds element-wise.']]","tf.compat.v1.assert_none_equal(
  x=x, y=y, data=data, summarize=summarize,
  message=message, name=name)
"
"tf.compat.v1.assert_positive(
    x, data=None, summarize=None, message=None, name=None
)
","[['Assert the condition ', 'x > 0', ' holds element-wise.']]","with tf.control_dependencies([tf.debugging.assert_positive(x, y)]):
  output = tf.reduce_sum(x)
"
"tf.debugging.assert_proper_iterable(
    values
)
",[],[]
"tf.compat.v1.assert_rank(
    x, rank, data=None, summarize=None, message=None, name=None
)
","[['Assert ', 'x', ' has rank equal to ', 'rank', '.']]","with tf.control_dependencies([tf.compat.v1.assert_rank(x, 2)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_rank_at_least(
    x, rank, data=None, summarize=None, message=None, name=None
)
","[['Assert ', 'x', ' has rank equal to ', 'rank', ' or higher.']]","with tf.control_dependencies([tf.compat.v1.assert_rank_at_least(x, 2)]):
  output = tf.reduce_sum(x)
"
"tf.compat.v1.assert_rank_in(
    x, ranks, data=None, summarize=None, message=None, name=None
)
","[['Assert ', 'x', ' has rank in ', 'ranks', '.']]","with tf.control_dependencies([tf.compat.v1.assert_rank_in(x, (2, 4))]):
  output = tf.reduce_sum(x)
"
"tf.debugging.assert_same_float_dtype(
    tensors=None, dtype=None
)
","[['Validate and return float type based on ', 'tensors', ' and ', 'dtype', '.']]",[]
"tf.compat.v1.assert_scalar(
    tensor, name=None, message=None
)
","[['Asserts that the given ', 'tensor', ' is a scalar (i.e. zero-dimensional).']]",[]
"tf.compat.v1.debugging.assert_shapes(
    shapes, data=None, summarize=None, message=None, name=None
)
",[],"n = 10
q = 3
d = 7
x = tf.zeros([n,q])
y = tf.ones([n,d])
param = tf.Variable([1.0, 2.0, 3.0])
scalar = 1.0
tf.debugging.assert_shapes([
 (x, ('N', 'Q')),
 (y, ('N', 'D')),
 (param, ('Q',)),
 (scalar, ()),
])"
"tf.compat.v1.assert_type(
    tensor, tf_type, message=None, name=None
)
","[['Statically asserts that the given ', 'Tensor', ' is of the specified type.']]",[]
"tf.debugging.check_numerics(
    tensor, message, name=None
)
",[],"a = tf.Variable(1.0)
tf.debugging.check_numerics(a, message='')

b = tf.Variable(np.nan)
try:
  tf.debugging.check_numerics(b, message='Checking b')
except Exception as e:
  assert ""Checking b : Tensor had NaN values"" in e.message

c = tf.Variable(np.inf)
try:
  tf.debugging.check_numerics(c, message='Checking c')
except Exception as e:
  assert ""Checking c : Tensor had Inf values"" in e.message
"
"tf.debugging.enable_check_numerics(
    stack_height_limit=30, path_length_limit=50
)
",[],"tf.config.set_soft_device_placement(True)
tf.debugging.enable_check_numerics()

resolver = tf.distribute.cluster_resolver.TPUClusterResolver(tpu='')
strategy = tf.distribute.TPUStrategy(resolver)
with strategy.scope():
  # ...
"
"tf.debugging.experimental.enable_dump_debug_info(
    dump_root,
    tensor_debug_mode=DEFAULT_TENSOR_DEBUG_MODE,
    circular_buffer_size=1000,
    op_regex=None,
    tensor_dtypes=None
)
","[[None, '\n']]","tf.debugging.experimental.enable_dump_debug_info('/tmp/my-tfdbg-dumps')

# Code to build, train and run your TensorFlow model...
"
"tf.math.is_finite(
    x, name=None
)
",[],"x = tf.constant([5.0, 4.8, 6.8, np.inf, np.nan])
tf.math.is_finite(x) ==> [True, True, True, False, False]
"
"tf.math.is_inf(
    x, name=None
)
",[],"x = tf.constant([5.0, np.inf, 6.8, np.inf])
tf.math.is_inf(x) ==> [False, True, False, True]
"
"tf.math.is_nan(
    x, name=None
)
",[],"x = tf.constant([5.0, np.nan, 6.8, np.nan, np.inf])
tf.math.is_nan(x) ==> [False, True, False, True, False]
"
"tf.math.is_non_decreasing(
    x, name=None
)
","[['Returns ', 'True', ' if ', 'x', ' is non-decreasing.']]","x1 = tf.constant([1.0, 1.0, 3.0])
tf.math.is_non_decreasing(x1)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
x2 = tf.constant([3.0, 1.0, 2.0])
tf.math.is_non_decreasing(x2)
<tf.Tensor: shape=(), dtype=bool, numpy=False>"
"tf.debugging.is_numeric_tensor(
    tensor
)
","[['Returns ', 'True', ' if the elements of ', 'tensor', ' are numbers.']]",[]
"tf.math.is_strictly_increasing(
    x, name=None
)
","[['Returns ', 'True', ' if ', 'x', ' is strictly increasing.']]","x1 = tf.constant([1.0, 2.0, 3.0])
tf.math.is_strictly_increasing(x1)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
x2 = tf.constant([3.0, 1.0, 2.0])
tf.math.is_strictly_increasing(x2)
<tf.Tensor: shape=(), dtype=bool, numpy=False>"
"tf.debugging.set_log_device_placement(
    enabled
)
",[],"tf.debugging.set_log_device_placement(True)
tf.ones([])
# [...] op Fill in device /job:localhost/replica:0/task:0/device:GPU:0
with tf.device(""CPU""):
 tf.ones([])
# [...] op Fill in device /job:localhost/replica:0/task:0/device:CPU:0
tf.debugging.set_log_device_placement(False)"
"tf.io.decode_base64(
    input, name=None
)
",[],[]
"tf.io.decode_compressed(
    bytes, compression_type='', name=None
)
",[],[]
"tf.compat.v1.decode_csv(
    records,
    record_defaults,
    field_delim=',',
    use_quote_delim=True,
    name=None,
    na_value='',
    select_cols=None
)
",[],[]
"tf.io.decode_json_example(
    json_examples, name=None
)
",[],"example = tf.train.Example(
  features=tf.train.Features(
      feature={
          ""a"": tf.train.Feature(
              int64_list=tf.train.Int64List(
                  value=[1, 1, 3]))}))"
"tf.compat.v1.decode_raw(
    input_bytes=None, out_type=None, little_endian=True, name=None, bytes=None
)
",[],[]
"tf.compat.v1.delete_session_tensor(
    handle, name=None
)
",[],[]
"tf.compat.v1.depth_to_space(
    input, block_size, name=None, data_format='NHWC'
)
",[],"x = [[[[1, 2, 3, 4]]]]

"
"tf.quantization.dequantize(
    input,
    min_range,
    max_range,
    mode='MIN_COMBINED',
    name=None,
    axis=None,
    narrow_range=False,
    dtype=tf.dtypes.float32
)
",[],"if T == qint8: in[i] += (range(T) + 1)/ 2.0
out[i] = min_range + (in[i]* (max_range - min_range) / range(T))
"
"tf.io.deserialize_many_sparse(
    serialized_sparse, dtype, rank=None, name=None
)
","[['Deserialize and concatenate ', 'SparseTensors', ' from a serialized minibatch.']]","index = [ 0]
        [10]
        [20]
values = [1, 2, 3]
shape = [50]
"
"tf.compat.v1.device(
    device_name_or_function
)
","[['Wrapper for ', 'Graph.device()', ' using the default graph.']]",[]
"tf.linalg.tensor_diag(
    diagonal, name=None
)
",[],"# 'diagonal' is [1, 2, 3, 4]
tf.diag(diagonal) ==> [[1, 0, 0, 0]
                       [0, 2, 0, 0]
                       [0, 0, 3, 0]
                       [0, 0, 0, 4]]
"
"tf.linalg.tensor_diag_part(
    input, name=None
)
",[],"x = [[[[1111,1112],[1121,1122]],
      [[1211,1212],[1221,1222]]],
     [[[2111, 2112], [2121, 2122]],
      [[2211, 2212], [2221, 2222]]]
     ]
tf.linalg.tensor_diag_part(x)
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[1111, 1212],
       [2121, 2222]], dtype=int32)>
tf.linalg.diag_part(x).shape
TensorShape([2, 2, 2])"
"tf.math.digamma(
    x, name=None
)
",[],[]
"tf.compat.dimension_at_index(
    shape, index
)
",[],"# If you had this in your V1 code:
dim = tensor_shape[i]

# Use `dimension_at_index` as direct replacement compatible with both V1 & V2:
dim = dimension_at_index(tensor_shape, i)

# Another possibility would be this, but WARNING: it only works if the
# tensor_shape instance has a defined rank.
dim = tensor_shape.dims[i]  # `dims` may be None if the rank is undefined!

# In native V2 code, we recommend instead being more explicit:
if tensor_shape.rank is None:
  dim = Dimension(None)
else:
  dim = tensor_shape.dims[i]

# Being more explicit will save you from the following trap (present in V1):
# you might do in-place modifications to `dim` and expect them to be reflected
# in `tensor_shape[i]`, but they would not be (as the Dimension object was
# instantiated on the fly.
"
"tf.compat.dimension_value(
    dimension
)
",[],"# If you had this in your V1 code:
value = tensor_shape[i].value

# Use `dimension_value` as direct replacement compatible with both V1 & V2:
value = dimension_value(tensor_shape[i])

# This would be the V2 equivalent:
value = tensor_shape[i]  # Warning: this will return the dim value in V2!
"
"tf.distribute.HierarchicalCopyAllReduce(
    num_packs=1
)
","[['Inherits From: ', 'CrossDeviceOps']]","  strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.HierarchicalCopyAllReduce())
"
"tf.distribute.InputContext(
    num_input_pipelines=1, input_pipeline_id=0, num_replicas_in_sync=1
)
",[],"get_per_replica_batch_size(
    global_batch_size
)
"
"tf.compat.v1.distribute.MirroredStrategy(
    devices=None, cross_device_ops=None
)
","[['Inherits From: ', 'Strategy']]","strategy = tf.distribute.MirroredStrategy([""GPU:0"", ""GPU:1""])
with strategy.scope():
  x = tf.Variable(1.)
x
MirroredVariable:{
  0: <tf.Variable ... shape=() dtype=float32, numpy=1.0>,
  1: <tf.Variable ... shape=() dtype=float32, numpy=1.0>
}"
"tf.distribute.NcclAllReduce(
    num_packs=1
)
","[['Inherits From: ', 'CrossDeviceOps']]","  strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.NcclAllReduce())
"
"tf.compat.v1.distribute.OneDeviceStrategy(
    device
)
","[['Inherits From: ', 'Strategy']]","tf.enable_eager_execution()
strategy = tf.distribute.OneDeviceStrategy(device=""/gpu:0"")

with strategy.scope():
  v = tf.Variable(1.0)
  print(v.device)  # /job:localhost/replica:0/task:0/device:GPU:0

def step_fn(x):
  return x * 2

result = 0
for i in range(10):
  result += strategy.run(step_fn, args=(i,))
print(result)  # 90
"
"tf.distribute.ReductionToOneDevice(
    reduce_to_device=None, accumulation_fn=None
)
","[['Inherits From: ', 'CrossDeviceOps']]","  strategy = tf.distribute.MirroredStrategy(
    cross_device_ops=tf.distribute.ReductionToOneDevice())
"
"tf.compat.v1.distribute.ReplicaContext(
    strategy, replica_id_in_sync_group
)
",[],"strategy = tf.distribute.MirroredStrategy(['GPU:0', 'GPU:1'])
def func():
  replica_context = tf.distribute.get_replica_context()
  return replica_context.replica_id_in_sync_group
strategy.run(func)
PerReplica:{
  0: <tf.Tensor: shape=(), dtype=int32, numpy=0>,
  1: <tf.Tensor: shape=(), dtype=int32, numpy=1>
}"
"tf.distribute.RunOptions(
    experimental_enable_dynamic_batch_size=True,
    experimental_bucketizing_dynamic_shape=False,
    experimental_xla_options=None
)
","[['Run options for ', 'strategy.run', '.']]",[]
"tf.distribute.Server(
    server_or_cluster_def,
    job_name=None,
    task_index=None,
    protocol=None,
    config=None,
    start=True
)
",[],"server = tf.distribute.Server(...)
with tf.compat.v1.Session(server.target):
  # ...
"
"tf.compat.v1.distribute.Strategy(
    extended
)
",[],"
os.environ['TF_CONFIG'] = json.dumps({
  'cluster': {
      'worker': [""localhost:12345"", ""localhost:23456""],
      'ps': [""localhost:34567""]
  },
  'task': {'type': 'worker', 'index': 0}
})

# This implicitly uses TF_CONFIG for the cluster and current task info.
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

...

if strategy.cluster_resolver.task_type == 'worker':
  # Perform something that's only applicable on workers. Since we set this
  # as a worker above, this block will run on this particular instance.
elif strategy.cluster_resolver.task_type == 'ps':
  # Perform something that's only applicable on parameter servers. Since we
  # set this as a worker above, this block will not run on this particular
  # instance.
"
"tf.compat.v1.distribute.StrategyExtended(
    container_strategy
)
","[['Inherits From: ', 'StrategyExtended']]","batch_reduce_to(
    reduce_op, value_destination_pairs, options=None
)
"
"tf.distribute.cluster_resolver.GCEClusterResolver(
    project,
    zone,
    instance_group,
    port,
    task_type='worker',
    task_id=0,
    rpc_layer='grpc',
    credentials='default',
    service=None
)
","[['Inherits From: ', 'ClusterResolver']]","  # On worker 0
  cluster_resolver = GCEClusterResolver(""my-project"", ""us-west1"",
                                        ""my-instance-group"",
                                        task_type=""worker"", task_id=0)
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)

  # On worker 1
  cluster_resolver = GCEClusterResolver(""my-project"", ""us-west1"",
                                        ""my-instance-group"",
                                        task_type=""worker"", task_id=1)
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)
"
"tf.distribute.cluster_resolver.KubernetesClusterResolver(
    job_to_label_mapping=None,
    tf_server_port=8470,
    rpc_layer='grpc',
    override_client=None
)
","[['Inherits From: ', 'ClusterResolver']]","  # On worker 0
  cluster_resolver = KubernetesClusterResolver(
      {""worker"": [""job-name=worker-cluster-a"", ""job-name=worker-cluster-b""]})
  cluster_resolver.task_type = ""worker""
  cluster_resolver.task_id = 0
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)

  # On worker 1
  cluster_resolver = KubernetesClusterResolver(
      {""worker"": [""job-name=worker-cluster-a"", ""job-name=worker-cluster-b""]})
  cluster_resolver.task_type = ""worker""
  cluster_resolver.task_id = 1
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)
"
"tf.distribute.cluster_resolver.SimpleClusterResolver(
    cluster_spec,
    master='',
    task_type=None,
    task_id=None,
    environment='',
    num_accelerators=None,
    rpc_layer=None
)
","[['Inherits From: ', 'ClusterResolver']]","  cluster = tf.train.ClusterSpec({""worker"": [""worker0.example.com:2222"",
                                             ""worker1.example.com:2222""]})

  # On worker 0
  cluster_resolver = SimpleClusterResolver(cluster, task_type=""worker"",
                                           task_id=0,
                                           num_accelerators={""GPU"": 8},
                                           rpc_layer=""grpc"")
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)

  # On worker 1
  cluster_resolver = SimpleClusterResolver(cluster, task_type=""worker"",
                                           task_id=1,
                                           num_accelerators={""GPU"": 8},
                                           rpc_layer=""grpc"")
  strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy(
      cluster_resolver=cluster_resolver)
"
"tf.distribute.cluster_resolver.SlurmClusterResolver(
    jobs=None,
    port_base=8888,
    gpus_per_node=None,
    gpus_per_task=None,
    tasks_per_node=None,
    auto_set_gpu=True,
    rpc_layer='grpc'
)
","[['Inherits From: ', 'ClusterResolver']]","cluster_spec = tf.train.ClusterSpec({
    ""ps"": [""localhost:2222"", ""localhost:2223""],
    ""worker"": [""localhost:2224"", ""localhost:2225"", ""localhost:2226""]
})

# SimpleClusterResolver is used here for illustration; other cluster
# resolvers may be used for other source of task type/id.
simple_resolver = SimpleClusterResolver(cluster_spec, task_type=""worker"",
                                        task_id=0)

...

if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:
  # Perform something that's only applicable on 'worker' type, id 0. This
  # block will run on this particular instance since we've specified this
  # task to be a 'worker', id 0 in above cluster resolver.
else:
  # Perform something that's only applicable on other ids. This block will
  # not run on this particular instance.
"
"tf.distribute.cluster_resolver.TFConfigClusterResolver(
    task_type=None, task_id=None, rpc_layer=None, environment=None
)
","[['Inherits From: ', 'ClusterResolver']]","  os.environ['TF_CONFIG'] = json.dumps({
    'cluster': {
        'worker': [""localhost:12345"", ""localhost:23456""]
    },
    'task': {'type': 'worker', 'index': 0}
  })
"
"tf.distribute.cluster_resolver.TPUClusterResolver(
    tpu=None,
    zone=None,
    project=None,
    job_name='worker',
    coordinator_name=None,
    coordinator_address=None,
    credentials='default',
    service=None,
    discovery_url=None
)
","[['Inherits From: ', 'ClusterResolver']]","cluster_spec = tf.train.ClusterSpec({
    ""ps"": [""localhost:2222"", ""localhost:2223""],
    ""worker"": [""localhost:2224"", ""localhost:2225"", ""localhost:2226""]
})

# SimpleClusterResolver is used here for illustration; other cluster
# resolvers may be used for other source of task type/id.
simple_resolver = SimpleClusterResolver(cluster_spec, task_type=""worker"",
                                        task_id=0)

...

if cluster_resolver.task_type == 'worker' and cluster_resolver.task_id == 0:
  # Perform something that's only applicable on 'worker' type, id 0. This
  # block will run on this particular instance since we've specified this
  # task to be a 'worker', id 0 in above cluster resolver.
else:
  # Perform something that's only applicable on other ids. This block will
  # not run on this particular instance.
"
"tf.distribute.cluster_resolver.UnionResolver(
    *args, **kwargs
)
","[['Inherits From: ', 'ClusterResolver']]","  cluster_0 = tf.train.ClusterSpec({""worker"": [""worker0.example.com:2222"",
                                               ""worker1.example.com:2222""]})
  cluster_resolver_0 = SimpleClusterResolver(cluster, task_type=""worker"",
                                             task_id=0,
                                             rpc_layer=""grpc"")

  cluster_1 = tf.train.ClusterSpec({""ps"": [""ps0.example.com:2222"",
                                           ""ps1.example.com:2222""]})
  cluster_resolver_1 = SimpleClusterResolver(cluster, task_type=""ps"",
                                             task_id=0,
                                             rpc_layer=""grpc"")

  # Its task type would be ""worker"".
  cluster_resolver = UnionClusterResolver(cluster_resolver_0,
                                          cluster_resolver_1)
"
"tf.compat.v1.distribute.experimental.CentralStorageStrategy(
    compute_devices=None, parameter_device=None
)
","[['Inherits From: ', 'Strategy']]","strategy = tf.distribute.experimental.CentralStorageStrategy()
# Create a dataset
ds = tf.data.Dataset.range(5).batch(2)
# Distribute that dataset
dist_dataset = strategy.experimental_distribute_dataset(ds)

with strategy.scope():
  @tf.function
  def train_step(val):
    return val + 1

  # Iterate over the distributed dataset
  for x in dist_dataset:
    # process dataset elements
    strategy.run(train_step, args=(x,))
"
"tf.distribute.experimental.CollectiveHints(
    bytes_per_pack=0, timeout_seconds=None
)
",[],"hints = tf.distribute.experimental.CollectiveHints(
    bytes_per_pack=50 * 1024 * 1024)
grads = tf.distribute.get_replica_context().all_reduce(
    'sum', grads, experimental_hints=hints)
optimizer.apply_gradients(zip(grads, vars),
    experimental_aggregate_gradients=False)
"
"tf.distribute.experimental.CommunicationOptions(
    bytes_per_pack=0,
    timeout_seconds=None,
    implementation=tf.distribute.experimental.CollectiveCommunication.AUTO
)
",[],"options = tf.distribute.experimental.CommunicationOptions(
    bytes_per_pack=50 * 1024 * 1024,
    timeout_seconds=120.0,
    implementation=tf.distribute.experimental.CommunicationImplementation.NCCL
)
grads = tf.distribute.get_replica_context().all_reduce(
    'sum', grads, options=options)
optimizer.apply_gradients(zip(grads, vars),
    experimental_aggregate_gradients=False)
"
"tf.compat.v1.distribute.experimental.MultiWorkerMirroredStrategy(
    communication=tf.distribute.experimental.CollectiveCommunication.AUTO,
    cluster_resolver=None
)
","[['Inherits From: ', 'Strategy']]","TF_CONFIG = '{""cluster"": {""worker"": [""localhost:12345"", ""localhost:23456""]}, ""task"": {""type"": ""worker"", ""index"": 0} }'
"
"tf.compat.v1.distribute.experimental.ParameterServerStrategy(
    cluster_resolver=None
)
","[['Inherits From: ', 'Strategy']]","strategy = tf.distribute.experimental.ParameterServerStrategy()
run_config = tf.estimator.RunConfig(
    experimental_distribute.train_distribute=strategy)
estimator = tf.estimator.Estimator(config=run_config)
tf.estimator.train_and_evaluate(estimator,...)
"
"tf.compat.v1.distribute.experimental.TPUStrategy(
    tpu_cluster_resolver=None, steps_per_run=None, device_assignment=None
)
","[['Inherits From: ', 'Strategy']]","
os.environ['TF_CONFIG'] = json.dumps({
  'cluster': {
      'worker': [""localhost:12345"", ""localhost:23456""],
      'ps': [""localhost:34567""]
  },
  'task': {'type': 'worker', 'index': 0}
})

# This implicitly uses TF_CONFIG for the cluster and current task info.
strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy()

...

if strategy.cluster_resolver.task_type == 'worker':
  # Perform something that's only applicable on workers. Since we set this
  # as a worker above, this block will run on this particular instance.
elif strategy.cluster_resolver.task_type == 'ps':
  # Perform something that's only applicable on parameter servers. Since we
  # set this as a worker above, this block will not run on this particular
  # instance.
"
"tf.distribute.experimental_set_strategy(
    strategy
)
","[['Set a ', 'tf.distribute.Strategy', ' as current without ', 'with strategy.scope()', '.']]","tf.distribute.experimental_set_strategy(strategy1)
f()
tf.distribute.experimental_set_strategy(strategy2)
g()
tf.distribute.experimental_set_strategy(None)
h()
"
"tf.compat.v1.distributions.Bernoulli(
    logits=None,
    probs=None,
    dtype=tf.dtypes.int32,
    validate_args=False,
    allow_nan_stats=True,
    name='Bernoulli'
)
","[['Inherits From: ', 'Distribution']]","batch_shape_tensor(
    name='batch_shape_tensor'
)
"
"tf.compat.v1.distributions.Beta(
    concentration1=None,
    concentration0=None,
    validate_args=False,
    allow_nan_stats=True,
    name='Beta'
)
","[['Inherits From: ', 'Distribution']]","import tensorflow_probability as tfp
tfd = tfp.distributions

# Create a batch of three Beta distributions.
alpha = [1, 2, 3]
beta = [1, 2, 3]
dist = tfd.Beta(alpha, beta)

dist.sample([4, 5])  # Shape [4, 5, 3]

# `x` has three batch entries, each with two samples.
x = [[.1, .4, .5],
     [.2, .3, .5]]
# Calculate the probability of each pair of samples under the corresponding
# distribution in `dist`.
dist.prob(x)         # Shape [2, 3]
"
"tf.compat.v1.distributions.Categorical(
    logits=None,
    probs=None,
    dtype=tf.dtypes.int32,
    validate_args=False,
    allow_nan_stats=True,
    name='Categorical'
)
","[['Inherits From: ', 'Distribution']]","K <= min(2**31-1, {
  tf.float16: 2**11,
  tf.float32: 2**24,
  tf.float64: 2**53 }[param.dtype])
"
"tf.compat.v1.distributions.Dirichlet(
    concentration,
    validate_args=False,
    allow_nan_stats=True,
    name='Dirichlet'
)
","[['Inherits From: ', 'Distribution']]","import tensorflow_probability as tfp
tfd = tfp.distributions

# Create a single trivariate Dirichlet, with the 3rd class being three times
# more frequent than the first. I.e., batch_shape=[], event_shape=[3].
alpha = [1., 2, 3]
dist = tfd.Dirichlet(alpha)

dist.sample([4, 5])  # shape: [4, 5, 3]

# x has one sample, one batch, three classes:
x = [.2, .3, .5]   # shape: [3]
dist.prob(x)       # shape: []

# x has two samples from one batch:
x = [[.1, .4, .5],
     [.2, .3, .5]]
dist.prob(x)         # shape: [2]

# alpha will be broadcast to shape [5, 7, 3] to match x.
x = [[...]]   # shape: [5, 7, 3]
dist.prob(x)  # shape: [5, 7]
"
"tf.compat.v1.distributions.DirichletMultinomial(
    total_count,
    concentration,
    validate_args=False,
    allow_nan_stats=True,
    name='DirichletMultinomial'
)
","[['Inherits From: ', 'Distribution']]","K <= min(2**31-1, {
  tf.float16: 2**11,
  tf.float32: 2**24,
  tf.float64: 2**53 }[param.dtype])
"
"tf.compat.v1.distributions.Distribution(
    dtype,
    reparameterization_type,
    validate_args,
    allow_nan_stats,
    parameters=None,
    graph_parents=None,
    name=None
)
",[],"@util.AppendDocstring(""Some other details."")
def _log_prob(self, value):
  ...
"
"tf.compat.v1.distributions.Exponential(
    rate,
    validate_args=False,
    allow_nan_stats=True,
    name='Exponential'
)
","[['Inherits From: ', 'Gamma', ', ', 'Distribution']]","Exponential(rate) = Gamma(concentration=1., rate)
"
"tf.compat.v1.distributions.Gamma(
    concentration,
    rate,
    validate_args=False,
    allow_nan_stats=True,
    name='Gamma'
)
","[['Inherits From: ', 'Distribution']]","import tensorflow_probability as tfp
tfd = tfp.distributions

dist = tfd.Gamma(concentration=3.0, rate=2.0)
dist2 = tfd.Gamma(concentration=[3.0, 4.0], rate=[2.0, 3.0])
"
"tf.compat.v1.distributions.Laplace(
    loc,
    scale,
    validate_args=False,
    allow_nan_stats=True,
    name='Laplace'
)
","[['The Laplace distribution with location ', 'loc', ' and ', 'scale', ' parameters.'], ['Inherits From: ', 'Distribution']]","batch_shape_tensor(
    name='batch_shape_tensor'
)
"
"tf.compat.v1.distributions.Multinomial(
    total_count,
    logits=None,
    probs=None,
    validate_args=False,
    allow_nan_stats=True,
    name='Multinomial'
)
","[['Inherits From: ', 'Distribution']]","K <= min(2**31-1, {
  tf.float16: 2**11,
  tf.float32: 2**24,
  tf.float64: 2**53 }[param.dtype])
"
"tf.compat.v1.distributions.Normal(
    loc,
    scale,
    validate_args=False,
    allow_nan_stats=True,
    name='Normal'
)
","[['The Normal distribution with location ', 'loc', ' and ', 'scale', ' parameters.'], ['Inherits From: ', 'Distribution']]","import tensorflow_probability as tfp
tfd = tfp.distributions

# Define a single scalar Normal distribution.
dist = tfd.Normal(loc=0., scale=3.)

# Evaluate the cdf at 1, returning a scalar.
dist.cdf(1.)

# Define a batch of two scalar valued Normals.
# The first has mean 1 and standard deviation 11, the second 2 and 22.
dist = tfd.Normal(loc=[1, 2.], scale=[11, 22.])

# Evaluate the pdf of the first distribution on 0, and the second on 1.5,
# returning a length two tensor.
dist.prob([0, 1.5])

# Get 3 samples, returning a 3 x 2 tensor.
dist.sample([3])
"
"tf.compat.v1.distributions.RegisterKL(
    dist_cls_a, dist_cls_b
)
",[],"__call__(
    kl_fn
)
"
"tf.compat.v1.distributions.ReparameterizationType(
    rep_type
)
",[],"__eq__(
    other
)
"
"tf.compat.v1.distributions.StudentT(
    df,
    loc,
    scale,
    validate_args=False,
    allow_nan_stats=True,
    name='StudentT'
)
","[['Inherits From: ', 'Distribution']]","import tensorflow_probability as tfp
tfd = tfp.distributions

# Define a single scalar Student t distribution.
single_dist = tfd.StudentT(df=3)

# Evaluate the pdf at 1, returning a scalar Tensor.
single_dist.prob(1.)

# Define a batch of two scalar valued Student t's.
# The first has degrees of freedom 2, mean 1, and scale 11.
# The second 3, 2 and 22.
multi_dist = tfd.StudentT(df=[2, 3], loc=[1, 2.], scale=[11, 22.])

# Evaluate the pdf of the first distribution on 0, and the second on 1.5,
# returning a length two tensor.
multi_dist.prob([0, 1.5])

# Get 3 samples, returning a 3 x 2 tensor.
multi_dist.sample(3)
"
"tf.compat.v1.distributions.Uniform(
    low=0.0,
    high=1.0,
    validate_args=False,
    allow_nan_stats=True,
    name='Uniform'
)
","[['Uniform distribution with ', 'low', ' and ', 'high', ' parameters.'], ['Inherits From: ', 'Distribution']]","# Without broadcasting:
u1 = Uniform(low=3.0, high=4.0)  # a single uniform distribution [3, 4]
u2 = Uniform(low=[1.0, 2.0],
             high=[3.0, 4.0])  # 2 distributions [1, 3], [2, 4]
u3 = Uniform(low=[[1.0, 2.0],
                  [3.0, 4.0]],
             high=[[1.5, 2.5],
                   [3.5, 4.5]])  # 4 distributions
"
"tf.compat.v1.distributions.kl_divergence(
    distribution_a, distribution_b, allow_nan_stats=True, name=None
)
",[],[]
"tf.compat.v1.div(
    x, y, name=None
)
",[],[]
"tf.math.divide_no_nan(
    x, y, name=None
)
","[['Computes a safe divide which returns 0 if ', 'y', ' (denominator) is zero.']]","tf.constant(3.0) / 0.0
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
tf.math.divide_no_nan(3.0, 0.0)
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
"tf.math.divide(
    x, y, name=None
)
","[['Computes Python style division of ', 'x', ' by ', 'y', '.']]","x = tf.constant([16, 12, 11])
y = tf.constant([4, 6, 2])
tf.divide(x,y)
<tf.Tensor: shape=(3,), dtype=float64,
numpy=array([4. , 2. , 5.5])>"
"tf.dtypes.as_dtype(
    type_value
)
","[['Converts the given ', 'type_value', ' to a ', 'DType', '.']]",[]
"tf.strings.as_string(
    input,
    precision=-1,
    scientific=False,
    shortest=False,
    width=-1,
    fill='',
    name=None
)
",[],"tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)"
"tf.cast(
    x, dtype, name=None
)
",[],"x = tf.constant([1.8, 2.2], dtype=tf.float32)
tf.cast(x, tf.int32)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([1, 2], dtype=int32)>"
"tf.dtypes.complex(
    real, imag, name=None
)
","[[None, '\n']]","real = tf.constant([2.25, 3.25])
imag = tf.constant([4.75, 5.75])
tf.complex(real, imag)  # [[2.25 + 4.75j], [3.25 + 5.75j]]
"
"tf.dtypes.saturate_cast(
    value, dtype, name=None
)
","[['Performs a safe saturating cast of ', 'value', ' to ', 'dtype', '.']]",[]
"tf.dynamic_partition(
    data, partitions, num_partitions, name=None
)
","[['Partitions ', 'data', ' into ', 'num_partitions', ' tensors using indices from ', 'partitions', '.']]","    outputs[i].shape = [sum(partitions == i)] + data.shape[partitions.ndim:]

    outputs[i] = pack([data[js, ...] for js if partitions[js] == i])
"
"tf.dynamic_stitch(
    indices, data, name=None
)
","[['Interleave the values from the ', 'data', ' tensors into a single tensor.']]","    merged[indices[m][i, ..., j], ...] = data[m][i, ..., j, ...]
"
"tf.edit_distance(
    hypothesis, truth, normalize=True, name='edit_distance'
)
",[],"hypothesis = tf.SparseTensor(
  [[0, 0, 0],
   [1, 0, 0]],
  [""a"", ""b""],
  (2, 1, 1))
truth = tf.SparseTensor(
  [[0, 1, 0],
   [1, 0, 0],
   [1, 0, 1],
   [1, 1, 0]],
   [""a"", ""b"", ""c"", ""a""],
   (2, 2, 2))
tf.edit_distance(hypothesis, truth, normalize=True)
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[inf, 1. ],
       [0.5, 1. ]], dtype=float32)>"
"tf.einsum(
    equation, *inputs, **kwargs
)
","[[None, '\n']]","C[i,k] = sum_j A[i,j] * B[j,k]
"
"tf.compat.v1.enable_eager_execution(
    config=None, device_policy=None, execution_mode=None
)
",[],[]
"tf.io.encode_base64(
    input, pad=False, name=None
)
",[],[]
"tf.ensure_shape(
    x, shape, name=None
)
",[],"x = tf.constant([[1, 2, 3],
                 [4, 5, 6]])
x = tf.ensure_shape(x, [2, 3])"
"tf.math.equal(
    x, y, name=None
)
",[],"x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>"
"tf.math.erf(
    x, name=None
)
","[[None, '\n'], ['Computes the ', 'Gauss error function', ' of ', 'x', ' element-wise. In statistics, for non-negative values of \\(x\\), the error function has the following interpretation: for a random variable \\(Y\\) that is normally distributed with mean 0 and variance \\(1/\\sqrt{2}\\), \\(erf(x)\\) is the probability that \\(Y\\) falls in the range \\([x, x]\\).']]","tf.math.erf([[1.0, 2.0, 3.0], [0.0, -1.0, -2.0]])
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.8427007,  0.9953223,  0.999978 ],
       [ 0.       , -0.8427007, -0.9953223]], dtype=float32)>"
"tf.math.erfc(
    x, name=None
)
","[['Computes the complementary error function of ', 'x', ' element-wise.']]",[]
"tf.errors.AbortedError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.AlreadyExistsError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.CancelledError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.DataLossError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.DeadlineExceededError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.FailedPreconditionError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.InternalError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.InvalidArgumentError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]","tf.reshape([1, 2, 3], (2,))
Traceback (most recent call last):
InvalidArgumentError: ..."
"tf.errors.NotFoundError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.OpError(
    node_def, op, message, error_code, *args
)
",[],[]
"tf.errors.OutOfRangeError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.PermissionDeniedError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.ResourceExhaustedError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.UnauthenticatedError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.UnavailableError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.UnimplementedError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.errors.UnknownError(
    node_def, op, message, *args
)
","[['Inherits From: ', 'OpError']]",[]
"tf.compat.v1.errors.error_code_from_exception_type(
    cls
)
",[],[]
"tf.compat.v1.errors.exception_type_from_error_code(
    error_code
)
",[],[]
"tf.compat.v1.estimator.BaselineClassifier(
    model_dir=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    optimizer='Ftrl',
    config=None,
    loss_reduction=tf.compat.v1.losses.Reduction.SUM
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]","
# Build BaselineClassifier
classifier = tf.estimator.BaselineClassifier(n_classes=3)

# Input builders
def input_fn_train:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass

def input_fn_eval:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass

# Fit model.
classifier.train(input_fn=input_fn_train)

# Evaluate cross entropy between the test and train labels.
loss = classifier.evaluate(input_fn=input_fn_eval)[""loss""]

# predict outputs the probability distribution of the classes as seen in
# training.
predictions = classifier.predict(new_samples)

"
"tf.compat.v1.estimator.BaselineEstimator(
    head, model_dir=None, optimizer='Ftrl', config=None
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]","
# Build baseline multi-label classifier.
estimator = tf.estimator.BaselineEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3))

# Input builders
def input_fn_train:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass

def input_fn_eval:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass

# Fit model.
estimator.train(input_fn=input_fn_train)

# Evaluates cross entropy between the test and train labels.
loss = estimator.evaluate(input_fn=input_fn_eval)[""loss""]

# For each class, predicts the ratio of training examples that contain the
# class.
predictions = estimator.predict(new_samples)

"
"tf.compat.v1.estimator.BaselineRegressor(
    model_dir=None,
    label_dimension=1,
    weight_column=None,
    optimizer='Ftrl',
    config=None,
    loss_reduction=tf.compat.v1.losses.Reduction.SUM
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]","
# Build BaselineRegressor
regressor = tf.estimator.BaselineRegressor()

# Input builders
def input_fn_train:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass

def input_fn_eval:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass

# Fit model.
regressor.train(input_fn=input_fn_train)

# Evaluate squared-loss between the test and train targets.
loss = regressor.evaluate(input_fn=input_fn_eval)[""loss""]

# predict outputs the mean value seen during training.
predictions = regressor.predict(new_samples)
"
"tf.estimator.BestExporter(
    name='best_exporter',
    serving_input_receiver_fn=None,
    event_file_pattern='eval/*.tfevents.*',
    compare_fn=_loss_smaller,
    assets_extra=None,
    as_text=False,
    exports_to_keep=5
)
","[['Inherits From: ', 'Exporter']]","export(
    estimator, export_path, checkpoint_path, eval_result, is_the_final_export
)
"
"tf.estimator.BinaryClassHead(
    weight_column=None,
    thresholds=None,
    label_vocabulary=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    loss_fn=None,
    name=None
)
","[['Creates a ', 'Head', ' for single label binary classification.'], ['Inherits From: ', 'Head']]","head = tf.estimator.BinaryClassHead()
logits = np.array(((45,), (-41,),), dtype=np.float32)
labels = np.array(((1,), (1,),), dtype=np.int32)
features = {'x': np.array(((42,),), dtype=np.float32)}
# expected_loss = sum(cross_entropy(labels, logits)) / batch_size
#               = sum(0, 41) / 2 = 41 / 2 = 20.50
loss = head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
20.50
eval_metrics = head.metrics()
updated_metrics = head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
 print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
  accuracy : 0.50
  accuracy_baseline : 1.00
  auc : 0.00
  auc_precision_recall : 1.00
  average_loss : 20.50
  label/mean : 1.00
  precision : 1.00
  prediction/mean : 0.50
  recall : 0.50
preds = head.predictions(logits)
print(preds['logits'])
tf.Tensor(
  [[ 45.]
   [-41.]], shape=(2, 1), dtype=float32)"
"tf.estimator.CheckpointSaverHook(
    checkpoint_dir,
    save_secs=None,
    save_steps=None,
    saver=None,
    checkpoint_basename='model.ckpt',
    scaffold=None,
    listeners=None,
    save_graph_def=True
)
","[['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.compat.v1.estimator.DNNClassifier(
    hidden_units,
    feature_columns,
    model_dir=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    optimizer='Adagrad',
    activation_fn=tf.nn.relu,
    dropout=None,
    input_layer_partitioner=None,
    config=None,
    warm_start_from=None,
    loss_reduction=tf.compat.v1.losses.Reduction.SUM,
    batch_norm=False
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]","categorical_feature_a = categorical_column_with_hash_bucket(...)
categorical_feature_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNClassifier(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256])

# Or estimator using the ProximalAdagradOptimizer optimizer with
# regularization.
estimator = tf.estimator.DNNClassifier(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=tf.compat.v1.train.ProximalAdagradOptimizer(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

# Or estimator using an optimizer with a learning rate decay.
estimator = tf.estimator.DNNClassifier(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=lambda: tf.keras.optimizers.Adam(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

# Or estimator with warm-starting from a previous checkpoint.
estimator = tf.estimator.DNNClassifier(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    warm_start_from=""/path/to/checkpoint/dir"")

# Input builders
def input_fn_train:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_eval:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_predict:
  # Returns tf.data.Dataset of (x, None) tuple.
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.compat.v1.estimator.DNNEstimator(
    head,
    hidden_units,
    feature_columns,
    model_dir=None,
    optimizer='Adagrad',
    activation_fn=tf.nn.relu,
    dropout=None,
    input_layer_partitioner=None,
    config=None,
    warm_start_from=None,
    batch_norm=False
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]","sparse_feature_a = sparse_column_with_hash_bucket(...)
sparse_feature_b = sparse_column_with_hash_bucket(...)

sparse_feature_a_emb = embedding_column(sparse_id_column=sparse_feature_a,
                                        ...)
sparse_feature_b_emb = embedding_column(sparse_id_column=sparse_feature_b,
                                        ...)

estimator = tf.estimator.DNNEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    hidden_units=[1024, 512, 256])

# Or estimator using the ProximalAdagradOptimizer optimizer with
# regularization.
estimator = tf.estimator.DNNEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=tf.compat.v1.train.ProximalAdagradOptimizer(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

# Or estimator using an optimizer with a learning rate decay.
estimator = tf.estimator.DNNEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=lambda: tf.keras.optimizers.Adam(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

# Or estimator with warm-starting from a previous checkpoint.
estimator = tf.estimator.DNNEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[sparse_feature_a_emb, sparse_feature_b_emb],
    hidden_units=[1024, 512, 256],
    warm_start_from=""/path/to/checkpoint/dir"")

# Input builders
def input_fn_train:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_eval:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_predict:
  # Returns tf.data.Dataset of (x, None) tuple.
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.compat.v1.estimator.DNNLinearCombinedClassifier(
    model_dir=None,
    linear_feature_columns=None,
    linear_optimizer='Ftrl',
    dnn_feature_columns=None,
    dnn_optimizer='Adagrad',
    dnn_hidden_units=None,
    dnn_activation_fn=tf.nn.relu,
    dnn_dropout=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    input_layer_partitioner=None,
    config=None,
    warm_start_from=None,
    loss_reduction=tf.compat.v1.losses.Reduction.SUM,
    batch_norm=False,
    linear_sparse_combiner='sum'
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]","numeric_feature = numeric_column(...)
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)
categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_id_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNLinearCombinedClassifier(
    # wide settings
    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],
    linear_optimizer=tf.keras.optimizers.Ftrl(...),
    # deep settings
    dnn_feature_columns=[
        categorical_feature_a_emb, categorical_feature_b_emb,
        numeric_feature],
    dnn_hidden_units=[1000, 500, 100],
    dnn_optimizer=tf.keras.optimizers.Adagrad(...),
    # warm-start settings
    warm_start_from=""/path/to/checkpoint/dir"")

# To apply L1 and L2 regularization, you can set dnn_optimizer to:
tf.compat.v1.train.ProximalAdagradOptimizer(
    learning_rate=0.1,
    l1_regularization_strength=0.001,
    l2_regularization_strength=0.001)
# To apply learning rate decay, you can set dnn_optimizer to a callable:
lambda: tf.keras.optimizers.Adam(
    learning_rate=tf.compat.v1.train.exponential_decay(
        learning_rate=0.1,
        global_step=tf.compat.v1.train.get_global_step(),
        decay_steps=10000,
        decay_rate=0.96)
# It is the same for linear_optimizer.

# Input builders
def input_fn_train:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_eval:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_predict:
  # Returns tf.data.Dataset of (x, None) tuple.
  pass
estimator.train(input_fn=input_fn_train, steps=100)
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.compat.v1.estimator.DNNLinearCombinedEstimator(
    head,
    model_dir=None,
    linear_feature_columns=None,
    linear_optimizer='Ftrl',
    dnn_feature_columns=None,
    dnn_optimizer='Adagrad',
    dnn_hidden_units=None,
    dnn_activation_fn=tf.nn.relu,
    dnn_dropout=None,
    input_layer_partitioner=None,
    config=None,
    batch_norm=False,
    linear_sparse_combiner='sum'
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]","numeric_feature = numeric_column(...)
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)
categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNLinearCombinedEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    # wide settings
    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],
    linear_optimizer=tf.keras.optimizers.Ftrl(...),
    # deep settings
    dnn_feature_columns=[
        categorical_feature_a_emb, categorical_feature_b_emb,
        numeric_feature],
    dnn_hidden_units=[1000, 500, 100],
    dnn_optimizer=tf.keras.optimizers.Adagrad(...))

# To apply L1 and L2 regularization, you can set dnn_optimizer to:
tf.compat.v1.train.ProximalAdagradOptimizer(
    learning_rate=0.1,
    l1_regularization_strength=0.001,
    l2_regularization_strength=0.001)
# To apply learning rate decay, you can set dnn_optimizer to a callable:
lambda: tf.keras.optimizers.Adam(
    learning_rate=tf.compat.v1.train.exponential_decay(
        learning_rate=0.1,
        global_step=tf.compat.v1.train.get_global_step(),
        decay_steps=10000,
        decay_rate=0.96)
# It is the same for linear_optimizer.

# Input builders
def input_fn_train:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_eval:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_predict:
  # Returns tf.data.Dataset of (x, None) tuple.
  pass
estimator.train(input_fn=input_fn_train, steps=100)
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.compat.v1.estimator.DNNLinearCombinedRegressor(
    model_dir=None,
    linear_feature_columns=None,
    linear_optimizer='Ftrl',
    dnn_feature_columns=None,
    dnn_optimizer='Adagrad',
    dnn_hidden_units=None,
    dnn_activation_fn=tf.nn.relu,
    dnn_dropout=None,
    label_dimension=1,
    weight_column=None,
    input_layer_partitioner=None,
    config=None,
    warm_start_from=None,
    loss_reduction=tf.compat.v1.losses.Reduction.SUM,
    batch_norm=False,
    linear_sparse_combiner='sum'
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]","numeric_feature = numeric_column(...)
categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)
categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNLinearCombinedRegressor(
    # wide settings
    linear_feature_columns=[categorical_feature_a_x_categorical_feature_b],
    linear_optimizer=tf.keras.optimizers.Ftrl(...),
    # deep settings
    dnn_feature_columns=[
        categorical_feature_a_emb, categorical_feature_b_emb,
        numeric_feature],
    dnn_hidden_units=[1000, 500, 100],
    dnn_optimizer=tf.keras.optimizers.Adagrad(...),
    # warm-start settings
    warm_start_from=""/path/to/checkpoint/dir"")

# To apply L1 and L2 regularization, you can set dnn_optimizer to:
tf.compat.v1.train.ProximalAdagradOptimizer(
    learning_rate=0.1,
    l1_regularization_strength=0.001,
    l2_regularization_strength=0.001)
# To apply learning rate decay, you can set dnn_optimizer to a callable:
lambda: tf.keras.optimizers.Adam(
    learning_rate=tf.compat.v1.train.exponential_decay(
        learning_rate=0.1,
        global_step=tf.compat.v1.train.get_global_step(),
        decay_steps=10000,
        decay_rate=0.96)
# It is the same for linear_optimizer.

# Input builders
def input_fn_train:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_eval:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_predict:
  # Returns tf.data.Dataset of (x, None) tuple.
  pass
estimator.train(input_fn=input_fn_train, steps=100)
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.compat.v1.estimator.DNNRegressor(
    hidden_units,
    feature_columns,
    model_dir=None,
    label_dimension=1,
    weight_column=None,
    optimizer='Adagrad',
    activation_fn=tf.nn.relu,
    dropout=None,
    input_layer_partitioner=None,
    config=None,
    warm_start_from=None,
    loss_reduction=tf.compat.v1.losses.Reduction.SUM,
    batch_norm=False
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]","categorical_feature_a = categorical_column_with_hash_bucket(...)
categorical_feature_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_emb = embedding_column(
    categorical_column=categorical_feature_a, ...)
categorical_feature_b_emb = embedding_column(
    categorical_column=categorical_feature_b, ...)

estimator = tf.estimator.DNNRegressor(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256])

# Or estimator using the ProximalAdagradOptimizer optimizer with
# regularization.
estimator = tf.estimator.DNNRegressor(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=tf.compat.v1.train.ProximalAdagradOptimizer(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

# Or estimator using an optimizer with a learning rate decay.
estimator = tf.estimator.DNNRegressor(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    optimizer=lambda: tf.keras.optimizers.Adam(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

# Or estimator with warm-starting from a previous checkpoint.
estimator = tf.estimator.DNNRegressor(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    warm_start_from=""/path/to/checkpoint/dir"")

# Input builders
def input_fn_train:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_eval:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_predict:
  # Returns tf.data.Dataset of (x, None) tuple.
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.compat.v1.estimator.Estimator(
    model_fn, model_dir=None, config=None, params=None, warm_start_from=None
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n']]","estimator = tf.estimator.DNNClassifier(
    feature_columns=[categorical_feature_a_emb, categorical_feature_b_emb],
    hidden_units=[1024, 512, 256],
    warm_start_from=""/path/to/checkpoint/dir"")
"
"tf.estimator.EstimatorSpec(
    mode,
    predictions=None,
    loss=None,
    train_op=None,
    eval_metric_ops=None,
    export_outputs=None,
    training_chief_hooks=None,
    training_hooks=None,
    scaffold=None,
    evaluation_hooks=None,
    prediction_hooks=None
)
","[['Ops and objects returned from a ', 'model_fn', ' and passed to an ', 'Estimator', '.']]",[]
"tf.estimator.EvalSpec(
    input_fn,
    steps=100,
    name=None,
    hooks=None,
    exporters=None,
    start_delay_secs=120,
    throttle_secs=600
)
","[['Configuration for the ""eval"" part for the ', 'train_and_evaluate', ' call.']]",[]
"tf.estimator.FeedFnHook(
    feed_fn
)
","[['Runs ', 'feed_fn', ' and sets the ', 'feed_dict', ' accordingly.'], ['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.estimator.FinalExporter(
    name, serving_input_receiver_fn, assets_extra=None, as_text=False
)
","[['Inherits From: ', 'Exporter']]","export(
    estimator, export_path, checkpoint_path, eval_result, is_the_final_export
)
"
"tf.estimator.FinalOpsHook(
    final_ops, final_ops_feed_dict=None
)
","[['A hook which evaluates ', 'Tensors', ' at the end of a session.'], ['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.estimator.GlobalStepWaiterHook(
    wait_until_step
)
","[['Delays execution until global step reaches ', 'wait_until_step', '.'], ['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.estimator.LatestExporter(
    name,
    serving_input_receiver_fn,
    assets_extra=None,
    as_text=False,
    exports_to_keep=5
)
","[['Inherits From: ', 'Exporter']]","export(
    estimator, export_path, checkpoint_path, eval_result, is_the_final_export
)
"
"tf.compat.v1.estimator.LinearClassifier(
    feature_columns,
    model_dir=None,
    n_classes=2,
    weight_column=None,
    label_vocabulary=None,
    optimizer='Ftrl',
    config=None,
    partitioner=None,
    warm_start_from=None,
    loss_reduction=tf.compat.v1.losses.Reduction.SUM,
    sparse_combiner='sum'
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]","categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)

# Estimator using the default optimizer.
estimator = tf.estimator.LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b])

# Or estimator using the FTRL optimizer with regularization.
estimator = tf.estimator.LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=tf.keras.optimizers.Ftrl(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

# Or estimator using an optimizer with a learning rate decay.
estimator = tf.estimator.LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=lambda: tf.keras.optimizers.Ftrl(
        learning_rate=tf.exponential_decay(
            learning_rate=0.1,
            global_step=tf.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

# Or estimator with warm-starting from a previous checkpoint.
estimator = tf.estimator.LinearClassifier(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    warm_start_from=""/path/to/checkpoint/dir"")


# Input builders
def input_fn_train:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_eval:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_predict:
  # Returns tf.data.Dataset of (x, None) tuple.
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.compat.v1.estimator.LinearEstimator(
    head,
    feature_columns,
    model_dir=None,
    optimizer='Ftrl',
    config=None,
    partitioner=None,
    sparse_combiner='sum',
    warm_start_from=None
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]","categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)

# Estimator using the default optimizer.
estimator = tf.estimator.LinearEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b])

# Or estimator using an optimizer with a learning rate decay.
estimator = tf.estimator.LinearEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=lambda: tf.keras.optimizers.Ftrl(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

# Or estimator using the FTRL optimizer with regularization.
estimator = tf.estimator.LinearEstimator(
    head=tf.estimator.MultiLabelHead(n_classes=3),
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b])
    optimizer=tf.keras.optimizers.Ftrl(
        learning_rate=0.1,
        l1_regularization_strength=0.001
    ))

def input_fn_train:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_eval:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_predict:
  # Returns tf.data.Dataset of (x, None) tuple.
  pass
estimator.train(input_fn=input_fn_train, steps=100)
metrics = estimator.evaluate(input_fn=input_fn_eval, steps=10)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.compat.v1.estimator.LinearRegressor(
    feature_columns,
    model_dir=None,
    label_dimension=1,
    weight_column=None,
    optimizer='Ftrl',
    config=None,
    partitioner=None,
    warm_start_from=None,
    loss_reduction=tf.compat.v1.losses.Reduction.SUM,
    sparse_combiner='sum'
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]","categorical_column_a = categorical_column_with_hash_bucket(...)
categorical_column_b = categorical_column_with_hash_bucket(...)

categorical_feature_a_x_categorical_feature_b = crossed_column(...)

# Estimator using the default optimizer.
estimator = tf.estimator.LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b])

# Or estimator using the FTRL optimizer with regularization.
estimator = tf.estimator.LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=tf.keras.optimizers.Ftrl(
      learning_rate=0.1,
      l1_regularization_strength=0.001
    ))

# Or estimator using an optimizer with a learning rate decay.
estimator = tf.estimator.LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    optimizer=lambda: tf.keras.optimizers.Ftrl(
        learning_rate=tf.compat.v1.train.exponential_decay(
            learning_rate=0.1,
            global_step=tf.compat.v1.train.get_global_step(),
            decay_steps=10000,
            decay_rate=0.96))

# Or estimator with warm-starting from a previous checkpoint.
estimator = tf.estimator.LinearRegressor(
    feature_columns=[categorical_column_a,
                     categorical_feature_a_x_categorical_feature_b],
    warm_start_from=""/path/to/checkpoint/dir"")


# Input builders
def input_fn_train:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_eval:
  # Returns tf.data.Dataset of (x, y) tuple where y represents label's class
  # index.
  pass
def input_fn_predict:
  # Returns tf.data.Dataset of (x, None) tuple.
  pass
estimator.train(input_fn=input_fn_train)
metrics = estimator.evaluate(input_fn=input_fn_eval)
predictions = estimator.predict(input_fn=input_fn_predict)
"
"tf.estimator.LoggingTensorHook(
    tensors, every_n_iter=None, every_n_secs=None, at_end=False, formatter=None
)
","[['Inherits From: ', 'SessionRunHook']]",[]
"tf.estimator.LogisticRegressionHead(
    weight_column=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    name=None
)
","[['Creates a ', 'Head', ' for logistic regression.'], ['Inherits From: ', 'RegressionHead', ', ', 'Head']]","my_head = tf.estimator.LogisticRegressionHead()
my_estimator = tf.estimator.DNNEstimator(
    head=my_head,
    hidden_units=...,
    feature_columns=...)
"
"tf.estimator.MultiClassHead(
    n_classes,
    weight_column=None,
    label_vocabulary=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    loss_fn=None,
    name=None
)
","[['Creates a ', 'Head', ' for multi class classification.'], ['Inherits From: ', 'Head']]","n_classes = 3
head = tf.estimator.MultiClassHead(n_classes)
logits = np.array(((10, 0, 0), (0, 10, 0),), dtype=np.float32)
labels = np.array(((1,), (1,)), dtype=np.int64)
features = {'x': np.array(((42,),), dtype=np.int32)}
# expected_loss = sum(cross_entropy(labels, logits)) / batch_size
#               = sum(10, 0) / 2 = 5.
loss = head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
5.00
eval_metrics = head.metrics()
updated_metrics = head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
  print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
accuracy : 0.50
average_loss : 5.00
preds = head.predictions(logits)
print(preds['logits'])
tf.Tensor(
  [[10.  0.  0.]
   [ 0. 10.  0.]], shape=(2, 3), dtype=float32)"
"tf.estimator.MultiHead(
    heads, head_weights=None
)
","[['Creates a ', 'Head', ' for multi-objective learning.'], ['Inherits From: ', 'Head']]","head1 = tf.estimator.MultiLabelHead(n_classes=2, name='head1')
head2 = tf.estimator.MultiLabelHead(n_classes=3, name='head2')
multi_head = tf.estimator.MultiHead([head1, head2])
logits = {
   'head1': np.array([[-10., 10.], [-15., 10.]], dtype=np.float32),
   'head2': np.array([[20., -20., 20.], [-30., 20., -20.]],
   dtype=np.float32),}
labels = {
   'head1': np.array([[1, 0], [1, 1]], dtype=np.int64),
   'head2': np.array([[0, 1, 0], [1, 1, 0]], dtype=np.int64),}
features = {'x': np.array(((42,),), dtype=np.float32)}
# For large logits, sigmoid cross entropy loss is approximated as:
# loss = labels * (logits < 0) * (-logits) +
#        (1 - labels) * (logits > 0) * logits =>
# head1: expected_unweighted_loss = [[10., 10.], [15., 0.]]
# loss1 = ((10 + 10) / 2 + (15 + 0) / 2) / 2 = 8.75
# head2: expected_unweighted_loss = [[20., 20., 20.], [30., 0., 0]]
# loss2 = ((20 + 20 + 20) / 3 + (30 + 0 + 0) / 3) / 2 = 15.00
# loss = loss1 + loss2 = 8.75 + 15.00 = 23.75
loss = multi_head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
23.75
eval_metrics = multi_head.metrics()
updated_metrics = multi_head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
 print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
auc/head1 : 0.17
auc/head2 : 0.33
auc_precision_recall/head1 : 0.60
auc_precision_recall/head2 : 0.40
average_loss/head1 : 8.75
average_loss/head2 : 15.00
loss/head1 : 8.75
loss/head2 : 15.00
preds = multi_head.predictions(logits)
print(preds[('head1', 'logits')])
tf.Tensor(
  [[-10.  10.]
   [-15.  10.]], shape=(2, 2), dtype=float32)"
"tf.estimator.MultiLabelHead(
    n_classes,
    weight_column=None,
    thresholds=None,
    label_vocabulary=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    loss_fn=None,
    classes_for_class_based_metrics=None,
    name=None
)
","[['Creates a ', 'Head', ' for multi-label classification.'], ['Inherits From: ', 'Head']]","n_classes = 2
head = tf.estimator.MultiLabelHead(n_classes)
logits = np.array([[-1., 1.], [-1.5, 1.5]], dtype=np.float32)
labels = np.array([[1, 0], [1, 1]], dtype=np.int64)
features = {'x': np.array([[41], [42]], dtype=np.int32)}
# expected_loss = sum(_sigmoid_cross_entropy(labels, logits)) / batch_size
#               = sum(1.31326169, 0.9514133) / 2 = 1.13
loss = head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
1.13
eval_metrics = head.metrics()
updated_metrics = head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
 print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
auc : 0.33
auc_precision_recall : 0.77
average_loss : 1.13
preds = head.predictions(logits)
print(preds['logits'])
tf.Tensor(
  [[-1.   1. ]
   [-1.5  1.5]], shape=(2, 2), dtype=float32)"
"tf.estimator.NanLossDuringTrainingError(
    *args, **kwargs
)
",[],[]
"tf.estimator.NanTensorHook(
    loss_tensor, fail_on_nan_loss=True
)
","[['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.estimator.PoissonRegressionHead(
    label_dimension=1,
    weight_column=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    compute_full_loss=True,
    name=None
)
","[['Creates a ', 'Head', ' for poisson regression using ', 'tf.nn.log_poisson_loss', '.'], ['Inherits From: ', 'RegressionHead', ', ', 'Head']]","my_head = tf.estimator.PoissonRegressionHead()
my_estimator = tf.estimator.DNNEstimator(
    head=my_head,
    hidden_units=...,
    feature_columns=...)
"
"tf.estimator.ProfilerHook(
    save_steps=None,
    save_secs=None,
    output_dir='',
    show_dataflow=True,
    show_memory=False
)
","[['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.estimator.RegressionHead(
    label_dimension=1,
    weight_column=None,
    loss_reduction=tf.losses.Reduction.SUM_OVER_BATCH_SIZE,
    loss_fn=None,
    inverse_link_fn=None,
    name=None
)
","[['Creates a ', 'Head', ' for regression using the ', 'mean_squared_error', ' loss.'], ['Inherits From: ', 'Head']]","head = tf.estimator.RegressionHead()
logits = np.array(((45,), (41,),), dtype=np.float32)
labels = np.array(((43,), (44,),), dtype=np.int32)
features = {'x': np.array(((42,),), dtype=np.float32)}
# expected_loss = weighted_loss / batch_size
#               = (43-45)^2 + (44-41)^2 / 2 = 6.50
loss = head.loss(labels, logits, features=features)
print('{:.2f}'.format(loss.numpy()))
6.50
eval_metrics = head.metrics()
updated_metrics = head.update_metrics(
  eval_metrics, features, logits, labels)
for k in sorted(updated_metrics):
 print('{} : {:.2f}'.format(k, updated_metrics[k].result().numpy()))
  average_loss : 6.50
  label/mean : 43.50
  prediction/mean : 43.00
preds = head.predictions(logits)
print(preds['predictions'])
tf.Tensor(
  [[45.]
   [41.]], shape=(2, 1), dtype=float32)"
"tf.estimator.RunConfig(
    model_dir=None,
    tf_random_seed=None,
    save_summary_steps=100,
    save_checkpoints_steps=_USE_DEFAULT,
    save_checkpoints_secs=_USE_DEFAULT,
    session_config=None,
    keep_checkpoint_max=5,
    keep_checkpoint_every_n_hours=10000,
    log_step_count_steps=100,
    train_distribute=None,
    device_fn=None,
    protocol=None,
    eval_distribute=None,
    experimental_distribute=None,
    experimental_max_worker_delay_secs=None,
    session_creation_timeout_secs=7200,
    checkpoint_save_graph_def=True
)
","[['This class specifies the configurations for an ', 'Estimator', ' run.']]","  cluster = {'chief': ['host0:2222'],
             'ps': ['host1:2222', 'host2:2222'],
             'worker': ['host3:2222', 'host4:2222', 'host5:2222']}
"
"tf.estimator.SecondOrStepTimer(
    every_secs=None, every_steps=None
)
",[],"last_triggered_step()
"
"tf.estimator.SessionRunArgs(
    fetches, feed_dict=None, options=None
)
","[['Represents arguments to be added to a ', 'Session.run()', ' call.']]",[]
"tf.estimator.SessionRunContext(
    original_args, session
)
","[['Provides information about the ', 'session.run()', ' call being made.']]","request_stop()
"
"tf.estimator.SessionRunValues(
    results, options, run_metadata
)
","[['Contains the results of ', 'Session.run()', '.']]",[]
"tf.estimator.StepCounterHook(
    every_n_steps=100, every_n_secs=None, output_dir=None, summary_writer=None
)
","[['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.estimator.StopAtStepHook(
    num_steps=None, last_step=None
)
","[['Inherits From: ', 'SessionRunHook']]",[]
"tf.estimator.SummarySaverHook(
    save_steps=None,
    save_secs=None,
    output_dir=None,
    summary_writer=None,
    scaffold=None,
    summary_op=None
)
","[['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.estimator.TrainSpec(
    input_fn, max_steps=None, hooks=None, saving_listeners=None
)
","[['Configuration for the ""train"" part for the ', 'train_and_evaluate', ' call.']]","train_spec = tf.estimator.TrainSpec(
   input_fn=lambda: 1,
   max_steps=100,
   hooks=[_StopAtSecsHook(stop_after_secs=10)],
   saving_listeners=[_NewCheckpointListenerForEvaluate(None, 20, None)])
train_spec.saving_listeners[0]._eval_throttle_secs
20
train_spec.hooks[0]._stop_after_secs
10
train_spec.max_steps
100"
"tf.estimator.WarmStartSettings(
    ckpt_to_initialize_from,
    vars_to_warm_start='.*',
    var_name_to_vocab_info=None,
    var_name_to_prev_var_name=None
)
","[['Settings for warm-starting in ', 'tf.estimator.Estimators', '.']]","emb_vocab_file = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_file(
        ""sc_vocab_file"", ""new_vocab.txt"", vocab_size=100),
    dimension=8)
emb_vocab_list = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_vocabulary_list(
        ""sc_vocab_list"", vocabulary_list=[""a"", ""b""]),
    dimension=8)
estimator = tf.estimator.DNNClassifier(
  hidden_units=[128, 64], feature_columns=[emb_vocab_file, emb_vocab_list],
  warm_start_from=ws)
"
"tf.estimator.add_metrics(
    estimator, metric_fn
)
","[['Creates a new ', 'tf.estimator.Estimator', ' which has given metrics.']]","  def my_auc(labels, predictions):
    auc_metric = tf.keras.metrics.AUC(name=""my_auc"")
    auc_metric.update_state(y_true=labels, y_pred=predictions['logistic'])
    return {'auc': auc_metric}

  estimator = tf.estimator.DNNClassifier(...)
  estimator = tf.estimator.add_metrics(estimator, my_auc)
  estimator.train(...)
  estimator.evaluate(...)
"
"tf.compat.v1.estimator.classifier_parse_example_spec(
    feature_columns,
    label_key,
    label_dtype=tf.dtypes.int64,
    label_default=None,
    weight_column=None
)
",[],"# Define features and transformations
feature_b = tf.feature_column.numeric_column(...)
feature_c_bucketized = tf.feature_column.bucketized_column(
  tf.feature_column.numeric_column(""feature_c""), ...)
feature_a_x_feature_c = tf.feature_column.crossed_column(
    columns=[""feature_a"", feature_c_bucketized], ...)

feature_columns = [feature_b, feature_c_bucketized, feature_a_x_feature_c]
parsing_spec = tf.estimator.classifier_parse_example_spec(
    feature_columns, label_key='my-label', label_dtype=tf.string)

# For the above example, classifier_parse_example_spec would return the dict:
assert parsing_spec == {
  ""feature_a"": parsing_ops.VarLenFeature(tf.string),
  ""feature_b"": parsing_ops.FixedLenFeature([1], dtype=tf.float32),
  ""feature_c"": parsing_ops.FixedLenFeature([1], dtype=tf.float32)
  ""my-label"" : parsing_ops.FixedLenFeature([1], dtype=tf.string)
}
"
"tf.estimator.experimental.InMemoryEvaluatorHook(
    estimator, input_fn, steps=None, hooks=None, name=None, every_n_iter=100
)
","[['Inherits From: ', 'SessionRunHook']]","def train_input_fn():
  ...
  return train_dataset

def eval_input_fn():
  ...
  return eval_dataset

estimator = tf.estimator.DNNClassifier(...)

evaluator = tf.estimator.experimental.InMemoryEvaluatorHook(
    estimator, eval_input_fn)
estimator.train(train_input_fn, hooks=[evaluator])
"
"tf.compat.v1.estimator.experimental.KMeans(
    num_clusters,
    model_dir=None,
    initial_clusters=RANDOM_INIT,
    distance_metric=SQUARED_EUCLIDEAN_DISTANCE,
    seed=None,
    use_mini_batch=True,
    mini_batch_steps_per_iteration=1,
    kmeans_plus_plus_num_retries=2,
    relative_tolerance=None,
    config=None,
    feature_columns=None
)
","[[None, '\n'], ['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]","import numpy as np
import tensorflow as tf

num_points = 100
dimensions = 2
points = np.random.uniform(0, 1000, [num_points, dimensions])

def input_fn():
  return tf.compat.v1.train.limit_epochs(
      tf.convert_to_tensor(points, dtype=tf.float32), num_epochs=1)

num_clusters = 5
kmeans = tf.compat.v1.estimator.experimental.KMeans(
    num_clusters=num_clusters, use_mini_batch=False)

# train
num_iterations = 10
previous_centers = None
for _ in xrange(num_iterations):
  kmeans.train(input_fn)
  cluster_centers = kmeans.cluster_centers()
  if previous_centers is not None:
    print 'delta:', cluster_centers - previous_centers
  previous_centers = cluster_centers
  print 'score:', kmeans.score(input_fn)
print 'cluster centers:', cluster_centers

# map the input points to their clusters
cluster_indices = list(kmeans.predict_cluster_index(input_fn))
for i, point in enumerate(points):
  cluster_index = cluster_indices[i]
  center = cluster_centers[cluster_index]
  print 'point:', point, 'is in cluster', cluster_index, 'centered at', center
"
"tf.estimator.experimental.LinearSDCA(
    example_id_column,
    num_loss_partitions=1,
    num_table_shards=None,
    symmetric_l1_regularization=0.0,
    symmetric_l2_regularization=1.0,
    adaptive=False
)
",[],"real_feature_column = numeric_column(...)
sparse_feature_column = categorical_column_with_hash_bucket(...)
linear_sdca = tf.estimator.experimental.LinearSDCA(
    example_id_column='example_id',
    num_loss_partitions=1,
    num_table_shards=1,
    symmetric_l2_regularization=2.0)
classifier = tf.estimator.LinearClassifier(
    feature_columns=[real_feature_column, sparse_feature_column],
    weight_column=...,
    optimizer=linear_sdca)
classifier.train(input_fn_train, steps=50)
classifier.evaluate(input_fn=input_fn_eval)
"
"tf.estimator.experimental.build_raw_supervised_input_receiver_fn(
    features, labels, default_batch_size=None
)
",[],[]
"tf.estimator.experimental.call_logit_fn(
    logit_fn, features, mode, params, config
)
",[],[]
"tf.compat.v1.estimator.experimental.dnn_logit_fn_builder(
    units,
    hidden_units,
    feature_columns,
    activation_fn,
    dropout,
    input_layer_partitioner,
    batch_norm
)
",[],[]
"tf.compat.v1.estimator.experimental.linear_logit_fn_builder(
    units, feature_columns, sparse_combiner='sum'
)
",[],[]
"tf.estimator.experimental.make_early_stopping_hook(
    estimator, should_stop_fn, run_every_secs=60, run_every_steps=None
)
",[],"estimator = ...
hook = early_stopping.make_early_stopping_hook(
    estimator, should_stop_fn=make_stop_fn(...))
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.experimental.stop_if_higher_hook(
    estimator,
    metric_name,
    threshold,
    eval_dir=None,
    min_steps=0,
    run_every_secs=60,
    run_every_steps=None
)
",[],"estimator = ...
# Hook to stop training if accuracy becomes higher than 0.9.
hook = early_stopping.stop_if_higher_hook(estimator, ""accuracy"", 0.9)
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.experimental.stop_if_lower_hook(
    estimator,
    metric_name,
    threshold,
    eval_dir=None,
    min_steps=0,
    run_every_secs=60,
    run_every_steps=None
)
",[],"estimator = ...
# Hook to stop training if loss becomes lower than 100.
hook = early_stopping.stop_if_lower_hook(estimator, ""loss"", 100)
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.experimental.stop_if_no_decrease_hook(
    estimator,
    metric_name,
    max_steps_without_decrease,
    eval_dir=None,
    min_steps=0,
    run_every_secs=60,
    run_every_steps=None
)
",[],"estimator = ...
# Hook to stop training if loss does not decrease in over 100000 steps.
hook = early_stopping.stop_if_no_decrease_hook(estimator, ""loss"", 100000)
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.experimental.stop_if_no_increase_hook(
    estimator,
    metric_name,
    max_steps_without_increase,
    eval_dir=None,
    min_steps=0,
    run_every_secs=60,
    run_every_steps=None
)
",[],"estimator = ...
# Hook to stop training if accuracy does not increase in over 100000 steps.
hook = early_stopping.stop_if_no_increase_hook(estimator, ""accuracy"", 100000)
train_spec = tf.estimator.TrainSpec(..., hooks=[hook])
tf.estimator.train_and_evaluate(estimator, train_spec, ...)
"
"tf.estimator.export.ClassificationOutput(
    scores=None, classes=None
)
","[['Inherits From: ', 'ExportOutput']]","as_signature_def(
    receiver_tensors
)
"
"tf.estimator.export.EvalOutput(
    loss=None, predictions=None, metrics=None
)
","[['Inherits From: ', 'ExportOutput']]","as_signature_def(
    receiver_tensors
)
"
"tf.estimator.export.PredictOutput(
    outputs
)
","[['Inherits From: ', 'ExportOutput']]","as_signature_def(
    receiver_tensors
)
"
"tf.estimator.export.RegressionOutput(
    value
)
","[['Inherits From: ', 'ExportOutput']]","as_signature_def(
    receiver_tensors
)
"
"tf.estimator.export.ServingInputReceiver(
    features, receiver_tensors, receiver_tensors_alternatives=None
)
",[],[]
"tf.estimator.export.TensorServingInputReceiver(
    features, receiver_tensors, receiver_tensors_alternatives=None
)
",[],[]
"tf.estimator.export.build_parsing_serving_input_receiver_fn(
    feature_spec, default_batch_size=None
)
",[],[]
"tf.estimator.export.build_raw_serving_input_receiver_fn(
    features, default_batch_size=None
)
",[],[]
"tf.compat.v1.estimator.inputs.numpy_input_fn(
    x,
    y=None,
    batch_size=128,
    num_epochs=1,
    shuffle=None,
    queue_capacity=1000,
    num_threads=1
)
",[],"age = np.arange(4) * 1.0
height = np.arange(32, 36)
x = {'age': age, 'height': height}
y = np.arange(-32, -28)

with tf.Session() as session:
  input_fn = numpy_io.numpy_input_fn(
      x, y, batch_size=2, shuffle=False, num_epochs=1)
"
"tf.compat.v1.estimator.inputs.pandas_input_fn(
    x,
    y=None,
    batch_size=128,
    num_epochs=1,
    shuffle=None,
    queue_capacity=1000,
    num_threads=1,
    target_column='target'
)
",[],[]
"tf.compat.v1.estimator.regressor_parse_example_spec(
    feature_columns,
    label_key,
    label_dtype=tf.dtypes.float32,
    label_default=None,
    label_dimension=1,
    weight_column=None
)
",[],"# Define features and transformations
feature_b = tf.feature_column.numeric_column(...)
feature_c_bucketized = tf.feature_column.bucketized_column(
  tf.feature_column.numeric_column(""feature_c""), ...)
feature_a_x_feature_c = tf.feature_column.crossed_column(
    columns=[""feature_a"", feature_c_bucketized], ...)

feature_columns = [feature_b, feature_c_bucketized, feature_a_x_feature_c]
parsing_spec = tf.estimator.regressor_parse_example_spec(
    feature_columns, label_key='my-label')

# For the above example, regressor_parse_example_spec would return the dict:
assert parsing_spec == {
  ""feature_a"": parsing_ops.VarLenFeature(tf.string),
  ""feature_b"": parsing_ops.FixedLenFeature([1], dtype=tf.float32),
  ""feature_c"": parsing_ops.FixedLenFeature([1], dtype=tf.float32)
  ""my-label"" : parsing_ops.FixedLenFeature([1], dtype=tf.float32)
}
"
"tf.compat.v1.estimator.tpu.RunConfig(
    tpu_config=None,
    evaluation_master=None,
    master=None,
    cluster=None,
    **kwargs
)
","[['Inherits From: ', 'RunConfig']]","  cluster = {'chief': ['host0:2222'],
             'ps': ['host1:2222', 'host2:2222'],
             'worker': ['host3:2222', 'host4:2222', 'host5:2222']}
"
"tf.compat.v1.estimator.tpu.TPUConfig(
    iterations_per_loop=2,
    num_shards=None,
    num_cores_per_replica=None,
    per_host_input_for_training=True,
    tpu_job_name=None,
    initial_infeed_sleep_secs=None,
    input_partition_dims=None,
    eval_training_input_configuration=InputPipelineConfig.PER_HOST_V1,
    experimental_host_call_every_n_steps=1,
    experimental_allow_per_host_v2_parallel_get_next=False,
    experimental_feed_hook=None
)
","[['TPU related configuration required by ', 'TPUEstimator', '.']]",[]
"tf.compat.v1.estimator.tpu.TPUEstimator(
    model_fn=None,
    model_dir=None,
    config=None,
    params=None,
    use_tpu=True,
    train_batch_size=None,
    eval_batch_size=None,
    predict_batch_size=None,
    batch_axis=None,
    eval_on_tpu=True,
    export_to_tpu=True,
    export_to_cpu=True,
    warm_start_from=None,
    embedding_config_spec=None,
    export_saved_model_api_version=ExportSavedModelApiVersion.V1
)
","[['Warning:', ' Estimators are not recommended for new code.  Estimators run\n'], ['Inherits From: ', 'Estimator']]",[]
"tf.compat.v1.estimator.tpu.TPUEstimatorSpec(
    mode,
    predictions=None,
    loss=None,
    train_op=None,
    eval_metrics=None,
    export_outputs=None,
    scaffold_fn=None,
    host_call=None,
    training_hooks=None,
    evaluation_hooks=None,
    prediction_hooks=None
)
","[['Ops and objects returned from a ', 'model_fn', ' and passed to ', 'TPUEstimator', '.']]",[]
"tf.compat.v1.estimator.tpu.experimental.EmbeddingConfigSpec(
    feature_columns=None,
    optimization_parameters=None,
    clipping_limit=None,
    pipeline_execution_with_tensor_core=False,
    experimental_gradient_multiplier_fn=None,
    feature_to_config_dict=None,
    table_to_config_dict=None,
    partition_strategy='div',
    profile_data_directory=None
)
",[],[]
"tf.estimator.train_and_evaluate(
    estimator, train_spec, eval_spec
)
","[[None, '\n'], ['Train and evaluate the ', 'estimator', '.']]","# Set up feature columns.
categorial_feature_a = categorial_column_with_hash_bucket(...)
categorial_feature_a_emb = embedding_column(
    categorical_column=categorial_feature_a, ...)
...  # other feature columns

estimator = DNNClassifier(
    feature_columns=[categorial_feature_a_emb, ...],
    hidden_units=[1024, 512, 256])

# Or set up the model directory
#   estimator = DNNClassifier(
#       config=tf.estimator.RunConfig(
#           model_dir='/my_model', save_summary_steps=100),
#       feature_columns=[categorial_feature_a_emb, ...],
#       hidden_units=[1024, 512, 256])

# Input pipeline for train and evaluate.
def train_input_fn(): # returns x, y
  # please shuffle the data.
  pass
def eval_input_fn(): # returns x, y
  pass

train_spec = tf.estimator.TrainSpec(input_fn=train_input_fn, max_steps=1000)
eval_spec = tf.estimator.EvalSpec(input_fn=eval_input_fn)

tf.estimator.train_and_evaluate(estimator, train_spec, eval_spec)
"
"tf.math.exp(
    x, name=None
)
","[[None, '\n']]","x = tf.constant(2.0)
tf.math.exp(x)
<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>"
"tf.compat.v1.expand_dims(
    input, axis=None, name=None, dim=None
)
","[['Returns a tensor with a length 1 axis inserted at index ', 'axis', '. (deprecated arguments)']]","image = tf.zeros([10,10,3])"
"tf.experimental.BatchableExtensionType(
    *args, **kwargs
)
","[['Inherits From: ', 'ExtensionType']]","class Vehicle(tf.experimental.BatchableExtensionType):
  top_speed: tf.Tensor
  mpg: tf.Tensor
batch = Vehicle([120, 150, 80], [30, 40, 12])
tf.map_fn(lambda vehicle: vehicle.top_speed * vehicle.mpg, batch,
          fn_output_signature=tf.int32).numpy()
array([3600, 6000,  960], dtype=int32)"
"tf.experimental.DynamicRaggedShape(
    row_partitions: Sequence[tf.experimental.RowPartition],
    inner_shape: tf.types.experimental.TensorLike,
    dtype: Optional[tf.dtypes.DType] = None,
    validate: bool = False,
    static_inner_shape: ... = None
)
","[['Inherits From: ', 'BatchableExtensionType', ', ', 'ExtensionType']]","@classmethod
from_lengths(
    lengths: Sequence[Union[Sequence[int], int]],
    num_row_partitions=None,
    dtype="
"tf.experimental.DynamicRaggedShape.Spec(
    row_partitions: Tuple[RowPartitionSpec, ...],
    static_inner_shape: tf.TensorShape,
    dtype: tf.dtypes.DType
)
","[['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.experimental.ExtensionType(
    *args, **kwargs
)
","[['Base class for TensorFlow ', 'ExtensionType', ' classes.']]","class MaskedTensor(ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor"
"tf.experimental.RowPartition(
    row_splits,
    row_lengths=None,
    value_rowids=None,
    nrows=None,
    uniform_row_length=None,
    nvals=None,
    internal=False
)
",[],"p1 = RowPartition.from_row_lengths([4, 0, 3, 1, 0])
p2 = RowPartition.from_row_splits([0, 4, 4, 7, 8, 8])
p3 = RowPartition.from_row_starts([0, 4, 4, 7, 8], nvals=8)
p4 = RowPartition.from_row_limits([4, 4, 7, 8, 8])
p5 = RowPartition.from_value_rowids([0, 0, 0, 0, 2, 2, 2, 3], nrows=5)"
"tf.experimental.StructuredTensor(
    fields: Mapping[str, _FieldValue],
    ragged_shape: tf.experimental.DynamicRaggedShape
)
","[['Inherits From: ', 'BatchableExtensionType', ', ', 'ExtensionType']]","# A scalar StructuredTensor describing a single person.
s1 = tf.experimental.StructuredTensor.from_pyval(
    {""age"": 82, ""nicknames"": [""Bob"", ""Bobby""]})
s1.shape
TensorShape([])
s1[""age""]
<tf.Tensor: shape=(), dtype=int32, numpy=82>"
"tf.experimental.StructuredTensor.Spec(
    _fields, _ragged_shape
)
","[['Inherits From: ', 'TypeSpec', ', ', 'TraceType']]","experimental_as_proto() -> struct_pb2.TypeSpecProto
"
"tf.experimental.dispatch_for_api(
    api, *signatures
)
",[],"class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor"
"tf.experimental.dispatch_for_binary_elementwise_apis(
    x_type, y_type
)
",[],"class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor
@dispatch_for_binary_elementwise_apis(MaskedTensor, MaskedTensor)
def binary_elementwise_api_handler(api_func, x, y):
  return MaskedTensor(api_func(x.values, y.values), x.mask & y.mask)
a = MaskedTensor([1, 2, 3, 4, 5], [True, True, True, True, False])
b = MaskedTensor([2, 4, 6, 8, 0], [True, True, True, False, True])
c = tf.add(a, b)
print(f""values={c.values.numpy()}, mask={c.mask.numpy()}"")
values=[ 3 6 9 12 5], mask=[ True True True False False]"
"tf.experimental.dispatch_for_binary_elementwise_assert_apis(
    x_type, y_type
)
",[],"class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor
@dispatch_for_binary_elementwise_assert_apis(MaskedTensor, MaskedTensor)
def binary_elementwise_assert_api_handler(assert_func, x, y):
  merged_mask = tf.logical_and(x.mask, y.mask)
  selected_x_values = tf.boolean_mask(x.values, merged_mask)
  selected_y_values = tf.boolean_mask(y.values, merged_mask)
  assert_func(selected_x_values, selected_y_values)
a = MaskedTensor([1, 1, 0, 1, 1], [False, False, True, True, True])
b = MaskedTensor([2, 2, 0, 2, 2], [True, True, True, False, False])
tf.debugging.assert_equal(a, b) # assert passed; no exception was thrown"
"tf.experimental.dispatch_for_unary_elementwise_apis(
    x_type
)
",[],"class MaskedTensor(tf.experimental.ExtensionType):
  values: tf.Tensor
  mask: tf.Tensor
@dispatch_for_unary_elementwise_apis(MaskedTensor)
def unary_elementwise_api_handler(api_func, x):
  return MaskedTensor(api_func(x.values), x.mask)
mt = MaskedTensor([1, -2, -3], [True, False, True])
abs_mt = tf.abs(mt)
print(f""values={abs_mt.values.numpy()}, mask={abs_mt.mask.numpy()}"")
values=[1 2 3], mask=[ True False True]"
"tf.compat.v1.experimental.output_all_intermediates(
    state
)
",[],[]
"tf.experimental.register_filesystem_plugin(
    plugin_location
)
",[],[]
"tf.experimental.unregister_dispatch_for(
    dispatch_target
)
","[['Unregisters a function that was registered with ', '@dispatch_for_*', '.']]","# Define a type and register a dispatcher to override `tf.abs`:
class MyTensor(tf.experimental.ExtensionType):
  value: tf.Tensor
@tf.experimental.dispatch_for_api(tf.abs)
def my_abs(x: MyTensor):
  return MyTensor(tf.abs(x.value))
tf.abs(MyTensor(5))
MyTensor(value=<tf.Tensor: shape=(), dtype=int32, numpy=5>)"
"tf.math.expm1(
    x, name=None
)
","[['Computes ', 'exp(x) - 1', ' element-wise.']]","  x = tf.constant(2.0)
  tf.math.expm1(x) ==> 6.389056

  x = tf.constant([2.0, 8.0])
  tf.math.expm1(x) ==> array([6.389056, 2979.958], dtype=float32)

  x = tf.constant(1 + 1j)
  tf.math.expm1(x) ==> (0.46869393991588515+2.2873552871788423j)
"
"tf.compat.v1.extract_image_patches(
    images,
    ksizes=None,
    strides=None,
    rates=None,
    padding=None,
    name=None,
    sizes=None
)
","[['Extract ', 'patches', ' from ', 'images', ' and put them in the ""depth"" output dimension.']]",[]
"tf.extract_volume_patches(
    input, ksizes, strides, padding, name=None
)
","[['Extract ', 'patches', ' from ', 'input', ' and put them in the ', '""depth""', ' output dimension. 3D extension of ', 'extract_image_patches', '.']]","ksizes = [1, ksize_planes, ksize_rows, ksize_cols, 1]
strides = [1, stride_planes, strides_rows, strides_cols, 1]
"
"tf.eye(
    num_rows,
    num_columns=None,
    batch_shape=None,
    dtype=tf.dtypes.float32,
    name=None
)
",[],"# Construct one identity matrix.
tf.eye(2)
==> [[1., 0.],
     [0., 1.]]

# Construct a batch of 3 identity matrices, each 2 x 2.
# batch_identity[i, :, :] is a 2 x 2 identity matrix, i = 0, 1, 2.
batch_identity = tf.eye(2, batch_shape=[3])

# Construct one 2 x 3 ""identity"" matrix
tf.eye(2, num_columns=3)
==> [[ 1.,  0.,  0.],
     [ 0.,  1.,  0.]]
"
"tf.quantization.fake_quant_with_min_max_args(
    inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_args_gradient(
    gradients, inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_vars(
    inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_vars_gradient(
    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_vars_per_channel(
    inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient(
    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.feature_column.bucketized_column(
    source_column, boundaries
)
","[['Represents discretized dense input bucketed by ', 'boundaries', '.']]","boundaries = [0, 10, 100]
input tensor = [[-5, 10000]
                [150,   10]
                [5,    100]]
"
"tf.feature_column.categorical_column_with_hash_bucket(
    key,
    hash_bucket_size,
    dtype=tf.dtypes.string
)
",[],"import tensorflow as tf
keywords = tf.feature_column.categorical_column_with_hash_bucket(""keywords"",
10000)
columns = [keywords]
features = {'keywords': tf.constant([['Tensorflow', 'Keras', 'RNN', 'LSTM',
'CNN'], ['LSTM', 'CNN', 'Tensorflow', 'Keras', 'RNN'], ['CNN', 'Tensorflow',
'LSTM', 'Keras', 'RNN']])}
linear_prediction, _, _ = tf.compat.v1.feature_column.linear_model(features,
columns)

# or
import tensorflow as tf
keywords = tf.feature_column.categorical_column_with_hash_bucket(""keywords"",
10000)
keywords_embedded = tf.feature_column.embedding_column(keywords, 16)
columns = [keywords_embedded]
features = {'keywords': tf.constant([['Tensorflow', 'Keras', 'RNN', 'LSTM',
'CNN'], ['LSTM', 'CNN', 'Tensorflow', 'Keras', 'RNN'], ['CNN', 'Tensorflow',
'LSTM', 'Keras', 'RNN']])}
input_layer = tf.keras.layers.DenseFeatures(columns)
dense_tensor = input_layer(features)
"
"tf.feature_column.categorical_column_with_identity(
    key, num_buckets, default_value=None
)
","[['A ', 'CategoricalColumn', ' that returns identity values.']]","import tensorflow as tf
video_id = tf.feature_column.categorical_column_with_identity(
    key='video_id', num_buckets=1000000, default_value=0)
columns = [video_id]
features = {'video_id': tf.sparse.from_dense([[2, 85, 0, 0, 0],
[33,78, 2, 73, 1]])}
linear_prediction = tf.compat.v1.feature_column.linear_model(features,
columns)
"
"tf.compat.v1.feature_column.categorical_column_with_vocabulary_file(
    key,
    vocabulary_file,
    vocabulary_size=None,
    num_oov_buckets=0,
    default_value=None,
    dtype=tf.dtypes.string
)
","[['A ', 'CategoricalColumn', ' with a vocabulary file.']]","import tensorflow as tf
states = tf.feature_column.categorical_column_with_vocabulary_file(
  key='states', vocabulary_file='states.txt', vocabulary_size=5,
  num_oov_buckets=1)
columns = [states]
features = {'states':tf.constant([['california', 'georgia', 'michigan',
'texas', 'new york'], ['new york', 'georgia', 'california', 'michigan',
'texas']])}
linear_prediction = tf.compat.v1.feature_column.linear_model(features,
columns)
"
"tf.feature_column.categorical_column_with_vocabulary_list(
    key, vocabulary_list, dtype=None, default_value=-1, num_oov_buckets=0
)
","[['A ', 'CategoricalColumn', ' with in-memory vocabulary.']]","colors = categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('R', 'G', 'B', 'Y'),
    num_oov_buckets=2)
columns = [colors, ...]
features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
linear_prediction, _, _ = linear_model(features, columns)
"
"tf.feature_column.crossed_column(
    keys, hash_bucket_size, hash_key=None
)
",[]," shape = [2, 2]
{
    [0, 0]: Hash64(""d"", Hash64(""a"")) % hash_bucket_size
    [1, 0]: Hash64(""e"", Hash64(""b"")) % hash_bucket_size
    [1, 1]: Hash64(""e"", Hash64(""c"")) % hash_bucket_size
}
"
"tf.feature_column.embedding_column(
    categorical_column,
    dimension,
    combiner='mean',
    initializer=None,
    ckpt_to_load_from=None,
    tensor_name_in_ckpt=None,
    max_norm=None,
    trainable=True,
    use_safe_embedding_lookup=True
)
","[['DenseColumn', ' that converts from sparse, categorical input.']]","video_id = categorical_column_with_identity(
    key='video_id', num_buckets=1000000, default_value=0)
columns = [embedding_column(video_id, 9),...]

estimator = tf.estimator.DNNClassifier(feature_columns=columns, ...)

label_column = ...
def input_fn():
  features = tf.io.parse_example(
      ..., features=make_parse_example_spec(columns + [label_column]))
  labels = features.pop(label_column.name)
  return features, labels

estimator.train(input_fn=input_fn, steps=100)
"
"tf.feature_column.indicator_column(
    categorical_column
)
",[],"name = indicator_column(categorical_column_with_vocabulary_list(
    'name', ['bob', 'george', 'wanda']))
columns = [name, ...]
features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)

dense_tensor == [[1, 0, 0]]  # If ""name"" bytes_list is [""bob""]
dense_tensor == [[1, 0, 1]]  # If ""name"" bytes_list is [""bob"", ""wanda""]
dense_tensor == [[2, 0, 0]]  # If ""name"" bytes_list is [""bob"", ""bob""]
"
"tf.compat.v1.feature_column.input_layer(
    features,
    feature_columns,
    weight_collections=None,
    trainable=True,
    cols_to_vars=None,
    cols_to_output_tensors=None
)
","[['Returns a dense ', 'Tensor', ' as input layer based on given ', 'feature_columns', '.']]","price = numeric_column('price')
keywords_embedded = embedding_column(
    categorical_column_with_hash_bucket(""keywords"", 10K), dimensions=16)
columns = [price, keywords_embedded, ...]
features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
dense_tensor = input_layer(features, columns)
for units in [128, 64, 32]:
  dense_tensor = tf.compat.v1.layers.dense(dense_tensor, units, tf.nn.relu)
prediction = tf.compat.v1.layers.dense(dense_tensor, 1)
"
"tf.compat.v1.feature_column.linear_model(
    features,
    feature_columns,
    units=1,
    sparse_combiner='sum',
    weight_collections=None,
    trainable=True,
    cols_to_vars=None
)
","[['Returns a linear prediction ', 'Tensor', ' based on given ', 'feature_columns', '.']]","  shape = [2, 2]
  {
      [0, 0]: ""a""
      [1, 0]: ""b""
      [1, 1]: ""c""
  }
"
"tf.compat.v1.feature_column.make_parse_example_spec(
    feature_columns
)
",[],"# Define features and transformations
feature_a = categorical_column_with_vocabulary_file(...)
feature_b = numeric_column(...)
feature_c_bucketized = bucketized_column(numeric_column(""feature_c""), ...)
feature_a_x_feature_c = crossed_column(
    columns=[""feature_a"", feature_c_bucketized], ...)

feature_columns = set(
    [feature_b, feature_c_bucketized, feature_a_x_feature_c])
features = tf.io.parse_example(
    serialized=serialized_examples,
    features=make_parse_example_spec(feature_columns))
"
"tf.feature_column.numeric_column(
    key,
    shape=(1,),
    default_value=None,
    dtype=tf.dtypes.float32,
    normalizer_fn=None
)
",[],"data = {'a': [15, 9, 17, 19, 21, 18, 25, 30],
   'b': [5.0, 6.4, 10.5, 13.6, 15.7, 19.9, 20.3 , 0.0]}"
"tf.feature_column.sequence_categorical_column_with_hash_bucket(
    key,
    hash_bucket_size,
    dtype=tf.dtypes.string
)
",[],"tokens = sequence_categorical_column_with_hash_bucket(
    'tokens', hash_bucket_size=1000)
tokens_embedding = embedding_column(tokens, dimension=10)
columns = [tokens_embedding]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.feature_column.sequence_categorical_column_with_identity(
    key, num_buckets, default_value=None
)
",[],"watches = sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = embedding_column(watches, dimension=10)
columns = [watches_embedding]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.feature_column.sequence_categorical_column_with_vocabulary_file(
    key,
    vocabulary_file,
    vocabulary_size=None,
    num_oov_buckets=0,
    default_value=None,
    dtype=tf.dtypes.string
)
",[],"states = sequence_categorical_column_with_vocabulary_file(
    key='states', vocabulary_file='/us/states.txt', vocabulary_size=50,
    num_oov_buckets=5)
states_embedding = embedding_column(states, dimension=10)
columns = [states_embedding]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.feature_column.sequence_categorical_column_with_vocabulary_list(
    key, vocabulary_list, dtype=None, default_value=-1, num_oov_buckets=0
)
",[],"colors = sequence_categorical_column_with_vocabulary_list(
    key='colors', vocabulary_list=('R', 'G', 'B', 'Y'),
    num_oov_buckets=2)
colors_embedding = embedding_column(colors, dimension=3)
columns = [colors_embedding]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.feature_column.sequence_numeric_column(
    key,
    shape=(1,),
    default_value=0.0,
    dtype=tf.dtypes.float32,
    normalizer_fn=None
)
",[],"temperature = sequence_numeric_column('temperature')
columns = [temperature]

features = tf.io.parse_example(..., features=make_parse_example_spec(columns))
sequence_feature_layer = SequenceFeatures(columns)
sequence_input, sequence_length = sequence_feature_layer(features)
sequence_length_mask = tf.sequence_mask(sequence_length)

rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.compat.v1.feature_column.shared_embedding_columns(
    categorical_columns,
    dimension,
    combiner='mean',
    initializer=None,
    shared_embedding_collection_name=None,
    ckpt_to_load_from=None,
    tensor_name_in_ckpt=None,
    max_norm=None,
    trainable=True,
    use_safe_embedding_lookup=True
)
",[],"watched_video_id = categorical_column_with_vocabulary_file(
    'watched_video_id', video_vocabulary_file, video_vocabulary_size)
impression_video_id = categorical_column_with_vocabulary_file(
    'impression_video_id', video_vocabulary_file, video_vocabulary_size)
columns = shared_embedding_columns(
    [watched_video_id, impression_video_id], dimension=10)

estimator = tf.estimator.DNNClassifier(feature_columns=columns, ...)

label_column = ...
def input_fn():
  features = tf.io.parse_example(
      ..., features=make_parse_example_spec(columns + [label_column]))
  labels = features.pop(label_column.name)
  return features, labels

estimator.train(input_fn=input_fn, steps=100)
"
"tf.feature_column.weighted_categorical_column(
    categorical_column,
    weight_feature_key,
    dtype=tf.dtypes.float32
)
","[['Applies weight values to a ', 'CategoricalColumn', '.']]","[
  features {
    feature {
      key: ""terms""
      value {bytes_list {value: ""very"" value: ""model""} }
    }
    feature {
      key: ""frequencies""
      value {float_list {value: 0.3 value: 0.1} }
    }
  },
  features {
    feature {
      key: ""terms""
      value {bytes_list {value: ""when"" value: ""course"" value: ""human""} }
    }
    feature {
      key: ""frequencies""
      value {float_list {value: 0.4 value: 0.1 value: 0.2} }
    }
  }
]
"
"tf.signal.fft(
    input, name=None
)
",[],[]
"tf.signal.fft2d(
    input, name=None
)
",[],[]
"tf.signal.fft3d(
    input, name=None
)
",[],[]
"tf.fill(
    dims, value, name=None
)
",[],"tf.fill([2, 3], 9)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[9, 9, 9],
       [9, 9, 9]], dtype=int32)>"
"tf.fingerprint(
    data, method='farmhash64', name=None
)
",[],"tf.fingerprint(data) == tf.fingerprint(tf.reshape(data, ...))
tf.fingerprint(data) == tf.fingerprint(tf.bitcast(data, ...))
"
"tf.compat.v1.fixed_size_partitioner(
    num_shards, axis=0
)
",[],"  x = tf.compat.v1.get_variable(
    ""x"", shape=(2,), partitioner=tf.compat.v1.fixed_size_partitioner(2)
  )
"
"tf.compat.v1.flags.BaseListParser(
    token=None, name=None
)
","[['Inherits From: ', 'ArgumentParser']]","super().__init__(token, name)
"
"tf.compat.v1.flags.BooleanFlag(
    name, default, help, short_name=None, **args
)
","[['Inherits From: ', 'Flag']]","flag_type()
"
"tf.compat.v1.flags.CsvListSerializer(
    list_sep
)
","[['Inherits From: ', 'ArgumentSerializer']]","serialize(
    value
)
"
"tf.compat.v1.flags.DEFINE(
    parser,
    name,
    default,
    help,
    flag_values=_flagvalues.FLAGS,
    serializer=None,
    module_name=None,
    required=False,
    **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_alias(
    name, original_name, flag_values=_flagvalues.FLAGS, module_name=None
)
",[],[]
"tf.compat.v1.flags.DEFINE_bool(
    name,
    default,
    help,
    flag_values=_flagvalues.FLAGS,
    module_name=None,
    required=False,
    **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_bool(
    name,
    default,
    help,
    flag_values=_flagvalues.FLAGS,
    module_name=None,
    required=False,
    **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_enum(
    name,
    default,
    enum_values,
    help,
    flag_values=_flagvalues.FLAGS,
    module_name=None,
    required=False,
    **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_enum_class(
    name,
    default,
    enum_class,
    help,
    flag_values=_flagvalues.FLAGS,
    module_name=None,
    case_sensitive=False,
    required=False,
    **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_flag(
    flag, flag_values=_flagvalues.FLAGS, module_name=None, required=False
)
","[['Registers a :class:', 'Flag', ' object with a :class:', 'FlagValues', ' object.']]",[]
"tf.compat.v1.flags.DEFINE_float(
    name,
    default,
    help,
    lower_bound=None,
    upper_bound=None,
    flag_values=_flagvalues.FLAGS,
    required=False,
    **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_integer(
    name,
    default,
    help,
    lower_bound=None,
    upper_bound=None,
    flag_values=_flagvalues.FLAGS,
    required=False,
    **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_list(
    name, default, help, flag_values=_flagvalues.FLAGS, required=False, **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_multi(
    parser,
    serializer,
    name,
    default,
    help,
    flag_values=_flagvalues.FLAGS,
    module_name=None,
    required=False,
    **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_multi_enum(
    name,
    default,
    enum_values,
    help,
    flag_values=_flagvalues.FLAGS,
    case_sensitive=True,
    required=False,
    **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_multi_enum_class(
    name,
    default,
    enum_class,
    help,
    flag_values=_flagvalues.FLAGS,
    module_name=None,
    case_sensitive=False,
    required=False,
    **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_multi_float(
    name,
    default,
    help,
    lower_bound=None,
    upper_bound=None,
    flag_values=_flagvalues.FLAGS,
    required=False,
    **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_multi_integer(
    name,
    default,
    help,
    lower_bound=None,
    upper_bound=None,
    flag_values=_flagvalues.FLAGS,
    required=False,
    **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_multi_string(
    name, default, help, flag_values=_flagvalues.FLAGS, required=False, **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_spaceseplist(
    name,
    default,
    help,
    comma_compat=False,
    flag_values=_flagvalues.FLAGS,
    required=False,
    **args
)
",[],[]
"tf.compat.v1.flags.DEFINE_string(
    name, default, help, flag_values=_flagvalues.FLAGS, required=False, **args
)
",[],[]
"tf.compat.v1.flags.EnumClassFlag(
    name,
    default,
    help,
    enum_class,
    short_name=None,
    case_sensitive=False,
    **args
)
","[['Inherits From: ', 'Flag']]","flag_type()
"
"tf.compat.v1.flags.EnumClassListSerializer(
    list_sep, **kwargs
)
","[['A serializer for :class:', 'MultiEnumClass', ' flags.'], ['Inherits From: ', 'ListSerializer', ', ', 'ArgumentSerializer']]","serialize(
    value
)
"
"tf.compat.v1.flags.EnumClassParser(
    enum_class, case_sensitive=True
)
","[['Inherits From: ', 'ArgumentParser']]","flag_type()
"
"tf.compat.v1.flags.EnumClassSerializer(
    lowercase
)
","[['Inherits From: ', 'ArgumentSerializer']]","serialize(
    value
)
"
"tf.compat.v1.flags.EnumFlag(
    name,
    default,
    help,
    enum_values,
    short_name=None,
    case_sensitive=True,
    **args
)
","[['Inherits From: ', 'Flag']]","flag_type()
"
"tf.compat.v1.flags.EnumParser(
    enum_values, case_sensitive=True
)
","[['Inherits From: ', 'ArgumentParser']]","flag_type()
"
"tf.compat.v1.flags.FLAGS(
    *args, **kwargs
)
","[['Registry of :class:', '~absl.flags.Flag', ' objects.']]"," FLAGS['longname'] = x   # register a new flag
"
"tf.compat.v1.flags.Flag(
    parser,
    serializer,
    name,
    default,
    help_string,
    short_name=None,
    boolean=False,
    allow_override=False,
    allow_override_cpp=False,
    allow_hide_cpp=False,
    allow_overwrite=True,
    allow_using_method_names=False
)
",[],"flag_type()
"
"tf.compat.v1.flags.FlagHolder(
    flag_values, flag, ensure_non_none_value=False
)
",[],"flags.DEFINE_integer('foo', ...)
flags.DEFINE_integer('bar', ...)

def method():
  # prints parsed value of 'bar' flag
  print(flags.FLAGS.foo)
  # runtime error due to typo or possibly bad coding style.
  print(flags.FLAGS.baz)
"
"tf.compat.v1.flags.FloatParser(
    lower_bound=None, upper_bound=None
)
","[['Inherits From: ', 'ArgumentParser']]","convert(
    argument
)
"
"tf.compat.v1.flags.IntegerParser(
    lower_bound=None, upper_bound=None
)
","[['Inherits From: ', 'ArgumentParser']]","convert(
    argument
)
"
"tf.compat.v1.flags.ListSerializer(
    list_sep
)
","[['Inherits From: ', 'ArgumentSerializer']]","serialize(
    value
)
"
"tf.compat.v1.flags.MultiEnumClassFlag(
    name, default, help_string, enum_class, case_sensitive=False, **args
)
","[['Inherits From: ', 'MultiFlag', ', ', 'Flag']]","flag_type()
"
"tf.compat.v1.flags.MultiFlag(
    *args, **kwargs
)
","[['Inherits From: ', 'Flag']]","flag_type()
"
"tf.compat.v1.flags.UnrecognizedFlagError(
    flagname, flagvalue='', suggestions=None
)
","[['Inherits From: ', 'Error']]",[]
"tf.compat.v1.flags.WhitespaceSeparatedListParser(
    comma_compat=False
)
","[['Inherits From: ', 'BaseListParser', ', ', 'ArgumentParser']]","flag_type()
"
"tf.compat.v1.flags.adopt_module_key_flags(
    module, flag_values=_flagvalues.FLAGS
)
",[],[]
"tf.compat.v1.flags.declare_key_flag(
    flag_name, flag_values=_flagvalues.FLAGS
)
",[],"flags.declare_key_flag('flag_1')
"
"tf.compat.v1.flags.doc_to_help(
    doc
)
","[['Takes a ', 'doc', ' string and reformats it as help.']]",[]
"tf.compat.v1.flags.flag_dict_to_args(
    flag_map, multi_flags=None
)
",[],[]
"tf.compat.v1.flags.mark_bool_flags_as_mutual_exclusive(
    flag_names, required=False, flag_values=_flagvalues.FLAGS
)
",[],[]
"tf.compat.v1.flags.mark_flag_as_required(
    flag_name, flag_values=_flagvalues.FLAGS
)
",[],"if __name__ == '__main__':
  flags.mark_flag_as_required('your_flag_name')
  app.run()
"
"tf.compat.v1.flags.mark_flags_as_mutual_exclusive(
    flag_names, required=False, flag_values=_flagvalues.FLAGS
)
",[],[]
"tf.compat.v1.flags.mark_flags_as_required(
    flag_names, flag_values=_flagvalues.FLAGS
)
",[],"if __name__ == '__main__':
  flags.mark_flags_as_required(['flag1', 'flag2', 'flag3'])
  app.run()
"
"tf.compat.v1.flags.multi_flags_validator(
    flag_names,
    message='Flag validation failed',
    flag_values=_flagvalues.FLAGS
)
",[],"@flags.multi_flags_validator(['foo', 'bar'])
def _CheckFooBar(flags_dict):
  ...
"
"tf.compat.v1.flags.register_multi_flags_validator(
    flag_names,
    multi_flags_checker,
    message='Flags validation failed',
    flag_values=_flagvalues.FLAGS
)
",[],[]
"tf.compat.v1.flags.register_validator(
    flag_name,
    checker,
    message='Flag validation failed',
    flag_values=_flagvalues.FLAGS
)
",[],[]
"tf.compat.v1.flags.set_default(
    flag_holder, value
)
",[],[]
"tf.compat.v1.flags.text_wrap(
    text, length=None, indent='', firstline_indent=None
)
",[],[]
"tf.compat.v1.flags.validator(
    flag_name,
    message='Flag validation failed',
    flag_values=_flagvalues.FLAGS
)
",[],"@flags.validator('foo')
def _CheckFoo(foo):
  ...
"
"tf.math.floor(
    x, name=None
)
",[],"x = tf.constant([1.3324, -1.5, 5.555, -2.532, 0.99, float(""inf"")])
tf.floor(x).numpy()
array([ 1., -2.,  5., -3.,  0., inf], dtype=float32)"
"tf.compat.v1.floor_div(
    x, y, name=None
)
",[],[]
"tf.math.floordiv(
    x, y, name=None
)
","[['Divides ', 'x / y', ' elementwise, rounding toward the most negative integer.']]",[]
"tf.math.floormod(
    x, y, name=None
)
",[],[]
"tf.compat.v1.foldl(
    fn,
    elems,
    initializer=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    name=None
)
","[['foldl on the list of tensors unpacked from ', 'elems', ' on dimension 0.']]","elems = tf.constant([1, 2, 3, 4, 5, 6])
sum = foldl(lambda a, x: a + x, elems)
# sum == 21
"
"tf.compat.v1.foldr(
    fn,
    elems,
    initializer=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    name=None
)
","[['foldr on the list of tensors unpacked from ', 'elems', ' on dimension 0.']]","elems = [1, 2, 3, 4, 5, 6]
sum = foldr(lambda a, x: a + x, elems)
# sum == 21
"
"tf.function(
    func=None,
    input_signature=None,
    autograph=True,
    jit_compile=None,
    reduce_retracing=False,
    experimental_implements=None,
    experimental_autograph_options=None,
    experimental_relax_shapes=None,
    experimental_compile=None,
    experimental_follow_type_hints=None
) -> tf.types.experimental.GenericFunction
",[],"@tf.function
def f(x, y):
  return x ** 2 + y
x = tf.constant([2, 3])
y = tf.constant([3, -2])
f(x, y)
<tf.Tensor: ... numpy=array([7, 7], ...)>"
"tf.compat.v1.gather(
    params, indices, validate_indices=None, name=None, axis=None, batch_dims=0
)
","[['Gather slices from params axis ', 'axis', ' according to indices. (deprecated arguments)']]","params = tf.constant(['p0', 'p1', 'p2', 'p3', 'p4', 'p5'])
params[3].numpy()
b'p3'
tf.gather(params, 3).numpy()
b'p3'"
"tf.compat.v1.gather_nd(
    params, indices, name=None, batch_dims=0
)
","[['Gather slices from ', 'params', ' into a Tensor with shape specified by ', 'indices', '.']]","tf.gather_nd(
    indices=[[0, 0],
             [1, 1]],
    params = [['a', 'b'],
              ['c', 'd']]).numpy()
array([b'a', b'd'], dtype=object)"
"tf.compat.v1.get_collection(
    key, scope=None
)
","[['Wrapper for ', 'Graph.get_collection()', ' using the default graph.']]",[]
"tf.compat.v1.get_collection_ref(
    key
)
","[['Wrapper for ', 'Graph.get_collection_ref()', ' using the default graph.']]",[]
"tf.compat.v1.get_local_variable(
    name,
    shape=None,
    dtype=None,
    initializer=None,
    regularizer=None,
    trainable=False,
    collections=None,
    caching_device=None,
    partitioner=None,
    validate_shape=True,
    use_resource=None,
    custom_getter=None,
    constraint=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE
)
","[['Gets an existing ', 'local', ' variable or creates a new one.']]",[]
"tf.compat.v1.get_seed(
    op_seed
)
",[],[]
"tf.compat.v1.get_session_handle(
    data, name=None
)
","[['Return the handle of ', 'data', '.']]","c = tf.multiply(a, b)
h = tf.compat.v1.get_session_handle(c)
h = sess.run(h)

p, a = tf.compat.v1.get_session_tensor(h.handle, tf.float32)
b = tf.multiply(a, 10)
c = sess.run(b, feed_dict={p: h.handle})
"
"tf.compat.v1.get_session_tensor(
    handle, dtype, name=None
)
","[['Get the tensor of type ', 'dtype', ' by feeding a tensor handle.']]","c = tf.multiply(a, b)
h = tf.compat.v1.get_session_handle(c)
h = sess.run(h)

p, a = tf.compat.v1.get_session_tensor(h.handle, tf.float32)
b = tf.multiply(a, 10)
c = sess.run(b, feed_dict={p: h.handle})
"
"tf.get_static_value(
    tensor, partial=False
)
",[],"a = tf.constant(10)
tf.get_static_value(a)
10
b = tf.constant(20)
tf.get_static_value(tf.add(a, b))
30"
"tf.compat.v1.get_variable(
    name,
    shape=None,
    dtype=None,
    initializer=None,
    regularizer=None,
    trainable=None,
    collections=None,
    caching_device=None,
    partitioner=None,
    validate_shape=True,
    use_resource=None,
    custom_getter=None,
    constraint=None,
    synchronization=tf.VariableSynchronization.AUTO,
    aggregation=tf.compat.v1.VariableAggregation.NONE
)
",[],[]
"tf.compat.v1.gfile.Copy(
    oldpath, newpath, overwrite=False
)
","[['Copies data from ', 'src', ' to ', 'dst', '.']]","with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.gfile.exists(""/tmp/x"")
True
tf.io.gfile.copy(""/tmp/x"", ""/tmp/y"")
tf.io.gfile.exists(""/tmp/y"")
True
tf.io.gfile.remove(""/tmp/y"")"
"tf.compat.v1.gfile.DeleteRecursively(
    dirname
)
",[],[]
"tf.compat.v1.gfile.Exists(
    filename
)
",[],"with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.gfile.exists(""/tmp/x"")
True"
"tf.compat.v1.gfile.FastGFile(
    name, mode='r'
)
",[],"close()
"
"tf.io.gfile.GFile(
    name, mode='r'
)
",[],"with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
with tf.io.gfile.GFile(""/tmp/x"") as f:
  f.read()
'asdf'"
"tf.compat.v1.gfile.Glob(
    filename
)
",[],[]
"tf.compat.v1.gfile.IsDirectory(
    dirname
)
",[],[]
"tf.compat.v1.gfile.ListDirectory(
    dirname
)
",[],[]
"tf.compat.v1.gfile.MakeDirs(
    dirname
)
",[],[]
"tf.compat.v1.gfile.MkDir(
    dirname
)
","[['Creates a directory with the name ', 'dirname', '.']]",[]
"tf.io.gfile.GFile(
    name, mode='r'
)
",[],"with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
with tf.io.gfile.GFile(""/tmp/x"") as f:
  f.read()
'asdf'"
"tf.compat.v1.gfile.Remove(
    filename
)
",[],[]
"tf.compat.v1.gfile.Rename(
    oldname, newname, overwrite=False
)
",[],[]
"tf.compat.v1.gfile.Stat(
    filename
)
",[],[]
"tf.compat.v1.gfile.Walk(
    top, in_order=True
)
",[],[]
"tf.linalg.global_norm(
    t_list, name=None
)
",[],[]
"tf.compat.v1.global_variables(
    scope=None
)
",[],[]
"tf.compat.v1.keras.initializers.glorot_normal(
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'VarianceScaling']]","@classmethod
from_config(
    config
)
"
"tf.compat.v1.keras.initializers.glorot_uniform(
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'VarianceScaling']]","@classmethod
from_config(
    config
)
"
"tf.grad_pass_through(
    f
)
",[],"x = tf.Variable(1.0, name=""x"")
z = tf.Variable(3.0, name=""z"")

with tf.GradientTape() as tape:
  # y will evaluate to 9.0
  y = tf.grad_pass_through(x.assign)(z**2)
# grads will evaluate to 6.0
grads = tape.gradient(y, z)
"
"tf.compat.v1.gradients(
    ys,
    xs,
    grad_ys=None,
    name='gradients',
    colocate_gradients_with_ops=False,
    gate_gradients=False,
    aggregation_method=None,
    stop_gradients=None,
    unconnected_gradients=tf.UnconnectedGradients.NONE
)
","[['Constructs symbolic derivatives of sum of ', 'ys', ' w.r.t. x in ', 'xs', '.']]","a = tf.constant(0.)
b = 2 * a
g = tf.gradients(a + b, [a, b], stop_gradients=[a, b])
"
"tf.compat.v1.graph_util.convert_variables_to_constants(
    sess,
    input_graph_def,
    output_node_names,
    variable_names_whitelist=None,
    variable_names_blacklist=None
)
",[],[]
"tf.compat.v1.graph_util.extract_sub_graph(
    graph_def, dest_nodes
)
",[],[]
"tf.graph_util.import_graph_def(
    graph_def,
    input_map=None,
    return_elements=None,
    name=None,
    op_dict=None,
    producer_op_list=None
)
","[['Imports the graph from ', 'graph_def', ' into the current default ', 'Graph', '. (deprecated arguments)']]",[]
"tf.compat.v1.graph_util.must_run_on_cpu(
    node, pin_variables_on_cpu=False
)
",[],[]
"tf.compat.v1.graph_util.remove_training_nodes(
    input_graph, protected_nodes=None
)
",[],[]
"tf.compat.v1.graph_util.tensor_shape_from_node_def_name(
    graph, input_name
)
",[],[]
"tf.math.greater(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5, 2, 5])
tf.math.greater(x, y) ==> [False, True, True]

x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.greater(x, y) ==> [False, False, True]
"
"tf.math.greater_equal(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6, 7])
y = tf.constant([5, 2, 5, 10])
tf.math.greater_equal(x, y) ==> [True, True, True, False]

x = tf.constant([5, 4, 6, 7])
y = tf.constant([5])
tf.math.greater_equal(x, y) ==> [True, False, True, True]
"
"tf.group(
    *inputs, **kwargs
)
",[],"with tf.control_dependencies([a, b]):
    c = tf.no_op()
"
"tf.guarantee_const(
    input, name=None
)
",[],[]
"tf.compat.v1.hessians(
    ys,
    xs,
    name='hessians',
    colocate_gradients_with_ops=False,
    gate_gradients=False,
    aggregation_method=None
)
","[['Constructs the Hessian of sum of ', 'ys', ' with respect to ', 'x', ' in ', 'xs', '.']]",[]
"tf.histogram_fixed_width(
    values,
    value_range,
    nbins=100,
    dtype=tf.dtypes.int32,
    name=None
)
",[],"# Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]
hist = tf.histogram_fixed_width(new_values, value_range, nbins=5)
hist.numpy()
array([2, 1, 1, 0, 2], dtype=int32)"
"tf.histogram_fixed_width_bins(
    values,
    value_range,
    nbins=100,
    dtype=tf.dtypes.int32,
    name=None
)
",[],"# Bins will be:  (-inf, 1), [1, 2), [2, 3), [3, 4), [4, inf)
nbins = 5
value_range = [0.0, 5.0]
new_values = [-1.0, 0.0, 1.5, 2.0, 5.0, 15]
indices = tf.histogram_fixed_width_bins(new_values, value_range, nbins=5)
indices.numpy()
array([0, 0, 1, 2, 4, 4], dtype=int32)"
"tf.identity(
    input, name=None
)
",[],"a = tf.constant([0.78])
a_identity = tf.identity(a)
a.numpy()
array([0.78], dtype=float32)
a_identity.numpy()
array([0.78], dtype=float32)"
"tf.identity_n(
    input, name=None
)
",[],"with tf.get_default_graph().gradient_override_map(
    {'IdentityN': 'OverrideGradientWithG'}):
  y, _ = identity_n([f(x), x])

@tf.RegisterGradient('OverrideGradientWithG')
def ApplyG(op, dy, _):
  return [None, g(dy)]  # Do not backprop to f(x).
"
"tf.signal.ifft(
    input, name=None
)
",[],[]
"tf.signal.ifft2d(
    input, name=None
)
",[],[]
"tf.signal.ifft3d(
    input, name=None
)
",[],[]
"tf.math.igamma(
    a, x, name=None
)
","[[None, '\n'], ['Compute the lower regularized incomplete Gamma function ', 'P(a, x)', '.']]",[]
"tf.math.igammac(
    a, x, name=None
)
","[[None, '\n'], ['Compute the upper regularized incomplete Gamma function ', 'Q(a, x)', '.']]",[]
"tf.math.imag(
    input, name=None
)
",[],"x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
tf.math.imag(x)  # [4.75, 5.75]
"
"tf.image.adjust_brightness(
    image, delta
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_brightness(x, delta=0.1)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.1,  2.1,  3.1],
        [ 4.1,  5.1,  6.1]],
       [[ 7.1,  8.1,  9.1],
        [10.1, 11.1, 12.1]]], dtype=float32)>"
"tf.image.adjust_contrast(
    images, contrast_factor
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_contrast(x, 2.)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[-3.5, -2.5, -1.5],
        [ 2.5,  3.5,  4.5]],
       [[ 8.5,  9.5, 10.5],
        [14.5, 15.5, 16.5]]], dtype=float32)>"
"tf.image.adjust_gamma(
    image, gamma=1, gain=1
)
","[['Performs ', 'Gamma Correction', '.']]","x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_gamma(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[1.       , 1.1486983, 1.2457309],
        [1.319508 , 1.3797297, 1.4309691]],
       [[1.4757731, 1.5157166, 1.5518456],
        [1.5848932, 1.6153942, 1.6437519]]], dtype=float32)>"
"tf.image.adjust_hue(
    image, delta, name=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_hue(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 2.3999996,  1.       ,  3.       ],
        [ 5.3999996,  4.       ,  6.       ]],
      [[ 8.4      ,  7.       ,  9.       ],
        [11.4      , 10.       , 12.       ]]], dtype=float32)>"
"tf.image.adjust_jpeg_quality(
    image, jpeg_quality, name=None
)
",[],"x = [[[0.01, 0.02, 0.03],
      [0.04, 0.05, 0.06]],
     [[0.07, 0.08, 0.09],
      [0.10, 0.11, 0.12]]]
x_jpeg = tf.image.adjust_jpeg_quality(x, 75)
x_jpeg.numpy()
array([[[0.00392157, 0.01960784, 0.03137255],
        [0.02745098, 0.04313726, 0.05490196]],
       [[0.05882353, 0.07450981, 0.08627451],
        [0.08235294, 0.09803922, 0.10980393]]], dtype=float32)"
"tf.image.adjust_saturation(
    image, saturation_factor, name=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.adjust_saturation(x, 0.5)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 2. ,  2.5,  3. ],
        [ 5. ,  5.5,  6. ]],
       [[ 8. ,  8.5,  9. ],
        [11. , 11.5, 12. ]]], dtype=float32)>"
"tf.image.central_crop(
    image, central_fraction
)
",[]," --------
|        |
|  XXXX  |
|  XXXX  |
|        |   where ""X"" is the central 50% of the image.
 --------
"
"tf.image.combined_non_max_suppression(
    boxes,
    scores,
    max_output_size_per_class,
    max_total_size,
    iou_threshold=0.5,
    score_threshold=float('-inf'),
    pad_per_class=False,
    clip_boxes=True,
    name=None
)
",[],[]
"tf.image.convert_image_dtype(
    image, dtype, saturate=False, name=None
)
","[['Convert ', 'image', ' to ', 'dtype', ', scaling its values if needed.']]","x = [[[1, 2, 3], [4, 5, 6]],
     [[7, 8, 9], [10, 11, 12]]]
x_int8 = tf.convert_to_tensor(x, dtype=tf.int8)
tf.image.convert_image_dtype(x_int8, dtype=tf.float16, saturate=False)
<tf.Tensor: shape=(2, 2, 3), dtype=float16, numpy=
array([[[0.00787, 0.01575, 0.02362],
        [0.0315 , 0.03937, 0.04724]],
       [[0.0551 , 0.063  , 0.07086],
        [0.07874, 0.0866 , 0.0945 ]]], dtype=float16)>"
"tf.compat.v1.image.crop_and_resize(
    image,
    boxes,
    box_ind=None,
    crop_size=None,
    method='bilinear',
    extrapolation_value=0,
    name=None,
    box_indices=None
)
",[],[]
"tf.image.crop_to_bounding_box(
    image, offset_height, offset_width, target_height, target_width
)
","[['Crops an ', 'image', ' to a specified bounding box.']]","image = tf.constant(np.arange(1, 28, dtype=np.float32), shape=[3, 3, 3])
image[:,:,0] # print the first channel of the 3-D tensor
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.,  4.,  7.],
       [10., 13., 16.],
       [19., 22., 25.]], dtype=float32)>
cropped_image = tf.image.crop_to_bounding_box(image, 0, 0, 2, 2)
cropped_image[:,:,0] # print the first channel of the cropped 3-D tensor
<tf.Tensor: shape=(2, 2), dtype=float32, numpy=
array([[ 1.,  4.],
       [10., 13.]], dtype=float32)>"
"tf.io.decode_and_crop_jpeg(
    contents,
    crop_window,
    channels=0,
    ratio=1,
    fancy_upscaling=True,
    try_recover_truncated=False,
    acceptable_fraction=1,
    dct_method='',
    name=None
)
",[],[]
"tf.io.decode_bmp(
    contents, channels=0, name=None
)
",[],[]
"tf.io.decode_gif(
    contents, name=None
)
","[[None, '\n']]","convert \\(src.gif -coalesce \\)dst.gif
"
"tf.io.decode_image(
    contents,
    channels=None,
    dtype=tf.dtypes.uint8,
    name=None,
    expand_animations=True
)
","[['Function for ', 'decode_bmp', ', ', 'decode_gif', ', ', 'decode_jpeg', ', and ', 'decode_png', '.']]",[]
"tf.io.decode_jpeg(
    contents,
    channels=0,
    ratio=1,
    fancy_upscaling=True,
    try_recover_truncated=False,
    acceptable_fraction=1,
    dct_method='',
    name=None
)
",[],[]
"tf.io.decode_png(
    contents,
    channels=0,
    dtype=tf.dtypes.uint8,
    name=None
)
",[],[]
"tf.compat.v1.image.draw_bounding_boxes(
    images, boxes, name=None, colors=None
)
",[],"# create an empty image
img = tf.zeros([1, 3, 3, 3])
# draw a box around the image
box = np.array([0, 0, 1, 1])
boxes = box.reshape([1, 1, 4])
# alternate between red and blue
colors = np.array([[1.0, 0.0, 0.0], [0.0, 0.0, 1.0]])
tf.image.draw_bounding_boxes(img, boxes, colors)
<tf.Tensor: shape=(1, 3, 3, 3), dtype=float32, numpy=
array([[[[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]],
        [[1., 0., 0.],
        [0., 0., 0.],
        [1., 0., 0.]],
        [[1., 0., 0.],
        [1., 0., 0.],
        [1., 0., 0.]]]], dtype=float32)>"
"tf.io.encode_jpeg(
    image,
    format='',
    quality=95,
    progressive=False,
    optimize_size=False,
    chroma_downsampling=True,
    density_unit='in',
    x_density=300,
    y_density=300,
    xmp_metadata='',
    name=None
)
",[],[]
"tf.io.encode_png(
    image, compression=-1, name=None
)
",[],[]
"tf.compat.v1.image.extract_glimpse(
    input,
    size,
    offsets,
    centered=True,
    normalized=True,
    uniform_noise=True,
    name=None
)
",[],"x = [[[[0.0],
          [1.0],
          [2.0]],
         [[3.0],
          [4.0],
          [5.0]],
         [[6.0],
          [7.0],
          [8.0]]]]
tf.compat.v1.image.extract_glimpse(x, size=(2, 2), offsets=[[1, 1]],
                                   centered=False, normalized=False)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
array([[[[0.],
         [1.]],
        [[3.],
         [4.]]]], dtype=float32)>"
"tf.compat.v1.extract_image_patches(
    images,
    ksizes=None,
    strides=None,
    rates=None,
    padding=None,
    name=None,
    sizes=None
)
","[['Extract ', 'patches', ' from ', 'images', ' and put them in the ""depth"" output dimension.']]",[]
"tf.io.extract_jpeg_shape(
    contents,
    output_type=tf.dtypes.int32,
    name=None
)
",[],[]
"tf.image.extract_patches(
    images, sizes, strides, rates, padding, name=None
)
","[['Extract ', 'patches', ' from ', 'images', '.']]","  n = 10
  # images is a 1 x 10 x 10 x 1 array that contains the numbers 1 through 100
  images = [[[[x * n + y + 1] for y in range(n)] for x in range(n)]]

  # We generate two outputs as follows:
  # 1. 3x3 patches with stride length 5
  # 2. Same as above, but the rate is increased to 2
  tf.image.extract_patches(images=images,
                           sizes=[1, 3, 3, 1],
                           strides=[1, 5, 5, 1],
                           rates=[1, 1, 1, 1],
                           padding='VALID')

  # Yields:
  [[[[ 1  2  3 11 12 13 21 22 23]
     [ 6  7  8 16 17 18 26 27 28]]
    [[51 52 53 61 62 63 71 72 73]
     [56 57 58 66 67 68 76 77 78]]]]
"
"tf.image.flip_left_right(
    image
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.flip_left_right(x)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 4.,  5.,  6.],
        [ 1.,  2.,  3.]],
       [[10., 11., 12.],
        [ 7.,  8.,  9.]]], dtype=float32)>"
"tf.image.flip_up_down(
    image
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.flip_up_down(x)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 7.,  8.,  9.],
        [10., 11., 12.]],
       [[ 1.,  2.,  3.],
        [ 4.,  5.,  6.]]], dtype=float32)>"
"tf.image.generate_bounding_box_proposals(
    scores,
    bbox_deltas,
    image_info,
    anchors,
    nms_threshold=0.7,
    pre_nms_topn=6000,
    min_size=16,
    post_nms_topn=300,
    name=None
)
",[],[]
"tf.image.grayscale_to_rgb(
    images, name=None
)
",[],"original = tf.constant([[[1.0], [2.0], [3.0]]])
converted = tf.image.grayscale_to_rgb(original)
print(converted.numpy())
[[[1. 1. 1.]
  [2. 2. 2.]
  [3. 3. 3.]]]"
"tf.image.hsv_to_rgb(
    images, name=None
)
",[],[]
"tf.image.image_gradients(
    image
)
",[],"BATCH_SIZE = 1
IMAGE_HEIGHT = 5
IMAGE_WIDTH = 5
CHANNELS = 1
image = tf.reshape(tf.range(IMAGE_HEIGHT * IMAGE_WIDTH * CHANNELS,
  delta=1, dtype=tf.float32),
  shape=(BATCH_SIZE, IMAGE_HEIGHT, IMAGE_WIDTH, CHANNELS))
dy, dx = tf.image.image_gradients(image)
print(image[0, :,:,0])
tf.Tensor(
  [[ 0.  1.  2.  3.  4.]
  [ 5.  6.  7.  8.  9.]
  [10. 11. 12. 13. 14.]
  [15. 16. 17. 18. 19.]
  [20. 21. 22. 23. 24.]], shape=(5, 5), dtype=float32)
print(dy[0, :,:,0])
tf.Tensor(
  [[5. 5. 5. 5. 5.]
  [5. 5. 5. 5. 5.]
  [5. 5. 5. 5. 5.]
  [5. 5. 5. 5. 5.]
  [0. 0. 0. 0. 0.]], shape=(5, 5), dtype=float32)
print(dx[0, :,:,0])
tf.Tensor(
  [[1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]
  [1. 1. 1. 1. 0.]], shape=(5, 5), dtype=float32)
"
"tf.io.is_jpeg(
    contents, name=None
)
",[],[]
"tf.image.non_max_suppression(
    boxes,
    scores,
    max_output_size,
    iou_threshold=0.5,
    score_threshold=float('-inf'),
    name=None
)
",[],"  selected_indices = tf.image.non_max_suppression(
      boxes, scores, max_output_size, iou_threshold)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.non_max_suppression_overlaps(
    overlaps,
    scores,
    max_output_size,
    overlap_threshold=0.5,
    score_threshold=float('-inf'),
    name=None
)
",[],"  selected_indices = tf.image.non_max_suppression_overlaps(
      overlaps, scores, max_output_size, iou_threshold)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.non_max_suppression_padded(
    boxes,
    scores,
    max_output_size,
    iou_threshold=0.5,
    score_threshold=float('-inf'),
    pad_to_max_output_size=False,
    name=None,
    sorted_input=False,
    canonicalized_coordinates=False,
    tile_size=512
)
",[],"  selected_indices_padded, num_valid = tf.image.non_max_suppression_padded(
      boxes, scores, max_output_size, iou_threshold,
      score_threshold, pad_to_max_output_size=True)
  selected_indices = tf.slice(
      selected_indices_padded, tf.constant([0]), num_valid)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.non_max_suppression_with_scores(
    boxes,
    scores,
    max_output_size,
    iou_threshold=0.5,
    score_threshold=float('-inf'),
    soft_nms_sigma=0.0,
    name=None
)
",[],"  selected_indices, selected_scores = tf.image.non_max_suppression_padded(
      boxes, scores, max_output_size, iou_threshold=1.0, score_threshold=0.1,
      soft_nms_sigma=0.5)
  selected_boxes = tf.gather(boxes, selected_indices)
"
"tf.image.pad_to_bounding_box(
    image, offset_height, offset_width, target_height, target_width
)
","[['Pad ', 'image', ' with zeros to the specified ', 'height', ' and ', 'width', '.']]","x = [[[1., 2., 3.],
      [4., 5., 6.]],
      [[7., 8., 9.],
      [10., 11., 12.]]]
padded_image = tf.image.pad_to_bounding_box(x, 1, 1, 4, 4)
padded_image
<tf.Tensor: shape=(4, 4, 3), dtype=float32, numpy=
array([[[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.]],
[[ 0.,  0.,  0.],
[ 1.,  2.,  3.],
[ 4.,  5.,  6.],
[ 0.,  0.,  0.]],
[[ 0.,  0.,  0.],
[ 7.,  8.,  9.],
[10., 11., 12.],
[ 0.,  0.,  0.]],
[[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.],
[ 0.,  0.,  0.]]], dtype=float32)>"
"tf.image.per_image_standardization(
    image
)
","[['Linearly scales each image in ', 'image', ' to have mean 0 and variance 1.']]","image = tf.constant(np.arange(1, 13, dtype=np.int32), shape=[2, 2, 3])
image # 3-D tensor
<tf.Tensor: shape=(2, 2, 3), dtype=int32, numpy=
array([[[ 1,  2,  3],
        [ 4,  5,  6]],
       [[ 7,  8,  9],
        [10, 11, 12]]], dtype=int32)>
new_image = tf.image.per_image_standardization(image)
new_image # 3-D tensor with mean ~= 0 and variance ~= 1
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[-1.593255  , -1.3035723 , -1.0138896 ],
        [-0.7242068 , -0.4345241 , -0.14484136]],
       [[ 0.14484136,  0.4345241 ,  0.7242068 ],
        [ 1.0138896 ,  1.3035723 ,  1.593255  ]]], dtype=float32)>"
"tf.image.psnr(
    a, b, max_val, name=None
)
",[],"    # Read images from file.
    im1 = tf.decode_png('path/to/im1.png')
    im2 = tf.decode_png('path/to/im2.png')
    # Compute PSNR over tf.uint8 Tensors.
    psnr1 = tf.image.psnr(im1, im2, max_val=255)

    # Compute PSNR over tf.float32 Tensors.
    im1 = tf.image.convert_image_dtype(im1, tf.float32)
    im2 = tf.image.convert_image_dtype(im2, tf.float32)
    psnr2 = tf.image.psnr(im1, im2, max_val=1.0)
    # psnr1 and psnr2 both have type tf.float32 and are almost equal.
"
"tf.image.random_brightness(
    image, max_delta, seed=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
     [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_brightness(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>"
"tf.image.random_contrast(
    image, lower, upper, seed=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_contrast(x, 0.2, 0.5)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>"
"tf.image.random_crop(
    value, size, seed=None, name=None
)
",[],"image = [[1, 2, 3], [4, 5, 6]]
result = tf.image.random_crop(value=image, size=(1, 3))
result.shape.as_list()
[1, 3]"
"tf.image.random_flip_left_right(
    image, seed=None
)
",[],"image = np.array([[[1], [2]], [[3], [4]]])
tf.image.random_flip_left_right(image, 5).numpy().tolist()
[[[2], [1]], [[4], [3]]]"
"tf.image.random_flip_up_down(
    image, seed=None
)
",[],"image = np.array([[[1], [2]], [[3], [4]]])
tf.image.random_flip_up_down(image, 3).numpy().tolist()
[[[3], [4]], [[1], [2]]]"
"tf.image.random_hue(
    image, max_delta, seed=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_hue(x, 0.2)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=...>"
"tf.image.random_jpeg_quality(
    image, min_jpeg_quality, max_jpeg_quality, seed=None
)
",[],"x = tf.constant([[[1, 2, 3],
                  [4, 5, 6]],
                 [[7, 8, 9],
                  [10, 11, 12]]], dtype=tf.uint8)
tf.image.random_jpeg_quality(x, 75, 95)
<tf.Tensor: shape=(2, 2, 3), dtype=uint8, numpy=...>"
"tf.image.random_saturation(
    image, lower, upper, seed=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.random_saturation(x, 5, 10)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 0. ,  1.5,  3. ],
        [ 0. ,  3. ,  6. ]],
       [[ 0. ,  4.5,  9. ],
        [ 0. ,  6. , 12. ]]], dtype=float32)>"
"tf.compat.v1.image.resize(
    images,
    size,
    method=ResizeMethodV1.BILINEAR,
    align_corners=False,
    preserve_aspect_ratio=False,
    name=None
)
","[['Resize ', 'images', ' to ', 'size', ' using the specified ', 'method', '.']]",[]
"tf.compat.v1.image.resize_area(
    images, size, align_corners=False, name=None
)
","[['Resize ', 'images', ' to ', 'size', ' using area interpolation.']]",[]
"tf.compat.v1.image.resize_bicubic(
    images, size, align_corners=False, name=None, half_pixel_centers=False
)
",[],[]
"tf.compat.v1.image.resize_bilinear(
    images, size, align_corners=False, name=None, half_pixel_centers=False
)
",[],[]
"tf.image.resize_with_crop_or_pad(
    image, target_height, target_width
)
",[],"image = np.arange(75).reshape(5, 5, 3)  # create 3-D image input
image[:,:,0]  # print first channel just for demo purposes
array([[ 0,  3,  6,  9, 12],
       [15, 18, 21, 24, 27],
       [30, 33, 36, 39, 42],
       [45, 48, 51, 54, 57],
       [60, 63, 66, 69, 72]])
image = tf.image.resize_with_crop_or_pad(image, 3, 3)  # crop
# print first channel for demo purposes; centrally cropped output
image[:,:,0]
<tf.Tensor: shape=(3, 3), dtype=int64, numpy=
array([[18, 21, 24],
       [33, 36, 39],
       [48, 51, 54]])>"
"tf.compat.v1.image.resize_image_with_pad(
    image,
    target_height,
    target_width,
    method=ResizeMethodV1.BILINEAR,
    align_corners=False
)
",[],[]
"tf.compat.v1.image.resize(
    images,
    size,
    method=ResizeMethodV1.BILINEAR,
    align_corners=False,
    preserve_aspect_ratio=False,
    name=None
)
","[['Resize ', 'images', ' to ', 'size', ' using the specified ', 'method', '.']]",[]
"tf.compat.v1.image.resize_nearest_neighbor(
    images, size, align_corners=False, name=None, half_pixel_centers=False
)
",[],[]
"tf.image.resize_with_crop_or_pad(
    image, target_height, target_width
)
",[],"image = np.arange(75).reshape(5, 5, 3)  # create 3-D image input
image[:,:,0]  # print first channel just for demo purposes
array([[ 0,  3,  6,  9, 12],
       [15, 18, 21, 24, 27],
       [30, 33, 36, 39, 42],
       [45, 48, 51, 54, 57],
       [60, 63, 66, 69, 72]])
image = tf.image.resize_with_crop_or_pad(image, 3, 3)  # crop
# print first channel for demo purposes; centrally cropped output
image[:,:,0]
<tf.Tensor: shape=(3, 3), dtype=int64, numpy=
array([[18, 21, 24],
       [33, 36, 39],
       [48, 51, 54]])>"
"tf.image.rgb_to_grayscale(
    images, name=None
)
",[],"original = tf.constant([[[1.0, 2.0, 3.0]]])
converted = tf.image.rgb_to_grayscale(original)
print(converted.numpy())
[[[1.81...]]]"
"tf.image.rgb_to_hsv(
    images, name=None
)
",[],"blue_image = tf.stack([
   tf.zeros([5,5]),
   tf.zeros([5,5]),
   tf.ones([5,5])],
   axis=-1)
blue_hsv_image = tf.image.rgb_to_hsv(blue_image)
blue_hsv_image[0,0].numpy()
array([0.6666667, 1. , 1. ], dtype=float32)"
"tf.image.rgb_to_yiq(
    images
)
",[],"x = tf.constant([[[1.0, 2.0, 3.0]]])
tf.image.rgb_to_yiq(x)
<tf.Tensor: shape=(1, 1, 3), dtype=float32,
numpy=array([[[ 1.815     , -0.91724455,  0.09962624]]], dtype=float32)>"
"tf.image.rgb_to_yuv(
    images
)
",[],[]
"tf.image.rot90(
    image, k=1, name=None
)
",[],"a=tf.constant([[[1],[2]],
               [[3],[4]]])
# rotating `a` counter clockwise by 90 degrees
a_rot=tf.image.rot90(a)
print(a_rot[...,0].numpy())
[[2 4]
 [1 3]]
# rotating `a` counter clockwise by 270 degrees
a_rot=tf.image.rot90(a, k=3)
print(a_rot[...,0].numpy())
[[3 1]
 [4 2]]
# rotating `a` clockwise by 180 degrees
a_rot=tf.image.rot90(a, k=-2)
print(a_rot[...,0].numpy())
[[4 3]
 [2 1]]"
"tf.compat.v1.image.sample_distorted_bounding_box(
    image_size,
    bounding_boxes,
    seed=None,
    seed2=None,
    min_object_covered=0.1,
    aspect_ratio_range=None,
    area_range=None,
    max_attempts=None,
    use_image_if_no_bounding_boxes=None,
    name=None
)
",[],"    # Generate a single distorted bounding box.
    begin, size, bbox_for_draw = tf.image.sample_distorted_bounding_box(
        tf.shape(image),
        bounding_boxes=bounding_boxes,
        min_object_covered=0.1)

    # Draw the bounding box in an image summary.
    image_with_box = tf.image.draw_bounding_boxes(tf.expand_dims(image, 0),
                                                  bbox_for_draw)
    tf.compat.v1.summary.image('images_with_box', image_with_box)

    # Employ the bounding box to distort the image.
    distorted_image = tf.slice(image, begin, size)
"
"tf.image.sobel_edges(
    image
)
",[],"image_bytes = tf.io.read_file(path_to_image_file)
image = tf.image.decode_image(image_bytes)
image = tf.cast(image, tf.float32)
image = tf.expand_dims(image, 0)
"
"tf.image.ssim(
    img1,
    img2,
    max_val,
    filter_size=11,
    filter_sigma=1.5,
    k1=0.01,
    k2=0.03,
    return_index_map=False
)
",[],"    # Read images (of size 255 x 255) from file.
    im1 = tf.image.decode_image(tf.io.read_file('path/to/im1.png'))
    im2 = tf.image.decode_image(tf.io.read_file('path/to/im2.png'))
    tf.shape(im1)  # `img1.png` has 3 channels; shape is `(255, 255, 3)`
    tf.shape(im2)  # `img2.png` has 3 channels; shape is `(255, 255, 3)`
    # Add an outer batch for each image.
    im1 = tf.expand_dims(im1, axis=0)
    im2 = tf.expand_dims(im2, axis=0)
    # Compute SSIM over tf.uint8 Tensors.
    ssim1 = tf.image.ssim(im1, im2, max_val=255, filter_size=11,
                          filter_sigma=1.5, k1=0.01, k2=0.03)

    # Compute SSIM over tf.float32 Tensors.
    im1 = tf.image.convert_image_dtype(im1, tf.float32)
    im2 = tf.image.convert_image_dtype(im2, tf.float32)
    ssim2 = tf.image.ssim(im1, im2, max_val=1.0, filter_size=11,
                          filter_sigma=1.5, k1=0.01, k2=0.03)
    # ssim1 and ssim2 both have type tf.float32 and are almost equal.
"
"tf.image.ssim_multiscale(
    img1,
    img2,
    max_val,
    power_factors=_MSSSIM_WEIGHTS,
    filter_size=11,
    filter_sigma=1.5,
    k1=0.01,
    k2=0.03
)
",[],[]
"tf.image.total_variation(
    images, name=None
)
",[],[]
"tf.image.transpose(
    image, name=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.transpose(x)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.,  2.,  3.],
        [ 7.,  8.,  9.]],
       [[ 4.,  5.,  6.],
        [10., 11., 12.]]], dtype=float32)>"
"tf.image.transpose(
    image, name=None
)
",[],"x = [[[1.0, 2.0, 3.0],
      [4.0, 5.0, 6.0]],
    [[7.0, 8.0, 9.0],
      [10.0, 11.0, 12.0]]]
tf.image.transpose(x)
<tf.Tensor: shape=(2, 2, 3), dtype=float32, numpy=
array([[[ 1.,  2.,  3.],
        [ 7.,  8.,  9.]],
       [[ 4.,  5.,  6.],
        [10., 11., 12.]]], dtype=float32)>"
"tf.image.yiq_to_rgb(
    images
)
",[],[]
"tf.image.yuv_to_rgb(
    images
)
",[],"yuv_images = tf.random.uniform(shape=[100, 64, 64, 3], maxval=255)
last_dimension_axis = len(yuv_images.shape) - 1
yuv_tensor_images = tf.truediv(
    tf.subtract(
        yuv_images,
        tf.reduce_min(yuv_images)
    ),
    tf.subtract(
        tf.reduce_max(yuv_images),
        tf.reduce_min(yuv_images)
     )
)
y, u, v = tf.split(yuv_tensor_images, 3, axis=last_dimension_axis)
target_uv_min, target_uv_max = -0.5, 0.5
u = u * (target_uv_max - target_uv_min) + target_uv_min
v = v * (target_uv_max - target_uv_min) + target_uv_min
preprocessed_yuv_images = tf.concat([y, u, v], axis=last_dimension_axis)
rgb_tensor_images = tf.image.yuv_to_rgb(preprocessed_yuv_images)
"
"tf.graph_util.import_graph_def(
    graph_def,
    input_map=None,
    return_elements=None,
    name=None,
    op_dict=None,
    producer_op_list=None
)
","[['Imports the graph from ', 'graph_def', ' into the current default ', 'Graph', '. (deprecated arguments)']]",[]
"tf.compat.v1.initialize_all_tables(
    name='init_all_tables'
)
",[],[]
"tf.compat.v1.initialize_variables(
    var_list, name='init'
)
","[['See ', 'tf.compat.v1.variables_initializer', '. (deprecated)']]",[]
"tf.compat.v1.keras.initializers.Constant(
    value=0,
    dtype=tf.dtypes.float32,
    verify_shape=False
)
",[],"value = [0, 1, 2, 3, 4, 5, 6, 7]
initializer = tf.compat.v1.constant_initializer(
    value=value,
    dtype=tf.float32,
    verify_shape=False)
variable = tf.Variable(initializer(shape=[2, 4]))
"
"tf.compat.v1.keras.initializers.glorot_normal(
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'VarianceScaling']]","@classmethod
from_config(
    config
)
"
"tf.compat.v1.keras.initializers.glorot_uniform(
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'VarianceScaling']]","@classmethod
from_config(
    config
)
"
"tf.compat.v1.initializers.he_normal(
    seed=None
)
",[],[]
"tf.compat.v1.initializers.he_uniform(
    seed=None
)
",[],[]
"tf.compat.v1.keras.initializers.Identity(
    gain=1.0,
    dtype=tf.dtypes.float32
)
",[],"@classmethod
from_config(
    config
)
"
"tf.compat.v1.initializers.lecun_normal(
    seed=None
)
",[],[]
"tf.compat.v1.initializers.lecun_uniform(
    seed=None
)
",[],[]
"tf.compat.v1.keras.initializers.Ones(
    dtype=tf.dtypes.float32
)
",[],">>> initializer = tf.compat.v1.keras.initializers.ones()
>>> initializer((1, 1))
<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>
"
"tf.compat.v1.keras.initializers.Orthogonal(
    gain=1.0,
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"@classmethod
from_config(
    config
)
"
"tf.compat.v1.random_normal_initializer(
    mean=0.0,
    stddev=1.0,
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"initializer = tf.compat.v1.random_normal_initializer(
  mean=mean,
  stddev=stddev,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.random_uniform_initializer(
    minval=0.0,
    maxval=None,
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"initializer = tf.compat.v1.random_uniform_initializer(
  minval=minval,
  maxval=maxval,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.tables_initializer(
    name='init_all_tables'
)
",[],"with tf.compat.v1.Session():
  init = tf.compat.v1.lookup.KeyValueTensorInitializer(['a', 'b'], [1, 2])
  table = tf.compat.v1.lookup.StaticHashTable(init, default_value=-1)
  tf.compat.v1.tables_initializer().run()
  result = table.lookup(tf.constant(['a', 'c'])).eval()
result
array([ 1, -1], dtype=int32)"
"tf.compat.v1.truncated_normal_initializer(
    mean=0.0,
    stddev=1.0,
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"initializer = tf.compat.v1.truncated_normal_initializer(
  mean=mean,
  stddev=stddev,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.uniform_unit_scaling_initializer(
    factor=1.0,
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"[-sqrt(3) / sqrt(dim), sqrt(3) / sqrt(dim)]
"
"tf.compat.v1.variables_initializer(
    var_list, name='init'
)
",[],[]
"tf.compat.v1.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"initializer = tf.compat.v1.variance_scaling_initializer(
  scale=scale,
  mode=mode,
  distribution=distribution
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.keras.initializers.Zeros(
    dtype=tf.dtypes.float32
)
",[],"initializer = tf.compat.v1.zeros_initializer(dtype=tf.float32)
variable = tf.Variable(initializer(shape=[3, 3]))
"
"tf.math.invert_permutation(
    x, name=None
)
",[],"# tensor `x` is [3, 4, 0, 2, 1]
invert_permutation(x) ==> [2, 4, 3, 0, 1]
"
"tf.io.FixedLenFeature(
    shape, dtype, default_value=None
)
",[],[]
"tf.io.FixedLenSequenceFeature(
    shape, dtype, allow_missing=False, default_value=None
)
","[['Configuration for parsing a variable-length input feature into a ', 'Tensor', '.']]",[]
"tf.queue.PaddingFIFOQueue(
    capacity,
    dtypes,
    shapes,
    names=None,
    shared_name=None,
    name='padding_fifo_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.queue.PriorityQueue(
    capacity,
    types,
    shapes=None,
    names=None,
    shared_name=None,
    name='priority_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.queue.QueueBase(
    dtypes, shapes, names, queue_ref
)
",[],"close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.io.RaggedFeature(
    dtype,
    value_key=None,
    partitions=(),
    row_splits_dtype=tf.dtypes.int32,
    validate=False
)
",[],"import google.protobuf.text_format as pbtext
example_batch = [
  pbtext.Merge(r'''
    features {
      feature {key: ""v"" value {int64_list {value: [3, 1, 4, 1, 5, 9]} } }
      feature {key: ""s1"" value {int64_list {value: [0, 2, 3, 3, 6]} } }
      feature {key: ""s2"" value {int64_list {value: [0, 2, 3, 4]} } }
    }''', tf.train.Example()).SerializeToString(),
  pbtext.Merge(r'''
    features {
      feature {key: ""v"" value {int64_list {value: [2, 7, 1, 8, 2, 8, 1]} } }
      feature {key: ""s1"" value {int64_list {value: [0, 3, 4, 5, 7]} } }
      feature {key: ""s2"" value {int64_list {value: [0, 1, 1, 4]} } }
    }''', tf.train.Example()).SerializeToString()]"
"tf.io.RaggedFeature.RowLengths(
    key
)
",[],[]
"tf.io.RaggedFeature.RowLimits(
    key
)
",[],[]
"tf.io.RaggedFeature.RowSplits(
    key
)
",[],[]
"tf.io.RaggedFeature.RowStarts(
    key
)
",[],[]
"tf.io.RaggedFeature.UniformRowLength(
    length
)
",[],[]
"tf.io.RaggedFeature.ValueRowIds(
    key
)
",[],[]
"tf.queue.RandomShuffleQueue(
    capacity,
    min_after_dequeue,
    dtypes,
    shapes=None,
    names=None,
    seed=None,
    shared_name=None,
    name='random_shuffle_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.io.SparseFeature(
    index_key, value_key, dtype, size, already_sorted=False
)
","[['Configuration for parsing a sparse input feature from an ', 'Example', '.']]","SparseTensor(indices=[[3, 1], [20, 0]],
             values=[0.5, -1.0]
             dense_shape=[100, 3])
"
"tf.io.TFRecordOptions(
    compression_type=None,
    flush_mode=None,
    input_buffer_size=None,
    output_buffer_size=None,
    window_bits=None,
    compression_level=None,
    compression_method=None,
    mem_level=None,
    compression_strategy=None
)
",[],"@classmethod
get_compression_type_string(
    options
)
"
"tf.io.TFRecordWriter(
    path, options=None
)
",[],"import tempfile
example_path = os.path.join(tempfile.gettempdir(), ""example.tfrecords"")
np.random.seed(0)"
"tf.io.VarLenFeature(
    dtype
)
",[],[]
"tf.io.decode_and_crop_jpeg(
    contents,
    crop_window,
    channels=0,
    ratio=1,
    fancy_upscaling=True,
    try_recover_truncated=False,
    acceptable_fraction=1,
    dct_method='',
    name=None
)
",[],[]
"tf.io.decode_base64(
    input, name=None
)
",[],[]
"tf.io.decode_bmp(
    contents, channels=0, name=None
)
",[],[]
"tf.io.decode_compressed(
    bytes, compression_type='', name=None
)
",[],[]
"tf.compat.v1.decode_csv(
    records,
    record_defaults,
    field_delim=',',
    use_quote_delim=True,
    name=None,
    na_value='',
    select_cols=None
)
",[],[]
"tf.io.decode_gif(
    contents, name=None
)
","[[None, '\n']]","convert \\(src.gif -coalesce \\)dst.gif
"
"tf.io.decode_image(
    contents,
    channels=None,
    dtype=tf.dtypes.uint8,
    name=None,
    expand_animations=True
)
","[['Function for ', 'decode_bmp', ', ', 'decode_gif', ', ', 'decode_jpeg', ', and ', 'decode_png', '.']]",[]
"tf.io.decode_jpeg(
    contents,
    channels=0,
    ratio=1,
    fancy_upscaling=True,
    try_recover_truncated=False,
    acceptable_fraction=1,
    dct_method='',
    name=None
)
",[],[]
"tf.io.decode_json_example(
    json_examples, name=None
)
",[],"example = tf.train.Example(
  features=tf.train.Features(
      feature={
          ""a"": tf.train.Feature(
              int64_list=tf.train.Int64List(
                  value=[1, 1, 3]))}))"
"tf.io.decode_png(
    contents,
    channels=0,
    dtype=tf.dtypes.uint8,
    name=None
)
",[],[]
"tf.io.decode_proto(
    bytes,
    message_type,
    field_names,
    output_types,
    descriptor_source='local://',
    message_format='binary',
    sanitize=False,
    name=None
)
",[],"from google.protobuf import text_format
# A Summary.Value contains: oneof {float simple_value; Image image}
values = [
   ""simple_value: 2.2"",
   ""simple_value: 1.2"",
   ""image { height: 128 width: 512 }"",
   ""image { height: 256 width: 256 }"",]
values = [
   text_format.Parse(v, tf.compat.v1.Summary.Value()).SerializeToString()
   for v in values]"
"tf.compat.v1.decode_raw(
    input_bytes=None, out_type=None, little_endian=True, name=None, bytes=None
)
",[],[]
"tf.io.deserialize_many_sparse(
    serialized_sparse, dtype, rank=None, name=None
)
","[['Deserialize and concatenate ', 'SparseTensors', ' from a serialized minibatch.']]","index = [ 0]
        [10]
        [20]
values = [1, 2, 3]
shape = [50]
"
"tf.io.encode_base64(
    input, pad=False, name=None
)
",[],[]
"tf.io.encode_jpeg(
    image,
    format='',
    quality=95,
    progressive=False,
    optimize_size=False,
    chroma_downsampling=True,
    density_unit='in',
    x_density=300,
    y_density=300,
    xmp_metadata='',
    name=None
)
",[],[]
"tf.io.encode_png(
    image, compression=-1, name=None
)
",[],[]
"tf.io.encode_proto(
    sizes,
    values,
    field_names,
    message_type,
    descriptor_source='local://',
    name=None
)
",[],[]
"tf.io.extract_jpeg_shape(
    contents,
    output_type=tf.dtypes.int32,
    name=None
)
",[],[]
"tf.io.gfile.GFile(
    name, mode='r'
)
",[],"with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
with tf.io.gfile.GFile(""/tmp/x"") as f:
  f.read()
'asdf'"
"tf.io.gfile.copy(
    src, dst, overwrite=False
)
","[['Copies data from ', 'src', ' to ', 'dst', '.']]","with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.gfile.exists(""/tmp/x"")
True
tf.io.gfile.copy(""/tmp/x"", ""/tmp/y"")
tf.io.gfile.exists(""/tmp/y"")
True
tf.io.gfile.remove(""/tmp/y"")"
"tf.io.gfile.exists(
    path
)
",[],"with open(""/tmp/x"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.gfile.exists(""/tmp/x"")
True"
"tf.io.gfile.glob(
    pattern
)
",[],"tf.io.gfile.glob(""*.py"")
# For example, ['__init__.py']"
"tf.io.gfile.isdir(
    path
)
",[],[]
"tf.io.gfile.join(
    path, *paths
)
",[],">>> tf.io.gfile.join(""gcs://folder"", ""file.py"")
'gcs://folder/file.py'
"
"tf.io.gfile.listdir(
    path
)
",[],[]
"tf.io.gfile.makedirs(
    path
)
",[],[]
"tf.io.gfile.mkdir(
    path
)
","[['Creates a directory with the name given by ', 'path', '.']]",[]
"tf.io.gfile.remove(
    path
)
",[],[]
"tf.io.gfile.rename(
    src, dst, overwrite=False
)
",[],[]
"tf.io.gfile.rmtree(
    path
)
",[],[]
"tf.io.gfile.stat(
    path
)
",[],[]
"tf.io.gfile.walk(
    top, topdown=True, onerror=None
)
",[],[]
"tf.io.is_jpeg(
    contents, name=None
)
",[],[]
"tf.io.match_filenames_once(
    pattern, name=None
)
",[],[]
"tf.io.matching_files(
    pattern, name=None
)
",[],[]
"tf.compat.v1.parse_example(
    serialized, features, name=None, example_names=None
)
","[['Parses ', 'Example', ' protos into a ', 'dict', ' of tensors.']]","serialized = [
  features
    { feature { key: ""ft"" value { float_list { value: [1.0, 2.0] } } } },
  features
    { feature []},
  features
    { feature { key: ""ft"" value { float_list { value: [3.0] } } }
]
"
"tf.io.parse_sequence_example(
    serialized,
    context_features=None,
    sequence_features=None,
    example_names=None,
    name=None
)
","[['Parses a batch of ', 'SequenceExample', ' protos.']]",[]
"tf.compat.v1.parse_single_example(
    serialized, features, name=None, example_names=None
)
","[['Parses a single ', 'Example', ' proto.']]",[]
"tf.io.parse_single_sequence_example(
    serialized,
    context_features=None,
    sequence_features=None,
    example_name=None,
    name=None
)
","[['Parses a single ', 'SequenceExample', ' proto.']]",[]
"tf.io.parse_tensor(
    serialized, out_type, name=None
)
",[],[]
"tf.io.read_file(
    filename, name=None
)
",[],"with open(""/tmp/file.txt"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.read_file(""/tmp/file.txt"")
<tf.Tensor: shape=(), dtype=string, numpy=b'asdf'>"
"tf.compat.v1.serialize_many_sparse(
    sp_input,
    name=None,
    out_type=tf.dtypes.string
)
","[['Serialize ', 'N', '-minibatch ', 'SparseTensor', ' into an ', '[N, 3]', ' ', 'Tensor', '.']]",[]
"tf.compat.v1.serialize_sparse(
    sp_input,
    name=None,
    out_type=tf.dtypes.string
)
","[['Serialize a ', 'SparseTensor', ' into a 3-vector (1-D ', 'Tensor', ') object.']]",[]
"tf.io.serialize_tensor(
    tensor, name=None
)
",[],"t = tf.constant(1)
tf.io.serialize_tensor(t)
<tf.Tensor: shape=(), dtype=string, numpy=b'\x08...\x00'>"
"tf.compat.v1.io.tf_record_iterator(
    path, options=None
)
",[],[]
"tf.io.write_file(
    filename, contents, name=None
)
","[['Writes ', 'contents', ' to the file at input ', 'filename', '.']]",[]
"tf.io.write_graph(
    graph_or_graph_def, logdir, name, as_text=True
)
",[],"v = tf.Variable(0, name='my_variable')
sess = tf.compat.v1.Session()
tf.io.write_graph(sess.graph_def, '/tmp/my-model', 'train.pbtxt')
"
"tf.math.is_finite(
    x, name=None
)
",[],"x = tf.constant([5.0, 4.8, 6.8, np.inf, np.nan])
tf.math.is_finite(x) ==> [True, True, True, False, False]
"
"tf.math.is_inf(
    x, name=None
)
",[],"x = tf.constant([5.0, np.inf, 6.8, np.inf])
tf.math.is_inf(x) ==> [False, True, False, True]
"
"tf.math.is_nan(
    x, name=None
)
",[],"x = tf.constant([5.0, np.nan, 6.8, np.nan, np.inf])
tf.math.is_nan(x) ==> [False, True, False, True, False]
"
"tf.math.is_non_decreasing(
    x, name=None
)
","[['Returns ', 'True', ' if ', 'x', ' is non-decreasing.']]","x1 = tf.constant([1.0, 1.0, 3.0])
tf.math.is_non_decreasing(x1)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
x2 = tf.constant([3.0, 1.0, 2.0])
tf.math.is_non_decreasing(x2)
<tf.Tensor: shape=(), dtype=bool, numpy=False>"
"tf.debugging.is_numeric_tensor(
    tensor
)
","[['Returns ', 'True', ' if the elements of ', 'tensor', ' are numbers.']]",[]
"tf.math.is_strictly_increasing(
    x, name=None
)
","[['Returns ', 'True', ' if ', 'x', ' is strictly increasing.']]","x1 = tf.constant([1.0, 2.0, 3.0])
tf.math.is_strictly_increasing(x1)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
x2 = tf.constant([3.0, 1.0, 2.0])
tf.math.is_strictly_increasing(x2)
<tf.Tensor: shape=(), dtype=bool, numpy=False>"
"tf.is_tensor(
    x
)
","[['Checks whether ', 'x', ' is a TF-native type that can be passed to many TF ops.']]","if not tf.is_tensor(t):
  t = tf.convert_to_tensor(t)
return t.shape, t.dtype
"
"tf.compat.v1.is_variable_initialized(
    variable
)
",[],[]
"tf.keras.Input(
    shape=None,
    batch_size=None,
    name=None,
    dtype=None,
    sparse=None,
    tensor=None,
    ragged=None,
    type_spec=None,
    **kwargs
)
","[['Input()', ' is used to instantiate a Keras tensor.']]","# this is a logistic regression in Keras
x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)
"
"tf.keras.Model(
    *args, **kwargs
)
","[['Model', ' groups layers into an object with training and inference features.'], ['Inherits From: ', 'Layer', ', ', 'Module']]","import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
"
"tf.keras.Sequential(
    layers=None, name=None
)
","[['Sequential', ' groups a linear stack of layers into a ', 'tf.keras.Model', '.'], ['Inherits From: ', 'Model', ', ', 'Layer', ', ', 'Module']]","# Optionally, the first layer can receive an `input_shape` argument:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
# Afterwards, we do automatic shape inference:
model.add(tf.keras.layers.Dense(4))

# This is identical to the following:
model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(8))

# Note that you can also omit the `input_shape` argument.
# In that case the model doesn't have any weights until the first call
# to a training/evaluation method (since it isn't yet built):
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
# model.weights not created yet

# Whereas if you specify the input shape, the model gets built
# continuously as you are adding layers:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))
len(model.weights)
# Returns ""4""

# When using the delayed-build pattern (no input shape specified), you can
# choose to manually build your model by calling
# `build(batch_input_shape)`:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
model.build((None, 16))
len(model.weights)
# Returns ""4""

# Note that when using the delayed-build pattern (no input shape specified),
# the model gets built the first time you call `fit`, `eval`, or `predict`,
# or the first time you call the model on some input data.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer='sgd', loss='mse')
# This builds the model for the first time:
model.fit(x, y, batch_size=32, epochs=10)
"
"tf.keras.activations.deserialize(
    name, custom_objects=None
)
",[],"tf.keras.activations.deserialize('linear')
 <function linear at 0x1239596a8>
tf.keras.activations.deserialize('sigmoid')
 <function sigmoid at 0x123959510>
tf.keras.activations.deserialize('abcd')
Traceback (most recent call last):
ValueError: Unknown activation function:abcd"
"tf.keras.activations.elu(
    x, alpha=1.0
)
",[],"import tensorflow as tf
model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='elu',
         input_shape=(28, 28, 1)))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))
model.add(tf.keras.layers.MaxPooling2D((2, 2)))
model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='elu'))"
"tf.keras.activations.exponential(
    x
)
",[],"a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.exponential(a)
b.numpy()
array([0.04978707,  0.36787945,  1.,  2.7182817 , 20.085537], dtype=float32)"
"tf.keras.activations.get(
    identifier
)
",[],"tf.keras.activations.get('softmax')
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(tf.keras.activations.softmax)
 <function softmax at 0x1222a3d90>
tf.keras.activations.get(None)
 <function linear at 0x1239596a8>
tf.keras.activations.get(abs)
 <built-in function abs>
tf.keras.activations.get('abcd')
Traceback (most recent call last):
ValueError: Unknown activation function:abcd"
"tf.keras.activations.hard_sigmoid(
    x
)
",[],"a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.hard_sigmoid(a)
b.numpy()
array([0. , 0.3, 0.5, 0.7, 1. ], dtype=float32)"
"tf.keras.activations.linear(
    x
)
",[],"a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.linear(a)
b.numpy()
array([-3., -1.,  0.,  1.,  3.], dtype=float32)"
"tf.keras.activations.relu(
    x, alpha=0.0, max_value=None, threshold=0.0
)
",[],"foo = tf.constant([-10, -5, 0.0, 5, 10], dtype = tf.float32)
tf.keras.activations.relu(foo).numpy()
array([ 0.,  0.,  0.,  5., 10.], dtype=float32)
tf.keras.activations.relu(foo, alpha=0.5).numpy()
array([-5. , -2.5,  0. ,  5. , 10. ], dtype=float32)
tf.keras.activations.relu(foo, max_value=5.).numpy()
array([0., 0., 0., 5., 5.], dtype=float32)
tf.keras.activations.relu(foo, threshold=5.).numpy()
array([-0., -0.,  0.,  0., 10.], dtype=float32)"
"tf.keras.activations.selu(
    x
)
",[],"num_classes = 10  # 10-class problem
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(64, kernel_initializer='lecun_normal',
                                activation='selu'))
model.add(tf.keras.layers.Dense(32, kernel_initializer='lecun_normal',
                                activation='selu'))
model.add(tf.keras.layers.Dense(16, kernel_initializer='lecun_normal',
                                activation='selu'))
model.add(tf.keras.layers.Dense(num_classes, activation='softmax'))"
"tf.keras.activations.serialize(
    activation
)
",[],"tf.keras.activations.serialize(tf.keras.activations.tanh)
'tanh'
tf.keras.activations.serialize(tf.keras.activations.sigmoid)
'sigmoid'
tf.keras.activations.serialize('abcd')
Traceback (most recent call last):
ValueError: ('Cannot serialize', 'abcd')"
"tf.keras.activations.sigmoid(
    x
)
","[['Sigmoid activation function, ', 'sigmoid(x) = 1 / (1 + exp(-x))', '.']]","a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.sigmoid(a)
b.numpy()
array([2.0611537e-09, 2.6894143e-01, 5.0000000e-01, 7.3105860e-01,
         1.0000000e+00], dtype=float32)"
"tf.keras.activations.softmax(
    x, axis=-1
)
",[],"inputs = tf.random.normal(shape=(32, 10))
outputs = tf.keras.activations.softmax(inputs)
tf.reduce_sum(outputs[0, :])  # Each sample in the batch now sums to 1
<tf.Tensor: shape=(), dtype=float32, numpy=1.0000001>"
"tf.keras.activations.softplus(
    x
)
","[['Softplus activation function, ', 'softplus(x) = log(exp(x) + 1)', '.']]","a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.softplus(a)
b.numpy()
array([2.0611537e-09, 3.1326166e-01, 6.9314718e-01, 1.3132616e+00,
         2.0000000e+01], dtype=float32)"
"tf.keras.activations.softsign(
    x
)
","[['Softsign activation function, ', 'softsign(x) = x / (abs(x) + 1)', '.']]","a = tf.constant([-1.0, 0.0, 1.0], dtype = tf.float32)
b = tf.keras.activations.softsign(a)
b.numpy()
array([-0.5,  0. ,  0.5], dtype=float32)"
"tf.keras.activations.swish(
    x
)
","[['Swish activation function, ', 'swish(x) = x * sigmoid(x)', '.']]","a = tf.constant([-20, -1.0, 0.0, 1.0, 20], dtype = tf.float32)
b = tf.keras.activations.swish(a)
b.numpy()
array([-4.1223075e-08, -2.6894143e-01,  0.0000000e+00,  7.3105860e-01,
          2.0000000e+01], dtype=float32)"
"tf.keras.activations.tanh(
    x
)
",[],"a = tf.constant([-3.0,-1.0, 0.0,1.0,3.0], dtype = tf.float32)
b = tf.keras.activations.tanh(a)
b.numpy()
array([-0.9950547, -0.7615942,  0.,  0.7615942,  0.9950547], dtype=float32)"
"tf.keras.applications.convnext.ConvNeXtBase(
    model_name='convnext_base',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtLarge(
    model_name='convnext_large',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtSmall(
    model_name='convnext_small',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtTiny(
    model_name='convnext_tiny',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtXLarge(
    model_name='convnext_xlarge',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.densenet.DenseNet121(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.densenet.DenseNet169(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.densenet.DenseNet201(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB0(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB1(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB3(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB4(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB5(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB6(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB7(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B0(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B1(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B3(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2L(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2M(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2S(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.inception_resnet_v2.InceptionResNetV2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.inception_v3.InceptionV3(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.mobilenet.MobileNet(
    input_shape=None,
    alpha=1.0,
    depth_multiplier=1,
    dropout=0.001,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.mobilenet_v2.MobileNetV2(
    input_shape=None,
    alpha=1.0,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.MobileNetV3Large(
    input_shape=None,
    alpha=1.0,
    minimalistic=False,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    classes=1000,
    pooling=None,
    dropout_rate=0.2,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.MobileNetV3Small(
    input_shape=None,
    alpha=1.0,
    minimalistic=False,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    classes=1000,
    pooling=None,
    dropout_rate=0.2,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.nasnet.NASNetLarge(
    input_shape=None,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.nasnet.NASNetMobile(
    input_shape=None,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX002(
    model_name='regnetx002',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX004(
    model_name='regnetx004',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX006(
    model_name='regnetx006',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX008(
    model_name='regnetx008',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX016(
    model_name='regnetx016',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX032(
    model_name='regnetx032',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX040(
    model_name='regnetx040',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX064(
    model_name='regnetx064',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX080(
    model_name='regnetx080',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX120(
    model_name='regnetx120',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX160(
    model_name='regnetx160',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX320(
    model_name='regnetx320',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY002(
    model_name='regnety002',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY004(
    model_name='regnety004',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY006(
    model_name='regnety006',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY008(
    model_name='regnety008',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY016(
    model_name='regnety016',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY032(
    model_name='regnety032',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY040(
    model_name='regnety040',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY064(
    model_name='regnety064',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY080(
    model_name='regnety080',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY120(
    model_name='regnety120',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY160(
    model_name='regnety160',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY320(
    model_name='regnety320',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet.ResNet101(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet_v2.ResNet101V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet.ResNet152(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet_v2.ResNet152V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet50.ResNet50(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet_v2.ResNet50V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS101(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS152(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS200(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS270(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS350(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS420(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS50(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.vgg16.VGG16(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.vgg19.VGG19(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.xception.Xception(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtBase(
    model_name='convnext_base',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtLarge(
    model_name='convnext_large',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtSmall(
    model_name='convnext_small',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtTiny(
    model_name='convnext_tiny',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.ConvNeXtXLarge(
    model_name='convnext_xlarge',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.convnext.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.convnext.preprocess_input(
    x, data_format=None
)
",[],[]
"tf.keras.applications.densenet.DenseNet121(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.densenet.DenseNet169(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.densenet.DenseNet201(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.densenet.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.densenet.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.efficientnet.EfficientNetB0(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB1(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB3(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB4(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB5(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB6(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.EfficientNetB7(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.efficientnet.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.efficientnet.preprocess_input(
    x, data_format=None
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B0(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B1(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2B3(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2L(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2M(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.EfficientNetV2S(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.efficientnet_v2.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.efficientnet_v2.preprocess_input(
    x, data_format=None
)
",[],[]
"tf.keras.applications.imagenet_utils.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.imagenet_utils.preprocess_input(
    x, data_format=None, mode='caffe'
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.inception_resnet_v2.InceptionResNetV2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.inception_resnet_v2.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.inception_resnet_v2.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.inception_v3.InceptionV3(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.inception_v3.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.inception_v3.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.mobilenet.MobileNet(
    input_shape=None,
    alpha=1.0,
    depth_multiplier=1,
    dropout=0.001,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.mobilenet.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.mobilenet.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.mobilenet_v2.MobileNetV2(
    input_shape=None,
    alpha=1.0,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax',
    **kwargs
)
",[],[]
"tf.keras.applications.mobilenet_v2.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.mobilenet_v2.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.mobilenet_v3.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.mobilenet_v3.preprocess_input(
    x, data_format=None
)
",[],[]
"tf.keras.applications.nasnet.NASNetLarge(
    input_shape=None,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.nasnet.NASNetMobile(
    input_shape=None,
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.nasnet.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.nasnet.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.regnet.RegNetX002(
    model_name='regnetx002',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX004(
    model_name='regnetx004',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX006(
    model_name='regnetx006',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX008(
    model_name='regnetx008',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX016(
    model_name='regnetx016',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX032(
    model_name='regnetx032',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX040(
    model_name='regnetx040',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX064(
    model_name='regnetx064',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX080(
    model_name='regnetx080',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX120(
    model_name='regnetx120',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX160(
    model_name='regnetx160',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetX320(
    model_name='regnetx320',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY002(
    model_name='regnety002',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY004(
    model_name='regnety004',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY006(
    model_name='regnety006',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY008(
    model_name='regnety008',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY016(
    model_name='regnety016',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY032(
    model_name='regnety032',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY040(
    model_name='regnety040',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY064(
    model_name='regnety064',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY080(
    model_name='regnety080',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY120(
    model_name='regnety120',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY160(
    model_name='regnety160',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.RegNetY320(
    model_name='regnety320',
    include_top=True,
    include_preprocessing=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.regnet.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.regnet.preprocess_input(
    x, data_format=None
)
",[],[]
"tf.keras.applications.resnet.ResNet101(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet.ResNet152(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet50.ResNet50(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet50.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.resnet50.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.resnet50.ResNet50(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    **kwargs
)
",[],[]
"tf.keras.applications.resnet50.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.resnet50.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.resnet_rs.ResNetRS101(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS152(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS200(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS270(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS350(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS420(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.ResNetRS50(
    include_top=True,
    weights='imagenet',
    classes=1000,
    input_shape=None,
    input_tensor=None,
    pooling=None,
    classifier_activation='softmax',
    include_preprocessing=True
)
",[],[]
"tf.keras.applications.resnet_rs.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.resnet_rs.preprocess_input(
    x, data_format=None
)
",[],[]
"tf.keras.applications.resnet_v2.ResNet101V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet_v2.ResNet152V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet_v2.ResNet50V2(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.resnet_v2.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.resnet_v2.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.vgg16.VGG16(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.vgg16.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.vgg16.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.vgg19.VGG19(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.vgg19.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.vgg19.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.keras.applications.xception.Xception(
    include_top=True,
    weights='imagenet',
    input_tensor=None,
    input_shape=None,
    pooling=None,
    classes=1000,
    classifier_activation='softmax'
)
",[],[]
"tf.keras.applications.xception.decode_predictions(
    preds, top=5
)
",[],[]
"tf.keras.applications.xception.preprocess_input(
    x, data_format=None
)
",[],"i = tf.keras.layers.Input([None, None, 3], dtype = tf.uint8)
x = tf.cast(i, tf.float32)
x = tf.keras.applications.mobilenet.preprocess_input(x)
core = tf.keras.applications.MobileNet()
x = core(x)
model = tf.keras.Model(inputs=[i], outputs=[x])

image = tf.image.decode_png(tf.io.read_file('file.png'))
result = model(image)
"
"tf.compat.v1.keras.backend.get_session(
    op_input_list=()
)
",[],[]
"tf.keras.backend.get_uid(
    prefix=''
)
",[],"get_uid('dense')
1
get_uid('dense')
2"
"tf.keras.backend.is_keras_tensor(
    x
)
","[['Returns whether ', 'x', ' is a Keras tensor.']]","np_var = np.array([1, 2])
# A numpy array is not a symbolic tensor.
tf.keras.backend.is_keras_tensor(np_var)
Traceback (most recent call last):
ValueError: Unexpectedly found an instance of type
`<class 'numpy.ndarray'>`.
Expected a symbolic tensor instance.
keras_var = tf.keras.backend.variable(np_var)
# A variable created with the keras backend is not a Keras tensor.
tf.keras.backend.is_keras_tensor(keras_var)
False
keras_placeholder = tf.keras.backend.placeholder(shape=(2, 4, 5))
# A placeholder is a Keras tensor.
tf.keras.backend.is_keras_tensor(keras_placeholder)
True
keras_input = tf.keras.layers.Input([10])
# An Input is a Keras tensor.
tf.keras.backend.is_keras_tensor(keras_input)
True
keras_layer_output = tf.keras.layers.Dense(10)(keras_input)
# Any Keras layer output is a Keras tensor.
tf.keras.backend.is_keras_tensor(keras_layer_output)
True"
"tf.compat.v1.keras.backend.name_scope(
    name, default_name=None, values=None
)
",[],"def my_op(a, b, c, name=None):
  with tf.name_scope(name, ""MyOp"", [a, b, c]) as scope:
    a = tf.convert_to_tensor(a, name=""a"")
    b = tf.convert_to_tensor(b, name=""b"")
    c = tf.convert_to_tensor(c, name=""c"")
    # Define some computation that uses `a`, `b`, and `c`.
    return foo_op(..., name=scope)
"
"tf.keras.backend.rnn(
    step_function,
    inputs,
    initial_states,
    go_backwards=False,
    mask=None,
    constants=None,
    unroll=False,
    input_length=None,
    time_major=False,
    zero_output_for_mask=False,
    return_all_outputs=True
)
",[],"- If `return_all_outputs=True`: a tensor with shape
  `(samples, time, ...)` where each entry `outputs[s, t]` is the
  output of the step function at time `t` for sample `s`
- Else, a tensor equal to `last_output` with shape
  `(samples, 1, ...)`
"
"tf.keras.backend.set_epsilon(
    value
)
",[],"tf.keras.backend.epsilon()
1e-07
tf.keras.backend.set_epsilon(1e-5)
tf.keras.backend.epsilon()
1e-05
tf.keras.backend.set_epsilon(1e-7)"
"tf.keras.backend.set_floatx(
    value
)
",[],"tf.keras.backend.floatx()
'float32'
tf.keras.backend.set_floatx('float64')
tf.keras.backend.floatx()
'float64'
tf.keras.backend.set_floatx('float32')"
"tf.keras.backend.set_image_data_format(
    data_format
)
",[],"tf.keras.backend.image_data_format()
'channels_last'
tf.keras.backend.set_image_data_format('channels_first')
tf.keras.backend.image_data_format()
'channels_first'
tf.keras.backend.set_image_data_format('channels_last')"
"tf.compat.v1.keras.backend.set_session(
    session
)
",[],[]
"tf.keras.callbacks.BaseLogger(
    stateful_metrics=None
)
","[['Inherits From: ', 'Callback']]","set_model(
    model
)
"
"tf.keras.callbacks.CSVLogger(
    filename, separator=',', append=False
)
","[['Inherits From: ', 'Callback']]","csv_logger = CSVLogger('training.log')
model.fit(X_train, Y_train, callbacks=[csv_logger])
"
"tf.keras.callbacks.CallbackList(
    callbacks=None, add_history=False, add_progbar=False, model=None, **params
)
",[],"append(
    callback
)
"
"tf.keras.callbacks.EarlyStopping(
    monitor='val_loss',
    min_delta=0,
    patience=0,
    verbose=0,
    mode='auto',
    baseline=None,
    restore_best_weights=False,
    start_from_epoch=0
)
","[['Inherits From: ', 'Callback']]","callback = tf.keras.callbacks.EarlyStopping(monitor='loss', patience=3)
# This callback will stop the training when there is no improvement in
# the loss for three consecutive epochs.
model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')
history = model.fit(np.arange(100).reshape(5, 20), np.zeros(5),
                    epochs=10, batch_size=1, callbacks=[callback],
                    verbose=0)
len(history.history['loss'])  # Only 4 epochs are run.
4"
"tf.keras.callbacks.LambdaCallback(
    on_epoch_begin=None,
    on_epoch_end=None,
    on_batch_begin=None,
    on_batch_end=None,
    on_train_begin=None,
    on_train_end=None,
    **kwargs
)
","[['Inherits From: ', 'Callback']]","# Print the batch number at the beginning of every batch.
batch_print_callback = LambdaCallback(
    on_batch_begin=lambda batch,logs: print(batch))

# Stream the epoch loss to a file in JSON format. The file content
# is not well-formed JSON but rather has a JSON object per line.
import json
json_log = open('loss_log.json', mode='wt', buffering=1)
json_logging_callback = LambdaCallback(
    on_epoch_end=lambda epoch, logs: json_log.write(
        json.dumps({'epoch': epoch, 'loss': logs['loss']}) + '\n'),
    on_train_end=lambda logs: json_log.close()
)

# Terminate some processes after having finished model training.
processes = ...
cleanup_callback = LambdaCallback(
    on_train_end=lambda logs: [
        p.terminate() for p in processes if p.is_alive()])

model.fit(...,
          callbacks=[batch_print_callback,
                     json_logging_callback,
                     cleanup_callback])
"
"tf.keras.callbacks.LearningRateScheduler(
    schedule, verbose=0
)
","[['Inherits From: ', 'Callback']]","# This function keeps the initial learning rate for the first ten epochs
# and decreases it exponentially after that.
def scheduler(epoch, lr):
  if epoch < 10:
    return lr
  else:
    return lr * tf.math.exp(-0.1)
model = tf.keras.models.Sequential([tf.keras.layers.Dense(10)])
model.compile(tf.keras.optimizers.SGD(), loss='mse')
round(model.optimizer.lr.numpy(), 5)
0.01"
"tf.keras.callbacks.ModelCheckpoint(
    filepath,
    monitor: str = 'val_loss',
    verbose: int = 0,
    save_best_only: bool = False,
    save_weights_only: bool = False,
    mode: str = 'auto',
    save_freq='epoch',
    options=None,
    initial_value_threshold=None,
    **kwargs
)
","[['Inherits From: ', 'Callback']]","model.compile(loss=..., optimizer=...,
              metrics=['accuracy'])

EPOCHS = 10
checkpoint_filepath = '/tmp/checkpoint'
model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(
    filepath=checkpoint_filepath,
    save_weights_only=True,
    monitor='val_accuracy',
    mode='max',
    save_best_only=True)

# Model weights are saved at the end of every epoch, if it's the best seen
# so far.
model.fit(epochs=EPOCHS, callbacks=[model_checkpoint_callback])

# The model weights (that are considered the best) are loaded into the
# model.
model.load_weights(checkpoint_filepath)
"
"tf.keras.callbacks.ProgbarLogger(
    count_mode: str = 'samples', stateful_metrics=None
)
","[['Inherits From: ', 'Callback']]","set_model(
    model
)
"
"tf.keras.callbacks.ReduceLROnPlateau(
    monitor='val_loss',
    factor=0.1,
    patience=10,
    verbose=0,
    mode='auto',
    min_delta=0.0001,
    cooldown=0,
    min_lr=0,
    **kwargs
)
","[['Inherits From: ', 'Callback']]","reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,
                              patience=5, min_lr=0.001)
model.fit(X_train, Y_train, callbacks=[reduce_lr])
"
"tf.keras.callbacks.RemoteMonitor(
    root='http://localhost:9000',
    path='/publish/epoch/end/',
    field='data',
    headers=None,
    send_as_json=False
)
","[['Inherits From: ', 'Callback']]","set_model(
    model
)
"
"tf.compat.v1.keras.callbacks.TensorBoard(
    log_dir='./logs',
    histogram_freq=0,
    batch_size=32,
    write_graph=True,
    write_grads=False,
    write_images=False,
    embeddings_freq=0,
    embeddings_layer_names=None,
    embeddings_metadata=None,
    embeddings_data=None,
    update_freq='epoch',
    profile_batch=2
)
","[['Inherits From: ', 'TensorBoard', ', ', 'Callback']]","tensorboard --logdir=path_to_your_logs
"
"tf.keras.constraints.MaxNorm(
    max_value=2, axis=0
)
","[['Inherits From: ', 'Constraint']]","@classmethod
from_config(
    config
)
"
"tf.keras.constraints.MinMaxNorm(
    min_value=0.0, max_value=1.0, rate=1.0, axis=0
)
","[['Inherits From: ', 'Constraint']]","@classmethod
from_config(
    config
)
"
"tf.keras.constraints.UnitNorm(
    axis=0
)
","[['Inherits From: ', 'Constraint']]","@classmethod
from_config(
    config
)
"
"tf.keras.constraints.deserialize(
    config, custom_objects=None
)
",[],[]
"tf.keras.constraints.get(
    identifier
)
",[],[]
"tf.keras.constraints.MaxNorm(
    max_value=2, axis=0
)
","[['Inherits From: ', 'Constraint']]","@classmethod
from_config(
    config
)
"
"tf.keras.constraints.MinMaxNorm(
    min_value=0.0, max_value=1.0, rate=1.0, axis=0
)
","[['Inherits From: ', 'Constraint']]","@classmethod
from_config(
    config
)
"
"tf.keras.constraints.serialize(
    constraint
)
",[],[]
"tf.keras.constraints.UnitNorm(
    axis=0
)
","[['Inherits From: ', 'Constraint']]","@classmethod
from_config(
    config
)
"
"tf.keras.datasets.boston_housing.load_data(
    path='boston_housing.npz', test_split=0.2, seed=113
)
","[[None, '\n']]",[]
"tf.keras.datasets.cifar100.load_data(
    label_mode='fine'
)
",[],"(x_train, y_train), (x_test, y_test) = keras.datasets.cifar100.load_data()
assert x_train.shape == (50000, 32, 32, 3)
assert x_test.shape == (10000, 32, 32, 3)
assert y_train.shape == (50000, 1)
assert y_test.shape == (10000, 1)
"
"tf.keras.datasets.imdb.get_word_index(
    path='imdb_word_index.json'
)
",[],"# Use the default parameters to keras.datasets.imdb.load_data
start_char = 1
oov_char = 2
index_from = 3
# Retrieve the training sequences.
(x_train, _), _ = keras.datasets.imdb.load_data(
    start_char=start_char, oov_char=oov_char, index_from=index_from
)
# Retrieve the word index file mapping words to indices
word_index = keras.datasets.imdb.get_word_index()
# Reverse the word index to obtain a dict mapping indices to words
# And add `index_from` to indices to sync with `x_train`
inverted_word_index = dict(
    (i + index_from, word) for (word, i) in word_index.items()
)
# Update `inverted_word_index` to include `start_char` and `oov_char`
inverted_word_index[start_char] = ""[START]""
inverted_word_index[oov_char] = ""[OOV]""
# Decode the first sequence in the dataset
decoded_sequence = "" "".join(inverted_word_index[i] for i in x_train[0])
"
"tf.keras.datasets.imdb.load_data(
    path='imdb.npz',
    num_words=None,
    skip_top=0,
    maxlen=None,
    seed=113,
    start_char=1,
    oov_char=2,
    index_from=3,
    **kwargs
)
","[['Loads the ', 'IMDB dataset', '.']]",[]
"tf.keras.datasets.mnist.load_data(
    path='mnist.npz'
)
",[],"(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()
assert x_train.shape == (60000, 28, 28)
assert x_test.shape == (10000, 28, 28)
assert y_train.shape == (60000,)
assert y_test.shape == (10000,)
"
"tf.keras.datasets.reuters.get_word_index(
    path='reuters_word_index.json'
)
",[],[]
"tf.keras.datasets.reuters.load_data(
    path='reuters.npz',
    num_words=None,
    skip_top=0,
    maxlen=None,
    test_split=0.2,
    seed=113,
    start_char=1,
    oov_char=2,
    index_from=3,
    **kwargs
)
",[],[]
"tf.compat.v1.keras.estimator.model_to_estimator(
    keras_model=None,
    keras_model_path=None,
    custom_objects=None,
    model_dir=None,
    config=None,
    checkpoint_format='saver',
    metric_names_map=None,
    export_outputs=None
)
","[['Constructs an ', 'Estimator', ' instance from given keras model.']]","keras_model = tf.keras.Model(...)
keras_model.compile(...)

estimator = tf.keras.estimator.model_to_estimator(keras_model)

def input_fn():
  return dataset_ops.Dataset.from_tensors(
      ({'features': features, 'sample_weights': sample_weights},
       targets))

estimator.train(input_fn, steps=1)
"
"tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate, decay_steps, alpha=0.0, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  step = min(step, decay_steps)
  cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))
  decayed = (1 - alpha) * cosine_decay + alpha
  return initial_learning_rate * decayed
"
"tf.keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate,
    first_decay_steps,
    t_mul=2.0,
    m_mul=1.0,
    alpha=0.0,
    name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","first_decay_steps = 1000
lr_decayed_fn = (
  tf.keras.optimizers.schedules.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps))
"
"tf.keras.experimental.LinearModel(
    units=1,
    activation=None,
    use_bias=True,
    kernel_initializer='zeros',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    **kwargs
)
","[[None, '\n'], ['Inherits From: ', 'Model', ', ', 'Layer', ', ', 'Module']]","model = LinearModel()
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, epochs=epochs)
"
"tf.keras.experimental.SequenceFeatures(
    feature_columns, trainable=True, name=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","
import tensorflow as tf

# Behavior of some cells or feature columns may depend on whether we are in
# training or inference mode, e.g. applying dropout.
training = True
rating = tf.feature_column.sequence_numeric_column('rating')
watches = tf.feature_column.sequence_categorical_column_with_identity(
    'watches', num_buckets=1000)
watches_embedding = tf.feature_column.embedding_column(watches,
                                            dimension=10)
columns = [rating, watches_embedding]

features = {
 'rating': tf.sparse.from_dense([[1.0,1.1, 0, 0, 0],
                                             [2.0,2.1,2.2, 2.3, 2.5]]),
 'watches': tf.sparse.from_dense([[2, 85, 0, 0, 0],[33,78, 2, 73, 1]])
}

sequence_input_layer = tf.keras.experimental.SequenceFeatures(columns)
sequence_input, sequence_length = sequence_input_layer(
   features, training=training)
sequence_length_mask = tf.sequence_mask(sequence_length)
hidden_size = 32
rnn_cell = tf.keras.layers.SimpleRNNCell(hidden_size)
rnn_layer = tf.keras.layers.RNN(rnn_cell)
outputs, state = rnn_layer(sequence_input, mask=sequence_length_mask)
"
"tf.keras.experimental.WideDeepModel(
    linear_model, dnn_model, activation=None, **kwargs
)
","[['Inherits From: ', 'Model', ', ', 'Layer', ', ', 'Module']]","linear_model = LinearModel()
dnn_model = keras.Sequential([keras.layers.Dense(units=64),
                             keras.layers.Dense(units=1)])
combined_model = WideDeepModel(linear_model, dnn_model)
combined_model.compile(optimizer=['sgd', 'adam'],
                       loss='mse', metrics=['mse'])
# define dnn_inputs and linear_inputs as separate numpy arrays or
# a single numpy array if dnn_inputs is same as linear_inputs.
combined_model.fit([linear_inputs, dnn_inputs], y, epochs)
# or define a single `tf.data.Dataset` that contains a single tensor or
# separate tensors for dnn_inputs and linear_inputs.
dataset = tf.data.Dataset.from_tensors(([linear_inputs, dnn_inputs], y))
combined_model.fit(dataset, epochs)
"
"tf.compat.v1.keras.initializers.Constant(
    value=0,
    dtype=tf.dtypes.float32,
    verify_shape=False
)
",[],"value = [0, 1, 2, 3, 4, 5, 6, 7]
initializer = tf.compat.v1.constant_initializer(
    value=value,
    dtype=tf.float32,
    verify_shape=False)
variable = tf.Variable(initializer(shape=[2, 4]))
"
"tf.compat.v1.keras.initializers.Identity(
    gain=1.0,
    dtype=tf.dtypes.float32
)
",[],"@classmethod
from_config(
    config
)
"
"tf.compat.v1.keras.initializers.Ones(
    dtype=tf.dtypes.float32
)
",[],">>> initializer = tf.compat.v1.keras.initializers.ones()
>>> initializer((1, 1))
<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>
"
"tf.compat.v1.keras.initializers.Orthogonal(
    gain=1.0,
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"@classmethod
from_config(
    config
)
"
"tf.compat.v1.keras.initializers.RandomNormal(
    mean=0.0,
    stddev=0.05,
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'random_normal_initializer']]","initializer = tf.compat.v1.keras.initializers.RandomNormal(
  mean=mean,
  stddev=stddev,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.keras.initializers.RandomUniform(
    minval=-0.05,
    maxval=0.05,
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'random_uniform_initializer']]","
initializer = tf.compat.v1.keras.initializers.RandomUniform(
  minval=minval,
  maxval=maxval,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.keras.initializers.TruncatedNormal(
    mean=0.0,
    stddev=0.05,
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'truncated_normal_initializer']]","initializer = tf.compat.v1.keras.initializers.TruncatedNormal(
  mean=mean,
  stddev=stddev,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"initializer = tf.compat.v1.variance_scaling_initializer(
  scale=scale,
  mode=mode,
  distribution=distribution
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.keras.initializers.Zeros(
    dtype=tf.dtypes.float32
)
",[],"initializer = tf.compat.v1.zeros_initializer(dtype=tf.float32)
variable = tf.Variable(initializer(shape=[3, 3]))
"
"tf.compat.v1.keras.initializers.Constant(
    value=0,
    dtype=tf.dtypes.float32,
    verify_shape=False
)
",[],"value = [0, 1, 2, 3, 4, 5, 6, 7]
initializer = tf.compat.v1.constant_initializer(
    value=value,
    dtype=tf.float32,
    verify_shape=False)
variable = tf.Variable(initializer(shape=[2, 4]))
"
"tf.keras.initializers.deserialize(
    config, custom_objects=None
)
","[['Return an ', 'Initializer', ' object from its config.']]",[]
"tf.keras.initializers.get(
    identifier
)
",[],"identifier = 'Ones'
tf.keras.initializers.deserialize(identifier)
<...keras.initializers.initializers_v2.Ones...>"
"tf.compat.v1.keras.initializers.glorot_normal(
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'VarianceScaling']]","@classmethod
from_config(
    config
)
"
"tf.compat.v1.keras.initializers.glorot_uniform(
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'VarianceScaling']]","@classmethod
from_config(
    config
)
"
"tf.compat.v1.keras.initializers.he_normal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling']]","initializer = tf.compat.v1.variance_scaling_initializer(
  scale=scale,
  mode=mode,
  distribution=distribution
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.keras.initializers.he_uniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling']]","initializer = tf.compat.v1.variance_scaling_initializer(
  scale=scale,
  mode=mode,
  distribution=distribution
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.keras.initializers.Identity(
    gain=1.0,
    dtype=tf.dtypes.float32
)
",[],"@classmethod
from_config(
    config
)
"
"tf.compat.v1.keras.initializers.lecun_normal(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling']]","initializer = tf.compat.v1.variance_scaling_initializer(
  scale=scale,
  mode=mode,
  distribution=distribution
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.keras.initializers.lecun_uniform(
    seed=None
)
","[['Inherits From: ', 'VarianceScaling']]","initializer = tf.compat.v1.variance_scaling_initializer(
  scale=scale,
  mode=mode,
  distribution=distribution
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.keras.initializers.RandomNormal(
    mean=0.0,
    stddev=0.05,
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'random_normal_initializer']]","initializer = tf.compat.v1.keras.initializers.RandomNormal(
  mean=mean,
  stddev=stddev,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.keras.initializers.Ones(
    dtype=tf.dtypes.float32
)
",[],">>> initializer = tf.compat.v1.keras.initializers.ones()
>>> initializer((1, 1))
<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>
"
"tf.compat.v1.keras.initializers.Orthogonal(
    gain=1.0,
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"@classmethod
from_config(
    config
)
"
"tf.compat.v1.keras.initializers.RandomNormal(
    mean=0.0,
    stddev=0.05,
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'random_normal_initializer']]","initializer = tf.compat.v1.keras.initializers.RandomNormal(
  mean=mean,
  stddev=stddev,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.keras.initializers.RandomUniform(
    minval=-0.05,
    maxval=0.05,
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'random_uniform_initializer']]","
initializer = tf.compat.v1.keras.initializers.RandomUniform(
  minval=minval,
  maxval=maxval,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.keras.initializers.serialize(
    initializer
)
",[],[]
"tf.compat.v1.keras.initializers.TruncatedNormal(
    mean=0.0,
    stddev=0.05,
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'truncated_normal_initializer']]","initializer = tf.compat.v1.keras.initializers.TruncatedNormal(
  mean=mean,
  stddev=stddev,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.keras.initializers.RandomUniform(
    minval=-0.05,
    maxval=0.05,
    seed=None,
    dtype=tf.dtypes.float32
)
","[['Inherits From: ', 'random_uniform_initializer']]","
initializer = tf.compat.v1.keras.initializers.RandomUniform(
  minval=minval,
  maxval=maxval,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.keras.initializers.Zeros(
    dtype=tf.dtypes.float32
)
",[],"initializer = tf.compat.v1.zeros_initializer(dtype=tf.float32)
variable = tf.Variable(initializer(shape=[3, 3]))
"
"tf.keras.layers.AbstractRNNCell(
    trainable=True, name=None, dtype=None, dynamic=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","  class MinimalRNNCell(AbstractRNNCell):

    def __init__(self, units, **kwargs):
      self.units = units
      super(MinimalRNNCell, self).__init__(**kwargs)

    @property
    def state_size(self):
      return self.units

    def build(self, input_shape):
      self.kernel = self.add_weight(shape=(input_shape[-1], self.units),
                                    initializer='uniform',
                                    name='kernel')
      self.recurrent_kernel = self.add_weight(
          shape=(self.units, self.units),
          initializer='uniform',
          name='recurrent_kernel')
      self.built = True

    def call(self, inputs, states):
      prev_output = states[0]
      h = backend.dot(inputs, self.kernel)
      output = h + backend.dot(prev_output, self.recurrent_kernel)
      return output, output
"
"tf.keras.layers.Activation(
    activation, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.Activation('relu')
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]
layer = tf.keras.layers.Activation(tf.nn.relu)
output = layer([-3.0, -1.0, 0.0, 2.0])
list(output.numpy())
[0.0, 0.0, 0.0, 2.0]"
"tf.keras.layers.ActivityRegularization(
    l1=0.0, l2=0.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Add(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 3, 4)
x1 = tf.random.normal(input_shape)
x2 = tf.random.normal(input_shape)
y = tf.keras.layers.Add()([x1, x2])
print(y.shape)
(2, 3, 4)"
"tf.keras.layers.AdditiveAttention(
    use_scale=True, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# Variable-length int sequences.
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

# Embedding lookup.
token_embedding = tf.keras.layers.Embedding(max_tokens, dimension)
# Query embeddings of shape [batch_size, Tq, dimension].
query_embeddings = token_embedding(query_input)
# Value embeddings of shape [batch_size, Tv, dimension].
value_embeddings = token_embedding(value_input)

# CNN layer.
cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    # Use 'same' padding so outputs have the same shape as inputs.
    padding='same')
# Query encoding of shape [batch_size, Tq, filters].
query_seq_encoding = cnn_layer(query_embeddings)
# Value encoding of shape [batch_size, Tv, filters].
value_seq_encoding = cnn_layer(value_embeddings)

# Query-value attention of shape [batch_size, Tq, filters].
query_value_attention_seq = tf.keras.layers.AdditiveAttention()(
    [query_seq_encoding, value_seq_encoding])

# Reduce over the sequence axis to produce encodings of shape
# [batch_size, filters].
query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

# Concatenate query and document encodings to produce a DNN input layer.
input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])

# Add DNN layers, and create Model.
# ...
"
"tf.keras.layers.AlphaDropout(
    rate, noise_shape=None, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Attention(
    use_scale=False, score_mode='dot', **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# Variable-length int sequences.
query_input = tf.keras.Input(shape=(None,), dtype='int32')
value_input = tf.keras.Input(shape=(None,), dtype='int32')

# Embedding lookup.
token_embedding = tf.keras.layers.Embedding(input_dim=1000, output_dim=64)
# Query embeddings of shape [batch_size, Tq, dimension].
query_embeddings = token_embedding(query_input)
# Value embeddings of shape [batch_size, Tv, dimension].
value_embeddings = token_embedding(value_input)

# CNN layer.
cnn_layer = tf.keras.layers.Conv1D(
    filters=100,
    kernel_size=4,
    # Use 'same' padding so outputs have the same shape as inputs.
    padding='same')
# Query encoding of shape [batch_size, Tq, filters].
query_seq_encoding = cnn_layer(query_embeddings)
# Value encoding of shape [batch_size, Tv, filters].
value_seq_encoding = cnn_layer(value_embeddings)

# Query-value attention of shape [batch_size, Tq, filters].
query_value_attention_seq = tf.keras.layers.Attention()(
    [query_seq_encoding, value_seq_encoding])

# Reduce over the sequence axis to produce encodings of shape
# [batch_size, filters].
query_encoding = tf.keras.layers.GlobalAveragePooling1D()(
    query_seq_encoding)
query_value_attention = tf.keras.layers.GlobalAveragePooling1D()(
    query_value_attention_seq)

# Concatenate query and document encodings to produce a DNN input layer.
input_layer = tf.keras.layers.Concatenate()(
    [query_encoding, query_value_attention])

# Add DNN layers, and create Model.
# ...
"
"tf.keras.layers.Average(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x1 = np.ones((2, 2))
x2 = np.zeros((2, 2))
y = tf.keras.layers.Average()([x1, x2])
y.numpy().tolist()
[[0.5, 0.5], [0.5, 0.5]]"
"tf.keras.layers.AveragePooling1D(
    pool_size=2,
    strides=None,
    padding='valid',
    data_format='channels_last',
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
   strides=1, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5]]], dtype=float32)>"
"tf.keras.layers.AveragePooling2D(
    pool_size=(2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([[1., 2., 3.],
                 [4., 5., 6.],
                 [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
   strides=(1, 1), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.]],
          [[6.],
           [7.]]]], dtype=float32)>"
"tf.keras.layers.AveragePooling3D(
    pool_size=(2, 2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","depth = 30
height = 30
width = 30
input_channels = 3

inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.AveragePooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
"
"tf.keras.layers.AveragePooling1D(
    pool_size=2,
    strides=None,
    padding='valid',
    data_format='channels_last',
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
x
<tf.Tensor: shape=(1, 5, 1), dtype=float32, numpy=
  array([[[1.],
          [2.],
          [3.],
          [4.],
          [5.]], dtype=float32)>
avg_pool_1d = tf.keras.layers.AveragePooling1D(pool_size=2,
   strides=1, padding='valid')
avg_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[1.5],
        [2.5],
        [3.5],
        [4.5]]], dtype=float32)>"
"tf.keras.layers.AveragePooling2D(
    pool_size=(2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([[1., 2., 3.],
                 [4., 5., 6.],
                 [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
avg_pool_2d = tf.keras.layers.AveragePooling2D(pool_size=(2, 2),
   strides=(1, 1), padding='valid')
avg_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[3.],
           [4.]],
          [[6.],
           [7.]]]], dtype=float32)>"
"tf.keras.layers.AveragePooling3D(
    pool_size=(2, 2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","depth = 30
height = 30
width = 30
input_channels = 3

inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.AveragePooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
"
"tf.compat.v1.keras.layers.BatchNormalization(
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer='zeros',
    gamma_initializer='ones',
    moving_mean_initializer='zeros',
    moving_variance_initializer='ones',
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    renorm=False,
    renorm_clipping=None,
    renorm_momentum=0.99,
    fused=None,
    trainable=True,
    virtual_batch_size=None,
    adjustment=None,
    name=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Bidirectional(
    layer,
    merge_mode='concat',
    weights=None,
    backward_layer=None,
    **kwargs
)
","[['Inherits From: ', 'Wrapper', ', ', 'Layer', ', ', 'Module']]","model = Sequential()
model.add(Bidirectional(LSTM(10, return_sequences=True),
                             input_shape=(5, 10)))
model.add(Bidirectional(LSTM(10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')

# With custom backward layer
model = Sequential()
forward_layer = LSTM(10, return_sequences=True)
backward_layer = LSTM(10, activation='relu', return_sequences=True,
                      go_backwards=True)
model.add(Bidirectional(forward_layer, backward_layer=backward_layer,
                        input_shape=(5, 10)))
model.add(Dense(5))
model.add(Activation('softmax'))
model.compile(loss='categorical_crossentropy', optimizer='rmsprop')
"
"tf.keras.layers.CategoryEncoding(
    num_tokens=None, output_mode='multi_hot', sparse=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode=""one_hot"")
layer([3, 2, 0, 1])
<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
  array([[0., 0., 0., 1.],
         [0., 0., 1., 0.],
         [1., 0., 0., 0.],
         [0., 1., 0., 0.]], dtype=float32)>"
"tf.keras.layers.CenterCrop(
    height, width, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Concatenate(
    axis=-1, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = np.arange(20).reshape(2, 2, 5)
print(x)
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]
 [[10 11 12 13 14]
  [15 16 17 18 19]]]
y = np.arange(20, 30).reshape(2, 1, 5)
print(y)
[[[20 21 22 23 24]]
 [[25 26 27 28 29]]]
tf.keras.layers.Concatenate(axis=1)([x, y])
<tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=
array([[[ 0,  1,  2,  3,  4],
        [ 5,  6,  7,  8,  9],
        [20, 21, 22, 23, 24]],
       [[10, 11, 12, 13, 14],
        [15, 16, 17, 18, 19],
        [25, 26, 27, 28, 29]]])>"
"tf.keras.layers.Conv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format='channels_last',
    dilation_rate=1,
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# The inputs are 128-length vectors with 10 timesteps, and the
# batch size is 4.
input_shape = (4, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu',input_shape=input_shape[1:])(x)
print(y.shape)
(4, 8, 32)"
"tf.keras.layers.Conv1DTranspose(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    output_padding=None,
    data_format=None,
    dilation_rate=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Conv1D', ', ', 'Layer', ', ', 'Module']]","new_timesteps = ((timesteps - 1) * strides + kernel_size -
2 * padding + output_padding)
"
"tf.keras.layers.Conv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# The inputs are 28x28 RGB images with `channels_last` and the batch
# size is 4.
input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 2)"
"tf.keras.layers.Conv2DTranspose(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    output_padding=None,
    data_format=None,
    dilation_rate=(1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Conv2D', ', ', 'Layer', ', ', 'Module']]","new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +
output_padding[0])
new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +
output_padding[1])
"
"tf.keras.layers.Conv3D(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# The inputs are 28x28x28 volumes with a single channel, and the
# batch size is 4
input_shape =(4, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 26, 2)"
"tf.keras.layers.Conv3DTranspose(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    output_padding=None,
    data_format=None,
    dilation_rate=(1, 1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Conv3D', ', ', 'Layer', ', ', 'Module']]","new_depth = ((depth - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +
output_padding[0])
new_rows = ((rows - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +
output_padding[1])
new_cols = ((cols - 1) * strides[2] + kernel_size[2] - 2 * padding[2] +
output_padding[2])
"
"tf.keras.layers.ConvLSTM1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format=None,
    dilation_rate=1,
    activation='tanh',
    recurrent_activation='hard_sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","reset_states(
    states=None
)
"
"tf.keras.layers.ConvLSTM2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    activation='tanh',
    recurrent_activation='hard_sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","reset_states(
    states=None
)
"
"tf.keras.layers.ConvLSTM3D(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1, 1),
    activation='tanh',
    recurrent_activation='hard_sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","reset_states(
    states=None
)
"
"tf.keras.layers.Conv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format='channels_last',
    dilation_rate=1,
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# The inputs are 128-length vectors with 10 timesteps, and the
# batch size is 4.
input_shape = (4, 10, 128)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv1D(
32, 3, activation='relu',input_shape=input_shape[1:])(x)
print(y.shape)
(4, 8, 32)"
"tf.keras.layers.Conv1DTranspose(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    output_padding=None,
    data_format=None,
    dilation_rate=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Conv1D', ', ', 'Layer', ', ', 'Module']]","new_timesteps = ((timesteps - 1) * strides + kernel_size -
2 * padding + output_padding)
"
"tf.keras.layers.Conv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# The inputs are 28x28 RGB images with `channels_last` and the batch
# size is 4.
input_shape = (4, 28, 28, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv2D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 2)"
"tf.keras.layers.Conv2DTranspose(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    output_padding=None,
    data_format=None,
    dilation_rate=(1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Conv2D', ', ', 'Layer', ', ', 'Module']]","new_rows = ((rows - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +
output_padding[0])
new_cols = ((cols - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +
output_padding[1])
"
"tf.keras.layers.Conv3D(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1, 1),
    groups=1,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# The inputs are 28x28x28 volumes with a single channel, and the
# batch size is 4
input_shape =(4, 28, 28, 28, 1)
x = tf.random.normal(input_shape)
y = tf.keras.layers.Conv3D(
2, 3, activation='relu', input_shape=input_shape[1:])(x)
print(y.shape)
(4, 26, 26, 26, 2)"
"tf.keras.layers.Conv3DTranspose(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    output_padding=None,
    data_format=None,
    dilation_rate=(1, 1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Conv3D', ', ', 'Layer', ', ', 'Module']]","new_depth = ((depth - 1) * strides[0] + kernel_size[0] - 2 * padding[0] +
output_padding[0])
new_rows = ((rows - 1) * strides[1] + kernel_size[1] - 2 * padding[1] +
output_padding[1])
new_cols = ((cols - 1) * strides[2] + kernel_size[2] - 2 * padding[2] +
output_padding[2])
"
"tf.keras.layers.Cropping1D(
    cropping=(1, 1), **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 3, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1]
  [ 2  3]
  [ 4  5]]
 [[ 6  7]
  [ 8  9]
  [10 11]]]
y = tf.keras.layers.Cropping1D(cropping=1)(x)
print(y)
tf.Tensor(
  [[[2 3]]
   [[8 9]]], shape=(2, 1, 2), dtype=int64)"
"tf.keras.layers.Cropping2D(
    cropping=((0, 0), (0, 0)), data_format=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 28, 28, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping2D(cropping=((2, 2), (4, 4)))(x)
print(y.shape)
(2, 24, 20, 3)"
"tf.keras.layers.Cropping3D(
    cropping=((1, 1), (1, 1), (1, 1)), data_format=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 28, 28, 10, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.Cropping3D(cropping=(2, 4, 2))(x)
print(y.shape)
(2, 24, 20, 6, 3)"
"tf.compat.v1.keras.layers.CuDNNGRU(
    units,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","get_losses_for(
    inputs=None
)
"
"tf.compat.v1.keras.layers.CuDNNLSTM(
    units,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","get_losses_for(
    inputs=None
)
"
"tf.keras.layers.Dense(
    units,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# Create a `Sequential` model and add a Dense layer as the first layer.
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(32, activation='relu'))
# Now the model will take as input arrays of shape (None, 16)
# and output arrays of shape (None, 32).
# Note that after the first layer, you don't need to specify
# the size of the input anymore:
model.add(tf.keras.layers.Dense(32))
model.output_shape
(None, 32)"
"tf.compat.v1.keras.layers.DenseFeatures(
    feature_columns, trainable=True, name=None, partitioner=None, **kwargs
)
","[['A layer that produces a dense ', 'Tensor', ' based on given ', 'feature_columns', '.'], ['Inherits From: ', 'Layer', ', ', 'Module']]","price = tf.feature_column.numeric_column('price')
keywords_embedded = tf.feature_column.embedding_column(
    tf.feature_column.categorical_column_with_hash_bucket(""keywords"", 10K),
    dimension=16)
columns = [price, keywords_embedded, ...]
partitioner = tf.compat.v1.fixed_size_partitioner(num_shards=4)
feature_layer = tf.compat.v1.keras.layers.DenseFeatures(
    feature_columns=columns, partitioner=partitioner)

features = tf.io.parse_example(
    ..., features=tf.feature_column.make_parse_example_spec(columns))
dense_tensor = feature_layer(features)
for units in [128, 64, 32]:
  dense_tensor = tf.compat.v1.keras.layers.Dense(
                     units, activation='relu')(dense_tensor)
prediction = tf.compat.v1.keras.layers.Dense(1)(dense_tensor)
"
"tf.keras.layers.DepthwiseConv1D(
    kernel_size,
    strides=1,
    padding='valid',
    depth_multiplier=1,
    data_format=None,
    dilation_rate=1,
    activation=None,
    use_bias=True,
    depthwise_initializer='glorot_uniform',
    bias_initializer='zeros',
    depthwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","convolution_op(
    inputs, kernel
)
"
"tf.keras.layers.DepthwiseConv2D(
    kernel_size,
    strides=(1, 1),
    padding='valid',
    depth_multiplier=1,
    data_format=None,
    dilation_rate=(1, 1),
    activation=None,
    use_bias=True,
    depthwise_initializer='glorot_uniform',
    bias_initializer='zeros',
    depthwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","convolution_op(
    inputs, kernel
)
"
"tf.keras.layers.Discretization(
    bin_boundaries=None,
    num_bins=None,
    epsilon=0.01,
    output_mode='int',
    sparse=False,
    **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]",">>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])
>>> layer = tf.keras.layers.Discretization(bin_boundaries=[0., 1., 2.])
>>> layer(input)
<tf.Tensor: shape=(2, 4), dtype=int64, numpy=
array([[0, 2, 3, 1],
       [1, 3, 2, 1]])>
"
"tf.keras.layers.Dot(
    axes, normalize=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = np.arange(10).reshape(1, 5, 2)
print(x)
[[[0 1]
  [2 3]
  [4 5]
  [6 7]
  [8 9]]]
y = np.arange(10, 20).reshape(1, 2, 5)
print(y)
[[[10 11 12 13 14]
  [15 16 17 18 19]]]
tf.keras.layers.Dot(axes=(1, 2))([x, y])
<tf.Tensor: shape=(1, 2, 2), dtype=int64, numpy=
array([[[260, 360],
        [320, 445]]])>"
"tf.keras.layers.Dropout(
    rate, noise_shape=None, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","tf.random.set_seed(0)
layer = tf.keras.layers.Dropout(.2, input_shape=(2,))
data = np.arange(10).reshape(5, 2).astype(np.float32)
print(data)
[[0. 1.]
 [2. 3.]
 [4. 5.]
 [6. 7.]
 [8. 9.]]
outputs = layer(data, training=True)
print(outputs)
tf.Tensor(
[[ 0.    1.25]
 [ 2.5   3.75]
 [ 5.    6.25]
 [ 7.5   8.75]
 [10.    0.  ]], shape=(5, 2), dtype=float32)"
"tf.keras.layers.ELU(
    alpha=1.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","  f(x) =  alpha * (exp(x) - 1.) for x < 0
  f(x) = x for x >= 0
"
"tf.keras.layers.EinsumDense(
    equation,
    output_shape,
    activation=None,
    bias_axes=None,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['A layer that uses ', 'tf.einsum', ' as the backing computation.'], ['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.EinsumDense(""ab,bc->ac"",
                                    output_shape=64,
                                    bias_axes=""c"")
input_tensor = tf.keras.Input(shape=[32])
output_tensor = layer(input_tensor)
output_tensor
<... shape=(None, 64) dtype=...>"
"tf.keras.layers.Embedding(
    input_dim,
    output_dim,
    embeddings_initializer='uniform',
    embeddings_regularizer=None,
    activity_regularizer=None,
    embeddings_constraint=None,
    mask_zero=False,
    input_length=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","model = tf.keras.Sequential()
model.add(tf.keras.layers.Embedding(1000, 64, input_length=10))
# The model will take as input an integer matrix of size (batch,
# input_length), and the largest integer (i.e. word index) in the input
# should be no larger than 999 (vocabulary size).
# Now model.output_shape is (None, 10, 64), where `None` is the batch
# dimension.
input_array = np.random.randint(1000, size=(32, 10))
model.compile('rmsprop', 'mse')
output_array = model.predict(input_array)
print(output_array.shape)
(32, 10, 64)"
"tf.keras.layers.Flatten(
    data_format=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","model = tf.keras.Sequential()
model.add(tf.keras.layers.Conv2D(64, 3, 3, input_shape=(3, 32, 32)))
model.output_shape
(None, 1, 10, 64)"
"tf.compat.v1.keras.layers.GRU(
    units,
    activation='tanh',
    recurrent_activation='hard_sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    unroll=False,
    reset_after=False,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","reset_states(
    states=None
)
"
"tf.compat.v1.keras.layers.GRUCell(
    units,
    activation='tanh',
    recurrent_activation='hard_sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    reset_after=False,
    **kwargs
)
","[['Inherits From: ', 'GRUCell', ', ', 'Layer', ', ', 'Module']]","get_dropout_mask_for_cell(
    inputs, training, count=1
)
"
"tf.keras.layers.GaussianDropout(
    rate, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.GaussianNoise(
    stddev, seed=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.GlobalAveragePooling1D(
    data_format='channels_last', **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 3, 4)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling1D()(x)
print(y.shape)
(2, 4)"
"tf.keras.layers.GlobalAveragePooling2D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalAveragePooling3D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.GlobalAveragePooling1D(
    data_format='channels_last', **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 3, 4)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling1D()(x)
print(y.shape)
(2, 4)"
"tf.keras.layers.GlobalAveragePooling2D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalAveragePooling2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalAveragePooling3D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.GlobalMaxPool1D(
    data_format='channels_last', keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])
x = tf.reshape(x, [3, 3, 1])
x
<tf.Tensor: shape=(3, 3, 1), dtype=float32, numpy=
array([[[1.], [2.], [3.]],
       [[4.], [5.], [6.]],
       [[7.], [8.], [9.]]], dtype=float32)>
max_pool_1d = tf.keras.layers.GlobalMaxPooling1D()
max_pool_1d(x)
<tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[3.],
       [6.],
       [9.], dtype=float32)>"
"tf.keras.layers.GlobalMaxPool2D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalMaxPool2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalMaxPool3D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.GlobalMaxPool1D(
    data_format='channels_last', keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([[1., 2., 3.], [4., 5., 6.], [7., 8., 9.]])
x = tf.reshape(x, [3, 3, 1])
x
<tf.Tensor: shape=(3, 3, 1), dtype=float32, numpy=
array([[[1.], [2.], [3.]],
       [[4.], [5.], [6.]],
       [[7.], [8.], [9.]]], dtype=float32)>
max_pool_1d = tf.keras.layers.GlobalMaxPooling1D()
max_pool_1d(x)
<tf.Tensor: shape=(3, 1), dtype=float32, numpy=
array([[3.],
       [6.],
       [9.], dtype=float32)>"
"tf.keras.layers.GlobalMaxPool2D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 4, 5, 3)
x = tf.random.normal(input_shape)
y = tf.keras.layers.GlobalMaxPool2D()(x)
print(y.shape)
(2, 3)"
"tf.keras.layers.GlobalMaxPool3D(
    data_format=None, keepdims=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Hashing(
    num_bins,
    mask_value=None,
    salt=None,
    output_mode='int',
    sparse=False,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.Hashing(num_bins=3)
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
  array([[1],
         [0],
         [1],
         [1],
         [2]])>"
"tf.keras.Input(
    shape=None,
    batch_size=None,
    name=None,
    dtype=None,
    sparse=None,
    tensor=None,
    ragged=None,
    type_spec=None,
    **kwargs
)
","[['Input()', ' is used to instantiate a Keras tensor.']]","# this is a logistic regression in Keras
x = Input(shape=(32,))
y = Dense(16, activation='softmax')(x)
model = Model(x, y)
"
"tf.keras.layers.InputLayer(
    input_shape=None,
    batch_size=None,
    dtype=None,
    input_tensor=None,
    sparse=None,
    name=None,
    ragged=None,
    type_spec=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# With explicit InputLayer.
model = tf.keras.Sequential([
  tf.keras.layers.InputLayer(input_shape=(4,)),
  tf.keras.layers.Dense(8)])
model.compile(tf.keras.optimizers.RMSprop(0.001), loss='mse')
model.fit(np.zeros((10, 4)),
          np.ones((10, 8)))

# Without InputLayer and let the first layer to have the input_shape.
# Keras will add a input for the model behind the scene.
model = tf.keras.Sequential([
  tf.keras.layers.Dense(8, input_shape=(4,))])
model.compile(tf.keras.optimizers.RMSprop(0.001), loss='mse')
model.fit(np.zeros((10, 4)),
          np.ones((10, 8)))
"
"tf.keras.layers.InputSpec(
    dtype=None,
    shape=None,
    ndim=None,
    max_ndim=None,
    min_ndim=None,
    axes=None,
    allow_last_axis_squeeze=False,
    name=None
)
",[],"class MyLayer(Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        # The layer will accept inputs with
        # shape (?, 28, 28) & (?, 28, 28, 1)
        # and raise an appropriate error message otherwise.
        self.input_spec = InputSpec(
            shape=(None, 28, 28, 1),
            allow_last_axis_squeeze=True)
"
"tf.compat.v1.keras.layers.LSTM(
    units,
    activation='tanh',
    recurrent_activation='hard_sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    unroll=False,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","reset_states(
    states=None
)
"
"tf.compat.v1.keras.layers.LSTMCell(
    units,
    activation='tanh',
    recurrent_activation='hard_sigmoid',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    unit_forget_bias=True,
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","[['Inherits From: ', 'LSTMCell', ', ', 'Layer', ', ', 'Module']]","get_dropout_mask_for_cell(
    inputs, training, count=1
)
"
"tf.keras.layers.Lambda(
    function, output_shape=None, mask=None, arguments=None, **kwargs
)
","[['Wraps arbitrary expressions as a ', 'Layer', ' object.'], ['Inherits From: ', 'Layer', ', ', 'Module']]","# add a x -> x^2 layer
model.add(Lambda(lambda x: x ** 2))
"
"tf.keras.layers.Layer(
    trainable=True, name=None, dtype=None, dynamic=False, **kwargs
)
","[['Inherits From: ', 'Module']]","class SimpleDense(Layer):

  def __init__(self, units=32):
      super(SimpleDense, self).__init__()
      self.units = units

  def build(self, input_shape):  # Create the state of the layer (weights)
    w_init = tf.random_normal_initializer()
    self.w = tf.Variable(
        initial_value=w_init(shape=(input_shape[-1], self.units),
                             dtype='float32'),
        trainable=True)
    b_init = tf.zeros_initializer()
    self.b = tf.Variable(
        initial_value=b_init(shape=(self.units,), dtype='float32'),
        trainable=True)

  def call(self, inputs):  # Defines the computation from inputs to outputs
      return tf.matmul(inputs, self.w) + self.b

# Instantiates the layer.
linear_layer = SimpleDense(4)

# This will also call `build(input_shape)` and create the weights.
y = linear_layer(tf.ones((2, 2)))
assert len(linear_layer.weights) == 2

# These weights are trainable, so they're listed in `trainable_weights`:
assert len(linear_layer.trainable_weights) == 2
"
"tf.keras.layers.LayerNormalization(
    axis=-1,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer='zeros',
    gamma_initializer='ones',
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","data = tf.constant(np.arange(10).reshape(5, 2) * 10, dtype=tf.float32)
print(data)
tf.Tensor(
[[ 0. 10.]
 [20. 30.]
 [40. 50.]
 [60. 70.]
 [80. 90.]], shape=(5, 2), dtype=float32)"
"tf.keras.layers.LeakyReLU(
    alpha=0.3, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","  f(x) = alpha * x if x < 0
  f(x) = x if x >= 0
"
"tf.keras.layers.LocallyConnected1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format=None,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    implementation=1,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","    # apply a unshared weight convolution 1d of length 3 to a sequence with
    # 10 timesteps, with 64 output filters
    model = Sequential()
    model.add(LocallyConnected1D(64, 3, input_shape=(10, 32)))
    # now model.output_shape == (None, 8, 64)
    # add a new conv1d on top
    model.add(LocallyConnected1D(32, 3))
    # now model.output_shape == (None, 6, 32)
"
"tf.keras.layers.LocallyConnected2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    activation=None,
    use_bias=True,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    implementation=1,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","    # apply a 3x3 unshared weights convolution with 64 output filters on a
    32x32 image
    # with `data_format=""channels_last""`:
    model = Sequential()
    model.add(LocallyConnected2D(64, (3, 3), input_shape=(32, 32, 3)))
    # now model.output_shape == (None, 30, 30, 64)
    # notice that this layer will consume (30*30)*(3*3*3*64) + (30*30)*64
    parameters

    # add a 3x3 unshared weights convolution on top, with 32 output filters:
    model.add(LocallyConnected2D(32, (3, 3)))
    # now model.output_shape == (None, 28, 28, 32)
"
"tf.keras.layers.Masking(
    mask_value=0.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","samples, timesteps, features = 32, 10, 8
inputs = np.random.random([samples, timesteps, features]).astype(np.float32)
inputs[:, 3, :] = 0.
inputs[:, 5, :] = 0.

model = tf.keras.models.Sequential()
model.add(tf.keras.layers.Masking(mask_value=0.,
                                  input_shape=(timesteps, features)))
model.add(tf.keras.layers.LSTM(32))

output = model(inputs)
# The time step 3 and 5 will be skipped from LSTM calculation.
"
"tf.keras.layers.MaxPool1D(
    pool_size=2,
    strides=None,
    padding='valid',
    data_format='channels_last',
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,
   strides=1, padding='valid')
max_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[2.],
        [3.],
        [4.],
        [5.]]], dtype=float32)>"
"tf.keras.layers.MaxPool2D(
    pool_size=(2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([[1., 2., 3.],
                 [4., 5., 6.],
                 [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
   strides=(1, 1), padding='valid')
max_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[5.],
           [6.]],
          [[8.],
           [9.]]]], dtype=float32)>"
"tf.keras.layers.MaxPool3D(
    pool_size=(2, 2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","depth = 30
height = 30
width = 30
input_channels = 3

inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.MaxPooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
"
"tf.keras.layers.MaxPool1D(
    pool_size=2,
    strides=None,
    padding='valid',
    data_format='channels_last',
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([1., 2., 3., 4., 5.])
x = tf.reshape(x, [1, 5, 1])
max_pool_1d = tf.keras.layers.MaxPooling1D(pool_size=2,
   strides=1, padding='valid')
max_pool_1d(x)
<tf.Tensor: shape=(1, 4, 1), dtype=float32, numpy=
array([[[2.],
        [3.],
        [4.],
        [5.]]], dtype=float32)>"
"tf.keras.layers.MaxPool2D(
    pool_size=(2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","x = tf.constant([[1., 2., 3.],
                 [4., 5., 6.],
                 [7., 8., 9.]])
x = tf.reshape(x, [1, 3, 3, 1])
max_pool_2d = tf.keras.layers.MaxPooling2D(pool_size=(2, 2),
   strides=(1, 1), padding='valid')
max_pool_2d(x)
<tf.Tensor: shape=(1, 2, 2, 1), dtype=float32, numpy=
  array([[[[5.],
           [6.]],
          [[8.],
           [9.]]]], dtype=float32)>"
"tf.keras.layers.MaxPool3D(
    pool_size=(2, 2, 2),
    strides=None,
    padding='valid',
    data_format=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","depth = 30
height = 30
width = 30
input_channels = 3

inputs = tf.keras.Input(shape=(depth, height, width, input_channels))
layer = tf.keras.layers.MaxPooling3D(pool_size=3)
outputs = layer(inputs)  # Shape: (batch_size, 10, 10, 10, 3)
"
"tf.keras.layers.Maximum(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","tf.keras.layers.Maximum()([np.arange(5).reshape(5, 1),
                           np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[5],
     [6],
     [7],
     [8],
     [9]])>"
"tf.keras.layers.Minimum(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","tf.keras.layers.Minimum()([np.arange(5).reshape(5, 1),
                           np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[0],
     [1],
     [2],
     [3],
     [4]])>"
"tf.keras.layers.MultiHeadAttention(
    num_heads,
    key_dim,
    value_dim=None,
    dropout=0.0,
    use_bias=True,
    output_shape=None,
    attention_axes=None,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = MultiHeadAttention(num_heads=2, key_dim=2)
target = tf.keras.Input(shape=[8, 16])
source = tf.keras.Input(shape=[4, 16])
output_tensor, weights = layer(target, source,
                               return_attention_scores=True)
print(output_tensor.shape)
(None, 8, 16)
print(weights.shape)
(None, 2, 8, 4)"
"tf.keras.layers.Multiply(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","tf.keras.layers.Multiply()([np.arange(5).reshape(5, 1),
                            np.arange(5, 10).reshape(5, 1)])
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
array([[ 0],
     [ 6],
     [14],
     [24],
     [36]])>"
"tf.keras.layers.Normalization(
    axis=-1, mean=None, variance=None, invert=False, **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]","adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')
input_data = np.array([1., 2., 3.], dtype='float32')
layer = tf.keras.layers.Normalization(axis=None)
layer.adapt(adapt_data)
layer(input_data)
<tf.Tensor: shape=(3,), dtype=float32, numpy=
array([-1.4142135, -0.70710677, 0.], dtype=float32)>"
"tf.keras.layers.PReLU(
    alpha_initializer='zeros',
    alpha_regularizer=None,
    alpha_constraint=None,
    shared_axes=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","  f(x) = alpha * x for x < 0
  f(x) = x for x >= 0
"
"tf.keras.layers.Permute(
    dims, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","model = Sequential()
model.add(Permute((2, 1), input_shape=(10, 64)))
# now: model.output_shape == (None, 64, 10)
# note: `None` is the batch dimension
"
"tf.keras.layers.RNN(
    cell,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    unroll=False,
    time_major=False,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","- Specify `stateful=True` in the layer constructor.
- Specify a fixed batch size for your model, by passing
  If sequential model:
    `batch_input_shape=(...)` to the first layer in your model.
  Else for functional model with 1 or more Input layers:
    `batch_shape=(...)` to all the first layers in your model.
  This is the expected shape of your inputs
  *including the batch size*.
  It should be a tuple of integers, e.g. `(32, 10, 100)`.
- Specify `shuffle=False` when calling `fit()`.
"
"tf.keras.layers.ReLU(
    max_value=None, negative_slope=0.0, threshold=0.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","  f(x) = max_value if x >= max_value
  f(x) = x if threshold <= x < max_value
  f(x) = negative_slope * (x - threshold) otherwise
"
"tf.keras.layers.RepeatVector(
    n, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","model = Sequential()
model.add(Dense(32, input_dim=32))
# now: model.output_shape == (None, 32)
# note: `None` is the batch dimension

model.add(RepeatVector(3))
# now: model.output_shape == (None, 3, 32)
"
"tf.keras.layers.Rescaling(
    scale, offset=0.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Reshape(
    target_shape, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","# as first layer in a Sequential model
model = tf.keras.Sequential()
model.add(tf.keras.layers.Reshape((3, 4), input_shape=(12,)))
# model.output_shape == (None, 3, 4), `None` is the batch size.
model.output_shape
(None, 3, 4)"
"tf.keras.layers.Resizing(
    height,
    width,
    interpolation='bilinear',
    crop_to_aspect_ratio=False,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.SeparableConv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format=None,
    dilation_rate=1,
    depth_multiplier=1,
    activation=None,
    use_bias=True,
    depthwise_initializer='glorot_uniform',
    pointwise_initializer='glorot_uniform',
    bias_initializer='zeros',
    depthwise_regularizer=None,
    pointwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    pointwise_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","convolution_op(
    inputs, kernel
)
"
"tf.keras.layers.SeparableConv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    depth_multiplier=1,
    activation=None,
    use_bias=True,
    depthwise_initializer='glorot_uniform',
    pointwise_initializer='glorot_uniform',
    bias_initializer='zeros',
    depthwise_regularizer=None,
    pointwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    pointwise_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","convolution_op(
    inputs, kernel
)
"
"tf.keras.layers.SeparableConv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format=None,
    dilation_rate=1,
    depth_multiplier=1,
    activation=None,
    use_bias=True,
    depthwise_initializer='glorot_uniform',
    pointwise_initializer='glorot_uniform',
    bias_initializer='zeros',
    depthwise_regularizer=None,
    pointwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    pointwise_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","convolution_op(
    inputs, kernel
)
"
"tf.keras.layers.SeparableConv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format=None,
    dilation_rate=(1, 1),
    depth_multiplier=1,
    activation=None,
    use_bias=True,
    depthwise_initializer='glorot_uniform',
    pointwise_initializer='glorot_uniform',
    bias_initializer='zeros',
    depthwise_regularizer=None,
    pointwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    pointwise_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","convolution_op(
    inputs, kernel
)
"
"tf.keras.layers.SimpleRNN(
    units,
    activation='tanh',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    return_sequences=False,
    return_state=False,
    go_backwards=False,
    stateful=False,
    unroll=False,
    **kwargs
)
","[['Inherits From: ', 'RNN', ', ', 'Layer', ', ', 'Module']]","inputs = np.random.random([32, 10, 8]).astype(np.float32)
simple_rnn = tf.keras.layers.SimpleRNN(4)

output = simple_rnn(inputs)  # The output has shape `[32, 4]`.

simple_rnn = tf.keras.layers.SimpleRNN(
    4, return_sequences=True, return_state=True)

# whole_sequence_output has shape `[32, 10, 4]`.
# final_state has shape `[32, 4]`.
whole_sequence_output, final_state = simple_rnn(inputs)
"
"tf.keras.layers.SimpleRNNCell(
    units,
    activation='tanh',
    use_bias=True,
    kernel_initializer='glorot_uniform',
    recurrent_initializer='orthogonal',
    bias_initializer='zeros',
    kernel_regularizer=None,
    recurrent_regularizer=None,
    bias_regularizer=None,
    kernel_constraint=None,
    recurrent_constraint=None,
    bias_constraint=None,
    dropout=0.0,
    recurrent_dropout=0.0,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","inputs = np.random.random([32, 10, 8]).astype(np.float32)
rnn = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(4))

output = rnn(inputs)  # The output has shape `[32, 4]`.

rnn = tf.keras.layers.RNN(
    tf.keras.layers.SimpleRNNCell(4),
    return_sequences=True,
    return_state=True)

# whole_sequence_output has shape `[32, 10, 4]`.
# final_state has shape `[32, 4]`.
whole_sequence_output, final_state = rnn(inputs)
"
"tf.keras.layers.Softmax(
    axis=-1, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","inp = np.asarray([1., 2., 1.])
layer = tf.keras.layers.Softmax()
layer(inp).numpy()
array([0.21194157, 0.5761169 , 0.21194157], dtype=float32)
mask = np.asarray([True, False, True], dtype=bool)
layer(inp, mask).numpy()
array([0.5, 0. , 0.5], dtype=float32)"
"tf.keras.layers.SpatialDropout1D(
    rate, **kwargs
)
","[['Inherits From: ', 'Dropout', ', ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.SpatialDropout2D(
    rate, data_format=None, **kwargs
)
","[['Inherits From: ', 'Dropout', ', ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.SpatialDropout3D(
    rate, data_format=None, **kwargs
)
","[['Inherits From: ', 'Dropout', ', ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.StackedRNNCells(
    cells, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","batch_size = 3
sentence_max_length = 5
n_features = 2
new_shape = (batch_size, sentence_max_length, n_features)
x = tf.constant(np.reshape(np.arange(30), new_shape), dtype = tf.float32)

rnn_cells = [tf.keras.layers.LSTMCell(128) for _ in range(2)]
stacked_lstm = tf.keras.layers.StackedRNNCells(rnn_cells)
lstm_layer = tf.keras.layers.RNN(stacked_lstm)

result = lstm_layer(x)
"
"tf.keras.layers.Subtract(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    # Equivalent to subtracted = keras.layers.subtract([x1, x2])
    subtracted = keras.layers.Subtract()([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
"
"tf.keras.layers.ThresholdedReLU(
    theta=1.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","  f(x) = x for x > theta
  f(x) = 0 otherwise`
"
"tf.keras.layers.TimeDistributed(
    layer, **kwargs
)
","[['Inherits From: ', 'Wrapper', ', ', 'Layer', ', ', 'Module']]","inputs = tf.keras.Input(shape=(10, 128, 128, 3))
conv_2d_layer = tf.keras.layers.Conv2D(64, (3, 3))
outputs = tf.keras.layers.TimeDistributed(conv_2d_layer)(inputs)
outputs.shape
TensorShape([None, 10, 126, 126, 64])"
"tf.keras.layers.UpSampling1D(
    size=2, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1  2]
  [ 3  4  5]]
 [[ 6  7  8]
  [ 9 10 11]]]
y = tf.keras.layers.UpSampling1D(size=2)(x)
print(y)
tf.Tensor(
  [[[ 0  1  2]
    [ 0  1  2]
    [ 3  4  5]
    [ 3  4  5]]
   [[ 6  7  8]
    [ 6  7  8]
    [ 9 10 11]
    [ 9 10 11]]], shape=(2, 4, 3), dtype=int64)"
"tf.keras.layers.UpSampling2D(
    size=(2, 2), data_format=None, interpolation='nearest', **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 2, 1, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[[ 0  1  2]]
  [[ 3  4  5]]]
 [[[ 6  7  8]]
  [[ 9 10 11]]]]
y = tf.keras.layers.UpSampling2D(size=(1, 2))(x)
print(y)
tf.Tensor(
  [[[[ 0  1  2]
     [ 0  1  2]]
    [[ 3  4  5]
     [ 3  4  5]]]
   [[[ 6  7  8]
     [ 6  7  8]]
    [[ 9 10 11]
     [ 9 10 11]]]], shape=(2, 2, 2, 3), dtype=int64)"
"tf.keras.layers.UpSampling3D(
    size=(2, 2, 2), data_format=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 1, 2, 1, 3)
x = tf.constant(1, shape=input_shape)
y = tf.keras.layers.UpSampling3D(size=2)(x)
print(y.shape)
(2, 2, 4, 2, 3)"
"tf.keras.layers.Wrapper(
    layer, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.ZeroPadding1D(
    padding=1, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[ 0  1  2]
  [ 3  4  5]]
 [[ 6  7  8]
  [ 9 10 11]]]
y = tf.keras.layers.ZeroPadding1D(padding=2)(x)
print(y)
tf.Tensor(
  [[[ 0  0  0]
    [ 0  0  0]
    [ 0  1  2]
    [ 3  4  5]
    [ 0  0  0]
    [ 0  0  0]]
   [[ 0  0  0]
    [ 0  0  0]
    [ 6  7  8]
    [ 9 10 11]
    [ 0  0  0]
    [ 0  0  0]]], shape=(2, 6, 3), dtype=int64)"
"tf.keras.layers.ZeroPadding2D(
    padding=(1, 1), data_format=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (1, 1, 2, 2)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
print(x)
[[[[0 1]
   [2 3]]]]
y = tf.keras.layers.ZeroPadding2D(padding=1)(x)
print(y)
tf.Tensor(
  [[[[0 0]
     [0 0]
     [0 0]
     [0 0]]
    [[0 0]
     [0 1]
     [2 3]
     [0 0]]
    [[0 0]
     [0 0]
     [0 0]
     [0 0]]]], shape=(1, 3, 4, 2), dtype=int64)"
"tf.keras.layers.ZeroPadding3D(
    padding=(1, 1, 1), data_format=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","input_shape = (1, 1, 2, 2, 3)
x = np.arange(np.prod(input_shape)).reshape(input_shape)
y = tf.keras.layers.ZeroPadding3D(padding=2)(x)
print(y.shape)
(1, 5, 6, 6, 3)"
"tf.keras.layers.add(
    inputs, **kwargs
)
","[['Functional interface to the ', 'tf.keras.layers.Add', ' layer.']]","input_shape = (2, 3, 4)
x1 = tf.random.normal(input_shape)
x2 = tf.random.normal(input_shape)
y = tf.keras.layers.add([x1, x2])
print(y.shape)
(2, 3, 4)"
"tf.keras.layers.average(
    inputs, **kwargs
)
","[['Functional interface to the ', 'tf.keras.layers.Average', ' layer.']]","x1 = np.ones((2, 2))
x2 = np.zeros((2, 2))
y = tf.keras.layers.Average()([x1, x2])
y.numpy().tolist()
[[0.5, 0.5], [0.5, 0.5]]"
"tf.keras.layers.concatenate(
    inputs, axis=-1, **kwargs
)
","[['Functional interface to the ', 'Concatenate', ' layer.']]","x = np.arange(20).reshape(2, 2, 5)
print(x)
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]
 [[10 11 12 13 14]
  [15 16 17 18 19]]]
y = np.arange(20, 30).reshape(2, 1, 5)
print(y)
[[[20 21 22 23 24]]
 [[25 26 27 28 29]]]
tf.keras.layers.concatenate([x, y],
                            axis=1)
<tf.Tensor: shape=(2, 3, 5), dtype=int64, numpy=
array([[[ 0,  1,  2,  3,  4],
      [ 5,  6,  7,  8,  9],
      [20, 21, 22, 23, 24]],
     [[10, 11, 12, 13, 14],
      [15, 16, 17, 18, 19],
      [25, 26, 27, 28, 29]]])>"
"tf.keras.layers.deserialize(
    config, custom_objects=None
)
",[],"# Configuration of Dense(32, activation='relu')
config = {
  'class_name': 'Dense',
  'config': {
    'activation': 'relu',
    'activity_regularizer': None,
    'bias_constraint': None,
    'bias_initializer': {'class_name': 'Zeros', 'config': {} },
    'bias_regularizer': None,
    'dtype': 'float32',
    'kernel_constraint': None,
    'kernel_initializer': {'class_name': 'GlorotUniform',
                           'config': {'seed': None} },
    'kernel_regularizer': None,
    'name': 'dense',
    'trainable': True,
    'units': 32,
    'use_bias': True
  }
}
dense_layer = tf.keras.layers.deserialize(config)
"
"tf.keras.layers.dot(
    inputs, axes, normalize=False, **kwargs
)
","[['Functional interface to the ', 'Dot', ' layer.']]",[]
"tf.keras.layers.EinsumDense(
    equation,
    output_shape,
    activation=None,
    bias_axes=None,
    kernel_initializer='glorot_uniform',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    **kwargs
)
","[['A layer that uses ', 'tf.einsum', ' as the backing computation.'], ['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.EinsumDense(""ab,bc->ac"",
                                    output_shape=64,
                                    bias_axes=""c"")
input_tensor = tf.keras.Input(shape=[32])
output_tensor = layer(input_tensor)
output_tensor
<... shape=(None, 64) dtype=...>"
"tf.keras.layers.experimental.RandomFourierFeatures(
    output_dim,
    kernel_initializer='gaussian',
    scale=None,
    trainable=False,
    name=None,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","model = keras.Sequential([
  keras.Input(shape=(784,)),
  RandomFourierFeatures(
      output_dim=4096,
      scale=10.,
      kernel_initializer='gaussian'),
  layers.Dense(units=10, activation='softmax'),
])
model.compile(
    optimizer='adam',
    loss='categorical_crossentropy',
    metrics=['categorical_accuracy']
)
"
"tf.keras.layers.CategoryEncoding(
    num_tokens=None, output_mode='multi_hot', sparse=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.CategoryEncoding(
          num_tokens=4, output_mode=""one_hot"")
layer([3, 2, 0, 1])
<tf.Tensor: shape=(4, 4), dtype=float32, numpy=
  array([[0., 0., 0., 1.],
         [0., 0., 1., 0.],
         [1., 0., 0., 0.],
         [0., 1., 0., 0.]], dtype=float32)>"
"tf.keras.layers.CenterCrop(
    height, width, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Discretization(
    bin_boundaries=None,
    num_bins=None,
    epsilon=0.01,
    output_mode='int',
    sparse=False,
    **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]",">>> input = np.array([[-1.5, 1.0, 3.4, .5], [0.0, 3.0, 1.3, 0.0]])
>>> layer = tf.keras.layers.Discretization(bin_boundaries=[0., 1., 2.])
>>> layer(input)
<tf.Tensor: shape=(2, 4), dtype=int64, numpy=
array([[0, 2, 3, 1],
       [1, 3, 2, 1]])>
"
"tf.keras.layers.experimental.preprocessing.HashedCrossing(
    num_bins, output_mode='int', sparse=False, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.experimental.preprocessing.HashedCrossing(
    num_bins=5)
feat1 = tf.constant(['A', 'B', 'A', 'B', 'A'])
feat2 = tf.constant([101, 101, 101, 102, 102])
layer((feat1, feat2))
<tf.Tensor: shape=(5,), dtype=int64, numpy=array([1, 4, 1, 1, 3])>"
"tf.keras.layers.Hashing(
    num_bins,
    mask_value=None,
    salt=None,
    output_mode='int',
    sparse=False,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","layer = tf.keras.layers.Hashing(num_bins=3)
inp = [['A'], ['B'], ['C'], ['D'], ['E']]
layer(inp)
<tf.Tensor: shape=(5, 1), dtype=int64, numpy=
  array([[1],
         [0],
         [1],
         [1],
         [2]])>"
"tf.keras.layers.Normalization(
    axis=-1, mean=None, variance=None, invert=False, **kwargs
)
","[['Inherits From: ', 'PreprocessingLayer', ', ', 'Layer', ', ', 'Module']]","adapt_data = np.array([1., 2., 3., 4., 5.], dtype='float32')
input_data = np.array([1., 2., 3.], dtype='float32')
layer = tf.keras.layers.Normalization(axis=None)
layer.adapt(adapt_data)
layer(input_data)
<tf.Tensor: shape=(3,), dtype=float32, numpy=
array([-1.4142135, -0.70710677, 0.], dtype=float32)>"
"tf.keras.layers.experimental.preprocessing.PreprocessingLayer(
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","adapt(
    data, batch_size=None, steps=None
)
"
"tf.keras.layers.Rescaling(
    scale, offset=0.0, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.Resizing(
    height,
    width,
    interpolation='bilinear',
    crop_to_aspect_ratio=False,
    **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]",[]
"tf.keras.layers.maximum(
    inputs, **kwargs
)
","[['Functional interface to compute maximum (element-wise) list of ', 'inputs', '.']]","input1 = tf.keras.layers.Input(shape=(16,))
x1 = tf.keras.layers.Dense(8, activation='relu')(input1) #shape=(None, 8)
input2 = tf.keras.layers.Input(shape=(32,))
x2 = tf.keras.layers.Dense(8, activation='relu')(input2) #shape=(None, 8)
max_inp=tf.keras.layers.maximum([x1,x2]) #shape=(None, 8)
out = tf.keras.layers.Dense(4)(max_inp)
model = tf.keras.models.Model(inputs=[input1, input2], outputs=out)
"
"tf.keras.layers.minimum(
    inputs, **kwargs
)
","[['Functional interface to the ', 'Minimum', ' layer.']]",[]
"tf.keras.layers.multiply(
    inputs, **kwargs
)
","[['Functional interface to the ', 'Multiply', ' layer.']]","x1 = np.arange(3.0)
x2 = np.arange(3.0)
tf.keras.layers.multiply([x1, x2])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0., 1., 4.], ...)>"
"tf.keras.layers.serialize(
    layer
)
","[['Serializes a ', 'Layer', ' object into a JSON-compatible representation.']]","from pprint import pprint
model = tf.keras.models.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(32, activation='relu'))

pprint(tf.keras.layers.serialize(model))
# prints the configuration of the model, as a dict.
"
"tf.keras.layers.subtract(
    inputs, **kwargs
)
","[['Functional interface to the ', 'Subtract', ' layer.']]","    import keras

    input1 = keras.layers.Input(shape=(16,))
    x1 = keras.layers.Dense(8, activation='relu')(input1)
    input2 = keras.layers.Input(shape=(32,))
    x2 = keras.layers.Dense(8, activation='relu')(input2)
    subtracted = keras.layers.subtract([x1, x2])

    out = keras.layers.Dense(4)(subtracted)
    model = keras.models.Model(inputs=[input1, input2], outputs=out)
"
"tf.keras.losses.BinaryCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_crossentropy'
)
","[['Inherits From: ', 'Loss']]","model.compile(
  loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
  ....
)
"
"tf.keras.losses.BinaryFocalCrossentropy(
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='binary_focal_crossentropy'
)
","[['Inherits From: ', 'Loss']]","model.compile(
  loss=tf.keras.losses.BinaryFocalCrossentropy(gamma=2.0, from_logits=True),
  ....
)
"
"tf.keras.losses.CategoricalCrossentropy(
    from_logits=False,
    label_smoothing=0.0,
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='categorical_crossentropy'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cce = tf.keras.losses.CategoricalCrossentropy()
cce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.CategoricalHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='categorical_hinge'
)
","[['Computes the categorical hinge loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
# Using 'auto'/'sum_over_batch_size' reduction type.
h = tf.keras.losses.CategoricalHinge()
h(y_true, y_pred).numpy()
1.4"
"tf.keras.losses.CosineSimilarity(
    axis=-1,
    reduction=losses_utils.ReductionV2.AUTO,
    name='cosine_similarity'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
cosine_loss = tf.keras.losses.CosineSimilarity(axis=1)
# l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]
# l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]
# l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]
# loss = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))
#       = -((0. + 0.) +  (0.5 + 0.5)) / 2
cosine_loss(y_true, y_pred).numpy()
-0.5"
"tf.keras.losses.Hinge(
    reduction=losses_utils.ReductionV2.AUTO, name='hinge'
)
","[['Computes the hinge loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
# Using 'auto'/'sum_over_batch_size' reduction type.
h = tf.keras.losses.Hinge()
h(y_true, y_pred).numpy()
1.3"
"tf.keras.losses.Huber(
    delta=1.0,
    reduction=losses_utils.ReductionV2.AUTO,
    name='huber_loss'
)
","[['Computes the Huber loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","loss = 0.5 * x^2                  if |x| <= d
loss = 0.5 * d^2 + d * (|x| - d)  if |x| > d
"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.losses.KLDivergence(
    reduction=losses_utils.ReductionV2.AUTO, name='kl_divergence'
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
# Using 'auto'/'sum_over_batch_size' reduction type.
kl = tf.keras.losses.KLDivergence()
kl(y_true, y_pred).numpy()
0.458"
"tf.keras.losses.LogCosh(
    reduction=losses_utils.ReductionV2.AUTO, name='log_cosh'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
l = tf.keras.losses.LogCosh()
l(y_true, y_pred).numpy()
0.108"
"tf.keras.losses.Loss(
    reduction=losses_utils.ReductionV2.AUTO, name=None
)
",[],"class MeanSquaredError(Loss):

  def call(self, y_true, y_pred):
    return tf.reduce_mean(tf.math.square(y_pred - y_true), axis=-1)
"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.losses.MeanAbsoluteError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_error'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
mae = tf.keras.losses.MeanAbsoluteError()
mae(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanAbsolutePercentageError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_absolute_percentage_error'
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[2., 1.], [2., 3.]]
y_pred = [[1., 1.], [1., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
mape = tf.keras.losses.MeanAbsolutePercentageError()
mape(y_true, y_pred).numpy()
50."
"tf.keras.losses.MeanSquaredError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_error'
)
","[['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
mse = tf.keras.losses.MeanSquaredError()
mse(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.MeanSquaredLogarithmicError(
    reduction=losses_utils.ReductionV2.AUTO,
    name='mean_squared_logarithmic_error'
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [1., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
msle = tf.keras.losses.MeanSquaredLogarithmicError()
msle(y_true, y_pred).numpy()
0.240"
"tf.keras.losses.Poisson(
    reduction=losses_utils.ReductionV2.AUTO, name='poisson'
)
","[['Computes the Poisson loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[1., 1.], [0., 0.]]
# Using 'auto'/'sum_over_batch_size' reduction type.
p = tf.keras.losses.Poisson()
p(y_true, y_pred).numpy()
0.5"
"tf.keras.losses.SparseCategoricalCrossentropy(
    from_logits=False,
    ignore_class=None,
    reduction=losses_utils.ReductionV2.AUTO,
    name='sparse_categorical_crossentropy'
)
","[['Inherits From: ', 'Loss']]","y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
# Using 'auto'/'sum_over_batch_size' reduction type.
scce = tf.keras.losses.SparseCategoricalCrossentropy()
scce(y_true, y_pred).numpy()
1.177"
"tf.keras.losses.SquaredHinge(
    reduction=losses_utils.ReductionV2.AUTO, name='squared_hinge'
)
","[['Computes the squared hinge loss between ', 'y_true', ' & ', 'y_pred', '.'], ['Inherits From: ', 'Loss']]","y_true = [[0., 1.], [0., 0.]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
# Using 'auto'/'sum_over_batch_size' reduction type.
h = tf.keras.losses.SquaredHinge()
h(y_true, y_pred).numpy()
1.86"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
",[],"y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
",[],"y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
",[],"y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.losses.categorical_hinge(
    y_true, y_pred
)
","[['Computes the categorical hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 3, size=(2,))
y_true = tf.keras.utils.to_categorical(y_true, num_classes=3)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.categorical_hinge(y_true, y_pred)
assert loss.shape == (2,)
pos = np.sum(y_true * y_pred, axis=-1)
neg = np.amax((1. - y_true) * y_pred, axis=-1)
assert np.array_equal(loss.numpy(), np.maximum(0., neg - pos + 1.))"
"tf.keras.losses.cosine_similarity(
    y_true, y_pred, axis=-1
)
",[],"y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)
loss.numpy()
array([-0., -0.999, 0.999], dtype=float32)"
"tf.keras.losses.cosine_similarity(
    y_true, y_pred, axis=-1
)
",[],"y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)
loss.numpy()
array([-0., -0.999, 0.999], dtype=float32)"
"tf.keras.losses.cosine_similarity(
    y_true, y_pred, axis=-1
)
",[],"y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)
loss.numpy()
array([-0., -0.999, 0.999], dtype=float32)"
"tf.keras.losses.deserialize(
    name, custom_objects=None
)
",[],[]
"tf.keras.losses.get(
    identifier
)
","[['Retrieves a Keras loss as a ', 'function', '/', 'Loss', ' class instance.']]","loss = tf.keras.losses.get(""categorical_crossentropy"")
type(loss)
<class 'function'>
loss = tf.keras.losses.get(""CategoricalCrossentropy"")
type(loss)
<class '...keras.losses.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","[['Computes the hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.losses.log_cosh(
    y_true, y_pred
)
",[],"y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.logcosh(y_true, y_pred)
assert loss.shape == (2,)
x = y_pred - y_true
assert np.allclose(
    loss.numpy(),
    np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),
            axis=-1),
    atol=1e-5)"
"tf.keras.losses.log_cosh(
    y_true, y_pred
)
",[],"y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.logcosh(y_true, y_pred)
assert loss.shape == (2,)
x = y_pred - y_true
assert np.allclose(
    loss.numpy(),
    np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),
            axis=-1),
    atol=1e-5)"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.losses.serialize(
    loss
)
","[['Serializes loss function or ', 'Loss', ' instance.']]",[]
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
",[],"y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","[['Computes the squared hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.keras.metrics.AUC(
    num_thresholds=200,
    curve='ROC',
    summation_method='interpolation',
    name=None,
    dtype=None,
    thresholds=None,
    multi_label=False,
    num_labels=None,
    label_weights=None,
    from_logits=False
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.AUC(num_thresholds=3)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
# threshold values are [0 - 1e-7, 0.5, 1 + 1e-7]
# tp = [2, 1, 0], fp = [2, 0, 0], fn = [0, 1, 2], tn = [0, 2, 2]
# tp_rate = recall = [1, 0.5, 0], fp_rate = [1, 0, 0]
# auc = ((((1+0.5)/2)*(1-0)) + (((0.5+0)/2)*(0-0))) = 0.75
m.result().numpy()
0.75"
"tf.keras.metrics.Accuracy(
    name='accuracy', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Accuracy()
m.update_state([[1], [2], [3], [4]], [[0], [2], [3], [4]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryAccuracy(
    name='binary_accuracy', dtype=None, threshold=0.5
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.BinaryAccuracy()
m.update_state([[1], [1], [0], [0]], [[0.98], [1], [0], [0.6]])
m.result().numpy()
0.75"
"tf.keras.metrics.BinaryCrossentropy(
    name='binary_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.BinaryCrossentropy()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.81492424"
"tf.keras.metrics.BinaryIoU(
    target_class_ids: Union[List[int], Tuple[int, ...]] = (0, 1),
    threshold=0.5,
    name=None,
    dtype=None
)
","[['Inherits From: ', 'IoU', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.CategoricalAccuracy(
    name='categorical_accuracy', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.CategoricalAccuracy()
m.update_state([[0, 0, 1], [0, 1, 0]], [[0.1, 0.9, 0.8],
                [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.CategoricalCrossentropy(
    name='categorical_crossentropy',
    dtype=None,
    from_logits=False,
    label_smoothing=0,
    axis=-1
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","# EPSILON = 1e-7, y = y_true, y` = y_pred
# y` = clip_ops.clip_by_value(output, EPSILON, 1. - EPSILON)
# y` = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]
# xent = -sum(y * log(y'), axis = -1)
#      = -((log 0.95), (log 0.1))
#      = [0.051, 2.302]
# Reduced xent = (0.051 + 2.302) / 2
m = tf.keras.metrics.CategoricalCrossentropy()
m.update_state([[0, 1, 0], [0, 0, 1]],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.CategoricalHinge(
    name='categorical_hinge', dtype=None
)
","[['Computes the categorical hinge metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.CategoricalHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.4000001"
"tf.keras.metrics.CosineSimilarity(
    name='cosine_similarity', dtype=None, axis=-1
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","# l2_norm(y_true) = [[0., 1.], [1./1.414, 1./1.414]]
# l2_norm(y_pred) = [[1., 0.], [1./1.414, 1./1.414]]
# l2_norm(y_true) . l2_norm(y_pred) = [[0., 0.], [0.5, 0.5]]
# result = mean(sum(l2_norm(y_true) . l2_norm(y_pred), axis=1))
#        = ((0. + 0.) +  (0.5 + 0.5)) / 2
m = tf.keras.metrics.CosineSimilarity(axis=1)
m.update_state([[0., 1.], [1., 1.]], [[1., 0.], [1., 1.]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.FalseNegatives(
    thresholds=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.FalseNegatives()
m.update_state([0, 1, 1, 1], [0, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.FalsePositives(
    thresholds=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.FalsePositives()
m.update_state([0, 1, 0, 0], [0, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.Hinge(
    name='hinge', dtype=None
)
","[['Computes the hinge metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Hinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.3"
"tf.keras.metrics.IoU(
    num_classes: int,
    target_class_ids: Union[List[int], Tuple[int, ...]],
    name: Optional[str] = None,
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    ignore_class: Optional[int] = None,
    sparse_y_true: bool = True,
    sparse_y_pred: bool = True,
    axis: int = -1
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.KLDivergence(
    name='kullback_leibler_divergence', dtype=None
)
","[['Computes Kullback-Leibler divergence metric between ', 'y_true', ' and'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.KLDivergence()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
0.45814306"
"tf.keras.metrics.LogCoshError(
    name='logcosh', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.LogCoshError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.10844523"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.Mean(
    name='mean', dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Mean()
m.update_state([1, 3, 5, 7])
m.result().numpy()
4.0
m.reset_state()
m.update_state([1, 3, 5, 7], sample_weight=[1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.MeanAbsoluteError(
    name='mean_absolute_error', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanAbsoluteError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanAbsolutePercentageError(
    name='mean_absolute_percentage_error', dtype=None
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' and'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanAbsolutePercentageError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
250000000.0"
"tf.keras.metrics.MeanIoU(
    num_classes: int,
    name: Optional[str] = None,
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    ignore_class: Optional[int] = None,
    sparse_y_true: bool = True,
    sparse_y_pred: bool = True,
    axis: int = -1
)
","[['Inherits From: ', 'IoU', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.MeanMetricWrapper(
    fn, name=None, dtype=None, **kwargs
)
","[['Inherits From: ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","def accuracy(y_true, y_pred):
  return tf.cast(tf.math.equal(y_true, y_pred), tf.float32)

accuracy_metric = tf.keras.metrics.MeanMetricWrapper(fn=accuracy)

keras_model.compile(..., metrics=accuracy_metric)
"
"tf.keras.metrics.MeanRelativeError(
    normalizer, name=None, dtype=None
)
","[['Inherits From: ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanRelativeError(normalizer=[1, 3, 2, 3])
m.update_state([1, 3, 2, 3], [2, 4, 6, 8])"
"tf.keras.metrics.MeanSquaredError(
    name='mean_squared_error', dtype=None
)
","[['Computes the mean squared error between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.25"
"tf.keras.metrics.MeanSquaredLogarithmicError(
    name='mean_squared_logarithmic_error', dtype=None
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' and'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanSquaredLogarithmicError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.12011322"
"tf.keras.metrics.MeanTensor(
    name='mean_tensor', dtype=None, shape=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.MeanTensor()
m.update_state([0, 1, 2, 3])
m.update_state([4, 5, 6, 7])
m.result().numpy()
array([2., 3., 4., 5.], dtype=float32)"
"tf.keras.metrics.Metric(
    name=None, dtype=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","m = SomeMetric(...)
for input in ...:
  m.update_state(input)
print('Final result: ', m.result().numpy())
"
"tf.keras.metrics.OneHotIoU(
    num_classes: int,
    target_class_ids: Union[List[int], Tuple[int, ...]],
    name=None,
    dtype=None,
    ignore_class: Optional[int] = None,
    sparse_y_pred: bool = False,
    axis: int = -1
)
","[['Inherits From: ', 'IoU', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.OneHotMeanIoU(
    num_classes: int,
    name: str = None,
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    ignore_class: Optional[int] = None,
    sparse_y_pred: bool = False,
    axis: int = -1
)
","[['Inherits From: ', 'MeanIoU', ', ', 'IoU', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","iou = true_positives / (true_positives + false_positives + false_negatives)
"
"tf.keras.metrics.Poisson(
    name='poisson', dtype=None
)
","[['Computes the Poisson metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Poisson()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.49999997"
"tf.keras.metrics.Precision(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Precision()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.PrecisionAtRecall(
    recall, num_thresholds=200, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.PrecisionAtRecall(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.Recall(
    thresholds=None, top_k=None, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Recall()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
0.6666667"
"tf.keras.metrics.RecallAtPrecision(
    precision, num_thresholds=200, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.RecallAtPrecision(0.8)
m.update_state([0, 0, 1, 1], [0, 0.5, 0.3, 0.9])
m.result().numpy()
0.5"
"tf.keras.metrics.RootMeanSquaredError(
    name='root_mean_squared_error', dtype=None
)
","[['Computes root mean squared error metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.RootMeanSquaredError()
m.update_state([[0, 1], [0, 0]], [[1, 1], [0, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SensitivityAtSpecificity(
    specificity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.SensitivityAtSpecificity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.5"
"tf.keras.metrics.SparseCategoricalAccuracy(
    name='sparse_categorical_accuracy', dtype=None
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","acc = np.dot(sample_weight, np.equal(y_true, np.argmax(y_pred, axis=1))
"
"tf.keras.metrics.SparseCategoricalCrossentropy(
    name: str = 'sparse_categorical_crossentropy',
    dtype: Optional[Union[str, tf.dtypes.DType]] = None,
    from_logits: bool = False,
    ignore_class: Optional[int] = None,
    axis: int = -1
)
","[['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","# y_true = one_hot(y_true) = [[0, 1, 0], [0, 0, 1]]
# logits = log(y_pred)
# softmax = exp(logits) / sum(exp(logits), axis=-1)
# softmax = [[0.05, 0.95, EPSILON], [0.1, 0.8, 0.1]]
# xent = -sum(y * log(softmax), 1)
# log(softmax) = [[-2.9957, -0.0513, -16.1181],
#                [-2.3026, -0.2231, -2.3026]]
# y_true * log(softmax) = [[0, -0.0513, 0], [0, 0, -2.3026]]
# xent = [0.0513, 2.3026]
# Reduced xent = (0.0513 + 2.3026) / 2
m = tf.keras.metrics.SparseCategoricalCrossentropy()
m.update_state([1, 2],
               [[0.05, 0.95, 0], [0.1, 0.8, 0.1]])
m.result().numpy()
1.1769392"
"tf.keras.metrics.SparseTopKCategoricalAccuracy(
    k=5, name='sparse_top_k_categorical_accuracy', dtype=None
)
","[['Computes how often integer targets are in the top ', 'K', ' predictions.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.SparseTopKCategoricalAccuracy(k=1)
m.update_state([2, 1], [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.SpecificityAtSensitivity(
    sensitivity, num_thresholds=200, class_id=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.SpecificityAtSensitivity(0.5)
m.update_state([0, 0, 0, 1, 1], [0, 0.3, 0.8, 0.3, 0.8])
m.result().numpy()
0.66666667"
"tf.keras.metrics.SquaredHinge(
    name='squared_hinge', dtype=None
)
","[['Computes the squared hinge metric between ', 'y_true', ' and ', 'y_pred', '.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.SquaredHinge()
m.update_state([[0, 1], [0, 0]], [[0.6, 0.4], [0.4, 0.6]])
m.result().numpy()
1.86"
"tf.keras.metrics.Sum(
    name='sum', dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.Sum()
m.update_state([1, 3, 5, 7])
m.result().numpy()
16.0"
"tf.keras.metrics.TopKCategoricalAccuracy(
    k=5, name='top_k_categorical_accuracy', dtype=None
)
","[['Computes how often targets are in the top ', 'K', ' predictions.'], ['Inherits From: ', 'MeanMetricWrapper', ', ', 'Mean', ', ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.TopKCategoricalAccuracy(k=1)
m.update_state([[0, 0, 1], [0, 1, 0]],
               [[0.1, 0.9, 0.8], [0.05, 0.95, 0]])
m.result().numpy()
0.5"
"tf.keras.metrics.TrueNegatives(
    thresholds=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.TrueNegatives()
m.update_state([0, 1, 0, 0], [1, 1, 0, 0])
m.result().numpy()
2.0"
"tf.keras.metrics.TruePositives(
    thresholds=None, name=None, dtype=None
)
","[['Inherits From: ', 'Metric', ', ', 'Layer', ', ', 'Module']]","m = tf.keras.metrics.TruePositives()
m.update_state([0, 1, 1, 1], [1, 0, 1, 1])
m.result().numpy()
2.0"
"tf.keras.metrics.binary_accuracy(
    y_true, y_pred, threshold=0.5
)
",[],"y_true = [[1], [1], [0], [0]]
y_pred = [[1], [1], [0], [0]]
m = tf.keras.metrics.binary_accuracy(y_true, y_pred)
assert m.shape == (4,)
m.numpy()
array([1., 1., 1., 1.], dtype=float32)"
"tf.keras.metrics.binary_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
",[],"y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.916 , 0.714], dtype=float32)"
"tf.keras.metrics.binary_focal_crossentropy(
    y_true,
    y_pred,
    apply_class_balancing=False,
    alpha=0.25,
    gamma=2.0,
    from_logits=False,
    label_smoothing=0.0,
    axis=-1
)
",[],"y_true = [[0, 1], [0, 0]]
y_pred = [[0.6, 0.4], [0.4, 0.6]]
loss = tf.keras.losses.binary_focal_crossentropy(y_true, y_pred,
                                                 gamma=2)
assert loss.shape == (2,)
loss.numpy()
array([0.330, 0.206], dtype=float32)"
"tf.keras.metrics.categorical_accuracy(
    y_true, y_pred
)
",[],"y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.categorical_crossentropy(
    y_true, y_pred, from_logits=False, label_smoothing=0.0, axis=-1
)
",[],"y_true = [[0, 1, 0], [0, 0, 1]]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.losses.cosine_similarity(
    y_true, y_pred, axis=-1
)
",[],"y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)
loss.numpy()
array([-0., -0.999, 0.999], dtype=float32)"
"tf.keras.losses.cosine_similarity(
    y_true, y_pred, axis=-1
)
",[],"y_true = [[0., 1.], [1., 1.], [1., 1.]]
y_pred = [[1., 0.], [1., 1.], [-1., -1.]]
loss = tf.keras.losses.cosine_similarity(y_true, y_pred, axis=1)
loss.numpy()
array([-0., -0.999, 0.999], dtype=float32)"
"tf.keras.metrics.deserialize(
    config, custom_objects=None
)
",[],[]
"tf.keras.metrics.get(
    identifier
)
","[['Retrieves a Keras metric as a ', 'function', '/', 'Metric', ' class instance.']]","metric = tf.keras.metrics.get(""categorical_crossentropy"")
type(metric)
<class 'function'>
metric = tf.keras.metrics.get(""CategoricalCrossentropy"")
type(metric)
<class '...metrics.CategoricalCrossentropy'>"
"tf.keras.metrics.hinge(
    y_true, y_pred
)
","[['Computes the hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.maximum(1. - y_true * y_pred, 0.), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.metrics.kl_divergence(
    y_true, y_pred
)
","[['Computes Kullback-Leibler divergence loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3)).astype(np.float64)
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.kullback_leibler_divergence(y_true, y_pred)
assert loss.shape == (2,)
y_true = tf.keras.backend.clip(y_true, 1e-7, 1)
y_pred = tf.keras.backend.clip(y_pred, 1e-7, 1)
assert np.array_equal(
    loss.numpy(), np.sum(y_true * np.log(y_true / y_pred), axis=-1))"
"tf.keras.losses.log_cosh(
    y_true, y_pred
)
",[],"y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.logcosh(y_true, y_pred)
assert loss.shape == (2,)
x = y_pred - y_true
assert np.allclose(
    loss.numpy(),
    np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),
            axis=-1),
    atol=1e-5)"
"tf.keras.losses.log_cosh(
    y_true, y_pred
)
",[],"y_true = np.random.random(size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.logcosh(y_true, y_pred)
assert loss.shape == (2,)
x = y_pred - y_true
assert np.allclose(
    loss.numpy(),
    np.mean(x + np.log(np.exp(-2. * x) + 1.) - tf.math.log(2.),
            axis=-1),
    atol=1e-5)"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_absolute_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.abs(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_absolute_percentage_error(
    y_true, y_pred
)
","[['Computes the mean absolute percentage error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.random(size=(2, 3))
y_true = np.maximum(y_true, 1e-7)  # Prevent division by zero
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_absolute_percentage_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    100. * np.mean(np.abs((y_true - y_pred) / y_true), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.mean_squared_error(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_error(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(), np.mean(np.square(y_true - y_pred), axis=-1))"
"tf.keras.metrics.mean_squared_logarithmic_error(
    y_true, y_pred
)
","[['Computes the mean squared logarithmic error between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.mean_squared_logarithmic_error(y_true, y_pred)
assert loss.shape == (2,)
y_true = np.maximum(y_true, 1e-7)
y_pred = np.maximum(y_pred, 1e-7)
assert np.allclose(
    loss.numpy(),
    np.mean(
        np.square(np.log(y_true + 1.) - np.log(y_pred + 1.)), axis=-1))"
"tf.keras.metrics.poisson(
    y_true, y_pred
)
",[],"y_true = np.random.randint(0, 2, size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.poisson(y_true, y_pred)
assert loss.shape == (2,)
y_pred = y_pred + 1e-7
assert np.allclose(
    loss.numpy(), np.mean(y_pred - y_true * np.log(y_pred), axis=-1),
    atol=1e-5)"
"tf.keras.metrics.serialize(
    metric
)
","[['Serializes metric function or ', 'Metric', ' instance.']]",[]
"tf.keras.metrics.sparse_categorical_accuracy(
    y_true, y_pred
)
",[],"y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_categorical_accuracy(y_true, y_pred)
assert m.shape == (2,)
m.numpy()
array([0., 1.], dtype=float32)"
"tf.keras.metrics.sparse_categorical_crossentropy(
    y_true, y_pred, from_logits=False, axis=-1, ignore_class=None
)
",[],"y_true = [1, 2]
y_pred = [[0.05, 0.95, 0], [0.1, 0.8, 0.1]]
loss = tf.keras.losses.sparse_categorical_crossentropy(y_true, y_pred)
assert loss.shape == (2,)
loss.numpy()
array([0.0513, 2.303], dtype=float32)"
"tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","[['Computes how often integer targets are in the top ', 'K', ' predictions.']]","y_true = [2, 1]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.sparse_top_k_categorical_accuracy(
    y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.keras.metrics.squared_hinge(
    y_true, y_pred
)
","[['Computes the squared hinge loss between ', 'y_true', ' & ', 'y_pred', '.']]","y_true = np.random.choice([-1, 1], size=(2, 3))
y_pred = np.random.random(size=(2, 3))
loss = tf.keras.losses.squared_hinge(y_true, y_pred)
assert loss.shape == (2,)
assert np.array_equal(
    loss.numpy(),
    np.mean(np.square(np.maximum(1. - y_true * y_pred, 0.)), axis=-1))"
"tf.keras.metrics.top_k_categorical_accuracy(
    y_true, y_pred, k=5
)
","[['Computes how often targets are in the top ', 'K', ' predictions.']]","y_true = [[0, 0, 1], [0, 1, 0]]
y_pred = [[0.1, 0.9, 0.8], [0.05, 0.95, 0]]
m = tf.keras.metrics.top_k_categorical_accuracy(y_true, y_pred, k=3)
assert m.shape == (2,)
m.numpy()
array([1., 1.], dtype=float32)"
"tf.keras.experimental.LinearModel(
    units=1,
    activation=None,
    use_bias=True,
    kernel_initializer='zeros',
    bias_initializer='zeros',
    kernel_regularizer=None,
    bias_regularizer=None,
    **kwargs
)
","[[None, '\n'], ['Inherits From: ', 'Model', ', ', 'Layer', ', ', 'Module']]","model = LinearModel()
model.compile(optimizer='sgd', loss='mse')
model.fit(x, y, epochs=epochs)
"
"tf.keras.Model(
    *args, **kwargs
)
","[['Model', ' groups layers into an object with training and inference features.'], ['Inherits From: ', 'Layer', ', ', 'Module']]","import tensorflow as tf

inputs = tf.keras.Input(shape=(3,))
x = tf.keras.layers.Dense(4, activation=tf.nn.relu)(inputs)
outputs = tf.keras.layers.Dense(5, activation=tf.nn.softmax)(x)
model = tf.keras.Model(inputs=inputs, outputs=outputs)
"
"tf.keras.Sequential(
    layers=None, name=None
)
","[['Sequential', ' groups a linear stack of layers into a ', 'tf.keras.Model', '.'], ['Inherits From: ', 'Model', ', ', 'Layer', ', ', 'Module']]","# Optionally, the first layer can receive an `input_shape` argument:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
# Afterwards, we do automatic shape inference:
model.add(tf.keras.layers.Dense(4))

# This is identical to the following:
model = tf.keras.Sequential()
model.add(tf.keras.Input(shape=(16,)))
model.add(tf.keras.layers.Dense(8))

# Note that you can also omit the `input_shape` argument.
# In that case the model doesn't have any weights until the first call
# to a training/evaluation method (since it isn't yet built):
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
# model.weights not created yet

# Whereas if you specify the input shape, the model gets built
# continuously as you are adding layers:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8, input_shape=(16,)))
model.add(tf.keras.layers.Dense(4))
len(model.weights)
# Returns ""4""

# When using the delayed-build pattern (no input shape specified), you can
# choose to manually build your model by calling
# `build(batch_input_shape)`:
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(4))
model.build((None, 16))
len(model.weights)
# Returns ""4""

# Note that when using the delayed-build pattern (no input shape specified),
# the model gets built the first time you call `fit`, `eval`, or `predict`,
# or the first time you call the model on some input data.
model = tf.keras.Sequential()
model.add(tf.keras.layers.Dense(8))
model.add(tf.keras.layers.Dense(1))
model.compile(optimizer='sgd', loss='mse')
# This builds the model for the first time:
model.fit(x, y, batch_size=32, epochs=10)
"
"tf.keras.experimental.WideDeepModel(
    linear_model, dnn_model, activation=None, **kwargs
)
","[['Inherits From: ', 'Model', ', ', 'Layer', ', ', 'Module']]","linear_model = LinearModel()
dnn_model = keras.Sequential([keras.layers.Dense(units=64),
                             keras.layers.Dense(units=1)])
combined_model = WideDeepModel(linear_model, dnn_model)
combined_model.compile(optimizer=['sgd', 'adam'],
                       loss='mse', metrics=['mse'])
# define dnn_inputs and linear_inputs as separate numpy arrays or
# a single numpy array if dnn_inputs is same as linear_inputs.
combined_model.fit([linear_inputs, dnn_inputs], y, epochs)
# or define a single `tf.data.Dataset` that contains a single tensor or
# separate tensors for dnn_inputs and linear_inputs.
dataset = tf.data.Dataset.from_tensors(([linear_inputs, dnn_inputs], y))
combined_model.fit(dataset, epochs)
"
"tf.keras.models.clone_model(
    model, input_tensors=None, clone_function=None
)
","[['Clone a Functional or Sequential ', 'Model', ' instance.']]","# Create a test Sequential model.
model = keras.Sequential([
    keras.Input(shape=(728,)),
    keras.layers.Dense(32, activation='relu'),
    keras.layers.Dense(1, activation='sigmoid'),
])
# Create a copy of the test model (with freshly initialized weights).
new_model = clone_model(model)
"
"tf.keras.models.load_model(
    filepath, custom_objects=None, compile=True, options=None
)
","[['Loads a model saved via ', 'model.save()', '.']]","model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
model.save('/tmp/model')
loaded_model = tf.keras.models.load_model('/tmp/model')
x = tf.random.uniform((10, 3))
assert np.allclose(model.predict(x), loaded_model.predict(x))"
"tf.keras.models.model_from_config(
    config, custom_objects=None
)
",[],"# for a Functional API model
tf.keras.Model().from_config(model.get_config())

# for a Sequential model
tf.keras.Sequential().from_config(model.get_config())
"
"tf.keras.models.model_from_json(
    json_string, custom_objects=None
)
",[],"model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
config = model.to_json()
loaded_model = tf.keras.models.model_from_json(config)"
"tf.keras.models.model_from_yaml(
    yaml_string, custom_objects=None
)
",[],[]
"tf.keras.models.save_model(
    model,
    filepath,
    overwrite=True,
    include_optimizer=True,
    save_format=None,
    signatures=None,
    options=None,
    save_traces=True
)
",[],"model = tf.keras.Sequential([
    tf.keras.layers.Dense(5, input_shape=(3,)),
    tf.keras.layers.Softmax()])
model.save('/tmp/model')
loaded_model = tf.keras.models.load_model('/tmp/model')
x = tf.random.uniform((10, 3))
assert np.allclose(model.predict(x), loaded_model.predict(x))"
"tf.keras.optimizers.legacy.Adadelta(
    learning_rate=0.001,
    rho=0.95,
    epsilon=1e-07,
    name='Adadelta',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_slot(
    var, slot_name, initializer='zeros', shape=None
)
"
"tf.keras.optimizers.legacy.Adagrad(
    learning_rate=0.001,
    initial_accumulator_value=0.1,
    epsilon=1e-07,
    name='Adagrad',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_slot(
    var, slot_name, initializer='zeros', shape=None
)
"
"tf.keras.optimizers.legacy.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    name='Adam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1
step_count = opt.minimize(loss, [var1]).numpy()
# The first step is `-learning_rate*sign(grad)`
var1.numpy()
9.9"
"tf.keras.optimizers.legacy.Adamax(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    name='Adamax',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","m = 0  # Initialize initial 1st moment vector
v = 0  # Initialize the exponentially weighted infinity norm
t = 0  # Initialize timestep
"
"tf.keras.optimizers.legacy.Ftrl(
    learning_rate=0.001,
    learning_rate_power=-0.5,
    initial_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    name='Ftrl',
    l2_shrinkage_regularization_strength=0.0,
    beta=0.0,
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","n = 0
sigma = 0
z = 0
"
"tf.keras.optimizers.legacy.Nadam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    name='Nadam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]",">>> opt = tf.keras.optimizers.legacy.Nadam(learning_rate=0.2)
>>> var1 = tf.Variable(10.0)
>>> loss = lambda: (var1 ** 2) / 2.0
>>> step_count = opt.minimize(loss, [var1]).numpy()
>>> ""{:.1f}"".format(var1.numpy())
9.8
"
"tf.keras.optimizers.legacy.Optimizer(
    name, gradient_aggregator=None, gradient_transformers=None, **kwargs
)
",[],"# Create an optimizer with the desired parameters.
opt = tf.keras.optimizers.legacy.SGD(learning_rate=0.1)
# `loss` is a callable that takes no argument and returns the value
# to minimize.
var1 = tf.Variable(2.0)
var2 = tf.Variable(5.0)
loss = lambda: 3 * var1 * var1 + 2 * var2 * var2
# In graph mode, returns op that minimizes the loss by updating the listed
# variables.
opt_op = opt.minimize(loss, var_list=[var1, var2])
opt_op.run()
# In eager mode, simply call minimize to update the list of variables.
opt.minimize(loss, var_list=[var1, var2])
"
"tf.keras.optimizers.legacy.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    name='RMSprop',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2) / 2.0    # d(loss) / d(var1) = var1
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
9.683772"
"tf.keras.optimizers.legacy.SGD(
    learning_rate=0.01,
    momentum=0.0,
    nesterov=False,
    name='SGD',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","w = w - learning_rate * g
"
"tf.keras.optimizers.deserialize(
    config, custom_objects=None, **kwargs
)
","[['Inverse of the ', 'serialize', ' function.']]",[]
"tf.keras.optimizers.get(
    identifier, **kwargs
)
",[],[]
"tf.keras.optimizers.legacy.Adadelta(
    learning_rate=0.001,
    rho=0.95,
    epsilon=1e-07,
    name='Adadelta',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_slot(
    var, slot_name, initializer='zeros', shape=None
)
"
"tf.keras.optimizers.legacy.Adagrad(
    learning_rate=0.001,
    initial_accumulator_value=0.1,
    epsilon=1e-07,
    name='Adagrad',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","add_slot(
    var, slot_name, initializer='zeros', shape=None
)
"
"tf.keras.optimizers.legacy.Adam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    amsgrad=False,
    name='Adam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","opt = tf.keras.optimizers.legacy.Adam(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2)/2.0       # d(loss)/d(var1) == var1
step_count = opt.minimize(loss, [var1]).numpy()
# The first step is `-learning_rate*sign(grad)`
var1.numpy()
9.9"
"tf.keras.optimizers.legacy.Adamax(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    name='Adamax',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","m = 0  # Initialize initial 1st moment vector
v = 0  # Initialize the exponentially weighted infinity norm
t = 0  # Initialize timestep
"
"tf.keras.optimizers.legacy.Ftrl(
    learning_rate=0.001,
    learning_rate_power=-0.5,
    initial_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    name='Ftrl',
    l2_shrinkage_regularization_strength=0.0,
    beta=0.0,
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","n = 0
sigma = 0
z = 0
"
"tf.keras.optimizers.legacy.Nadam(
    learning_rate=0.001,
    beta_1=0.9,
    beta_2=0.999,
    epsilon=1e-07,
    name='Nadam',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]",">>> opt = tf.keras.optimizers.legacy.Nadam(learning_rate=0.2)
>>> var1 = tf.Variable(10.0)
>>> loss = lambda: (var1 ** 2) / 2.0
>>> step_count = opt.minimize(loss, [var1]).numpy()
>>> ""{:.1f}"".format(var1.numpy())
9.8
"
"tf.keras.optimizers.legacy.Optimizer(
    name, gradient_aggregator=None, gradient_transformers=None, **kwargs
)
",[],"# Create an optimizer with the desired parameters.
opt = tf.keras.optimizers.legacy.SGD(learning_rate=0.1)
# `loss` is a callable that takes no argument and returns the value
# to minimize.
var1 = tf.Variable(2.0)
var2 = tf.Variable(5.0)
loss = lambda: 3 * var1 * var1 + 2 * var2 * var2
# In graph mode, returns op that minimizes the loss by updating the listed
# variables.
opt_op = opt.minimize(loss, var_list=[var1, var2])
opt_op.run()
# In eager mode, simply call minimize to update the list of variables.
opt.minimize(loss, var_list=[var1, var2])
"
"tf.keras.optimizers.legacy.RMSprop(
    learning_rate=0.001,
    rho=0.9,
    momentum=0.0,
    epsilon=1e-07,
    centered=False,
    name='RMSprop',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","opt = tf.keras.optimizers.legacy.RMSprop(learning_rate=0.1)
var1 = tf.Variable(10.0)
loss = lambda: (var1 ** 2) / 2.0    # d(loss) / d(var1) = var1
step_count = opt.minimize(loss, [var1]).numpy()
var1.numpy()
9.683772"
"tf.keras.optimizers.legacy.SGD(
    learning_rate=0.01,
    momentum=0.0,
    nesterov=False,
    name='SGD',
    **kwargs
)
","[['Inherits From: ', 'Optimizer']]","w = w - learning_rate * g
"
"tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate, decay_steps, alpha=0.0, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  step = min(step, decay_steps)
  cosine_decay = 0.5 * (1 + cos(pi * step / decay_steps))
  decayed = (1 - alpha) * cosine_decay + alpha
  return initial_learning_rate * decayed
"
"tf.keras.optimizers.schedules.CosineDecayRestarts(
    initial_learning_rate,
    first_decay_steps,
    t_mul=2.0,
    m_mul=1.0,
    alpha=0.0,
    name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","first_decay_steps = 1000
lr_decayed_fn = (
  tf.keras.optimizers.schedules.CosineDecayRestarts(
      initial_learning_rate,
      first_decay_steps))
"
"tf.keras.optimizers.schedules.ExponentialDecay(
    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  return initial_learning_rate * decay_rate ^ (step / decay_steps)
"
"tf.keras.optimizers.schedules.InverseTimeDecay(
    initial_learning_rate, decay_steps, decay_rate, staircase=False, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  return initial_learning_rate / (1 + decay_rate * step / decay_step)
"
"tf.keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values, name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","step = tf.Variable(0, trainable=False)
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate_fn = keras.optimizers.schedules.PiecewiseConstantDecay(
    boundaries, values)

# Later, whenever we perform an optimization step, we pass in the step.
learning_rate = learning_rate_fn(step)
"
"tf.keras.optimizers.schedules.PolynomialDecay(
    initial_learning_rate,
    decay_steps,
    end_learning_rate=0.0001,
    power=1.0,
    cycle=False,
    name=None
)
","[['Inherits From: ', 'LearningRateSchedule']]","def decayed_learning_rate(step):
  step = min(step, decay_steps)
  return ((initial_learning_rate - end_learning_rate) *
          (1 - step / decay_steps) ^ (power)
         ) + end_learning_rate
"
"tf.keras.optimizers.schedules.deserialize(
    config, custom_objects=None
)
","[['Instantiates a ', 'LearningRateSchedule', ' object from a serialized form.']]","# Configuration for PolynomialDecay
config = {
  'class_name': 'PolynomialDecay',
  'config': {'cycle': False,
    'decay_steps': 10000,
    'end_learning_rate': 0.01,
    'initial_learning_rate': 0.1,
    'name': None,
    'power': 0.5} }
lr_schedule = tf.keras.optimizers.schedules.deserialize(config)
"
"tf.keras.optimizers.schedules.serialize(
    learning_rate_schedule
)
","[['Serializes a ', 'LearningRateSchedule', ' into a JSON-compatible representation.']]","lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(
  0.1, decay_steps=100000, decay_rate=0.96, staircase=True)
tf.keras.optimizers.schedules.serialize(lr_schedule)
{'class_name': 'ExponentialDecay', 'config': {...} }"
"tf.keras.optimizers.serialize(
    optimizer
)
",[],"tf.keras.optimizers.serialize(tf.keras.optimizers.legacy.SGD())
{'class_name': 'SGD', 'config': {'name': 'SGD', 'learning_rate': 0.01,
                                 'decay': 0.0, 'momentum': 0.0,
                                 'nesterov': False} }"
"tf.keras.preprocessing.image.DirectoryIterator(
    directory,
    image_data_generator,
    target_size=(256, 256),
    color_mode='rgb',
    classes=None,
    class_mode='categorical',
    batch_size=32,
    shuffle=True,
    seed=None,
    data_format=None,
    save_to_dir=None,
    save_prefix='',
    save_format='png',
    follow_links=False,
    subset=None,
    interpolation='nearest',
    keep_aspect_ratio=False,
    dtype=None
)
","[['Inherits From: ', 'Iterator', ', ', 'Sequence']]","next()
"
"tf.keras.preprocessing.image.ImageDataGenerator(
    featurewise_center=False,
    samplewise_center=False,
    featurewise_std_normalization=False,
    samplewise_std_normalization=False,
    zca_whitening=False,
    zca_epsilon=1e-06,
    rotation_range=0,
    width_shift_range=0.0,
    height_shift_range=0.0,
    brightness_range=None,
    shear_range=0.0,
    zoom_range=0.0,
    channel_shift_range=0.0,
    fill_mode='nearest',
    cval=0.0,
    horizontal_flip=False,
    vertical_flip=False,
    rescale=None,
    preprocessing_function=None,
    data_format=None,
    validation_split=0.0,
    interpolation_order=1,
    dtype=None
)
",[],"(x_train, y_train), (x_test, y_test) = cifar10.load_data()
y_train = utils.to_categorical(y_train, num_classes)
y_test = utils.to_categorical(y_test, num_classes)
datagen = ImageDataGenerator(
    featurewise_center=True,
    featurewise_std_normalization=True,
    rotation_range=20,
    width_shift_range=0.2,
    height_shift_range=0.2,
    horizontal_flip=True,
    validation_split=0.2)
# compute quantities required for featurewise normalization
# (std, mean, and principal components if ZCA whitening is applied)
datagen.fit(x_train)
# fits the model on batches with real-time data augmentation:
model.fit(datagen.flow(x_train, y_train, batch_size=32,
         subset='training'),
         validation_data=datagen.flow(x_train, y_train,
         batch_size=8, subset='validation'),
         steps_per_epoch=len(x_train) / 32, epochs=epochs)
# here's a more ""manual"" example
for e in range(epochs):
    print('Epoch', e)
    batches = 0
    for x_batch, y_batch in datagen.flow(x_train, y_train, batch_size=32):
        model.fit(x_batch, y_batch)
        batches += 1
        if batches >= len(x_train) / 32:
            # we need to break the loop by hand because
            # the generator loops indefinitely
            break
"
"tf.keras.preprocessing.image.Iterator(
    n, batch_size, shuffle, seed
)
","[['Inherits From: ', 'Sequence']]","next()
"
"tf.keras.preprocessing.image.NumpyArrayIterator(
    x,
    y,
    image_data_generator,
    batch_size=32,
    shuffle=False,
    sample_weight=None,
    seed=None,
    data_format=None,
    save_to_dir=None,
    save_prefix='',
    save_format='png',
    subset=None,
    ignore_class_split=False,
    dtype=None
)
","[['Inherits From: ', 'Iterator', ', ', 'Sequence']]","next()
"
"tf.keras.preprocessing.image.apply_affine_transform(
    x,
    theta=0,
    tx=0,
    ty=0,
    shear=0,
    zx=1,
    zy=1,
    row_axis=1,
    col_axis=2,
    channel_axis=0,
    fill_mode='nearest',
    cval=0.0,
    order=1
)
",[],[]
"tf.keras.preprocessing.image.apply_brightness_shift(
    x, brightness, scale=True
)
",[],[]
"tf.keras.preprocessing.image.apply_channel_shift(
    x, intensity, channel_axis=0
)
",[],[]
"tf.keras.utils.array_to_img(
    x, data_format=None, scale=True, dtype=None
)
",[],"from PIL import Image
img = np.random.random(size=(100, 100, 3))
pil_img = tf.keras.utils.array_to_img(img)
"
"tf.keras.utils.img_to_array(
    img, data_format=None, dtype=None
)
",[],"from PIL import Image
img_data = np.random.random(size=(100, 100, 3))
img = tf.keras.utils.array_to_img(img_data)
array = tf.keras.utils.image.img_to_array(img)
"
"tf.keras.utils.load_img(
    path,
    grayscale=False,
    color_mode='rgb',
    target_size=None,
    interpolation='nearest',
    keep_aspect_ratio=False
)
",[],"image = tf.keras.utils.load_img(image_path)
input_arr = tf.keras.utils.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)
"
"tf.keras.preprocessing.image.random_brightness(
    x, brightness_range, scale=True
)
",[],[]
"tf.keras.preprocessing.image.random_channel_shift(
    x, intensity_range, channel_axis=0
)
",[],[]
"tf.keras.preprocessing.image.random_rotation(
    x,
    rg,
    row_axis=1,
    col_axis=2,
    channel_axis=0,
    fill_mode='nearest',
    cval=0.0,
    interpolation_order=1
)
",[],[]
"tf.keras.preprocessing.image.random_shear(
    x,
    intensity,
    row_axis=1,
    col_axis=2,
    channel_axis=0,
    fill_mode='nearest',
    cval=0.0,
    interpolation_order=1
)
",[],[]
"tf.keras.preprocessing.image.random_shift(
    x,
    wrg,
    hrg,
    row_axis=1,
    col_axis=2,
    channel_axis=0,
    fill_mode='nearest',
    cval=0.0,
    interpolation_order=1
)
",[],[]
"tf.keras.preprocessing.image.random_zoom(
    x,
    zoom_range,
    row_axis=1,
    col_axis=2,
    channel_axis=0,
    fill_mode='nearest',
    cval=0.0,
    interpolation_order=1
)
",[],[]
"tf.keras.utils.save_img(
    path, x, data_format=None, file_format=None, scale=True, **kwargs
)
",[],[]
"tf.keras.preprocessing.sequence.TimeseriesGenerator(
    data,
    targets,
    length,
    sampling_rate=1,
    stride=1,
    start_index=0,
    end_index=None,
    shuffle=False,
    reverse=False,
    batch_size=128
)
","[['Inherits From: ', 'Sequence']]","from keras.preprocessing.sequence import TimeseriesGenerator
import numpy as np
data = np.array([[i] for i in range(50)])
targets = np.array([[i] for i in range(50)])
data_gen = TimeseriesGenerator(data, targets,
                               length=10, sampling_rate=2,
                               batch_size=2)
assert len(data_gen) == 20
batch_0 = data_gen[0]
x, y = batch_0
assert np.array_equal(x,
                      np.array([[[0], [2], [4], [6], [8]],
                                [[1], [3], [5], [7], [9]]]))
assert np.array_equal(y,
                      np.array([[10], [11]]))
"
"tf.keras.preprocessing.sequence.make_sampling_table(
    size, sampling_factor=1e-05
)
",[],"p(word) = (min(1, sqrt(word_frequency / sampling_factor) /
    (word_frequency / sampling_factor)))
"
"tf.keras.utils.pad_sequences(
    sequences,
    maxlen=None,
    dtype='int32',
    padding='pre',
    truncating='pre',
    value=0.0
)
",[],"sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)
array([[0, 0, 1],
       [0, 2, 3],
       [4, 5, 6]], dtype=int32)"
"tf.keras.preprocessing.sequence.skipgrams(
    sequence,
    vocabulary_size,
    window_size=4,
    negative_samples=1.0,
    shuffle=True,
    categorical=False,
    sampling_table=None,
    seed=None
)
",[],[]
"tf.keras.preprocessing.text.Tokenizer(
    num_words=None,
    filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
    lower=True,
    split=' ',
    char_level=False,
    oov_token=None,
    analyzer=None,
    **kwargs
)
","[[None, '\n']]","fit_on_sequences(
    sequences
)
"
"tf.keras.preprocessing.text.hashing_trick(
    text,
    n,
    hash_function=None,
    filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
    lower=True,
    split=' ',
    analyzer=None
)
","[[None, '\n']]",[]
"tf.keras.preprocessing.text.one_hot(
    input_text,
    n,
    filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
    lower=True,
    split=' ',
    analyzer=None
)
","[[None, '\n'], ['One-hot encodes a text into a list of word indexes of size ', 'n', '.']]",[]
"tf.keras.preprocessing.text.text_to_word_sequence(
    input_text,
    filters='!""#$%&()*+,-./:;<=>?@[\\]^_`{|}~\t\n',
    lower=True,
    split=' '
)
","[[None, '\n']]","sample_text = 'This is a sample sentence.'
tf.keras.preprocessing.text.text_to_word_sequence(sample_text)
['this', 'is', 'a', 'sample', 'sentence']"
"tf.keras.preprocessing.text.tokenizer_from_json(
    json_string
)
",[],[]
"tf.keras.regularizers.L1(
    l1=0.01, **kwargs
)
","[['Inherits From: ', 'Regularizer']]","dense = tf.keras.layers.Dense(3, kernel_regularizer='l1')"
"tf.keras.regularizers.L1L2(
    l1=0.0, l2=0.0
)
","[['Inherits From: ', 'Regularizer']]","dense = tf.keras.layers.Dense(3, kernel_regularizer='l1_l2')"
"tf.keras.regularizers.L2(
    l2=0.01, **kwargs
)
","[['Inherits From: ', 'Regularizer']]","dense = tf.keras.layers.Dense(3, kernel_regularizer='l2')"
"tf.keras.regularizers.deserialize(
    config, custom_objects=None
)
",[],[]
"tf.keras.regularizers.get(
    identifier
)
",[],[]
"tf.keras.regularizers.L1(
    l1=0.01, **kwargs
)
","[['Inherits From: ', 'Regularizer']]","dense = tf.keras.layers.Dense(3, kernel_regularizer='l1')"
"tf.keras.regularizers.l1_l2(
    l1=0.01, l2=0.01
)
",[],[]
"tf.keras.regularizers.L2(
    l2=0.01, **kwargs
)
","[['Inherits From: ', 'Regularizer']]","dense = tf.keras.layers.Dense(3, kernel_regularizer='l2')"
"tf.keras.regularizers.serialize(
    regularizer
)
",[],[]
"tf.keras.utils.custom_object_scope(
    *args
)
",[],"layer = Dense(3, kernel_regularizer=my_regularizer)
# Config contains a reference to `my_regularizer`
config = layer.get_config()
...
# Later:
with custom_object_scope({'my_regularizer': my_regularizer}):
  layer = Dense.from_config(config)
"
"tf.compat.v1.keras.utils.DeterministicRandomTestTool(
    seed: int = 42, mode='constant'
)
",[],"scope()
"
"tf.keras.utils.GeneratorEnqueuer(
    generator, use_multiprocessing=False, random_seed=None
)
","[['Inherits From: ', 'SequenceEnqueuer']]","get()
"
"tf.keras.utils.OrderedEnqueuer(
    sequence, use_multiprocessing=False, shuffle=False
)
","[['Inherits From: ', 'SequenceEnqueuer']]","get()
"
"tf.keras.utils.Progbar(
    target,
    width=30,
    verbose=1,
    interval=0.05,
    stateful_metrics=None,
    unit_name='step'
)
",[],"add(
    n, values=None
)
"
"tf.keras.utils.SequenceEnqueuer(
    sequence, use_multiprocessing=False
)
",[],"    enqueuer = SequenceEnqueuer(...)
    enqueuer.start()
    datas = enqueuer.get()
    for data in datas:
        # Use the inputs; training, evaluating, predicting.
        # ... stop sometime.
    enqueuer.stop()
"
"tf.keras.utils.array_to_img(
    x, data_format=None, scale=True, dtype=None
)
",[],"from PIL import Image
img = np.random.random(size=(100, 100, 3))
pil_img = tf.keras.utils.array_to_img(img)
"
"tf.keras.utils.custom_object_scope(
    *args
)
",[],"layer = Dense(3, kernel_regularizer=my_regularizer)
# Config contains a reference to `my_regularizer`
config = layer.get_config()
...
# Later:
with custom_object_scope({'my_regularizer': my_regularizer}):
  layer = Dense.from_config(config)
"
"tf.keras.utils.deserialize_keras_object(
    identifier,
    module_objects=None,
    custom_objects=None,
    printable_module_name='object'
)
",[],"def deserialize(config, custom_objects=None):
   return deserialize_keras_object(
     identifier,
     module_objects=globals(),
     custom_objects=custom_objects,
     name=""MyObjectType"",
   )
"
"tf.keras.utils.get_file(
    fname=None,
    origin=None,
    untar=False,
    md5_hash=None,
    file_hash=None,
    cache_subdir='datasets',
    hash_algorithm='auto',
    extract=False,
    archive_format='auto',
    cache_dir=None
)
",[],"path_to_downloaded_file = tf.keras.utils.get_file(
    ""flower_photos"",
    ""https://storage.googleapis.com/download.tensorflow.org/example_images/flower_photos.tgz"",
    untar=True)
"
"tf.compat.v1.keras.utils.get_or_create_layer(
    name, create_layer_method
)
",[],"class NestedLayer(tf.keras.layers.Layer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  def build_model(self):
    inp = tf.keras.Input(shape=(5, 5))
    dense_layer = tf.keras.layers.Dense(
        10, name=""dense"", kernel_regularizer=""l2"",
        kernel_initializer=tf.compat.v1.ones_initializer())
    model = tf.keras.Model(inputs=inp, outputs=dense_layer(inp))
    return model

  @tf.compat.v1.keras.utils.track_tf1_style_variables
  def call(self, inputs):
    model = tf.compat.v1.keras.utils.get_or_create_layer(
        ""dense_model"", self.build_model)
    return model(inputs)
"
"tf.keras.utils.get_registered_name(
    obj
)
",[],[]
"tf.keras.utils.get_registered_object(
    name, custom_objects=None, module_objects=None
)
","[['Returns the class associated with ', 'name', ' if it is registered with Keras.']]","def from_config(cls, config, custom_objects=None):
  if 'my_custom_object_name' in config:
    config['hidden_cls'] = tf.keras.utils.get_registered_object(
        config['my_custom_object_name'], custom_objects=custom_objects)
"
"tf.keras.utils.get_source_inputs(
    tensor, layer=None, node_index=None
)
","[['Returns the list of input tensors necessary to compute ', 'tensor', '.']]",[]
"tf.keras.utils.img_to_array(
    img, data_format=None, dtype=None
)
",[],"from PIL import Image
img_data = np.random.random(size=(100, 100, 3))
img = tf.keras.utils.array_to_img(img_data)
array = tf.keras.utils.image.img_to_array(img)
"
"tf.keras.utils.load_img(
    path,
    grayscale=False,
    color_mode='rgb',
    target_size=None,
    interpolation='nearest',
    keep_aspect_ratio=False
)
",[],"image = tf.keras.utils.load_img(image_path)
input_arr = tf.keras.utils.img_to_array(image)
input_arr = np.array([input_arr])  # Convert single image to a batch.
predictions = model.predict(input_arr)
"
"tf.keras.utils.model_to_dot(
    model,
    show_shapes=False,
    show_dtype=False,
    show_layer_names=True,
    rankdir='TB',
    expand_nested=False,
    dpi=96,
    subgraph=False,
    layer_range=None,
    show_layer_activations=False
)
",[],[]
"tf.keras.utils.normalize(
    x, axis=-1, order=2
)
",[],[]
"tf.keras.utils.pad_sequences(
    sequences,
    maxlen=None,
    dtype='int32',
    padding='pre',
    truncating='pre',
    value=0.0
)
",[],"sequence = [[1], [2, 3], [4, 5, 6]]
tf.keras.preprocessing.sequence.pad_sequences(sequence)
array([[0, 0, 1],
       [0, 2, 3],
       [4, 5, 6]], dtype=int32)"
"tf.keras.utils.plot_model(
    model,
    to_file='model.png',
    show_shapes=False,
    show_dtype=False,
    show_layer_names=True,
    rankdir='TB',
    expand_nested=False,
    dpi=96,
    layer_range=None,
    show_layer_activations=False
)
",[],"input = tf.keras.Input(shape=(100,), dtype='int32', name='input')
x = tf.keras.layers.Embedding(
    output_dim=512, input_dim=10000, input_length=100)(input)
x = tf.keras.layers.LSTM(32)(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
x = tf.keras.layers.Dense(64, activation='relu')(x)
output = tf.keras.layers.Dense(1, activation='sigmoid', name='output')(x)
model = tf.keras.Model(inputs=[input], outputs=[output])
dot_img_file = '/tmp/model_1.png'
tf.keras.utils.plot_model(model, to_file=dot_img_file, show_shapes=True)
"
"tf.keras.utils.register_keras_serializable(
    package='Custom', name=None
)
",[],"# Note that `'my_package'` is used as the `package` argument here, and since
# the `name` argument is not provided, `'MyDense'` is used as the `name`.
@keras.utils.register_keras_serializable('my_package')
class MyDense(keras.layers.Dense):
  pass

assert keras.utils.get_registered_object('my_package>MyDense') == MyDense
assert keras.utils.get_registered_name(MyDense) == 'my_package>MyDense'
"
"tf.keras.utils.save_img(
    path, x, data_format=None, file_format=None, scale=True, **kwargs
)
",[],[]
"tf.keras.utils.serialize_keras_object(
    instance
)
",[],[]
"tf.keras.utils.to_categorical(
    y, num_classes=None, dtype='float32'
)
",[],"a = tf.keras.utils.to_categorical([0, 1, 2, 3], num_classes=4)
a = tf.constant(a, shape=[4, 4])
print(a)
tf.Tensor(
  [[1. 0. 0. 0.]
   [0. 1. 0. 0.]
   [0. 0. 1. 0.]
   [0. 0. 0. 1.]], shape=(4, 4), dtype=float32)"
"tf.compat.v1.keras.utils.track_tf1_style_variables(
    method
)
",[],"class WrappedDoubleDenseLayer(tf.keras.layers.Layer):

  def __init__(self, units, *args, **kwargs):
    super().__init__(*args, **kwargs)
    self.units = units

  @tf.compat.v1.keras.utils.track_tf1_style_variables
  def call(self, inputs):
    with tf.compat.v1.variable_scope(""double_dense_layer""):
      out = tf.compat.v1.layers.dense(
          inputs, self.units, name=""dense_one"",
          kernel_initializer=tf.compat.v1.random_normal_initializer,
          kernel_regularizer=""l2"")
      out = tf.compat.v1.layers.dense(
          out, self.units, name=""dense_two"",
          kernel_initializer=tf.compat.v1.random_normal_initializer(),
          kernel_regularizer=""l2"")
    return out

# Create a layer that can be used as a standard keras layer
layer = WrappedDoubleDenseLayer(10)

# call the layer on inputs
layer(...)

# Variables created/used within the scope will be tracked by the layer
layer.weights
layer.trainable_variables

# Regularization losses will be captured in layer.losses after a call,
# just like any other Keras layer
reg_losses = layer.losses
"
"tf.keras.utils.warmstart_embedding_matrix(
    base_vocabulary,
    new_vocabulary,
    base_embeddings,
    new_embeddings_initializer='uniform'
)
",[],">>> import keras
>>> vocab_base = tf.convert_to_tensor([""unk"", ""a"", ""b"", ""c""])
>>> vocab_new = tf.convert_to_tensor(
...        [""unk"", ""unk"", ""a"", ""b"", ""c"", ""d"", ""e""])
>>> vectorized_vocab_base = np.random.rand(vocab_base.shape[0], 3)
>>> vectorized_vocab_new = np.random.rand(vocab_new.shape[0], 3)
>>> warmstarted_embedding_matrix = warmstart_embedding_matrix(
...       base_vocabulary=vocab_base,
...       new_vocabulary=vocab_new,
...       base_embeddings=vectorized_vocab_base,
...       new_embeddings_initializer=keras.initializers.Constant(
...         vectorized_vocab_new))
"
"tf.compat.v1.layers.AveragePooling1D(
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None,
    **kwargs
)
","[['Inherits From: ', 'AveragePooling1D', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," pooling = tf.compat.v1.layers.AveragePooling1D(pool_size=2, strides=2)
"
"tf.compat.v1.layers.AveragePooling2D(
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None,
    **kwargs
)
","[['Inherits From: ', 'AveragePooling2D', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," pooling = tf.compat.v1.layers.AveragePooling2D(pool_size=2, strides=2)
"
"tf.compat.v1.layers.AveragePooling3D(
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None,
    **kwargs
)
","[['Inherits From: ', 'AveragePooling3D', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," pooling = tf.compat.v1.layers.AveragePooling3D(pool_size=2, strides=2)
"
"tf.compat.v1.layers.BatchNormalization(
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer=tf.compat.v1.zeros_initializer(),
    gamma_initializer=tf.compat.v1.ones_initializer(),
    moving_mean_initializer=tf.compat.v1.zeros_initializer(),
    moving_variance_initializer=tf.compat.v1.ones_initializer(),
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    renorm=False,
    renorm_clipping=None,
    renorm_momentum=0.99,
    fused=None,
    trainable=True,
    virtual_batch_size=None,
    adjustment=None,
    name=None,
    **kwargs
)
","[['Inherits From: ', 'BatchNormalization', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," bn = tf.compat.v1.layers.BatchNormalization()
"
"tf.compat.v1.layers.Conv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format='channels_last',
    dilation_rate=1,
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    **kwargs
)
","[['Inherits From: ', 'Conv1D', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," conv = tf.compat.v1.layers.Conv1D(filters=3, kernel_size=3)
"
"tf.compat.v1.layers.Conv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format='channels_last',
    dilation_rate=(1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    **kwargs
)
","[['Inherits From: ', 'Conv2D', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," conv = tf.compat.v1.layers.Conv2D(filters=3, kernel_size=3)
"
"tf.compat.v1.layers.Conv2DTranspose(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format='channels_last',
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    **kwargs
)
","[['Inherits From: ', 'Conv2DTranspose', ', ', 'Conv2D', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," conv = tf.compat.v1.layers.Conv2DTranspose(filters=3, kernel_size=3)
"
"tf.compat.v1.layers.Conv3D(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format='channels_last',
    dilation_rate=(1, 1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    **kwargs
)
","[['Inherits From: ', 'Conv3D', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," conv = tf.compat.v1.layers.Conv3D(filters=3, kernel_size=3)
"
"tf.compat.v1.layers.Conv3DTranspose(
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format='channels_last',
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    **kwargs
)
","[['Inherits From: ', 'Conv3DTranspose', ', ', 'Conv3D', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," conv = tf.compat.v1.layers.Conv3DTranspose(filters=3, kernel_size=3)
"
"tf.compat.v1.layers.Dense(
    units,
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    **kwargs
)
","[['Inherits From: ', 'Dense', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," dense = tf.compat.v1.layers.Dense(units=3)
"
"tf.compat.v1.layers.Dropout(
    rate=0.5, noise_shape=None, seed=None, name=None, **kwargs
)
","[['Inherits From: ', 'Dropout', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," dropout = tf.compat.v1.layers.Dropout()
"
"tf.compat.v1.layers.Flatten(
    data_format=None, **kwargs
)
","[['Inherits From: ', 'Flatten', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," flatten = tf.compat.v1.layers.Flatten()
"
"tf.keras.layers.InputSpec(
    dtype=None,
    shape=None,
    ndim=None,
    max_ndim=None,
    min_ndim=None,
    axes=None,
    allow_last_axis_squeeze=False,
    name=None
)
",[],"class MyLayer(Layer):
    def __init__(self):
        super(MyLayer, self).__init__()
        # The layer will accept inputs with
        # shape (?, 28, 28) & (?, 28, 28, 1)
        # and raise an appropriate error message otherwise.
        self.input_spec = InputSpec(
            shape=(None, 28, 28, 1),
            allow_last_axis_squeeze=True)
"
"tf.compat.v1.layers.Layer(
    trainable=True, name=None, dtype=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Module']]","apply(
    *args, **kwargs
)
"
"tf.compat.v1.layers.MaxPooling1D(
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None,
    **kwargs
)
","[['Inherits From: ', 'MaxPool1D', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," pooling = tf.compat.v1.layers.MaxPooling1D(pool_size=2, strides=2)
"
"tf.compat.v1.layers.MaxPooling2D(
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None,
    **kwargs
)
","[['Inherits From: ', 'MaxPool2D', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," pooling = tf.compat.v1.layers.MaxPooling2D(pool_size=2, strides=2)
"
"tf.compat.v1.layers.MaxPooling3D(
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None,
    **kwargs
)
","[['Inherits From: ', 'MaxPool3D', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," pooling = tf.compat.v1.layers.MaxPooling3D(pool_size=2, strides=2)
"
"tf.compat.v1.layers.SeparableConv1D(
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format='channels_last',
    dilation_rate=1,
    depth_multiplier=1,
    activation=None,
    use_bias=True,
    depthwise_initializer=None,
    pointwise_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    depthwise_regularizer=None,
    pointwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    pointwise_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    **kwargs
)
","[['Inherits From: ', 'SeparableConv1D', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," conv = tf.compat.v1.layers.SeparableConv1D(filters=3, kernel_size=3)
"
"tf.compat.v1.layers.SeparableConv2D(
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format='channels_last',
    dilation_rate=(1, 1),
    depth_multiplier=1,
    activation=None,
    use_bias=True,
    depthwise_initializer=None,
    pointwise_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    depthwise_regularizer=None,
    pointwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    pointwise_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    **kwargs
)
","[['Inherits From: ', 'SeparableConv2D', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]"," conv = tf.compat.v1.layers.SeparableConv2D(filters=3, kernel_size=3)
"
"tf.compat.v1.layers.average_pooling1d(
    inputs,
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None
)
",[]," y = tf.compat.v1.layers.average_pooling1d(x, pool_size=2, strides=2)
"
"tf.compat.v1.layers.average_pooling2d(
    inputs,
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None
)
",[]," y = tf.compat.v1.layers.average_pooling2d(x, pool_size=2, strides=2)
"
"tf.compat.v1.layers.average_pooling3d(
    inputs,
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None
)
",[]," y = tf.compat.v1.layers.average_pooling3d(x, pool_size=2, strides=2)
"
"tf.compat.v1.layers.batch_normalization(
    inputs,
    axis=-1,
    momentum=0.99,
    epsilon=0.001,
    center=True,
    scale=True,
    beta_initializer=tf.compat.v1.zeros_initializer(),
    gamma_initializer=tf.compat.v1.ones_initializer(),
    moving_mean_initializer=tf.compat.v1.zeros_initializer(),
    moving_variance_initializer=tf.compat.v1.ones_initializer(),
    beta_regularizer=None,
    gamma_regularizer=None,
    beta_constraint=None,
    gamma_constraint=None,
    training=False,
    trainable=True,
    name=None,
    reuse=None,
    renorm=False,
    renorm_clipping=None,
    renorm_momentum=0.99,
    fused=None,
    virtual_batch_size=None,
    adjustment=None
)
",[]," x_norm = tf.compat.v1.layers.batch_normalization(x)
"
"tf.compat.v1.layers.conv1d(
    inputs,
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format='channels_last',
    dilation_rate=1,
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    reuse=None
)
",[]," y = tf.compat.v1.layers.conv1d(x, filters=3, kernel_size=3)
"
"tf.compat.v1.layers.conv2d(
    inputs,
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format='channels_last',
    dilation_rate=(1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    reuse=None
)
",[]," y = tf.compat.v1.layers.conv2d(x, filters=3, kernel_size=3)
"
"tf.compat.v1.layers.conv2d_transpose(
    inputs,
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format='channels_last',
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    reuse=None
)
",[]," y = tf.compat.v1.layers.conv2d_transpose(x, filters=3, kernel_size=3)
"
"tf.compat.v1.layers.conv3d(
    inputs,
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format='channels_last',
    dilation_rate=(1, 1, 1),
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    reuse=None
)
",[]," y = tf.compat.v1.layers.conv3d(x, filters=3, kernel_size=3)
"
"tf.compat.v1.layers.conv3d_transpose(
    inputs,
    filters,
    kernel_size,
    strides=(1, 1, 1),
    padding='valid',
    data_format='channels_last',
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    reuse=None
)
",[]," y = tf.compat.v1.layers.conv3d_transpose(x, filters=3, kernel_size=3)
"
"tf.compat.v1.layers.dense(
    inputs,
    units,
    activation=None,
    use_bias=True,
    kernel_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    kernel_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    kernel_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    reuse=None
)
",[]," y = tf.compat.v1.layers.dense(x, units=3)
"
"tf.compat.v1.layers.dropout(
    inputs, rate=0.5, noise_shape=None, seed=None, training=False, name=None
)
",[]," y = tf.compat.v1.layers.dropout(x)
"
"tf.compat.v1.layers.flatten(
    inputs, name=None, data_format='channels_last'
)
",[]," y = tf.compat.v1.layers.flatten(x)
"
"tf.compat.v1.layers.max_pooling1d(
    inputs,
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None
)
",[]," y = tf.compat.v1.layers.max_pooling1d(x, pool_size=2, strides=2)
"
"tf.compat.v1.layers.max_pooling2d(
    inputs,
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None
)
",[]," y = tf.compat.v1.layers.max_pooling2d(x, pool_size=2, strides=2)
"
"tf.compat.v1.layers.max_pooling3d(
    inputs,
    pool_size,
    strides,
    padding='valid',
    data_format='channels_last',
    name=None
)
",[]," y = tf.compat.v1.layers.max_pooling3d(x, pool_size=2, strides=2)
"
"tf.compat.v1.layers.separable_conv1d(
    inputs,
    filters,
    kernel_size,
    strides=1,
    padding='valid',
    data_format='channels_last',
    dilation_rate=1,
    depth_multiplier=1,
    activation=None,
    use_bias=True,
    depthwise_initializer=None,
    pointwise_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    depthwise_regularizer=None,
    pointwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    pointwise_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    reuse=None
)
",[]," y = tf.compat.v1.layers.separable_conv1d(x, filters=3, kernel_size=3)
"
"tf.compat.v1.layers.separable_conv2d(
    inputs,
    filters,
    kernel_size,
    strides=(1, 1),
    padding='valid',
    data_format='channels_last',
    dilation_rate=(1, 1),
    depth_multiplier=1,
    activation=None,
    use_bias=True,
    depthwise_initializer=None,
    pointwise_initializer=None,
    bias_initializer=tf.compat.v1.zeros_initializer(),
    depthwise_regularizer=None,
    pointwise_regularizer=None,
    bias_regularizer=None,
    activity_regularizer=None,
    depthwise_constraint=None,
    pointwise_constraint=None,
    bias_constraint=None,
    trainable=True,
    name=None,
    reuse=None
)
",[]," y = tf.compat.v1.layers.separable_conv2d(x, filters=3, kernel_size=3)
"
"tf.math.lbeta(
    x, name=None
)
","[[None, '\n']]",[]
"tf.math.less(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less(x, y) ==> [False, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 7])
tf.math.less(x, y) ==> [False, True, True]
"
"tf.math.less_equal(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less_equal(x, y) ==> [True, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 6])
tf.math.less_equal(x, y) ==> [True, True, True]
"
"tf.math.lgamma(
    x, name=None
)
","[['Computes the log of the absolute value of ', 'Gamma(x)', ' element-wise.']]","x = tf.constant([0, 0.5, 1, 4.5, -4, -5.6])
tf.math.lgamma(x) ==> [inf, 0.5723649, 0., 2.4537368, inf, -4.6477685]
"
"tf.linspace(
    start, stop, num, name=None, axis=0
)
",[],"tf.linspace(10.0, 12.0, 3, name=""linspace"") => [ 10.0  11.0  12.0]
"
"tf.linalg.LinearOperator(
    dtype,
    graph_parents=None,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None,
    parameters=None
)
","[['Inherits From: ', 'Module']]","operator.shape = [B1,...,Bb] + [M, N],  b >= 0,
x.shape =   [B1,...,Bb] + [N, R]
"
"tf.linalg.LinearOperatorAdjoint(
    operator,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","[['LinearOperator', ' representing the adjoint of another operator.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 linear operator.
operator = LinearOperatorFullMatrix([[1 - i., 3.], [0., 1. + i]])
operator_adjoint = LinearOperatorAdjoint(operator)

operator_adjoint.to_dense()
==> [[1. + i, 0.]
     [3., 1 - i]]

operator_adjoint.shape
==> [2, 2]

operator_adjoint.log_abs_determinant()
==> - log(2)

x = ... Shape [2, 4] Tensor
operator_adjoint.matmul(x)
==> Shape [2, 4] Tensor, equal to operator.matmul(x, adjoint=True)
"
"tf.linalg.LinearOperatorBlockDiag(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=True,
    name=None
)
","[['Combines one or more ', 'LinearOperators', ' in to a Block Diagonal matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 4 x 4 linear operator combined of two 2 x 2 operators.
operator_1 = LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorFullMatrix([[1., 0.], [0., 1.]])
operator = LinearOperatorBlockDiag([operator_1, operator_2])

operator.to_dense()
==> [[1., 2., 0., 0.],
     [3., 4., 0., 0.],
     [0., 0., 1., 0.],
     [0., 0., 0., 1.]]

operator.shape
==> [4, 4]

operator.log_abs_determinant()
==> scalar Tensor

x1 = ... # Shape [2, 2] Tensor
x2 = ... # Shape [2, 2] Tensor
x = tf.concat([x1, x2], 0)  # Shape [2, 4] Tensor
operator.matmul(x)
==> tf.concat([operator_1.matmul(x1), operator_2.matmul(x2)])

# Create a 5 x 4 linear operator combining three blocks.
operator_1 = LinearOperatorFullMatrix([[1.], [3.]])
operator_2 = LinearOperatorFullMatrix([[1., 6.]])
operator_3 = LinearOperatorFullMatrix([[2.], [7.]])
operator = LinearOperatorBlockDiag([operator_1, operator_2, operator_3])

operator.to_dense()
==> [[1., 0., 0., 0.],
     [3., 0., 0., 0.],
     [0., 1., 6., 0.],
     [0., 0., 0., 2.]]
     [0., 0., 0., 7.]]

operator.shape
==> [5, 4]


# Create a [2, 3] batch of 4 x 4 linear operators.
matrix_44 = tf.random.normal(shape=[2, 3, 4, 4])
operator_44 = LinearOperatorFullMatrix(matrix)

# Create a [1, 3] batch of 5 x 5 linear operators.
matrix_55 = tf.random.normal(shape=[1, 3, 5, 5])
operator_55 = LinearOperatorFullMatrix(matrix_55)

# Combine to create a [2, 3] batch of 9 x 9 operators.
operator_99 = LinearOperatorBlockDiag([operator_44, operator_55])

# Create a shape [2, 3, 9] vector.
x = tf.random.normal(shape=[2, 3, 9])
operator_99.matmul(x)
==> Shape [2, 3, 9] Tensor

# Create a blockwise list of vectors.
x = [tf.random.normal(shape=[2, 3, 4]), tf.random.normal(shape=[2, 3, 5])]
operator_99.matmul(x)
==> [Shape [2, 3, 4] Tensor, Shape [2, 3, 5] Tensor]
"
"tf.linalg.LinearOperatorBlockLowerTriangular(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorBlockLowerTriangular'
)
","[['Combines ', 'LinearOperators', ' into a blockwise lower-triangular matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]",">>> operator_0 = tf.linalg.LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
>>> operator_1 = tf.linalg.LinearOperatorFullMatrix([[1., 0.], [0., 1.]])
>>> operator_2 = tf.linalg.LinearOperatorLowerTriangular([[5., 6.], [7., 8]])
>>> operator = LinearOperatorBlockLowerTriangular(
...   [[operator_0], [operator_1, operator_2]])
"
"tf.linalg.LinearOperatorCirculant(
    spectrum,
    input_output_dtype=tf.dtypes.complex64,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=True,
    name='LinearOperatorCirculant'
)
","[['LinearOperator', ' acting like a circulant matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","A = |w z y x|
    |x w z y|
    |y x w z|
    |z y x w|
"
"tf.linalg.LinearOperatorCirculant2D(
    spectrum,
    input_output_dtype=tf.dtypes.complex64,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=True,
    name='LinearOperatorCirculant2D'
)
","[['LinearOperator', ' acting like a block circulant matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","A = |W Z Y X|
    |X W Z Y|
    |Y X W Z|
    |Z Y X W|
"
"tf.linalg.LinearOperatorCirculant3D(
    spectrum,
    input_output_dtype=tf.dtypes.complex64,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=True,
    name='LinearOperatorCirculant3D'
)
","[['LinearOperator', ' acting like a nested block circulant matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","A = |W Z Y X|
    |X W Z Y|
    |Y X W Z|
    |Z Y X W|
"
"tf.linalg.LinearOperatorComposition(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","[['Composes one or more ', 'LinearOperators', '.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","op_composed(x) := op1(op2(...(opJ(x)...))
"
"tf.linalg.LinearOperatorDiag(
    diag,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorDiag'
)
","[['LinearOperator', ' acting like a [batch] square diagonal matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 diagonal linear operator.
diag = [1., -1.]
operator = LinearOperatorDiag(diag)

operator.to_dense()
==> [[1.,  0.]
     [0., -1.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
diag = tf.random.normal(shape=[2, 3, 4])
operator = LinearOperatorDiag(diag)

# Create a shape [2, 1, 4, 2] vector.  Note that this shape is compatible
# since the batch dimensions, [2, 1], are broadcast to
# operator.batch_shape = [2, 3].
y = tf.random.normal(shape=[2, 1, 4, 2])
x = operator.solve(y)
==> operator.matmul(x) = y
"
"tf.linalg.LinearOperatorFullMatrix(
    matrix,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorFullMatrix'
)
","[['LinearOperator', ' that wraps a [batch] matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 linear operator.
matrix = [[1., 2.], [3., 4.]]
operator = LinearOperatorFullMatrix(matrix)

operator.to_dense()
==> [[1., 2.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
matrix = tf.random.normal(shape=[2, 3, 4, 4])
operator = LinearOperatorFullMatrix(matrix)
"
"tf.linalg.LinearOperatorHouseholder(
    reflection_axis,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorHouseholder'
)
","[['LinearOperator', ' acting like a [batch] of Householder transformations.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 householder transform.
vec = [1 / np.sqrt(2), 1. / np.sqrt(2)]
operator = LinearOperatorHouseholder(vec)

operator.to_dense()
==> [[0.,  -1.]
     [-1., -0.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor
"
"tf.linalg.LinearOperatorIdentity(
    num_rows,
    batch_shape=None,
    dtype=None,
    is_non_singular=True,
    is_self_adjoint=True,
    is_positive_definite=True,
    is_square=True,
    assert_proper_shapes=False,
    name='LinearOperatorIdentity'
)
","[['LinearOperator', ' acting like a [batch] square identity matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 identity matrix.
operator = LinearOperatorIdentity(num_rows=2, dtype=tf.float32)

operator.to_dense()
==> [[1., 0.]
     [0., 1.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> 0.

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor, same as x.

y = tf.random.normal(shape=[3, 2, 4])
# Note that y.shape is compatible with operator.shape because operator.shape
# is broadcast to [3, 2, 2].
# This broadcast does NOT require copying data, since we can infer that y
# will be passed through without changing shape.  We are always able to infer
# this if the operator has no batch_shape.
x = operator.solve(y)
==> Shape [3, 2, 4] Tensor, same as y.

# Create a 2-batch of 2x2 identity matrices
operator = LinearOperatorIdentity(num_rows=2, batch_shape=[2])
operator.to_dense()
==> [[[1., 0.]
      [0., 1.]],
     [[1., 0.]
      [0., 1.]]]

# Here, even though the operator has a batch shape, the input is the same as
# the output, so x can be passed through without a copy.  The operator is able
# to detect that no broadcast is necessary because both x and the operator
# have statically defined shape.
x = ... Shape [2, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, same as x

# Here the operator and x have different batch_shape, and are broadcast.
# This requires a copy, since the output is different size than the input.
x = ... Shape [1, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, equal to [x, x]
"
"tf.linalg.LinearOperatorInversion(
    operator,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","[['LinearOperator', ' representing the inverse of another operator.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 linear operator.
operator = LinearOperatorFullMatrix([[1., 0.], [0., 2.]])
operator_inv = LinearOperatorInversion(operator)

operator_inv.to_dense()
==> [[1., 0.]
     [0., 0.5]]

operator_inv.shape
==> [2, 2]

operator_inv.log_abs_determinant()
==> - log(2)

x = ... Shape [2, 4] Tensor
operator_inv.matmul(x)
==> Shape [2, 4] Tensor, equal to operator.solve(x)
"
"tf.linalg.LinearOperatorKronecker(
    operators,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name=None
)
","[['Kronecker product between two ', 'LinearOperators', '.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 4 x 4 linear operator composed of two 2 x 2 operators.
operator_1 = LinearOperatorFullMatrix([[1., 2.], [3., 4.]])
operator_2 = LinearOperatorFullMatrix([[1., 0.], [2., 1.]])
operator = LinearOperatorKronecker([operator_1, operator_2])

operator.to_dense()
==> [[1., 0., 2., 0.],
     [2., 1., 4., 2.],
     [3., 0., 4., 0.],
     [6., 3., 8., 4.]]

operator.shape
==> [4, 4]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [4, 2] Tensor
operator.matmul(x)
==> Shape [4, 2] Tensor

# Create a [2, 3] batch of 4 x 5 linear operators.
matrix_45 = tf.random.normal(shape=[2, 3, 4, 5])
operator_45 = LinearOperatorFullMatrix(matrix)

# Create a [2, 3] batch of 5 x 6 linear operators.
matrix_56 = tf.random.normal(shape=[2, 3, 5, 6])
operator_56 = LinearOperatorFullMatrix(matrix_56)

# Compose to create a [2, 3] batch of 20 x 30 operators.
operator_large = LinearOperatorKronecker([operator_45, operator_56])

# Create a shape [2, 3, 20, 2] vector.
x = tf.random.normal(shape=[2, 3, 6, 2])
operator_large.matmul(x)
==> Shape [2, 3, 30, 2] Tensor
"
"tf.linalg.LinearOperatorLowRankUpdate(
    base_operator,
    u,
    diag_update=None,
    v=None,
    is_diag_update_positive=None,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorLowRankUpdate'
)
","[['Perturb a ', 'LinearOperator', ' with a rank ', 'K', ' update.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","L, is a LinearOperator representing [batch] M x N matrices
U, is a [batch] M x K matrix.  Typically K << M.
D, is a [batch] K x K matrix.
V, is a [batch] N x K matrix.  Typically K << N.
V^H is the Hermitian transpose (adjoint) of V.
"
"tf.linalg.LinearOperatorLowerTriangular(
    tril,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorLowerTriangular'
)
","[['LinearOperator', ' acting like a [batch] square lower triangular matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 lower-triangular linear operator.
tril = [[1., 2.], [3., 4.]]
operator = LinearOperatorLowerTriangular(tril)

# The upper triangle is ignored.
operator.to_dense()
==> [[1., 0.]
     [3., 4.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor

# Create a [2, 3] batch of 4 x 4 linear operators.
tril = tf.random.normal(shape=[2, 3, 4, 4])
operator = LinearOperatorLowerTriangular(tril)
"
"tf.linalg.LinearOperatorPermutation(
    perm,
    dtype=tf.dtypes.float32,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorPermutation'
)
","[['LinearOperator', ' acting like a [batch] of permutation matrices.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 3 x 3 permutation matrix that swaps the last two columns.
vec = [0, 2, 1]
operator = LinearOperatorPermutation(vec)

operator.to_dense()
==> [[1., 0., 0.]
     [0., 0., 1.]
     [0., 1., 0.]]

operator.shape
==> [3, 3]

# This will be zero.
operator.log_abs_determinant()
==> scalar Tensor

x = ... Shape [3, 4] Tensor
operator.matmul(x)
==> Shape [3, 4] Tensor
"
"tf.linalg.LinearOperatorScaledIdentity(
    num_rows,
    multiplier,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=True,
    assert_proper_shapes=False,
    name='LinearOperatorScaledIdentity'
)
","[['LinearOperator', ' acting like a scaled [batch] identity matrix ', 'A = c I', '.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 scaled identity matrix.
operator = LinearOperatorIdentity(num_rows=2, multiplier=3.)

operator.to_dense()
==> [[3., 0.]
     [0., 3.]]

operator.shape
==> [2, 2]

operator.log_abs_determinant()
==> 2 * Log[3]

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> 3 * x

y = tf.random.normal(shape=[3, 2, 4])
# Note that y.shape is compatible with operator.shape because operator.shape
# is broadcast to [3, 2, 2].
x = operator.solve(y)
==> 3 * x

# Create a 2-batch of 2x2 identity matrices
operator = LinearOperatorIdentity(num_rows=2, multiplier=5.)
operator.to_dense()
==> [[[5., 0.]
      [0., 5.]],
     [[5., 0.]
      [0., 5.]]]

x = ... Shape [2, 2, 3]
operator.matmul(x)
==> 5 * x

# Here the operator and x have different batch_shape, and are broadcast.
x = ... Shape [1, 2, 3]
operator.matmul(x)
==> 5 * x
"
"tf.linalg.LinearOperatorToeplitz(
    col,
    row,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorToeplitz'
)
","[['LinearOperator', ' acting like a [batch] of toeplitz matrices.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","A = |a b c d|
    |e a b c|
    |f e a b|
    |g f e a|
"
"tf.linalg.LinearOperatorTridiag(
    diagonals,
    diagonals_format=_COMPACT,
    is_non_singular=None,
    is_self_adjoint=None,
    is_positive_definite=None,
    is_square=None,
    name='LinearOperatorTridiag'
)
","[['LinearOperator', ' acting like a [batch] square tridiagonal matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","superdiag = [3., 4., 5.]
diag = [1., -1., 2.]
subdiag = [6., 7., 8]
operator = tf.linalg.LinearOperatorTridiag(
   [superdiag, diag, subdiag],
   diagonals_format='sequence')
operator.to_dense()
<tf.Tensor: shape=(3, 3), dtype=float32, numpy=
array([[ 1.,  3.,  0.],
       [ 7., -1.,  4.],
       [ 0.,  8.,  2.]], dtype=float32)>
operator.shape
TensorShape([3, 3])"
"tf.linalg.LinearOperatorZeros(
    num_rows,
    num_columns=None,
    batch_shape=None,
    dtype=None,
    is_non_singular=False,
    is_self_adjoint=True,
    is_positive_definite=False,
    is_square=True,
    assert_proper_shapes=False,
    name='LinearOperatorZeros'
)
","[['LinearOperator', ' acting like a [batch] zero matrix.'], ['Inherits From: ', 'LinearOperator', ', ', 'Module']]","# Create a 2 x 2 zero matrix.
operator = LinearOperatorZero(num_rows=2, dtype=tf.float32)

operator.to_dense()
==> [[0., 0.]
     [0., 0.]]

operator.shape
==> [2, 2]

operator.determinant()
==> 0.

x = ... Shape [2, 4] Tensor
operator.matmul(x)
==> Shape [2, 4] Tensor, same as x.

# Create a 2-batch of 2x2 zero matrices
operator = LinearOperatorZeros(num_rows=2, batch_shape=[2])
operator.to_dense()
==> [[[0., 0.]
      [0., 0.]],
     [[0., 0.]
      [0., 0.]]]

# Here, even though the operator has a batch shape, the input is the same as
# the output, so x can be passed through without a copy.  The operator is able
# to detect that no broadcast is necessary because both x and the operator
# have statically defined shape.
x = ... Shape [2, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, same as tf.zeros_like(x)

# Here the operator and x have different batch_shape, and are broadcast.
# This requires a copy, since the output is different size than the input.
x = ... Shape [1, 2, 3]
operator.matmul(x)
==> Shape [2, 2, 3] Tensor, equal to tf.zeros_like([x, x])
"
"tf.linalg.adjoint(
    matrix, name=None
)
","[['Transposes the last two dimensions of and conjugates tensor ', 'matrix', '.']]","x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],
                 [4 + 4j, 5 + 5j, 6 + 6j]])
tf.linalg.adjoint(x)  # [[1 - 1j, 4 - 4j],
                      #  [2 - 2j, 5 - 5j],
                      #  [3 - 3j, 6 - 6j]]
"
"tf.linalg.band_part(
    input, num_lower, num_upper, name=None
)
",[],"# if 'input' is [[ 0,  1,  2, 3]
#                [-1,  0,  1, 2]
#                [-2, -1,  0, 1]
#                [-3, -2, -1, 0]],

tf.linalg.band_part(input, 1, -1) ==> [[ 0,  1,  2, 3]
                                       [-1,  0,  1, 2]
                                       [ 0, -1,  0, 1]
                                       [ 0,  0, -1, 0]],

tf.linalg.band_part(input, 2, 1) ==> [[ 0,  1,  0, 0]
                                      [-1,  0,  1, 0]
                                      [-2, -1,  0, 1]
                                      [ 0, -2, -1, 0]]
"
"tf.linalg.cholesky(
    input, name=None
)
",[],[]
"tf.linalg.cholesky_solve(
    chol, rhs, name=None
)
","[['Solves systems of linear eqns ', 'A X = RHS', ', given Cholesky factorizations.']]","# Solve 10 separate 2x2 linear systems:
A = ... # shape 10 x 2 x 2
RHS = ... # shape 10 x 2 x 1
chol = tf.linalg.cholesky(A)  # shape 10 x 2 x 2
X = tf.linalg.cholesky_solve(chol, RHS)  # shape 10 x 2 x 1
# tf.matmul(A, X) ~ RHS
X[3, :, 0]  # Solution to the linear system A[3, :, :] x = RHS[3, :, 0]

# Solve five linear systems (K = 5) for every member of the length 10 batch.
A = ... # shape 10 x 2 x 2
RHS = ... # shape 10 x 2 x 5
...
X[3, :, 2]  # Solution to the linear system A[3, :, :] x = RHS[3, :, 2]
"
"tf.linalg.cross(
    a, b, name=None
)
",[],[]
"tf.linalg.det(
    input, name=None
)
",[],[]
"tf.linalg.diag(
    diagonal,
    name='diag',
    k=0,
    num_rows=-1,
    num_cols=-1,
    padding_value=0,
    align='RIGHT_LEFT'
)
",[],"output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper
    padding_value                             ; otherwise
"
"tf.linalg.diag_part(
    input,
    name='diag_part',
    k=0,
    padding_value=0,
    align='RIGHT_LEFT'
)
",[],"diagonal[i, j, ..., l, n]
  = input[i, j, ..., l, n+y, n+x] ; if 0 <= n+y < M and 0 <= n+x < N,
    padding_value                 ; otherwise.
"
"tf.linalg.eigh(
    tensor, name=None
)
",[],[]
"tf.linalg.eigh_tridiagonal(
    alpha,
    beta,
    eigvals_only=True,
    select='a',
    select_range=None,
    tol=None,
    name=None
)
",[],"import numpy
eigvals = tf.linalg.eigh_tridiagonal([0.0, 0.0, 0.0], [1.0, 1.0])
eigvals_expected = [-numpy.sqrt(2.0), 0.0, numpy.sqrt(2.0)]
tf.assert_near(eigvals_expected, eigvals)
# ==> True
"
"tf.linalg.eigvalsh(
    tensor, name=None
)
",[],[]
"tf.einsum(
    equation, *inputs, **kwargs
)
","[[None, '\n']]","C[i,k] = sum_j A[i,j] * B[j,k]
"
"tf.linalg.experimental.conjugate_gradient(
    operator,
    rhs,
    preconditioner=None,
    x=None,
    tol=1e-05,
    max_iter=20,
    name='conjugate_gradient'
)
","[[None, '\n']]",[]
"tf.linalg.expm(
    input, name=None
)
","[[None, '\n']]",[]
"tf.eye(
    num_rows,
    num_columns=None,
    batch_shape=None,
    dtype=tf.dtypes.float32,
    name=None
)
",[],"# Construct one identity matrix.
tf.eye(2)
==> [[1., 0.],
     [0., 1.]]

# Construct a batch of 3 identity matrices, each 2 x 2.
# batch_identity[i, :, :] is a 2 x 2 identity matrix, i = 0, 1, 2.
batch_identity = tf.eye(2, batch_shape=[3])

# Construct one 2 x 3 ""identity"" matrix
tf.eye(2, num_columns=3)
==> [[ 1.,  0.,  0.],
     [ 0.,  1.,  0.]]
"
"tf.linalg.global_norm(
    t_list, name=None
)
",[],[]
"tf.linalg.inv(
    input, adjoint=False, name=None
)
",[],[]
"tf.math.l2_normalize(
    x, axis=None, epsilon=1e-12, name=None, dim=None
)
","[['Normalizes along dimension ', 'axis', ' using an L2 norm. (deprecated arguments)']]","output = x / sqrt(max(sum(x**2), epsilon))
"
"tf.linalg.logdet(
    matrix, name=None
)
",[],"# Compute the determinant of a matrix while reducing the chance of over- or
underflow:
A = ... # shape 10 x 10
det = tf.exp(tf.linalg.logdet(A))  # scalar
"
"tf.linalg.logm(
    input, name=None
)
","[[None, '\n']]",[]
"tf.linalg.lstsq(
    matrix, rhs, l2_regularizer=0.0, fast=True, name=None
)
","[[None, '\n']]",[]
"tf.linalg.lu(
    input,
    output_idx_type=tf.dtypes.int32,
    name=None
)
",[],[]
"tf.linalg.lu_matrix_inverse(
    lower_upper, perm, validate_args=False, name=None
)
",[],"inv_X = tf.lu_matrix_inverse(*tf.linalg.lu(X))
tf.assert_near(tf.matrix_inverse(X), inv_X)
# ==> True
"
"tf.linalg.lu_reconstruct(
    lower_upper, perm, validate_args=False, name=None
)
",[],"import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp

x = [[[3., 4], [1, 2]],
     [[7., 8], [3, 4]]]
x_reconstructed = tf.linalg.lu_reconstruct(*tf.linalg.lu(x))
tf.assert_near(x, x_reconstructed)
# ==> True
"
"tf.linalg.lu_solve(
    lower_upper, perm, rhs, validate_args=False, name=None
)
","[['Solves systems of linear eqns ', 'A X = RHS', ', given LU factorizations.']]","import numpy as np
import tensorflow as tf
import tensorflow_probability as tfp

x = [[[1., 2],
      [3, 4]],
     [[7, 8],
      [3, 4]]]
inv_x = tf.linalg.lu_solve(*tf.linalg.lu(x), rhs=tf.eye(2))
tf.assert_near(tf.matrix_inverse(x), inv_x)
# ==> True
"
"tf.linalg.matmul(
    a,
    b,
    transpose_a=False,
    transpose_b=False,
    adjoint_a=False,
    adjoint_b=False,
    a_is_sparse=False,
    b_is_sparse=False,
    output_type=None,
    name=None
)
","[['Multiplies matrix ', 'a', ' by matrix ', 'b', ', producing ', 'a', ' * ', 'b', '.']]","a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
a  # 2-D tensor
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)>
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
b  # 2-D tensor
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 7,  8],
       [ 9, 10],
       [11, 12]], dtype=int32)>
c = tf.matmul(a, b)
c  # `a` * `b`
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 58,  64],
       [139, 154]], dtype=int32)>"
"tf.linalg.matrix_rank(
    a, tol=None, validate_args=False, name=None
)
",[],[]
"tf.linalg.matrix_transpose(
    a, name='matrix_transpose', conjugate=False
)
","[['Transposes last two dimensions of tensor ', 'a', '.']]","x = tf.constant([[1, 2, 3], [4, 5, 6]])
tf.linalg.matrix_transpose(x)  # [[1, 4],
                               #  [2, 5],
                               #  [3, 6]]

x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],
                 [4 + 4j, 5 + 5j, 6 + 6j]])
tf.linalg.matrix_transpose(x, conjugate=True)  # [[1 - 1j, 4 - 4j],
                                               #  [2 - 2j, 5 - 5j],
                                               #  [3 - 3j, 6 - 6j]]

# Matrix with two batch dimensions.
# x.shape is [1, 2, 3, 4]
# tf.linalg.matrix_transpose(x) is shape [1, 2, 4, 3]
"
"tf.linalg.matvec(
    a,
    b,
    transpose_a=False,
    adjoint_a=False,
    a_is_sparse=False,
    b_is_sparse=False,
    name=None
)
","[['Multiplies matrix ', 'a', ' by vector ', 'b', ', producing ', 'a', ' * ', 'b', '.']]","# 2-D tensor `a`
# [[1, 2, 3],
#  [4, 5, 6]]
a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])

# 1-D tensor `b`
# [7, 9, 11]
b = tf.constant([7, 9, 11], shape=[3])

# `a` * `b`
# [ 58,  64]
c = tf.linalg.matvec(a, b)


# 3-D tensor `a`
# [[[ 1,  2,  3],
#   [ 4,  5,  6]],
#  [[ 7,  8,  9],
#   [10, 11, 12]]]
a = tf.constant(np.arange(1, 13, dtype=np.int32),
                shape=[2, 2, 3])

# 2-D tensor `b`
# [[13, 14, 15],
#  [16, 17, 18]]
b = tf.constant(np.arange(13, 19, dtype=np.int32),
                shape=[2, 3])

# `a` * `b`
# [[ 86, 212],
#  [410, 563]]
c = tf.linalg.matvec(a, b)
"
"tf.compat.v1.norm(
    tensor,
    ord='euclidean',
    axis=None,
    keepdims=None,
    name=None,
    keep_dims=None
)
",[],[]
"tf.linalg.normalize(
    tensor, ord='euclidean', axis=None, name=None
)
","[['Normalizes ', 'tensor', ' along dimension ', 'axis', ' using specified norm.']]",[]
"tf.linalg.pinv(
    a, rcond=None, validate_args=False, name=None
)
",[],"import tensorflow as tf
import tensorflow_probability as tfp

a = tf.constant([[1.,  0.4,  0.5],
                 [0.4, 0.2,  0.25],
                 [0.5, 0.25, 0.35]])
tf.matmul(tf.linalg.pinv(a), a)
# ==> array([[1., 0., 0.],
             [0., 1., 0.],
             [0., 0., 1.]], dtype=float32)

a = tf.constant([[1.,  0.4,  0.5,  1.],
                 [0.4, 0.2,  0.25, 2.],
                 [0.5, 0.25, 0.35, 3.]])
tf.matmul(tf.linalg.pinv(a), a)
# ==> array([[ 0.76,  0.37,  0.21, -0.02],
             [ 0.37,  0.43, -0.33,  0.02],
             [ 0.21, -0.33,  0.81,  0.01],
             [-0.02,  0.02,  0.01,  1.  ]], dtype=float32)
"
"tf.linalg.qr(
    input, full_matrices=False, name=None
)
",[],"# a is a tensor.
# q is a tensor of orthonormal matrices.
# r is a tensor of upper triangular matrices.
q, r = qr(a)
q_full, r_full = qr(a, full_matrices=True)
"
"tf.linalg.set_diag(
    input,
    diagonal,
    name='set_diag',
    k=0,
    align='RIGHT_LEFT'
)
",[],"output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]
    input[i, j, ..., l, m, n]              ; otherwise
"
"tf.linalg.slogdet(
    input, name=None
)
",[],[]
"tf.linalg.solve(
    matrix, rhs, adjoint=False, name=None
)
",[],[]
"tf.linalg.sqrtm(
    input, name=None
)
",[],[]
"tf.linalg.svd(
    tensor, full_matrices=False, compute_uv=True, name=None
)
","[[None, '\n']]","# a is a tensor.
# s is a tensor of singular values.
# u is a tensor of left singular vectors.
# v is a tensor of right singular vectors.
s, u, v = svd(a)
s = svd(a, compute_uv=False)
"
"tf.linalg.tensor_diag(
    diagonal, name=None
)
",[],"# 'diagonal' is [1, 2, 3, 4]
tf.diag(diagonal) ==> [[1, 0, 0, 0]
                       [0, 2, 0, 0]
                       [0, 0, 3, 0]
                       [0, 0, 0, 4]]
"
"tf.linalg.tensor_diag_part(
    input, name=None
)
",[],"x = [[[[1111,1112],[1121,1122]],
      [[1211,1212],[1221,1222]]],
     [[[2111, 2112], [2121, 2122]],
      [[2211, 2212], [2221, 2222]]]
     ]
tf.linalg.tensor_diag_part(x)
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[1111, 1212],
       [2121, 2222]], dtype=int32)>
tf.linalg.diag_part(x).shape
TensorShape([2, 2, 2])"
"tf.tensordot(
    a, b, axes, name=None
)
","[[None, '\n']]",[]
"tf.linalg.trace(
    x, name=None
)
","[['Compute the trace of a tensor ', 'x', '.']]","x = tf.constant([[1, 2], [3, 4]])
tf.linalg.trace(x)  # 5

x = tf.constant([[1, 2, 3],
                 [4, 5, 6],
                 [7, 8, 9]])
tf.linalg.trace(x)  # 15

x = tf.constant([[[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9]],
                 [[-1, -2, -3],
                  [-4, -5, -6],
                  [-7, -8, -9]]])
tf.linalg.trace(x)  # [15, -15]
"
"tf.linalg.matrix_transpose(
    a, name='matrix_transpose', conjugate=False
)
","[['Transposes last two dimensions of tensor ', 'a', '.']]","x = tf.constant([[1, 2, 3], [4, 5, 6]])
tf.linalg.matrix_transpose(x)  # [[1, 4],
                               #  [2, 5],
                               #  [3, 6]]

x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],
                 [4 + 4j, 5 + 5j, 6 + 6j]])
tf.linalg.matrix_transpose(x, conjugate=True)  # [[1 - 1j, 4 - 4j],
                                               #  [2 - 2j, 5 - 5j],
                                               #  [3 - 3j, 6 - 6j]]

# Matrix with two batch dimensions.
# x.shape is [1, 2, 3, 4]
# tf.linalg.matrix_transpose(x) is shape [1, 2, 4, 3]
"
"tf.linalg.triangular_solve(
    matrix, rhs, lower=True, adjoint=False, name=None
)
",[],"a = tf.constant([[3,  0,  0,  0],
  [2,  1,  0,  0],
  [1,  0,  1,  0],
  [1,  1,  1,  1]], dtype=tf.float32)"
"tf.linalg.tridiagonal_matmul(
    diagonals, rhs, diagonals_format='compact', name=None
)
",[],"superdiag = tf.constant([-1, -1, 0], dtype=tf.float64)
maindiag = tf.constant([2, 2, 2], dtype=tf.float64)
subdiag = tf.constant([0, -1, -1], dtype=tf.float64)
diagonals = [superdiag, maindiag, subdiag]
rhs = tf.constant([[1, 1], [1, 1], [1, 1]], dtype=tf.float64)
x = tf.linalg.tridiagonal_matmul(diagonals, rhs, diagonals_format='sequence')
"
"tf.linalg.tridiagonal_solve(
    diagonals,
    rhs,
    diagonals_format='compact',
    transpose_rhs=False,
    conjugate_rhs=False,
    name=None,
    partial_pivoting=True,
    perturb_singular=False
)
",[],"rhs = tf.constant([...])
matrix = tf.constant([[...]])
m = matrix.shape[0]
dummy_idx = [0, 0]  # An arbitrary element to use as a dummy
indices = [[[i, i + 1] for i in range(m - 1)] + [dummy_idx],  # Superdiagonal
         [[i, i] for i in range(m)],                          # Diagonal
         [dummy_idx] + [[i + 1, i] for i in range(m - 1)]]    # Subdiagonal
diagonals=tf.gather_nd(matrix, indices)
x = tf.linalg.tridiagonal_solve(diagonals, rhs)
"
"tf.linspace(
    start, stop, num, name=None, axis=0
)
",[],"tf.linspace(10.0, 12.0, 3, name=""linspace"") => [ 10.0  11.0  12.0]
"
"tf.lite.Interpreter(
    model_path=None,
    model_content=None,
    experimental_delegates=None,
    num_threads=None,
    experimental_op_resolver_type=tf.lite.experimental.OpResolverType.AUTO,
    experimental_preserve_all_tensors=False
)
",[],"x = np.array([[1.], [2.]])
y = np.array([[2.], [4.]])
model = tf.keras.models.Sequential([
          tf.keras.layers.Dropout(0.2),
          tf.keras.layers.Dense(units=1, input_shape=[1])
        ])
model.compile(optimizer='sgd', loss='mean_squared_error')
model.fit(x, y, epochs=1)
converter = tf.lite.TFLiteConverter.from_keras_model(model)
tflite_model = converter.convert()"
"tf.compat.v1.lite.OpHint(
    function_name, level=1, children_inputs_mappings=None, **kwargs
)
",[],"add_input(
    *args, **kwargs
)
"
"tf.compat.v1.lite.OpHint.OpHintArgumentTracker(
    function_name,
    unique_function_id,
    node_name_prefix,
    attr_name,
    level=1,
    children_inputs_mappings=None
)
",[],"add(
    arg, tag=None, name=None, aggregate=None, index_override=None
)
"
"tf.lite.RepresentativeDataset(
    input_gen
)
",[],[]
"tf.compat.v1.lite.TFLiteConverter(
    graph_def,
    input_tensors,
    output_tensors,
    input_arrays_with_shape=None,
    output_arrays=None,
    experimental_debug_info_func=None
)
","[['Convert a TensorFlow model into ', 'output_format', '.']]","# Converting a GraphDef from session.
converter = tf.compat.v1.lite.TFLiteConverter.from_session(
  sess, in_tensors, out_tensors)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)

# Converting a GraphDef from file.
converter = tf.compat.v1.lite.TFLiteConverter.from_frozen_graph(
  graph_def_file, input_arrays, output_arrays)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)

# Converting a SavedModel.
converter = tf.compat.v1.lite.TFLiteConverter.from_saved_model(
    saved_model_dir)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)

# Converting a tf.keras model.
converter = tf.compat.v1.lite.TFLiteConverter.from_keras_model_file(
    keras_model)
tflite_model = converter.convert()
open(""converted_model.tflite"", ""wb"").write(tflite_model)
"
"tf.lite.TargetSpec(
    supported_ops=None,
    supported_types=None,
    experimental_select_user_tf_ops=None,
    experimental_supported_backends=None
)
",[],[]
"tf.lite.experimental.QuantizationDebugOptions(
    layer_debug_metrics: Optional[Mapping[str, Callable[[np.ndarray], float]]] = None,
    model_debug_metrics: Optional[Mapping[str, Callable[[Sequence[np.ndarray], Sequence[np.ndarray]],
        float]]] = None,
    layer_direct_compare_metrics: Optional[Mapping[str, Callable[[Sequence[np.ndarray], Sequence[np.ndarray],
        float, int], float]]] = None,
    denylisted_ops: Optional[List[str]] = None,
    denylisted_nodes: Optional[List[str]] = None,
    fully_quantize: bool = False
) -> None
",[],[]
"tf.lite.experimental.QuantizationDebugger(
    quant_debug_model_path: Optional[str] = None,
    quant_debug_model_content: Optional[bytes] = None,
    float_model_path: Optional[str] = None,
    float_model_content: Optional[bytes] = None,
    debug_dataset: Optional[Callable[[], Iterable[Sequence[np.ndarray]]]] = None,
    debug_options: Optional[tf.lite.experimental.QuantizationDebugOptions] = None,
    converter: Optional[TFLiteConverter] = None
) -> None
",[],"get_debug_quantized_model() -> bytes
"
"tf.lite.experimental.authoring.compatible(
    target=None, converter_target_spec=None, **kwargs
)
","[['Wraps ', 'tf.function', ' into a callable function with TFLite compatibility checking.']]","@tf.lite.experimental.authoring.compatible
@tf.function(input_signature=[
    tf.TensorSpec(shape=[None], dtype=tf.float32)
])
def f(x):
    return tf.cosh(x)

result = f(tf.constant([0.0]))
# COMPATIBILITY WARNING: op 'tf.Cosh' require(s) ""Select TF Ops"" for model
# conversion for TensorFlow Lite.
# Op: tf.Cosh
#   - tensorflow/python/framework/op_def_library.py:748
#   - tensorflow/python/ops/gen_math_ops.py:2458
#   - <stdin>:6
"
"tf.compat.v1.lite.experimental.convert_op_hints_to_stubs(
    session=None,
    graph_def=None,
    write_callback=(lambda graph_def, comments: None)
)
",[],[]
"tf.lite.experimental.load_delegate(
    library, options=None
)
",[],"import tensorflow as tf

try:
  delegate = tf.lite.experimental.load_delegate('delegate.so')
except ValueError:
  // Fallback to CPU

if delegate:
  interpreter = tf.lite.Interpreter(
      model_path='model.tflite',
      experimental_delegates=[delegate])
else:
  interpreter = tf.lite.Interpreter(model_path='model.tflite')
"
"tf.compat.v1.lite.toco_convert(
    input_data, input_tensors, output_tensors, *args, **kwargs
)
",[],[]
"tf.compat.v1.load_file_system_library(
    library_filename
)
",[],[]
"tf.load_library(
    library_location
)
",[],[]
"tf.load_op_library(
    library_filename
)
",[],[]
"tf.compat.v1.local_variables(
    scope=None
)
",[],[]
"tf.math.log(
    x, name=None
)
","[[None, '\n']]","x = tf.constant([0, 0.5, 1, 5])
tf.math.log(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([      -inf, -0.6931472,  0.       ,  1.609438 ], dtype=float32)>"
"tf.math.log1p(
    x, name=None
)
","[[None, '\n']]","x = tf.constant([0, 0.5, 1, 5])
tf.math.log1p(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.       , 0.4054651, 0.6931472, 1.7917595], dtype=float32)>"
"tf.math.log_sigmoid(
    x, name=None
)
","[['Computes log sigmoid of ', 'x', ' element-wise.']]","x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.log_sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=
array([-6.9314718e-01, -3.1326169e-01, -1.9287499e-22, -0.0000000e+00],
      dtype=float32)>"
"tf.compat.v1.logging.TaskLevelStatusMessage(
    msg
)
",[],[]
"tf.compat.v1.logging.debug(
    msg, *args, **kwargs
)
",[],[]
"tf.compat.v1.logging.error(
    msg, *args, **kwargs
)
",[],[]
"tf.compat.v1.logging.fatal(
    msg, *args, **kwargs
)
",[],[]
"tf.compat.v1.logging.info(
    msg, *args, **kwargs
)
",[],[]
"tf.compat.v1.logging.log(
    level, msg, *args, **kwargs
)
",[],[]
"tf.compat.v1.logging.log_every_n(
    level, msg, n, *args
)
",[],[]
"tf.compat.v1.logging.log_first_n(
    level, msg, n, *args
)
",[],[]
"tf.compat.v1.logging.log_if(
    level, msg, condition, *args
)
",[],[]
"tf.compat.v1.logging.set_verbosity(
    v
)
",[],[]
"tf.compat.v1.logging.vlog(
    level, msg, *args, **kwargs
)
",[],[]
"tf.compat.v1.logging.warn(
    msg, *args, **kwargs
)
",[],[]
"tf.compat.v1.logging.warning(
    msg, *args, **kwargs
)
",[],[]
"tf.math.logical_and(
    x, y, name=None
)
",[],">>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_and(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
>>> a & b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
"
"tf.math.logical_not(
    x, name=None
)
","[['Returns the truth value of ', 'NOT x', ' element-wise.']]","tf.math.logical_not(tf.constant([True, False]))
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.math.logical_or(
    x, y, name=None
)
",[],">>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_or(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
>>> a | b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
"
"tf.math.logical_xor(
    x, y, name='LogicalXor'
)
",[],"a = tf.constant([True])
b = tf.constant([False])
tf.math.logical_xor(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>"
"tf.lookup.KeyValueTensorInitializer(
    keys, values, key_dtype=None, value_dtype=None, name=None
)
","[['Table initializers given ', 'keys', ' and ', 'values', ' tensors.']]","keys_tensor = tf.constant(['a', 'b', 'c'])
vals_tensor = tf.constant([7, 8, 9])
input_tensor = tf.constant(['a', 'f'])
init = tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor)
table = tf.lookup.StaticHashTable(
    init,
    default_value=-1)
table.lookup(input_tensor).numpy()
array([ 7, -1], dtype=int32)"
"tf.compat.v1.lookup.StaticHashTable(
    initializer, default_value, name=None, experimental_is_anonymous=False
)
","[['Inherits From: ', 'StaticHashTable', ', ', 'TrackableResource']]","keys_tensor = tf.constant([1, 2])
vals_tensor = tf.constant([3, 4])
input_tensor = tf.constant([1, 5])
table = tf.lookup.StaticHashTable(
    tf.lookup.KeyValueTensorInitializer(keys_tensor, vals_tensor), -1)
out = table.lookup(input_tensor)
with tf.Session() as sess:
    sess.run(tf.tables_initializer())
    print(sess.run(out))
"
"tf.compat.v1.lookup.StaticVocabularyTable(
    initializer,
    num_oov_buckets,
    lookup_key_dtype=None,
    name=None,
    experimental_is_anonymous=False
)
","[['Inherits From: ', 'StaticVocabularyTable', ', ', 'TrackableResource']]","init = tf.lookup.KeyValueTensorInitializer(
    keys=tf.constant(['emerson', 'lake', 'palmer']),
    values=tf.constant([0, 1, 2], dtype=tf.int64))
table = tf.lookup.StaticVocabularyTable(
   init,
   num_oov_buckets=5)"
"tf.lookup.TextFileInitializer(
    filename,
    key_dtype,
    key_index,
    value_dtype,
    value_index,
    vocab_size=None,
    delimiter='\t',
    name=None,
    value_index_offset=0
)
",[],"import tempfile
f = tempfile.NamedTemporaryFile(delete=False)
content='\n'.join([""emerson 10"", ""lake 20"", ""palmer 30"",])
f.file.write(content.encode('utf-8'))
f.file.close()"
"tf.lookup.experimental.DenseHashTable(
    key_dtype,
    value_dtype,
    default_value,
    empty_key,
    deleted_key,
    initial_num_buckets=None,
    name='MutableDenseHashTable',
    checkpoint=True,
    experimental_is_anonymous=False
)
","[[None, '\n'], ['Inherits From: ', 'TrackableResource']]","table = tf.lookup.experimental.DenseHashTable(
    key_dtype=tf.string,
    value_dtype=tf.int64,
    default_value=-1,
    empty_key='',
    deleted_key='$')
keys = tf.constant(['a', 'b', 'c'])
values = tf.constant([0, 1, 2], dtype=tf.int64)
table.insert(keys, values)
table.remove(tf.constant(['c']))
table.lookup(tf.constant(['a', 'b', 'c','d'])).numpy()
array([ 0,  1, -1, -1])"
"tf.lookup.experimental.MutableHashTable(
    key_dtype,
    value_dtype,
    default_value,
    name='MutableHashTable',
    checkpoint=True,
    experimental_is_anonymous=False
)
","[['Inherits From: ', 'TrackableResource']]","table = tf.lookup.experimental.MutableHashTable(key_dtype=tf.string,
                                                value_dtype=tf.int64,
                                                default_value=-1)
keys_tensor = tf.constant(['a', 'b', 'c'])
vals_tensor = tf.constant([7, 8, 9], dtype=tf.int64)
input_tensor = tf.constant(['a', 'f'])
table.insert(keys_tensor, vals_tensor)
table.lookup(input_tensor).numpy()
array([ 7, -1])
table.remove(tf.constant(['c']))
table.lookup(keys_tensor).numpy()
array([ 7, 8, -1])
sorted(table.export()[0].numpy())
[b'a', b'b']
sorted(table.export()[1].numpy())
[7, 8]"
"tf.compat.v1.losses.absolute_difference(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=ops.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
",[],[]
"tf.compat.v1.losses.add_loss(
    loss, loss_collection=ops.GraphKeys.LOSSES
)
",[],[]
"tf.compat.v1.losses.compute_weighted_loss(
    losses,
    weights=1.0,
    scope=None,
    loss_collection=ops.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
",[],[]
"tf.compat.v1.losses.cosine_distance(
    labels,
    predictions,
    axis=None,
    weights=1.0,
    scope=None,
    loss_collection=ops.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS,
    dim=None
)
",[],[]
"tf.compat.v1.losses.get_losses(
    scope=None, loss_collection=ops.GraphKeys.LOSSES
)
",[],[]
"tf.compat.v1.losses.get_regularization_loss(
    scope=None, name='total_regularization_loss'
)
",[],[]
"tf.compat.v1.losses.get_regularization_losses(
    scope=None
)
",[],[]
"tf.compat.v1.losses.get_total_loss(
    add_regularization_losses=True, name='total_loss', scope=None
)
",[],[]
"tf.compat.v1.losses.hinge_loss(
    labels,
    logits,
    weights=1.0,
    scope=None,
    loss_collection=ops.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
",[],[]
"tf.compat.v1.losses.huber_loss(
    labels,
    predictions,
    weights=1.0,
    delta=1.0,
    scope=None,
    loss_collection=ops.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
","[['Adds a ', 'Huber Loss', ' term to the training procedure.']]","  0.5 * x^2                  if |x| <= d
  0.5 * d^2 + d * (|x| - d)  if |x| > d
"
"tf.compat.v1.losses.log_loss(
    labels,
    predictions,
    weights=1.0,
    epsilon=1e-07,
    scope=None,
    loss_collection=ops.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
",[],[]
"tf.compat.v1.losses.mean_pairwise_squared_error(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=ops.GraphKeys.LOSSES
)
",[],[]
"tf.compat.v1.losses.mean_squared_error(
    labels,
    predictions,
    weights=1.0,
    scope=None,
    loss_collection=ops.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
",[],"loss = tf.compat.v1.losses.mean_squared_error(
  labels=labels,
  predictions=predictions,
  weights=weights,
  reduction=reduction)
"
"tf.compat.v1.losses.sigmoid_cross_entropy(
    multi_class_labels,
    logits,
    weights=1.0,
    label_smoothing=0,
    scope=None,
    loss_collection=ops.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
",[],"new_multiclass_labels = multiclass_labels * (1 - label_smoothing)

                        + 0.5 * label_smoothing
"
"tf.compat.v1.losses.softmax_cross_entropy(
    onehot_labels,
    logits,
    weights=1.0,
    label_smoothing=0,
    scope=None,
    loss_collection=ops.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
",[],"loss = tf.compat.v1.losses.softmax_cross_entropy(
  onehot_labels=onehot_labels,
  logits=logits,
  weights=weights,
  label_smoothing=smoothing)
"
"tf.compat.v1.losses.sparse_softmax_cross_entropy(
    labels,
    logits,
    weights=1.0,
    scope=None,
    loss_collection=ops.GraphKeys.LOSSES,
    reduction=Reduction.SUM_BY_NONZERO_WEIGHTS
)
","[['Cross-entropy loss using ', 'tf.nn.sparse_softmax_cross_entropy_with_logits', '.']]",[]
"tf.make_ndarray(
    tensor
)
",[],"# Tensor a has shape (2,3)
a = tf.constant([[1,2,3],[4,5,6]])
proto_tensor = tf.make_tensor_proto(a)  # convert `tensor a` to a proto tensor
tf.make_ndarray(proto_tensor) # output: array([[1, 2, 3],
#                                              [4, 5, 6]], dtype=int32)
# output has shape (2,3)
"
"tf.compat.v1.make_template(
    name_,
    func_,
    create_scope_now_=False,
    unique_name_=None,
    custom_getter_=None,
    **kwargs
)
",[],[]
"tf.make_tensor_proto(
    values, dtype=None, shape=None, verify_shape=False, allow_broadcast=False
)
",[],"  request = tensorflow_serving.apis.predict_pb2.PredictRequest()
  request.model_spec.name = ""my_model""
  request.model_spec.signature_name = ""serving_default""
  request.inputs[""images""].CopyFrom(tf.make_tensor_proto(X_new))
"
"tf.compat.v1.batch_to_space_nd(
    input, block_shape, crops, name=None
)
",[],"[[[[1]]], [[[2]]], [[[3]]], [[[4]]]]
"
"tf.compat.v1.gather_nd(
    params, indices, name=None, batch_dims=0
)
","[['Gather slices from ', 'params', ' into a Tensor with shape specified by ', 'indices', '.']]","tf.gather_nd(
    indices=[[0, 0],
             [1, 1]],
    params = [['a', 'b'],
              ['c', 'd']]).numpy()
array([b'a', b'd'], dtype=object)"
"tf.reshape(
    tensor, shape, name=None
)
",[],"t1 = [[1, 2, 3],
      [4, 5, 6]]
print(tf.shape(t1).numpy())
[2 3]
t2 = tf.reshape(t1, [6])
t2
<tf.Tensor: shape=(6,), dtype=int32,
  numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>
tf.reshape(t2, [3, 2])
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
  array([[1, 2],
         [3, 4],
         [5, 6]], dtype=int32)>"
"tf.reverse(
    tensor, axis, name=None
)
",[],"# tensor 't' is [[[[ 0,  1,  2,  3],
#                  [ 4,  5,  6,  7],
#                  [ 8,  9, 10, 11]],
#                 [[12, 13, 14, 15],
#                  [16, 17, 18, 19],
#                  [20, 21, 22, 23]]]]
# tensor 't' shape is [1, 2, 3, 4]

# 'dims' is [3] or 'dims' is [-1]
reverse(t, dims) ==> [[[[ 3,  2,  1,  0],
                        [ 7,  6,  5,  4],
                        [ 11, 10, 9, 8]],
                       [[15, 14, 13, 12],
                        [19, 18, 17, 16],
                        [23, 22, 21, 20]]]]

# 'dims' is '[1]' (or 'dims' is '[-3]')
reverse(t, dims) ==> [[[[12, 13, 14, 15],
                        [16, 17, 18, 19],
                        [20, 21, 22, 23]
                       [[ 0,  1,  2,  3],
                        [ 4,  5,  6,  7],
                        [ 8,  9, 10, 11]]]]

# 'dims' is '[2]' (or 'dims' is '[-2]')
reverse(t, dims) ==> [[[[8, 9, 10, 11],
                        [4, 5, 6, 7],
                        [0, 1, 2, 3]]
                       [[20, 21, 22, 23],
                        [16, 17, 18, 19],
                        [12, 13, 14, 15]]]]
"
"tf.roll(
    input, shift, axis, name=None
)
",[],"# 't' is [0, 1, 2, 3, 4]
roll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]

# shifting along multiple dimensions
# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
roll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]

# shifting along the same axis multiple times
# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
roll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]
"
"tf.scatter_nd(
    indices, updates, shape, name=None
)
","[['Scatters ', 'updates', ' into a tensor of shape ', 'shape', ' according to ', 'indices', '.']]","indices.shape[-1] <= shape.rank
"
"tf.space_to_batch_nd(
    input, block_shape, paddings, name=None
)
",[],"x = [[[[1], [2]], [[3], [4]]]]
"
"tf.tile(
    input, multiples, name=None
)
",[],"a = tf.constant([[1,2,3],[4,5,6]], tf.int32)
b = tf.constant([1,2], tf.int32)
tf.tile(a, b)
<tf.Tensor: shape=(2, 6), dtype=int32, numpy=
array([[1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6]], dtype=int32)>
c = tf.constant([2,1], tf.int32)
tf.tile(a, c)
<tf.Tensor: shape=(4, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6],
       [1, 2, 3],
       [4, 5, 6]], dtype=int32)>
d = tf.constant([2,2], tf.int32)
tf.tile(a, d)
<tf.Tensor: shape=(4, 6), dtype=int32, numpy=
array([[1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6],
       [1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6]], dtype=int32)>"
"tf.compat.v1.map_fn(
    fn,
    elems,
    dtype=None,
    parallel_iterations=None,
    back_prop=True,
    swap_memory=False,
    infer_shape=True,
    name=None,
    fn_output_signature=None
)
","[['Transforms ', 'elems', ' by applying ', 'fn', ' to each element unstacked on axis 0. (deprecated arguments)']]","tf.map_fn(fn=lambda t: tf.range(t, t + 3), elems=tf.constant([3, 5, 2]))
<tf.Tensor: shape=(3, 3), dtype=int32, numpy=
  array([[3, 4, 5],
         [5, 6, 7],
         [2, 3, 4]], dtype=int32)>"
"tf.io.matching_files(
    pattern, name=None
)
",[],[]
"tf.math.abs(
    x, name=None
)
","[[None, '\n']]","# real number
x = tf.constant([-2.25, 3.25])
tf.abs(x)
<tf.Tensor: shape=(2,), dtype=float32,
numpy=array([2.25, 3.25], dtype=float32)>"
"tf.math.accumulate_n(
    inputs, shape=None, tensor_dtype=None, name=None
)
",[],"a = tf.constant([[1, 2], [3, 4]])
b = tf.constant([[5, 0], [0, 6]])
tf.math.accumulate_n([a, b, a]).numpy()
array([[ 7, 4],
       [ 6, 14]], dtype=int32)"
"tf.math.acos(
    x, name=None
)
",[],"x = tf.constant([1.0, -0.5, 3.4, 0.2, 0.0, -2], dtype = tf.float32)
tf.math.acos(x)
<tf.Tensor: shape=(6,), dtype=float32,
numpy= array([0. , 2.0943952, nan, 1.3694383, 1.5707964, nan],
dtype=float32)>"
"tf.math.acosh(
    x, name=None
)
",[],"x = tf.constant([-2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
tf.math.acosh(x) ==> [nan nan 0. 0.62236255 5.9914584 9.903487 inf]
"
"tf.math.add(
    x, y, name=None
)
",[],"x = [1, 2, 3, 4, 5]
y = 1
tf.add(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([2, 3, 4, 5, 6],
dtype=int32)>"
"tf.math.add_n(
    inputs, name=None
)
",[],"a = tf.constant([[3, 5], [4, 8]])
b = tf.constant([[1, 6], [2, 9]])
tf.math.add_n([a, b, a]).numpy()
array([[ 7, 16],
       [10, 25]], dtype=int32)"
"tf.math.angle(
    input, name=None
)
","[[None, '\n']]","input = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j], dtype=tf.complex64)
tf.math.angle(input).numpy()
# ==> array([2.0131705, 1.056345 ], dtype=float32)
"
"tf.math.approx_max_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","[['Returns max ', 'k', ' values and their indices of the input ', 'operand', ' in an approximate manner.']]","import tensorflow as tf
@tf.function(jit_compile=True)
def mips(qy, db, k=10, recall_target=0.95):
  dists = tf.einsum('ik,jk->ij', qy, db)
  # returns (f32[qy_size, k], i32[qy_size, k])
  return tf.nn.approx_max_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
dot_products, neighbors = mips(qy, db, k=20)"
"tf.math.approx_min_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","[['Returns min ', 'k', ' values and their indices of the input ', 'operand', ' in an approximate manner.']]","import tensorflow as tf
@tf.function(jit_compile=True)
def l2_ann(qy, db, half_db_norms, k=10, recall_target=0.95):
  dists = half_db_norms - tf.einsum('ik,jk->ij', qy, db)
  return tf.nn.approx_min_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
half_db_norms = tf.norm(db, axis=1) / 2
dists, neighbors = l2_ann(qy, db, half_db_norms)"
"tf.compat.v1.argmax(
    input,
    axis=None,
    name=None,
    dimension=None,
    output_type=tf.dtypes.int64
)
",[],"import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmax(input = a)
c = tf.keras.backend.eval(b)
# c = 4
# here a[4] = 166.32 which is the largest element of a across axis 0
"
"tf.compat.v1.argmin(
    input,
    axis=None,
    name=None,
    dimension=None,
    output_type=tf.dtypes.int64
)
",[],"import tensorflow as tf
a = [1, 10, 26.9, 2.8, 166.32, 62.3]
b = tf.math.argmin(input = a)
c = tf.keras.backend.eval(b)
# c = 0
# here a[0] = 1 which is the smallest element of a across axis 0
"
"tf.math.asin(
    x, name=None
)
",[],"# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]
x = tf.constant([1.047, 0.785])
y = tf.math.sin(x) # [0.8659266, 0.7068252]

tf.math.asin(y) # [1.047, 0.785] = x
"
"tf.math.asinh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -2, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.asinh(x) ==> [-inf -1.4436355 -0.4812118 0.8813736 1.0159732 5.991471 9.903487 inf]
"
"tf.math.atan(
    x, name=None
)
",[],"# Note: [1.047, 0.785] ~= [(pi/3), (pi/4)]
x = tf.constant([1.047, 0.785])
y = tf.math.tan(x) # [1.731261, 0.99920404]

tf.math.atan(y) # [1.047, 0.785] = x
"
"tf.math.atan2(
    y, x, name=None
)
","[[None, '\n'], ['Computes arctangent of ', 'y/x', ' element-wise, respecting signs of the arguments.']]","x = [1., 1.]
y = [1., -1.]
print((tf.math.atan2(y,x) * (180 / np.pi)).numpy())
[ 45. -45.]"
"tf.math.atanh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -1, -0.5, 1, 0, 0.5, 10, float(""inf"")])
  tf.math.atanh(x) ==> [nan -inf -0.54930615 inf  0. 0.54930615 nan nan]
"
"tf.math.bessel_i0(
    x, name=None
)
","[['Computes the Bessel i0 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i0([-1., -0.5, 0.5, 1.]).numpy()
array([1.26606588, 1.06348337, 1.06348337, 1.26606588], dtype=float32)"
"tf.math.bessel_i0e(
    x, name=None
)
","[['Computes the Bessel i0e function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i0e([-1., -0.5, 0.5, 1.]).numpy()
array([0.46575961, 0.64503527, 0.64503527, 0.46575961], dtype=float32)"
"tf.math.bessel_i1(
    x, name=None
)
","[['Computes the Bessel i1 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i1([-1., -0.5, 0.5, 1.]).numpy()
array([-0.5651591 , -0.25789431,  0.25789431,  0.5651591 ], dtype=float32)"
"tf.math.bessel_i1e(
    x, name=None
)
","[['Computes the Bessel i1e function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i1e([-1., -0.5, 0.5, 1.]).numpy()
array([-0.20791042, -0.15642083,  0.15642083,  0.20791042], dtype=float32)"
"tf.math.betainc(
    a, b, x, name=None
)
","[[None, '\n']]",[]
"tf.compat.v1.bincount(
    arr,
    weights=None,
    minlength=None,
    maxlength=None,
    dtype=tf.dtypes.int32
)
",[],[]
"tf.math.ceil(
    x, name=None
)
",[],"tf.math.ceil([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0])
<tf.Tensor: shape=(7,), dtype=float32,
numpy=array([-1., -1., -0.,  1.,  2.,  2.,  2.], dtype=float32)>"
"tf.compat.v1.confusion_matrix(
    labels,
    predictions,
    num_classes=None,
    dtype=tf.dtypes.int32,
    name=None,
    weights=None
)
",[],"  tf.math.confusion_matrix([1, 2, 4], [2, 2, 4]) ==>
      [[0 0 0 0 0]
       [0 0 1 0 0]
       [0 0 1 0 0]
       [0 0 0 0 0]
       [0 0 0 0 1]]
"
"tf.math.conj(
    x, name=None
)
","[[None, '\n']]","x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
tf.math.conj(x)
<tf.Tensor: shape=(2,), dtype=complex128,
numpy=array([-2.25-4.75j,  3.25-5.75j])>"
"tf.math.cos(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.cos(x) ==> [nan -0.91113025 0.87758255 0.5403023 0.36235774 0.48718765 -0.95215535 nan]
"
"tf.math.cosh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.cosh(x) ==> [inf 4.0515420e+03 1.1276259e+00 1.5430807e+00 1.8106556e+00 3.7621956e+00 1.1013233e+04 inf]
"
"tf.compat.v1.count_nonzero(
    input_tensor=None,
    axis=None,
    keepdims=None,
    dtype=tf.dtypes.int64,
    name=None,
    reduction_indices=None,
    keep_dims=None,
    input=None
)
",[],"x = tf.constant([[0, 1, 0], [1, 1, 0]])
tf.math.count_nonzero(x)  # 3
tf.math.count_nonzero(x, 0)  # [1, 2, 0]
tf.math.count_nonzero(x, 1)  # [1, 2]
tf.math.count_nonzero(x, 1, keepdims=True)  # [[1], [2]]
tf.math.count_nonzero(x, [0, 1])  # 3
"
"tf.math.cumprod(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","[['Compute the cumulative product of the tensor ', 'x', ' along ', 'axis', '.']]","tf.math.cumprod([a, b, c])  # [a, a * b, a * b * c]
"
"tf.math.cumsum(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","[['Compute the cumulative sum of the tensor ', 'x', ' along ', 'axis', '.']]","# tf.cumsum([a, b, c])   # [a, a + b, a + b + c]
x = tf.constant([2, 4, 6, 8])
tf.cumsum(x)
<tf.Tensor: shape=(4,), dtype=int32,
numpy=array([ 2,  6, 12, 20], dtype=int32)>"
"tf.math.cumulative_logsumexp(
    x, axis=0, exclusive=False, reverse=False, name=None
)
","[['Compute the cumulative log-sum-exp of the tensor ', 'x', ' along ', 'axis', '.']]","log(sum(exp(x))) == log(sum(exp(x - max(x)))) + max(x)
"
"tf.math.digamma(
    x, name=None
)
",[],[]
"tf.math.divide(
    x, y, name=None
)
","[['Computes Python style division of ', 'x', ' by ', 'y', '.']]","x = tf.constant([16, 12, 11])
y = tf.constant([4, 6, 2])
tf.divide(x,y)
<tf.Tensor: shape=(3,), dtype=float64,
numpy=array([4. , 2. , 5.5])>"
"tf.math.divide_no_nan(
    x, y, name=None
)
","[['Computes a safe divide which returns 0 if ', 'y', ' (denominator) is zero.']]","tf.constant(3.0) / 0.0
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
tf.math.divide_no_nan(3.0, 0.0)
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>"
"tf.math.equal(
    x, y, name=None
)
",[],"x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  False])>"
"tf.math.erf(
    x, name=None
)
","[[None, '\n'], ['Computes the ', 'Gauss error function', ' of ', 'x', ' element-wise. In statistics, for non-negative values of \\(x\\), the error function has the following interpretation: for a random variable \\(Y\\) that is normally distributed with mean 0 and variance \\(1/\\sqrt{2}\\), \\(erf(x)\\) is the probability that \\(Y\\) falls in the range \\([x, x]\\).']]","tf.math.erf([[1.0, 2.0, 3.0], [0.0, -1.0, -2.0]])
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.8427007,  0.9953223,  0.999978 ],
       [ 0.       , -0.8427007, -0.9953223]], dtype=float32)>"
"tf.math.erfc(
    x, name=None
)
","[['Computes the complementary error function of ', 'x', ' element-wise.']]",[]
"tf.math.erfcinv(
    x, name=None
)
",[],"tf.math.erfcinv([0., 0.2, 1., 1.5, 2.])
<tf.Tensor: shape=(5,), dtype=float32, numpy=
array([       inf,  0.9061935, -0.       , -0.4769363,       -inf],
      dtype=float32)>"
"tf.math.erfinv(
    x, name=None
)
",[],[]
"tf.math.exp(
    x, name=None
)
","[[None, '\n']]","x = tf.constant(2.0)
tf.math.exp(x)
<tf.Tensor: shape=(), dtype=float32, numpy=7.389056>"
"tf.math.expm1(
    x, name=None
)
","[['Computes ', 'exp(x) - 1', ' element-wise.']]","  x = tf.constant(2.0)
  tf.math.expm1(x) ==> 6.389056

  x = tf.constant([2.0, 8.0])
  tf.math.expm1(x) ==> array([6.389056, 2979.958], dtype=float32)

  x = tf.constant(1 + 1j)
  tf.math.expm1(x) ==> (0.46869393991588515+2.2873552871788423j)
"
"tf.math.floor(
    x, name=None
)
",[],"x = tf.constant([1.3324, -1.5, 5.555, -2.532, 0.99, float(""inf"")])
tf.floor(x).numpy()
array([ 1., -2.,  5., -3.,  0., inf], dtype=float32)"
"tf.math.floordiv(
    x, y, name=None
)
","[['Divides ', 'x / y', ' elementwise, rounding toward the most negative integer.']]",[]
"tf.math.floormod(
    x, y, name=None
)
",[],[]
"tf.math.greater(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5, 2, 5])
tf.math.greater(x, y) ==> [False, True, True]

x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.greater(x, y) ==> [False, False, True]
"
"tf.math.greater_equal(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6, 7])
y = tf.constant([5, 2, 5, 10])
tf.math.greater_equal(x, y) ==> [True, True, True, False]

x = tf.constant([5, 4, 6, 7])
y = tf.constant([5])
tf.math.greater_equal(x, y) ==> [True, False, True, True]
"
"tf.math.igamma(
    a, x, name=None
)
","[[None, '\n'], ['Compute the lower regularized incomplete Gamma function ', 'P(a, x)', '.']]",[]
"tf.math.igammac(
    a, x, name=None
)
","[[None, '\n'], ['Compute the upper regularized incomplete Gamma function ', 'Q(a, x)', '.']]",[]
"tf.math.imag(
    input, name=None
)
",[],"x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
tf.math.imag(x)  # [4.75, 5.75]
"
"tf.compat.v1.math.in_top_k(
    predictions, targets, k, name=None
)
","[[None, '\n'], ['Says whether the targets are in the top ', 'K', ' predictions.']]",[]
"tf.math.invert_permutation(
    x, name=None
)
",[],"# tensor `x` is [3, 4, 0, 2, 1]
invert_permutation(x) ==> [2, 4, 3, 0, 1]
"
"tf.math.is_finite(
    x, name=None
)
",[],"x = tf.constant([5.0, 4.8, 6.8, np.inf, np.nan])
tf.math.is_finite(x) ==> [True, True, True, False, False]
"
"tf.math.is_inf(
    x, name=None
)
",[],"x = tf.constant([5.0, np.inf, 6.8, np.inf])
tf.math.is_inf(x) ==> [False, True, False, True]
"
"tf.math.is_nan(
    x, name=None
)
",[],"x = tf.constant([5.0, np.nan, 6.8, np.nan, np.inf])
tf.math.is_nan(x) ==> [False, True, False, True, False]
"
"tf.math.is_non_decreasing(
    x, name=None
)
","[['Returns ', 'True', ' if ', 'x', ' is non-decreasing.']]","x1 = tf.constant([1.0, 1.0, 3.0])
tf.math.is_non_decreasing(x1)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
x2 = tf.constant([3.0, 1.0, 2.0])
tf.math.is_non_decreasing(x2)
<tf.Tensor: shape=(), dtype=bool, numpy=False>"
"tf.math.is_strictly_increasing(
    x, name=None
)
","[['Returns ', 'True', ' if ', 'x', ' is strictly increasing.']]","x1 = tf.constant([1.0, 2.0, 3.0])
tf.math.is_strictly_increasing(x1)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
x2 = tf.constant([3.0, 1.0, 2.0])
tf.math.is_strictly_increasing(x2)
<tf.Tensor: shape=(), dtype=bool, numpy=False>"
"tf.math.l2_normalize(
    x, axis=None, epsilon=1e-12, name=None, dim=None
)
","[['Normalizes along dimension ', 'axis', ' using an L2 norm. (deprecated arguments)']]","output = x / sqrt(max(sum(x**2), epsilon))
"
"tf.math.lbeta(
    x, name=None
)
","[[None, '\n']]",[]
"tf.math.less(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less(x, y) ==> [False, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 7])
tf.math.less(x, y) ==> [False, True, True]
"
"tf.math.less_equal(
    x, y, name=None
)
",[],"x = tf.constant([5, 4, 6])
y = tf.constant([5])
tf.math.less_equal(x, y) ==> [True, True, False]

x = tf.constant([5, 4, 6])
y = tf.constant([5, 6, 6])
tf.math.less_equal(x, y) ==> [True, True, True]
"
"tf.math.lgamma(
    x, name=None
)
","[['Computes the log of the absolute value of ', 'Gamma(x)', ' element-wise.']]","x = tf.constant([0, 0.5, 1, 4.5, -4, -5.6])
tf.math.lgamma(x) ==> [inf, 0.5723649, 0., 2.4537368, inf, -4.6477685]
"
"tf.math.log(
    x, name=None
)
","[[None, '\n']]","x = tf.constant([0, 0.5, 1, 5])
tf.math.log(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([      -inf, -0.6931472,  0.       ,  1.609438 ], dtype=float32)>"
"tf.math.log1p(
    x, name=None
)
","[[None, '\n']]","x = tf.constant([0, 0.5, 1, 5])
tf.math.log1p(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0.       , 0.4054651, 0.6931472, 1.7917595], dtype=float32)>"
"tf.math.log_sigmoid(
    x, name=None
)
","[['Computes log sigmoid of ', 'x', ' element-wise.']]","x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.log_sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32, numpy=
array([-6.9314718e-01, -3.1326169e-01, -1.9287499e-22, -0.0000000e+00],
      dtype=float32)>"
"tf.compat.v1.math.log_softmax(
    logits, axis=None, name=None, dim=None
)
",[],"logsoftmax = logits - log(reduce_sum(exp(logits), axis))
"
"tf.math.logical_and(
    x, y, name=None
)
",[],">>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_and(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
>>> a & b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([False])>
"
"tf.math.logical_not(
    x, name=None
)
","[['Returns the truth value of ', 'NOT x', ' element-wise.']]","tf.math.logical_not(tf.constant([True, False]))
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.math.logical_or(
    x, y, name=None
)
",[],">>> a = tf.constant([True])
>>> b = tf.constant([False])
>>> tf.math.logical_or(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
>>> a | b
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>
"
"tf.math.logical_xor(
    x, y, name='LogicalXor'
)
",[],"a = tf.constant([True])
b = tf.constant([False])
tf.math.logical_xor(a, b)
<tf.Tensor: shape=(1,), dtype=bool, numpy=array([ True])>"
"tf.math.maximum(
    x, y, name=None
)
",[],"x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-2., 0., 2., 5.])
tf.math.maximum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>"
"tf.math.minimum(
    x, y, name=None
)
",[],"x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-5., -2., 0., 3.])
tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>"
"tf.math.floormod(
    x, y, name=None
)
",[],[]
"tf.math.multiply(
    x, y, name=None
)
",[],"x = tf.constant(([1, 2, 3, 4]))
tf.math.multiply(x, x)
<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>"
"tf.math.multiply_no_nan(
    x, y, name=None
)
",[],[]
"tf.math.ndtri(
    x, name=None
)
",[],[]
"tf.math.negative(
    x, name=None
)
","[[None, '\n']]",[]
"tf.math.nextafter(
    x1, x2, name=None
)
","[['Returns the next representable value of ', 'x1', ' in the direction of ', 'x2', ', element-wise.']]",[]
"tf.math.not_equal(
    x, y, name=None
)
",[],"x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.not_equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.math.polygamma(
    a, x, name=None
)
","[[None, '\n']]",[]
"tf.math.polyval(
    coeffs, x, name=None
)
",[],"p(x) = coeffs[n-1] + x * (coeffs[n-2] + ... + x * (coeffs[1] + x * coeffs[0]))
"
"tf.math.pow(
    x, y, name=None
)
","[[None, '\n']]","x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]
"
"tf.math.real(
    input, name=None
)
",[],"x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
tf.math.real(x)  # [-2.25, 3.25]
"
"tf.math.reciprocal(
    x, name=None
)
","[[None, '\n']]",[]
"tf.math.reciprocal_no_nan(
    x, name=None
)
",[],"x = tf.constant([2.0, 0.5, 0, 1], dtype=tf.float32)
tf.math.reciprocal_no_nan(x)  # [ 0.5, 2, 0.0, 1.0 ]
"
"tf.compat.v1.reduce_all(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
","[['Computes ', 'tf.math.logical_and', ' of elements across dimensions of a tensor. (deprecated arguments)']]",">>> x = tf.constant([[True,  True], [False, False]])
>>> tf.math.reduce_all(x)
<tf.Tensor: shape=(), dtype=bool, numpy=False>
>>> tf.math.reduce_all(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False, False])>
>>> tf.math.reduce_all(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.compat.v1.reduce_any(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
","[['Computes ', 'tf.math.logical_or', ' of elements across dimensions of a tensor. (deprecated arguments)']]",">>> x = tf.constant([[True,  True], [False, False]])
>>> tf.reduce_any(x)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
>>> tf.reduce_any(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>
>>> tf.reduce_any(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.math.reduce_euclidean_norm(
    input_tensor, axis=None, keepdims=False, name=None
)
",[],"x = tf.constant([[1, 2, 3], [1, 1, 1]]) # x.dtype is tf.int32
tf.math.reduce_euclidean_norm(x)  # returns 4 as dtype is tf.int32
y = tf.constant([[1, 2, 3], [1, 1, 1]], dtype = tf.float32)
tf.math.reduce_euclidean_norm(y)  # returns 4.1231055 which is sqrt(17)
tf.math.reduce_euclidean_norm(y, 0)  # [sqrt(2), sqrt(5), sqrt(10)]
tf.math.reduce_euclidean_norm(y, 1)  # [sqrt(14), sqrt(3)]
tf.math.reduce_euclidean_norm(y, 1, keepdims=True)  # [[sqrt(14)], [sqrt(3)]]
tf.math.reduce_euclidean_norm(y, [0, 1])  # sqrt(17)
"
"tf.compat.v1.reduce_logsumexp(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
",[],"x = tf.constant([[0., 0., 0.], [0., 0., 0.]])
tf.reduce_logsumexp(x)  # log(6)
tf.reduce_logsumexp(x, 0)  # [log(2), log(2), log(2)]
tf.reduce_logsumexp(x, 1)  # [log(3), log(3)]
tf.reduce_logsumexp(x, 1, keepdims=True)  # [[log(3)], [log(3)]]
tf.reduce_logsumexp(x, [0, 1])  # log(6)
"
"tf.compat.v1.reduce_max(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
","[['Computes ', 'tf.math.maximum', ' of elements across dimensions of a tensor. (deprecated arguments)']]",">>> x = tf.constant([5, 1, 2, 4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=5>
>>> x = tf.constant([-5, -1, -2, -4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=-1>
>>> x = tf.constant([4, float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('nan'), float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('-inf'), float('inf')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
"
"tf.compat.v1.reduce_mean(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
",[],"x = tf.constant([[1., 1.], [2., 2.]])
tf.reduce_mean(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.5>
tf.reduce_mean(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>
tf.reduce_mean(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
"tf.compat.v1.reduce_min(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
","[['Computes the ', 'tf.math.minimum', ' of elements across dimensions of a tensor. (deprecated arguments)']]",">>> x = tf.constant([5, 1, 2, 4])
>>> tf.reduce_min(x)
<tf.Tensor: shape=(), dtype=int32, numpy=1>
>>> x = tf.constant([-5, -1, -2, -4])
>>> tf.reduce_min(x)
<tf.Tensor: shape=(), dtype=int32, numpy=-5>
>>> x = tf.constant([4, float('nan')])
>>> tf.reduce_min(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('nan'), float('nan')])
>>> tf.reduce_min(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('-inf'), float('inf')])
>>> tf.reduce_min(x)
<tf.Tensor: shape=(), dtype=float32, numpy=-inf>
"
"tf.compat.v1.reduce_prod(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
","[['Computes ', 'tf.math.multiply', ' of elements across dimensions of a tensor. (deprecated arguments)']]",">>> x = tf.constant([[1., 2.], [3., 4.]])
>>> tf.math.reduce_prod(x)
<tf.Tensor: shape=(), dtype=float32, numpy=24.>
>>> tf.math.reduce_prod(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 8.], dtype=float32)>
>>> tf.math.reduce_prod(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 12.],
dtype=float32)>
"
"tf.math.reduce_std(
    input_tensor, axis=None, keepdims=False, name=None
)
",[],"x = tf.constant([[1., 2.], [3., 4.]])
tf.math.reduce_std(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.118034>
tf.math.reduce_std(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], dtype=float32)>
tf.math.reduce_std(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.5, 0.5], dtype=float32)>"
"tf.compat.v1.reduce_sum(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
",[],">>> # x has a shape of (2, 3) (two rows and three columns):
>>> x = tf.constant([[1, 1, 1], [1, 1, 1]])
>>> x.numpy()
array([[1, 1, 1],
       [1, 1, 1]], dtype=int32)
>>> # sum all the elements
>>> # 1 + 1 + 1 + 1 + 1+ 1 = 6
>>> tf.reduce_sum(x).numpy()
6
>>> # reduce along the first dimension
>>> # the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]
>>> tf.reduce_sum(x, 0).numpy()
array([2, 2, 2], dtype=int32)
>>> # reduce along the second dimension
>>> # the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]
>>> tf.reduce_sum(x, 1).numpy()
array([3, 3], dtype=int32)
>>> # keep the original dimensions
>>> tf.reduce_sum(x, 1, keepdims=True).numpy()
array([[3],
       [3]], dtype=int32)
>>> # reduce along both dimensions
>>> # the result is 1 + 1 + 1 + 1 + 1 + 1 = 6
>>> # or, equivalently, reduce along rows, then reduce the resultant array
>>> # [1, 1, 1] + [1, 1, 1] = [2, 2, 2]
>>> # 2 + 2 + 2 = 6
>>> tf.reduce_sum(x, [0, 1]).numpy()
6
"
"tf.math.reduce_variance(
    input_tensor, axis=None, keepdims=False, name=None
)
",[],"x = tf.constant([[1., 2.], [3., 4.]])
tf.math.reduce_variance(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.25>
tf.math.reduce_variance(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 1.], ...)>
tf.math.reduce_variance(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0.25, 0.25], ...)>"
"tf.math.rint(
    x, name=None
)
",[],"rint(-1.5) ==> -2.0
rint(0.5000001) ==> 1.0
rint([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0]) ==> [-2., -2., -0., 0., 2., 2., 2.]
"
"tf.math.round(
    x, name=None
)
",[],"x = tf.constant([0.9, 2.5, 2.3, 1.5, -4.5])
tf.round(x)  # [ 1.0, 2.0, 2.0, 2.0, -4.0 ]
"
"tf.math.rsqrt(
    x, name=None
)
",[],"x = tf.constant([2., 0., -2.])
tf.math.rsqrt(x)
<tf.Tensor: shape=(3,), dtype=float32,
numpy=array([0.707, inf, nan], dtype=float32)>"
"tf.compat.v1.scalar_mul(
    scalar, x, name=None
)
","[['Multiplies a scalar times a ', 'Tensor', ' or ', 'IndexedSlices', ' object.']]","x = tf.reshape(tf.range(30, dtype=tf.float32), [10, 3])
with tf.GradientTape() as g:
  g.watch(x)
  y = tf.gather(x, [1, 2])  # IndexedSlices
  z = tf.math.scalar_mul(10.0, y)"
"tf.math.segment_max(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_max(c, tf.constant([0, 0, 1])).numpy()
array([[4, 3, 3, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_mean(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1.0,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_mean(c, tf.constant([0, 0, 1])).numpy()
array([[2.5, 2.5, 2.5, 2.5],
       [5., 6., 7., 8.]], dtype=float32)"
"tf.math.segment_min(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_min(c, tf.constant([0, 0, 1])).numpy()
array([[1, 2, 2, 1],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_prod(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_prod(c, tf.constant([0, 0, 1])).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_sum(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_sum(c, tf.constant([0, 0, 1])).numpy()
array([[5, 5, 5, 5],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.sigmoid(
    x, name=None
)
","[[None, '\n'], ['Computes sigmoid of ', 'x', ' element-wise.']]","x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32,
numpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>"
"tf.math.sign(
    x, name=None
)
",[],"# real number
tf.math.sign([0., 2., -3.])
<tf.Tensor: shape=(3,), dtype=float32,
numpy=array([ 0.,  1., -1.], dtype=float32)>"
"tf.math.sin(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10, float(""inf"")])
  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]
"
"tf.math.sinh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.sinh(x) ==> [-inf -4.0515420e+03 -5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf]
"
"tf.math.sobol_sample(
    dim,
    num_results,
    skip=0,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.compat.v1.math.softmax(
    logits, axis=None, name=None, dim=None
)
",[],"softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis, keepdims=True)
"
"tf.math.softplus(
    features, name=None
)
","[['Computes elementwise softplus: ', 'softplus(x) = log(exp(x) + 1)', '.']]","import tensorflow as tf
tf.math.softplus(tf.range(0, 2, dtype=tf.float32)).numpy()
array([0.6931472, 1.3132616], dtype=float32)"
"tf.nn.softsign(
    features, name=None
)
","[['Computes softsign: ', 'features / (abs(features) + 1)', '.']]",[]
"tf.math.bessel_i0(
    x, name=None
)
","[['Computes the Bessel i0 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i0([-1., -0.5, 0.5, 1.]).numpy()
array([1.26606588, 1.06348337, 1.06348337, 1.26606588], dtype=float32)"
"tf.math.bessel_i0e(
    x, name=None
)
","[['Computes the Bessel i0e function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i0e([-1., -0.5, 0.5, 1.]).numpy()
array([0.46575961, 0.64503527, 0.64503527, 0.46575961], dtype=float32)"
"tf.math.bessel_i1(
    x, name=None
)
","[['Computes the Bessel i1 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i1([-1., -0.5, 0.5, 1.]).numpy()
array([-0.5651591 , -0.25789431,  0.25789431,  0.5651591 ], dtype=float32)"
"tf.math.bessel_i1e(
    x, name=None
)
","[['Computes the Bessel i1e function of ', 'x', ' element-wise.']]","tf.math.special.bessel_i1e([-1., -0.5, 0.5, 1.]).numpy()
array([-0.20791042, -0.15642083,  0.15642083,  0.20791042], dtype=float32)"
"tf.math.special.bessel_j0(
    x, name=None
)
","[['Computes the Bessel j0 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_j0([0.5, 1., 2., 4.]).numpy()
array([ 0.93846981,  0.76519769,  0.22389078, -0.39714981], dtype=float32)"
"tf.math.special.bessel_j1(
    x, name=None
)
","[['Computes the Bessel j1 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_j1([0.5, 1., 2., 4.]).numpy()
array([ 0.24226846,  0.44005059,  0.57672481, -0.06604333], dtype=float32)"
"tf.math.special.bessel_k0(
    x, name=None
)
","[['Computes the Bessel k0 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_k0([0.5, 1., 2., 4.]).numpy()
array([0.92441907, 0.42102444, 0.11389387, 0.01115968], dtype=float32)"
"tf.math.special.bessel_k0e(
    x, name=None
)
","[['Computes the Bessel k0e function of ', 'x', ' element-wise.']]","tf.math.special.bessel_k0e([0.5, 1., 2., 4.]).numpy()
array([1.52410939, 1.14446308, 0.84156822, 0.60929767], dtype=float32)"
"tf.math.special.bessel_k1(
    x, name=None
)
","[['Computes the Bessel k1 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_k1([0.5, 1., 2., 4.]).numpy()
array([1.65644112, 0.60190723, 0.13986588, 0.0124835 ], dtype=float32)"
"tf.math.special.bessel_k1e(
    x, name=None
)
","[['Computes the Bessel k1e function of ', 'x', ' element-wise.']]","tf.math.special.bessel_k1e([0.5, 1., 2., 4.]).numpy()
array([2.73100971, 1.63615349, 1.03347685, 0.68157595], dtype=float32)"
"tf.math.special.bessel_y0(
    x, name=None
)
","[['Computes the Bessel y0 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_y0([0.5, 1., 2., 4.]).numpy()
array([-0.44451873,  0.08825696,  0.51037567, -0.01694074], dtype=float32)"
"tf.math.special.bessel_y1(
    x, name=None
)
","[['Computes the Bessel y1 function of ', 'x', ' element-wise.']]","tf.math.special.bessel_y1([0.5, 1., 2., 4.]).numpy()
array([-1.47147239, -0.78121282, -0.10703243,  0.39792571], dtype=float32)"
"tf.math.special.dawsn(
    x, name=None
)
","[[""Computes Dawson's integral of "", 'x', ' element-wise.']]",">>> tf.math.special.dawsn([-1., -0.5, 0.5, 1.]).numpy()
array([-0.5380795, -0.4244364, 0.4244364,  0.5380795], dtype=float32)
"
"tf.math.special.expint(
    x, name=None
)
","[['Computes the Exponential integral of ', 'x', ' element-wise.']]","tf.math.special.expint([1., 1.1, 2.1, 4.1]).numpy()
array([ 1.8951179,  2.1673784,  5.3332353, 21.048464], dtype=float32)"
"tf.math.special.fresnel_cos(
    x, name=None
)
","[[""Computes Fresnel's cosine integral of "", 'x', ' element-wise.']]",">>> tf.math.special.fresnel_cos([-1., -0.1, 0.1, 1.]).numpy()
array([-0.7798934 , -0.09999753,  0.09999753,  0.7798934 ], dtype=float32)
"
"tf.math.special.fresnel_sin(
    x, name=None
)
","[[""Computes Fresnel's sine integral of "", 'x', ' element-wise.']]","tf.math.special.fresnel_sin([-1., -0.1, 0.1, 1.]).numpy()
array([-0.43825912, -0.00052359,  0.00052359,  0.43825912], dtype=float32)"
"tf.math.special.spence(
    x, name=None
)
","[[""Computes Spence's integral of "", 'x', ' element-wise.']]","tf.math.special.spence([0.5, 1., 2., 3.]).numpy()
array([ 0.58224034,  0.        , -0.82246685, -1.4367464], dtype=float32)"
"tf.math.sqrt(
    x, name=None
)
",[],"x = tf.constant([[4.0], [16.0]])
tf.sqrt(x)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[2.],
         [4.]], dtype=float32)>
y = tf.constant([[-4.0], [16.0]])
tf.sqrt(y)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[nan],
         [ 4.]], dtype=float32)>
z = tf.constant([[-1.0], [16.0]], dtype=tf.complex128)
tf.sqrt(z)
<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=
  array([[0.0+1.j],
         [4.0+0.j]])>"
"tf.math.square(
    x, name=None
)
","[[None, '\n']]","tf.math.square([-2., 0., 3.])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>"
"tf.math.squared_difference(
    x, y, name=None
)
",[],[]
"tf.math.subtract(
    x, y, name=None
)
",[],"x = [1, 2, 3, 4, 5]
y = 1
tf.subtract(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>
tf.subtract(y, x)
<tf.Tensor: shape=(5,), dtype=int32,
numpy=array([ 0, -1, -2, -3, -4], dtype=int32)>"
"tf.math.tan(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.tan(x) ==> [nan 0.45231566 -0.5463025 1.5574077 2.572152 -1.7925274 0.32097113 nan]
"
"tf.math.tanh(
    x, name=None
)
","[['Computes hyperbolic tangent of ', 'x', ' element-wise.']]",[]
"tf.math.top_k(
    input, k=1, sorted=True, name=None
)
","[['Finds values and indices of the ', 'k', ' largest entries for the last dimension.']]","result = tf.math.top_k([1, 2, 98, 1, 1, 99, 3, 1, 3, 96, 4, 1],
                        k=3)
result.values.numpy()
array([99, 98, 96], dtype=int32)
result.indices.numpy()
array([5, 2, 9], dtype=int32)"
"tf.math.truediv(
    x, y, name=None
)
",[],[]
"tf.math.unsorted_segment_max(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_max(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[4, 3, 3, 4],
       [5,  6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_mean(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]",[]
"tf.math.unsorted_segment_min(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_min(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[1, 2, 2, 1],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_prod(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_prod(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_sqrt_n(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]",[]
"tf.math.unsorted_segment_sum(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = [[1,2,3,4], [5,6,7,8], [4,3,2,1]]
tf.math.unsorted_segment_sum(c, [0, 1, 0], num_segments=2).numpy()
array([[5, 5, 5, 5],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.xdivy(
    x, y, name=None
)
",[],[]
"tf.math.xlog1py(
    x, y, name=None
)
",[],"tf.math.xlog1py(0., 1.)
<tf.Tensor: shape=(), dtype=float32, numpy=0.>
tf.math.xlog1py(1., 1.)
<tf.Tensor: shape=(), dtype=float32, numpy=0.6931472>
tf.math.xlog1py(2., 2.)
<tf.Tensor: shape=(), dtype=float32, numpy=2.1972246>
tf.math.xlog1py(0., -1.)
<tf.Tensor: shape=(), dtype=float32, numpy=0.>"
"tf.math.xlogy(
    x, y, name=None
)
",[],[]
"tf.math.zero_fraction(
    value, name=None
)
","[['Returns the fraction of zeros in ', 'value', '.']]","    z = tf.nn.relu(...)
    summ = tf.compat.v1.summary.scalar('sparsity', tf.nn.zero_fraction(z))
"
"tf.math.zeta(
    x, q, name=None
)
","[[None, '\n']]",[]
"tf.linalg.matmul(
    a,
    b,
    transpose_a=False,
    transpose_b=False,
    adjoint_a=False,
    adjoint_b=False,
    a_is_sparse=False,
    b_is_sparse=False,
    output_type=None,
    name=None
)
","[['Multiplies matrix ', 'a', ' by matrix ', 'b', ', producing ', 'a', ' * ', 'b', '.']]","a = tf.constant([1, 2, 3, 4, 5, 6], shape=[2, 3])
a  # 2-D tensor
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)>
b = tf.constant([7, 8, 9, 10, 11, 12], shape=[3, 2])
b  # 2-D tensor
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[ 7,  8],
       [ 9, 10],
       [11, 12]], dtype=int32)>
c = tf.matmul(a, b)
c  # `a` * `b`
<tf.Tensor: shape=(2, 2), dtype=int32, numpy=
array([[ 58,  64],
       [139, 154]], dtype=int32)>"
"tf.linalg.band_part(
    input, num_lower, num_upper, name=None
)
",[],"# if 'input' is [[ 0,  1,  2, 3]
#                [-1,  0,  1, 2]
#                [-2, -1,  0, 1]
#                [-3, -2, -1, 0]],

tf.linalg.band_part(input, 1, -1) ==> [[ 0,  1,  2, 3]
                                       [-1,  0,  1, 2]
                                       [ 0, -1,  0, 1]
                                       [ 0,  0, -1, 0]],

tf.linalg.band_part(input, 2, 1) ==> [[ 0,  1,  0, 0]
                                      [-1,  0,  1, 0]
                                      [-2, -1,  0, 1]
                                      [ 0, -2, -1, 0]]
"
"tf.linalg.det(
    input, name=None
)
",[],[]
"tf.linalg.diag(
    diagonal,
    name='diag',
    k=0,
    num_rows=-1,
    num_cols=-1,
    padding_value=0,
    align='RIGHT_LEFT'
)
",[],"output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(d_upper, 0)] ; if n - m == d_upper
    padding_value                             ; otherwise
"
"tf.linalg.diag_part(
    input,
    name='diag_part',
    k=0,
    padding_value=0,
    align='RIGHT_LEFT'
)
",[],"diagonal[i, j, ..., l, n]
  = input[i, j, ..., l, n+y, n+x] ; if 0 <= n+y < M and 0 <= n+x < N,
    padding_value                 ; otherwise.
"
"tf.linalg.inv(
    input, adjoint=False, name=None
)
",[],[]
"tf.linalg.set_diag(
    input,
    diagonal,
    name='set_diag',
    k=0,
    align='RIGHT_LEFT'
)
",[],"output[i, j, ..., l, m, n]
  = diagonal[i, j, ..., l, n-max(k[1], 0)] ; if n - m == k[1]
    input[i, j, ..., l, m, n]              ; otherwise
"
"tf.linalg.solve(
    matrix, rhs, adjoint=False, name=None
)
",[],[]
"tf.linalg.lstsq(
    matrix, rhs, l2_regularizer=0.0, fast=True, name=None
)
","[[None, '\n']]",[]
"tf.linalg.sqrtm(
    input, name=None
)
",[],[]
"tf.linalg.matrix_transpose(
    a, name='matrix_transpose', conjugate=False
)
","[['Transposes last two dimensions of tensor ', 'a', '.']]","x = tf.constant([[1, 2, 3], [4, 5, 6]])
tf.linalg.matrix_transpose(x)  # [[1, 4],
                               #  [2, 5],
                               #  [3, 6]]

x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],
                 [4 + 4j, 5 + 5j, 6 + 6j]])
tf.linalg.matrix_transpose(x, conjugate=True)  # [[1 - 1j, 4 - 4j],
                                               #  [2 - 2j, 5 - 5j],
                                               #  [3 - 3j, 6 - 6j]]

# Matrix with two batch dimensions.
# x.shape is [1, 2, 3, 4]
# tf.linalg.matrix_transpose(x) is shape [1, 2, 4, 3]
"
"tf.linalg.triangular_solve(
    matrix, rhs, lower=True, adjoint=False, name=None
)
",[],"a = tf.constant([[3,  0,  0,  0],
  [2,  1,  0,  0],
  [1,  0,  1,  0],
  [1,  1,  1,  1]], dtype=tf.float32)"
"tf.math.maximum(
    x, y, name=None
)
",[],"x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-2., 0., 2., 5.])
tf.math.maximum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([0., 0., 2., 5.], dtype=float32)>"
"tf.meshgrid(
    *args, **kwargs
)
",[],"x = [1, 2, 3]
y = [4, 5, 6]
X, Y = tf.meshgrid(x, y)
# X = [[1, 2, 3],
#      [1, 2, 3],
#      [1, 2, 3]]
# Y = [[4, 4, 4],
#      [5, 5, 5],
#      [6, 6, 6]]
"
"tf.compat.v1.metrics.accuracy(
    labels,
    predictions,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
","[['Calculates how often ', 'predictions', ' matches ', 'labels', '.']]","accuracy, update_op = tf.compat.v1.metrics.accuracy(
  labels=labels,
  predictions=predictions,
  weights=weights,
  metrics_collections=metrics_collections,
  update_collections=update_collections,
  name=name)
"
"tf.compat.v1.metrics.auc(
    labels,
    predictions,
    weights=None,
    num_thresholds=200,
    metrics_collections=None,
    updates_collections=None,
    curve='ROC',
    name=None,
    summation_method='trapezoidal',
    thresholds=None
)
",[],[]
"tf.compat.v1.metrics.average_precision_at_k(
    labels,
    predictions,
    k,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.false_negatives(
    labels,
    predictions,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.false_negatives_at_thresholds(
    labels,
    predictions,
    thresholds,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.false_positives(
    labels,
    predictions,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.false_positives_at_thresholds(
    labels,
    predictions,
    thresholds,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.mean(
    values,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],"mean, update_op = tf.compat.v1.metrics.mean(
  values=values,
  weights=weights,
  metrics_collections=metrics_collections,
  update_collections=update_collections,
  name=name)
"
"tf.compat.v1.metrics.mean_absolute_error(
    labels,
    predictions,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.mean_cosine_distance(
    labels,
    predictions,
    dim,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.mean_iou(
    labels,
    predictions,
    num_classes,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.mean_per_class_accuracy(
    labels,
    predictions,
    num_classes,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.mean_relative_error(
    labels,
    predictions,
    normalizer,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.mean_squared_error(
    labels,
    predictions,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.mean_tensor(
    values,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.percentage_below(
    values,
    threshold,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.precision(
    labels,
    predictions,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.precision_at_k(
    labels,
    predictions,
    k,
    class_id=None,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.precision_at_thresholds(
    labels,
    predictions,
    thresholds,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
","[['Computes precision values for different ', 'thresholds', ' on ', 'predictions', '.']]",[]
"tf.compat.v1.metrics.precision_at_top_k(
    labels,
    predictions_idx,
    k=None,
    class_id=None,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.recall(
    labels,
    predictions,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.recall_at_k(
    labels,
    predictions,
    k,
    class_id=None,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.recall_at_thresholds(
    labels,
    predictions,
    thresholds,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
","[['Computes various recall values for different ', 'thresholds', ' on ', 'predictions', '.']]",[]
"tf.compat.v1.metrics.recall_at_top_k(
    labels,
    predictions_idx,
    k=None,
    class_id=None,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.root_mean_squared_error(
    labels,
    predictions,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.sensitivity_at_specificity(
    labels,
    predictions,
    specificity,
    weights=None,
    num_thresholds=200,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.sparse_average_precision_at_k(
    labels,
    predictions,
    k,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
","[['Renamed to ', 'average_precision_at_k', ', please use that method instead. (deprecated)']]",[]
"tf.compat.v1.metrics.sparse_precision_at_k(
    labels,
    predictions,
    k,
    class_id=None,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
","[['Renamed to ', 'precision_at_k', ', please use that method instead. (deprecated)']]",[]
"tf.compat.v1.metrics.specificity_at_sensitivity(
    labels,
    predictions,
    sensitivity,
    weights=None,
    num_thresholds=200,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.true_negatives(
    labels,
    predictions,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.true_negatives_at_thresholds(
    labels,
    predictions,
    thresholds,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.true_positives(
    labels,
    predictions,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.metrics.true_positives_at_thresholds(
    labels,
    predictions,
    thresholds,
    weights=None,
    metrics_collections=None,
    updates_collections=None,
    name=None
)
",[],[]
"tf.compat.v1.min_max_variable_partitioner(
    max_partitions=1,
    axis=0,
    min_slice_size=(256 << 10),
    bytes_per_string_element=16
)
",[],[]
"tf.math.minimum(
    x, y, name=None
)
",[],"x = tf.constant([0., 0., 0., 0.])
y = tf.constant([-5., -2., 0., 3.])
tf.math.minimum(x, y)
<tf.Tensor: shape=(4,), dtype=float32, numpy=array([-5., -2., 0., 0.], dtype=float32)>"
"tf.compat.v1.mixed_precision.DynamicLossScale(
    initial_loss_scale=(2 ** 15), increment_period=2000, multiplier=2.0
)
","[['Inherits From: ', 'LossScale']]","@classmethod
from_config(
    config
)
"
"tf.compat.v1.mixed_precision.FixedLossScale(
    loss_scale_value
)
","[['Inherits From: ', 'LossScale']]","@classmethod
from_config(
    config
)
"
"tf.compat.v1.mixed_precision.MixedPrecisionLossScaleOptimizer(
    opt, loss_scale
)
","[['Inherits From: ', 'Optimizer']]","loss = ...
loss *= loss_scale
grads = gradients(loss, vars)
grads /= loss_scale
"
"tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(
    opt, loss_scale='dynamic'
)
",[],"model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(64, activation='softmax'),
])

opt = tf.keras.optimizers.SGD()
opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)
model.compile(loss=""mse"", optimizer=opt)

x_train = np.random.random((1024, 64))
y_train = np.random.random((1024, 64))
model.fit(x_train, y_train)
"
"tf.compat.v1.mixed_precision.DynamicLossScale(
    initial_loss_scale=(2 ** 15), increment_period=2000, multiplier=2.0
)
","[['Inherits From: ', 'LossScale']]","@classmethod
from_config(
    config
)
"
"tf.compat.v1.mixed_precision.FixedLossScale(
    loss_scale_value
)
","[['Inherits From: ', 'LossScale']]","@classmethod
from_config(
    config
)
"
"tf.mlir.experimental.convert_function(
    concrete_function,
    pass_pipeline='tf-standard-pipeline',
    show_debug_info=False
)
",[],"@tf.function
def add(a, b):
  return a + b"
"tf.mlir.experimental.convert_graph_def(
    graph_def,
    pass_pipeline='tf-standard-pipeline',
    show_debug_info=False
)
",[],[]
"tf.math.floormod(
    x, y, name=None
)
",[],[]
"tf.compat.v1.model_variables(
    scope=None
)
",[],[]
"tf.compat.v1.moving_average_variables(
    scope=None
)
",[],[]
"tf.compat.v1.multinomial(
    logits, num_samples, seed=None, name=None, output_dtype=None
)
",[],"# samples has shape [1, 5], where each value is either 0 or 1 with equal
# probability.
samples = tf.random.categorical(tf.math.log([[0.5, 0.5]]), 5)
"
"tf.math.multiply(
    x, y, name=None
)
",[],"x = tf.constant(([1, 2, 3, 4]))
tf.math.multiply(x, x)
<tf.Tensor: shape=(4,), dtype=..., numpy=array([ 1,  4,  9, 16], dtype=int32)>"
"tf.compat.v1.keras.backend.name_scope(
    name, default_name=None, values=None
)
",[],"def my_op(a, b, c, name=None):
  with tf.name_scope(name, ""MyOp"", [a, b, c]) as scope:
    a = tf.convert_to_tensor(a, name=""a"")
    b = tf.convert_to_tensor(b, name=""b"")
    c = tf.convert_to_tensor(c, name=""c"")
    # Define some computation that uses `a`, `b`, and `c`.
    return foo_op(..., name=scope)
"
"tf.math.negative(
    x, name=None
)
","[[None, '\n']]",[]
"tf.nest.assert_same_structure(
    nest1, nest2, check_types=True, expand_composites=False
)
",[],[]
"tf.nest.flatten(
    structure, expand_composites=False
)
",[],[]
"tf.nest.is_nested(
    seq
)
",[],[]
"tf.nest.map_structure(
    func, *structure, **kwargs
)
","[['Creates a new structure by applying ', 'func', ' to each atom in ', 'structure', '.']]","a = {""hello"": 24, ""world"": 76}
tf.nest.map_structure(lambda p: p * 2, a)
{'hello': 48, 'world': 152}"
"tf.nest.pack_sequence_as(
    structure, flat_sequence, expand_composites=False
)
",[],[]
"tf.random.all_candidate_sampler(
    true_classes, num_true, num_sampled, unique, seed=None, name=None
)
",[],[]
"tf.math.approx_max_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","[['Returns max ', 'k', ' values and their indices of the input ', 'operand', ' in an approximate manner.']]","import tensorflow as tf
@tf.function(jit_compile=True)
def mips(qy, db, k=10, recall_target=0.95):
  dists = tf.einsum('ik,jk->ij', qy, db)
  # returns (f32[qy_size, k], i32[qy_size, k])
  return tf.nn.approx_max_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
dot_products, neighbors = mips(qy, db, k=20)"
"tf.math.approx_min_k(
    operand,
    k,
    reduction_dimension=-1,
    recall_target=0.95,
    reduction_input_size_override=-1,
    aggregate_to_topk=True,
    name=None
)
","[['Returns min ', 'k', ' values and their indices of the input ', 'operand', ' in an approximate manner.']]","import tensorflow as tf
@tf.function(jit_compile=True)
def l2_ann(qy, db, half_db_norms, k=10, recall_target=0.95):
  dists = half_db_norms - tf.einsum('ik,jk->ij', qy, db)
  return tf.nn.approx_min_k(dists, k=k, recall_target=recall_target)
qy = tf.random.uniform((256,128))
db = tf.random.uniform((2048,128))
half_db_norms = tf.norm(db, axis=1) / 2
dists, neighbors = l2_ann(qy, db, half_db_norms)"
"tf.nn.atrous_conv2d(
    value, filters, rate, padding, name=None
)
",[],"output[batch, height, width, out_channel] =
    sum_{dheight, dwidth, in_channel} (
        filters[dheight, dwidth, in_channel, out_channel] *
        value[batch, height + rate*dheight, width + rate*dwidth, in_channel]
    )
"
"tf.nn.atrous_conv2d_transpose(
    value, filters, output_shape, rate, padding, name=None
)
","[['The transpose of ', 'atrous_conv2d', '.']]",[]
"tf.compat.v1.nn.avg_pool(
    value,
    ksize,
    strides,
    padding,
    data_format='NHWC',
    name=None,
    input=None
)
",[],[]
"tf.nn.avg_pool1d(
    input, ksize, strides, padding, data_format='NWC', name=None
)
",[],[]
"tf.compat.v1.nn.avg_pool(
    value,
    ksize,
    strides,
    padding,
    data_format='NHWC',
    name=None,
    input=None
)
",[],[]
"tf.nn.avg_pool3d(
    input, ksize, strides, padding, data_format='NDHWC', name=None
)
",[],[]
"tf.nn.avg_pool(
    input, ksize, strides, padding, data_format=None, name=None
)
",[],[]
"tf.compat.v1.nn.batch_norm_with_global_normalization(
    t=None,
    m=None,
    v=None,
    beta=None,
    gamma=None,
    variance_epsilon=None,
    scale_after_normalization=None,
    name=None,
    input=None,
    mean=None,
    variance=None
)
",[],[]
"tf.nn.batch_normalization(
    x, mean, variance, offset, scale, variance_epsilon, name=None
)
","[[None, '\n']]",[]
"tf.nn.bias_add(
    value, bias, data_format=None, name=None
)
","[['Adds ', 'bias', ' to ', 'value', '.']]",[]
"tf.compat.v1.nn.bidirectional_dynamic_rnn(
    cell_fw,
    cell_bw,
    inputs,
    sequence_length=None,
    initial_state_fw=None,
    initial_state_bw=None,
    dtype=None,
    parallel_iterations=None,
    swap_memory=False,
    time_major=False,
    scope=None
)
",[],[]
"tf.nn.collapse_repeated(
    labels, seq_length, name=None
)
",[],[]
"tf.nn.compute_accidental_hits(
    true_classes, sampled_candidates, num_true, seed=None, name=None
)
","[['Compute the position ids in ', 'sampled_candidates', ' matching ', 'true_classes', '.']]",[]
"tf.nn.compute_average_loss(
    per_example_loss, sample_weight=None, global_batch_size=None
)
",[],"with strategy.scope():
  def compute_loss(labels, predictions, sample_weight=None):

    # If you are using a `Loss` class instead, set reduction to `NONE` so that
    # we can do the reduction afterwards and divide by global batch size.
    per_example_loss = tf.keras.losses.sparse_categorical_crossentropy(
        labels, predictions)

    # Compute loss that is scaled by sample_weight and by global batch size.
    return tf.nn.compute_average_loss(
        per_example_loss,
        sample_weight=sample_weight,
        global_batch_size=GLOBAL_BATCH_SIZE)
"
"tf.compat.v1.nn.conv1d(
    value=None,
    filters=None,
    stride=None,
    padding=None,
    use_cudnn_on_gpu=None,
    data_format=None,
    name=None,
    input=None,
    dilations=None
)
","[[None, '\n'], ['Computes a 1-D convolution of input with rank ', '>=3', ' and a ', '3-D', ' filter. (deprecated argument values) (deprecated argument values)']]",[]
"tf.nn.conv1d_transpose(
    input,
    filters,
    output_shape,
    strides,
    padding='SAME',
    data_format='NWC',
    dilations=None,
    name=None
)
","[['The transpose of ', 'conv1d', '.']]",[]
"tf.compat.v1.nn.conv2d(
    input,
    filter=None,
    strides=None,
    padding=None,
    use_cudnn_on_gpu=True,
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None,
    filters=None
)
","[['Computes a 2-D convolution given 4-D ', 'input', ' and ', 'filter', ' tensors.']]","output[b, i, j, k] =
    sum_{di, dj, q} input[b, strides[1] * i + di, strides[2] * j + dj, q]

                    * filter[di, dj, q, k]
"
"tf.compat.v1.nn.conv2d_backprop_filter(
    input,
    filter_sizes,
    out_backprop,
    strides,
    padding,
    use_cudnn_on_gpu=True,
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.compat.v1.nn.conv2d_backprop_input(
    input_sizes,
    filter=None,
    out_backprop=None,
    strides=None,
    padding=None,
    use_cudnn_on_gpu=True,
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None,
    filters=None
)
",[],[]
"tf.compat.v1.nn.conv2d_transpose(
    value=None,
    filter=None,
    output_shape=None,
    strides=None,
    padding='SAME',
    data_format='NHWC',
    name=None,
    input=None,
    filters=None,
    dilations=None
)
","[['The transpose of ', 'conv2d', '.']]",[]
"tf.compat.v1.nn.conv3d(
    input,
    filter=None,
    strides=None,
    padding=None,
    data_format='NDHWC',
    dilations=[1, 1, 1, 1, 1],
    name=None,
    filters=None
)
","[['Computes a 3-D convolution given 5-D ', 'input', ' and ', 'filter', ' tensors.']]",[]
"tf.compat.v1.nn.conv3d_backprop_filter(
    input,
    filter_sizes,
    out_backprop,
    strides,
    padding,
    data_format='NDHWC',
    dilations=[1, 1, 1, 1, 1],
    name=None
)
",[],[]
"tf.compat.v1.nn.conv3d_backprop_filter(
    input,
    filter_sizes,
    out_backprop,
    strides,
    padding,
    data_format='NDHWC',
    dilations=[1, 1, 1, 1, 1],
    name=None
)
",[],[]
"tf.compat.v1.nn.conv3d_transpose(
    value,
    filter=None,
    output_shape=None,
    strides=None,
    padding='SAME',
    data_format='NDHWC',
    name=None,
    input=None,
    filters=None,
    dilations=None
)
","[['The transpose of ', 'conv3d', '.']]",[]
"tf.nn.conv_transpose(
    input,
    filters,
    output_shape,
    strides,
    padding='SAME',
    data_format=None,
    dilations=None,
    name=None
)
","[['The transpose of ', 'convolution', '.']]",[]
"tf.compat.v1.nn.convolution(
    input,
    filter,
    padding,
    strides=None,
    dilation_rate=None,
    name=None,
    data_format=None,
    filters=None,
    dilations=None
)
",[],"output[b, x[0], ..., x[N-1], k] =
    sum_{z[0], ..., z[N-1], q}
        filter[z[0], ..., z[N-1], q, k] *
        padded_input[b,
                     x[0]*strides[0] + dilation_rate[0]*z[0],
                     ...,
                     x[N-1]*strides[N-1] + dilation_rate[N-1]*z[N-1],
                     q]
"
"tf.compat.v1.nn.crelu(
    features, name=None, axis=-1
)
",[],[]
"tf.compat.v1.nn.ctc_beam_search_decoder(
    inputs, sequence_length, beam_width=100, top_paths=1, merge_repeated=True
)
",[],[]
"tf.nn.ctc_beam_search_decoder(
    inputs, sequence_length, beam_width=100, top_paths=1
)
",[],[]
"tf.nn.ctc_greedy_decoder(
    inputs, sequence_length, merge_repeated=True, blank_index=None
)
",[],"inf = float(""inf"")
logits = tf.constant([[[   0., -inf, -inf],
                       [ -2.3, -inf, -0.1]],
                      [[ -inf, -0.5, -inf],
                       [ -inf, -inf, -0.1]],
                      [[ -inf, -inf, -inf],
                       [ -0.1, -inf, -2.3]]])
seq_lens = tf.constant([2, 3])
outputs = tf.nn.ctc_greedy_decoder(
    logits,
    seq_lens,
    blank_index=1)"
"tf.compat.v1.nn.ctc_loss(
    labels,
    inputs=None,
    sequence_length=None,
    preprocess_collapse_repeated=False,
    ctc_merge_repeated=True,
    ignore_longer_outputs_than_inputs=False,
    time_major=True,
    logits=None
)
",[],"sequence_length(b) <= time for all b

max(labels.indices(labels.indices[:, 1] == b, 2))
  <= sequence_length(b) for all b.
"
"tf.compat.v1.nn.ctc_loss_v2(
    labels,
    logits,
    label_length,
    logit_length,
    logits_time_major=True,
    unique=None,
    blank_index=None,
    name=None
)
",[],[]
"tf.nn.ctc_unique_labels(
    labels, name=None
)
","[['Get unique labels and indices for batched labels for ', 'tf.nn.ctc_loss', '.']]",[]
"tf.compat.v1.depth_to_space(
    input, block_size, name=None, data_format='NHWC'
)
",[],"x = [[[[1, 2, 3, 4]]]]

"
"tf.compat.v1.nn.depthwise_conv2d(
    input,
    filter,
    strides,
    padding,
    rate=None,
    name=None,
    data_format=None,
    dilations=None
)
",[],"output[b, i, j, k * channel_multiplier + q] = sum_{di, dj}
     filter[di, dj, k, q] * input[b, strides[1] * i + rate[0] * di,
                                     strides[2] * j + rate[1] * dj, k]
"
"tf.nn.depthwise_conv2d_backprop_filter(
    input,
    filter_sizes,
    out_backprop,
    strides,
    padding,
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.nn.depthwise_conv2d_backprop_input(
    input_sizes,
    filter,
    out_backprop,
    strides,
    padding,
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.compat.v1.nn.depthwise_conv2d_native(
    input,
    filter,
    strides,
    padding,
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
",[],"for k in 0..in_channels-1
  for q in 0..channel_multiplier-1
    output[b, i, j, k * channel_multiplier + q] =
      sum_{di, dj} input[b, strides[1] * i + di, strides[2] * j + dj, k] *
                        filter[di, dj, k, q]
"
"tf.nn.depthwise_conv2d_backprop_filter(
    input,
    filter_sizes,
    out_backprop,
    strides,
    padding,
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.nn.depthwise_conv2d_backprop_input(
    input_sizes,
    filter,
    out_backprop,
    strides,
    padding,
    data_format='NHWC',
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.compat.v1.nn.dilation2d(
    input,
    filter=None,
    strides=None,
    rates=None,
    padding=None,
    name=None,
    filters=None,
    dilations=None
)
","[['Computes the grayscale dilation of 4-D ', 'input', ' and 3-D ', 'filter', ' tensors.']]","output[b, y, x, c] =
   max_{dy, dx} input[b,
                      strides[1] * y + rates[1] * dy,
                      strides[2] * x + rates[2] * dx,
                      c] +
                filter[dy, dx, c]
"
"tf.compat.v1.nn.dropout(
    x, keep_prob=None, noise_shape=None, seed=None, name=None, rate=None
)
",[],[]
"tf.compat.v1.nn.dynamic_rnn(
    cell,
    inputs,
    sequence_length=None,
    initial_state=None,
    dtype=None,
    parallel_iterations=None,
    swap_memory=False,
    time_major=False,
    scope=None
)
","[['Creates a recurrent neural network specified by RNNCell ', 'cell', '. (deprecated)']]","# create 2 LSTMCells
rnn_layers = [tf.compat.v1.nn.rnn_cell.LSTMCell(size) for size in [128, 256]]

# create a RNN cell composed sequentially of a number of RNNCells
multi_rnn_cell = tf.compat.v1.nn.rnn_cell.MultiRNNCell(rnn_layers)

# 'outputs' is a tensor of shape [batch_size, max_time, 256]
# 'state' is a N-tuple where N is the number of LSTMCells containing a
# tf.nn.rnn_cell.LSTMStateTuple for each cell
outputs, state = tf.compat.v1.nn.dynamic_rnn(cell=multi_rnn_cell,
                                             inputs=data,
                                             dtype=tf.float32)
"
"tf.nn.elu(
    features, name=None
)
","[[None, '\n']]","tf.nn.elu(1.0)
<tf.Tensor: shape=(), dtype=float32, numpy=1.0>
tf.nn.elu(0.0)
<tf.Tensor: shape=(), dtype=float32, numpy=0.0>
tf.nn.elu(-1000.0)
<tf.Tensor: shape=(), dtype=float32, numpy=-1.0>"
"tf.compat.v1.nn.embedding_lookup(
    params,
    ids,
    partition_strategy='mod',
    name=None,
    validate_indices=True,
    max_norm=None
)
","[['Looks up embeddings for the given ', 'ids', ' from a list of tensors.']]",[]
"tf.compat.v1.nn.embedding_lookup_sparse(
    params,
    sp_ids,
    sp_weights,
    partition_strategy='mod',
    name=None,
    combiner=None,
    max_norm=None
)
",[],"  [0, 0]: id 1, weight 2.0
  [0, 1]: id 3, weight 0.5
  [1, 0]: id 0, weight 1.0
  [2, 3]: id 1, weight 3.0
"
"tf.compat.v1.nn.erosion2d(
    value, kernel, strides, rates, padding, name=None
)
","[['Computes the grayscale erosion of 4-D ', 'value', ' and 3-D ', 'kernel', ' tensors.']]","output[b, y, x, c] =
   min_{dy, dx} value[b,
                      strides[1] * y - rates[1] * dy,
                      strides[2] * x - rates[2] * dx,
                      c] -
                kernel[dy, dx, c]
"
"tf.nn.experimental.stateless_dropout(
    x, rate, seed, rng_alg=None, noise_shape=None, name=None
)
",[],"x = tf.ones([3,5])
tf.nn.experimental.stateless_dropout(x, rate=0.5, seed=[1, 0])
<tf.Tensor: shape=(3, 5), dtype=float32, numpy=
array([[2., 0., 2., 0., 0.],
       [0., 0., 2., 0., 2.],
       [0., 0., 0., 0., 2.]], dtype=float32)>"
"tf.random.fixed_unigram_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    vocab_file='',
    distortion=1.0,
    num_reserved_ids=0,
    num_shards=1,
    shard=0,
    unigrams=(),
    seed=None,
    name=None
)
",[],[]
"tf.compat.v1.nn.fractional_avg_pool(
    value,
    pooling_ratio,
    pseudo_random=False,
    overlapping=False,
    deterministic=False,
    seed=0,
    seed2=0,
    name=None
)
",[],[]
"tf.compat.v1.nn.fractional_max_pool(
    value,
    pooling_ratio,
    pseudo_random=False,
    overlapping=False,
    deterministic=False,
    seed=0,
    seed2=0,
    name=None
)
",[],[]
"tf.compat.v1.nn.fused_batch_norm(
    x,
    scale,
    offset,
    mean=None,
    variance=None,
    epsilon=0.001,
    data_format='NHWC',
    is_training=True,
    name=None,
    exponential_avg_factor=1.0
)
",[],[]
"tf.compat.v1.math.in_top_k(
    predictions, targets, k, name=None
)
","[[None, '\n'], ['Says whether the targets are in the top ', 'K', ' predictions.']]",[]
"tf.nn.l2_loss(
    t, name=None
)
",[],"output = sum(t ** 2) / 2
"
"tf.math.l2_normalize(
    x, axis=None, epsilon=1e-12, name=None, dim=None
)
","[['Normalizes along dimension ', 'axis', ' using an L2 norm. (deprecated arguments)']]","output = x / sqrt(max(sum(x**2), epsilon))
"
"tf.nn.leaky_relu(
    features, alpha=0.2, name=None
)
",[],[]
"tf.random.learned_unigram_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=None,
    name=None
)
",[],[]
"tf.nn.local_response_normalization(
    input, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None
)
",[],"sqr_sum[a, b, c, d] =
    sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)
output = input / (bias + alpha * sqr_sum) ** beta
"
"tf.nn.log_poisson_loss(
    targets, log_input, compute_full_loss=False, name=None
)
","[['Computes log Poisson loss given ', 'log_input', '.']]","  -log(exp(-x) * (x^z) / z!)
= -log(exp(-x) * (x^z)) + log(z!)
~ -log(exp(-x)) - log(x^z) [+ z * log(z) - z + 0.5 * log(2 * pi * z)]
    [ Note the second term is the Stirling's Approximation for log(z!).
      It is invariant to x and does not affect optimization, though
      important for correct relative loss comparisons. It is only
      computed when compute_full_loss == True. ]
= x - z * log(x) [+ z * log(z) - z + 0.5 * log(2 * pi * z)]
= exp(c) - z * c [+ z * log(z) - z + 0.5 * log(2 * pi * z)]
"
"tf.compat.v1.math.log_softmax(
    logits, axis=None, name=None, dim=None
)
",[],"logsoftmax = logits - log(reduce_sum(exp(logits), axis))
"
"tf.random.log_uniform_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=None,
    name=None
)
",[],[]
"tf.nn.local_response_normalization(
    input, depth_radius=5, bias=1, alpha=1, beta=0.5, name=None
)
",[],"sqr_sum[a, b, c, d] =
    sum(input[a, b, c, d - depth_radius : d + depth_radius + 1] ** 2)
output = input / (bias + alpha * sqr_sum) ** beta
"
"tf.compat.v1.nn.max_pool(
    value,
    ksize,
    strides,
    padding,
    data_format='NHWC',
    name=None,
    input=None
)
",[],[]
"tf.nn.max_pool1d(
    input, ksize, strides, padding, data_format='NWC', name=None
)
",[],[]
"tf.nn.max_pool2d(
    input, ksize, strides, padding, data_format='NHWC', name=None
)
",[],"x = tf.constant([[1., 2., 3., 4.],
                 [5., 6., 7., 8.],
                 [9., 10., 11., 12.]])
# Add the `batch` and `channels` dimensions.
x = x[tf.newaxis, :, :, tf.newaxis]
result = tf.nn.max_pool2d(x, ksize=(2, 2), strides=(2, 2),
                          padding=""VALID"")
result[0, :, :, 0]
<tf.Tensor: shape=(1, 2), dtype=float32, numpy=
array([[6., 8.]], dtype=float32)>"
"tf.nn.max_pool3d(
    input, ksize, strides, padding, data_format='NDHWC', name=None
)
",[],[]
"tf.nn.max_pool(
    input, ksize, strides, padding, data_format=None, name=None
)
",[],"matrix = tf.constant([
    [0, 0, 1, 7],
    [0, 2, 0, 0],
    [5, 2, 0, 0],
    [0, 0, 9, 8],
])
reshaped = tf.reshape(matrix, (1, 4, 4, 1))
tf.nn.max_pool(reshaped, ksize=2, strides=2, padding=""SAME"")
<tf.Tensor: shape=(1, 2, 2, 1), dtype=int32, numpy=
array([[[[2],
         [7]],
        [[5],
         [9]]]], dtype=int32)>"
"tf.compat.v1.nn.max_pool_with_argmax(
    input,
    ksize,
    strides,
    padding,
    data_format='NHWC',
    Targmax=None,
    name=None,
    output_dtype=None,
    include_batch_in_index=False
)
",[],[]
"tf.compat.v1.nn.moments(
    x, axes, shift=None, name=None, keep_dims=None, keepdims=None
)
","[['Calculate the mean and variance of ', 'x', '.']]",[]
"tf.compat.v1.nn.nce_loss(
    weights,
    biases,
    labels,
    inputs,
    num_sampled,
    num_classes,
    num_true=1,
    sampled_values=None,
    remove_accidental_hits=False,
    partition_strategy='mod',
    name='nce_loss'
)
",[],"if mode == ""train"":
  loss = tf.nn.nce_loss(
      weights=weights,
      biases=biases,
      labels=labels,
      inputs=inputs,
      ...,
      partition_strategy=""div"")
elif mode == ""eval"":
  logits = tf.matmul(inputs, tf.transpose(weights))
  logits = tf.nn.bias_add(logits, biases)
  labels_one_hot = tf.one_hot(labels, n_classes)
  loss = tf.nn.sigmoid_cross_entropy_with_logits(
      labels=labels_one_hot,
      logits=logits)
  loss = tf.reduce_sum(loss, axis=1)
"
"tf.nn.normalize_moments(
    counts, mean_ss, variance_ss, shift, name=None
)
",[],[]
"tf.compat.v1.nn.pool(
    input,
    window_shape,
    pooling_type,
    padding,
    dilation_rate=None,
    strides=None,
    name=None,
    data_format=None,
    dilations=None
)
",[],"output[b, x[0], ..., x[N-1], c] =
  REDUCE_{z[0], ..., z[N-1]}
    input[b,
          x[0] * strides[0] - pad_before[0] + dilation_rate[0]*z[0],
          ...
          x[N-1]*strides[N-1] - pad_before[N-1] + dilation_rate[N-1]*z[N-1],
          c],
"
"tf.compat.v1.nn.quantized_avg_pool(
    input, min_input, max_input, ksize, strides, padding, name=None
)
",[],[]
"tf.compat.v1.nn.quantized_conv2d(
    input,
    filter,
    min_input,
    max_input,
    min_filter,
    max_filter,
    strides,
    padding,
    out_type=tf.dtypes.qint32,
    dilations=[1, 1, 1, 1],
    name=None
)
",[],[]
"tf.compat.v1.nn.quantized_max_pool(
    input, min_input, max_input, ksize, strides, padding, name=None
)
",[],[]
"tf.compat.v1.nn.quantized_relu_x(
    features,
    max_value,
    min_features,
    max_features,
    out_type=tf.dtypes.quint8,
    name=None
)
","[['Computes Quantized Rectified Linear X: ', 'min(max(features, 0), max_value)']]",[]
"tf.compat.v1.nn.raw_rnn(
    cell, loop_fn, parallel_iterations=None, swap_memory=False, scope=None
)
","[['Creates an ', 'RNN', ' specified by RNNCell ', 'cell', ' and loop function ', 'loop_fn', '.']]","time = tf.constant(0, dtype=tf.int32)
(finished, next_input, initial_state, emit_structure, loop_state) = loop_fn(
    time=time, cell_output=None, cell_state=None, loop_state=None)
emit_ta = TensorArray(dynamic_size=True, dtype=initial_state.dtype)
state = initial_state
while not all(finished):
  (output, cell_state) = cell(next_input, state)
  (next_finished, next_input, next_state, emit, loop_state) = loop_fn(
      time=time + 1, cell_output=output, cell_state=cell_state,
      loop_state=loop_state)
  # Emit zeros and copy forward state for minibatch entries that are finished.
  state = tf.where(finished, state, next_state)
  emit = tf.where(finished, tf.zeros_like(emit_structure), emit)
  emit_ta = emit_ta.write(time, emit)
  # If any new minibatch entries are marked as finished, mark these.
  finished = tf.logical_or(finished, next_finished)
  time += 1
return (emit_ta, state, loop_state)
"
"tf.nn.relu(
    features, name=None
)
","[['Computes rectified linear: ', 'max(features, 0)', '.']]",">>> tf.nn.relu([-2., 0., 3.]).numpy()
array([0., 0., 3.], dtype=float32)
"
"tf.nn.relu6(
    features, name=None
)
","[['Computes Rectified Linear 6: ', 'min(max(features, 0), 6)', '.']]","x = tf.constant([-3.0, -1.0, 0.0, 6.0, 10.0], dtype=tf.float32)
y = tf.nn.relu6(x)
y.numpy()
array([0., 0., 0., 6., 6.], dtype=float32)"
"tf.compat.v1.nn.relu_layer(
    x, weights, biases, name=None
)
",[],[]
"tf.compat.v1.nn.rnn_cell.BasicLSTMCell(
    num_units,
    forget_bias=1.0,
    state_is_tuple=True,
    activation=None,
    reuse=None,
    name=None,
    dtype=None,
    **kwargs
)
","[['Deprecated:', ' Please use '], ['Inherits From: ', 'RNNCell', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]","apply(
    *args, **kwargs
)
"
"tf.compat.v1.nn.rnn_cell.BasicRNNCell(
    num_units, activation=None, reuse=None, name=None, dtype=None, **kwargs
)
","[['Inherits From: ', 'RNNCell', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]","apply(
    *args, **kwargs
)
"
"tf.compat.v1.nn.rnn_cell.DeviceWrapper(
    cell, device, **kwargs
)
","[['Inherits From: ', 'RNNCell', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]","apply(
    *args, **kwargs
)
"
"tf.compat.v1.nn.rnn_cell.DropoutWrapper(
    cell,
    input_keep_prob=1.0,
    output_keep_prob=1.0,
    state_keep_prob=1.0,
    variational_recurrent=False,
    input_size=None,
    dtype=None,
    seed=None,
    dropout_state_filter_visitor=None,
    **kwargs
)
","[['Inherits From: ', 'RNNCell', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]","def dropout_state_filter_visitor(s):
  # Never perform dropout on the c state.
  if isinstance(s, LSTMCellState):
    return LSTMCellState(c=False, h=True)
  elif isinstance(s, TensorArray):
    return False
  return True
"
"tf.compat.v1.nn.rnn_cell.GRUCell(
    num_units,
    activation=None,
    reuse=None,
    kernel_initializer=None,
    bias_initializer=None,
    name=None,
    dtype=None,
    **kwargs
)
","[['Inherits From: ', 'RNNCell', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]","apply(
    *args, **kwargs
)
"
"tf.compat.v1.nn.rnn_cell.LSTMCell(
    num_units,
    use_peepholes=False,
    cell_clip=None,
    initializer=None,
    num_proj=None,
    proj_clip=None,
    num_unit_shards=None,
    num_proj_shards=None,
    forget_bias=1.0,
    state_is_tuple=True,
    activation=None,
    reuse=None,
    name=None,
    dtype=None,
    **kwargs
)
","[['Inherits From: ', 'RNNCell', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]","apply(
    *args, **kwargs
)
"
"tf.compat.v1.nn.rnn_cell.LSTMStateTuple(
    c, h
)
","[['Tuple used by LSTM Cells for ', 'state_size', ', ', 'zero_state', ', and output state.']]",[]
"tf.compat.v1.nn.rnn_cell.MultiRNNCell(
    cells, state_is_tuple=True
)
","[['Inherits From: ', 'RNNCell', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]","num_units = [128, 64]
cells = [BasicLSTMCell(num_units=n) for n in num_units]
stacked_rnn_cell = MultiRNNCell(cells)
"
"tf.compat.v1.nn.rnn_cell.RNNCell(
    trainable=True, name=None, dtype=None, **kwargs
)
","[['Inherits From: ', 'Layer', ', ', 'Layer', ', ', 'Module']]","apply(
    *args, **kwargs
)
"
"tf.compat.v1.nn.rnn_cell.ResidualWrapper(
    cell, residual_fn=None, **kwargs
)
","[['Inherits From: ', 'RNNCell', ', ', 'Layer', ', ', 'Layer', ', ', 'Module']]","apply(
    *args, **kwargs
)
"
"tf.compat.v1.nn.safe_embedding_lookup_sparse(
    embedding_weights,
    sparse_ids,
    sparse_weights=None,
    combiner='mean',
    default_id=None,
    name=None,
    partition_strategy='div',
    max_norm=None
)
",[],"  [0, 0]: id 1, weight 2.0
  [0, 1]: id 3, weight 0.5
  [1, 0]: id -1, weight 1.0
  [2, 3]: id 1, weight 3.0
"
"tf.compat.v1.nn.sampled_softmax_loss(
    weights,
    biases,
    labels,
    inputs,
    num_sampled,
    num_classes,
    num_true=1,
    sampled_values=None,
    remove_accidental_hits=True,
    partition_strategy='mod',
    name='sampled_softmax_loss',
    seed=None
)
",[],"if mode == ""train"":
  loss = tf.nn.sampled_softmax_loss(
      weights=weights,
      biases=biases,
      labels=labels,
      inputs=inputs,
      ...,
      partition_strategy=""div"")
elif mode == ""eval"":
  logits = tf.matmul(inputs, tf.transpose(weights))
  logits = tf.nn.bias_add(logits, biases)
  labels_one_hot = tf.one_hot(labels, n_classes)
  loss = tf.nn.softmax_cross_entropy_with_logits(
      labels=labels_one_hot,
      logits=logits)
"
"tf.nn.scale_regularization_loss(
    regularization_loss
)
",[],"with strategy.scope():
  def compute_loss(self, label, predictions):
    per_example_loss = tf.keras.losses.sparse_categorical_crossentropy(
        labels, predictions)

    # Compute loss that is scaled by sample_weight and by global batch size.
    loss = tf.nn.compute_average_loss(
        per_example_loss,
        sample_weight=sample_weight,
        global_batch_size=GLOBAL_BATCH_SIZE)

    # Add scaled regularization losses.
    loss += tf.nn.scale_regularization_loss(tf.nn.l2_loss(weights))
    return loss
"
"tf.nn.selu(
    features, name=None
)
","[['Computes scaled exponential linear: ', 'scale * alpha * (exp(features) - 1)']]",[]
"tf.compat.v1.nn.separable_conv2d(
    input,
    depthwise_filter,
    pointwise_filter,
    strides,
    padding,
    rate=None,
    name=None,
    data_format=None,
    dilations=None
)
",[],"output[b, i, j, k] = sum_{di, dj, q, r}
    input[b, strides[1] * i + di, strides[2] * j + dj, q] *
    depthwise_filter[di, dj, q, r] *
    pointwise_filter[0, 0, q * channel_multiplier + r, k]
"
"tf.math.sigmoid(
    x, name=None
)
","[[None, '\n'], ['Computes sigmoid of ', 'x', ' element-wise.']]","x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32,
numpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>"
"tf.compat.v1.nn.sigmoid_cross_entropy_with_logits(
    _sentinel=None, labels=None, logits=None, name=None
)
","[[None, '\n'], ['Computes sigmoid cross entropy given ', 'logits', '.']]","  z * -log(sigmoid(x)) + (1 - z) * -log(1 - sigmoid(x))
= z * -log(1 / (1 + exp(-x))) + (1 - z) * -log(exp(-x) / (1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (-log(exp(-x)) + log(1 + exp(-x)))
= z * log(1 + exp(-x)) + (1 - z) * (x + log(1 + exp(-x))
= (1 - z) * x + log(1 + exp(-x))
= x - x * z + log(1 + exp(-x))
"
"tf.nn.silu(
    features, beta=1.0
)
","[['Computes the SiLU or Swish activation function: ', 'x * sigmoid(beta * x)', '.']]",[]
"tf.compat.v1.math.softmax(
    logits, axis=None, name=None, dim=None
)
",[],"softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis, keepdims=True)
"
"tf.compat.v1.nn.softmax_cross_entropy_with_logits(
    _sentinel=None, labels=None, logits=None, dim=-1, name=None, axis=None
)
","[['Computes softmax cross entropy between ', 'logits', ' and ', 'labels', '. (deprecated)']]",[]
"tf.compat.v1.nn.softmax_cross_entropy_with_logits_v2(
    labels, logits, axis=None, name=None, dim=None
)
","[['Computes softmax cross entropy between ', 'logits', ' and ', 'labels', '. (deprecated arguments)']]",[]
"tf.math.softplus(
    features, name=None
)
","[['Computes elementwise softplus: ', 'softplus(x) = log(exp(x) + 1)', '.']]","import tensorflow as tf
tf.math.softplus(tf.range(0, 2, dtype=tf.float32)).numpy()
array([0.6931472, 1.3132616], dtype=float32)"
"tf.nn.softsign(
    features, name=None
)
","[['Computes softsign: ', 'features / (abs(features) + 1)', '.']]",[]
"tf.compat.v1.space_to_batch(
    input, paddings, block_size=None, name=None, block_shape=None
)
",[],"[batch*block_size*block_size, height_pad/block_size, width_pad/block_size,
 depth]
"
"tf.compat.v1.space_to_depth(
    input, block_size, name=None, data_format='NHWC'
)
",[],"x = [[[[1], [2]],
      [[3], [4]]]]
"
"tf.compat.v1.nn.sparse_softmax_cross_entropy_with_logits(
    _sentinel=None, labels=None, logits=None, name=None
)
","[['Computes sparse softmax cross entropy between ', 'logits', ' and ', 'labels', '.']]",[]
"tf.compat.v1.nn.static_bidirectional_rnn(
    cell_fw,
    cell_bw,
    inputs,
    initial_state_fw=None,
    initial_state_bw=None,
    dtype=None,
    sequence_length=None,
    scope=None
)
",[],[]
"tf.compat.v1.nn.static_rnn(
    cell,
    inputs,
    initial_state=None,
    dtype=None,
    sequence_length=None,
    scope=None
)
","[['Creates a recurrent neural network specified by RNNCell ', 'cell', '. (deprecated)']]","  state = cell.zero_state(...)
  outputs = []
  for input_ in inputs:
    output, state = cell(input_, state)
    outputs.append(output)
  return (outputs, state)
"
"tf.compat.v1.nn.static_state_saving_rnn(
    cell, inputs, state_saver, state_name, sequence_length=None, scope=None
)
",[],[]
"tf.compat.v1.nn.sufficient_statistics(
    x, axes, shift=None, keep_dims=None, name=None, keepdims=None
)
","[['Calculate the sufficient statistics for the mean and variance of ', 'x', '.']]","t = [[1, 2, 3], [4, 5, 6]]
sufficient_statistics(t, [1])
(<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(2,),
dtype=int32, numpy=array([ 6, 15], dtype=int32)>, <tf.Tensor: shape=(2,),
dtype=int32, numpy=array([14, 77], dtype=int32)>, None)
sufficient_statistics(t, [-1])
(<tf.Tensor: shape=(), dtype=int32, numpy=3>, <tf.Tensor: shape=(2,),
dtype=int32, numpy=array([ 6, 15], dtype=int32)>, <tf.Tensor: shape=(2,),
dtype=int32, numpy=array([14, 77], dtype=int32)>, None)"
"tf.nn.silu(
    features, beta=1.0
)
","[['Computes the SiLU or Swish activation function: ', 'x * sigmoid(beta * x)', '.']]",[]
"tf.math.tanh(
    x, name=None
)
","[['Computes hyperbolic tangent of ', 'x', ' element-wise.']]",[]
"tf.math.top_k(
    input, k=1, sorted=True, name=None
)
","[['Finds values and indices of the ', 'k', ' largest entries for the last dimension.']]","result = tf.math.top_k([1, 2, 98, 1, 1, 99, 3, 1, 3, 96, 4, 1],
                        k=3)
result.values.numpy()
array([99, 98, 96], dtype=int32)
result.indices.numpy()
array([5, 2, 9], dtype=int32)"
"tf.random.uniform_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=None,
    name=None
)
",[],[]
"tf.compat.v1.nn.weighted_cross_entropy_with_logits(
    labels=None, logits=None, pos_weight=None, name=None, targets=None
)
",[],"labels * -log(sigmoid(logits)) +
    (1 - labels) * -log(1 - sigmoid(logits))
"
"tf.compat.v1.nn.weighted_moments(
    x, axes, frequency_weights, name=None, keep_dims=None, keepdims=None
)
","[['Returns the frequency-weighted mean and variance of ', 'x', '.']]",[]
"tf.nn.with_space_to_batch(
    input,
    dilation_rate,
    padding,
    op,
    filter_shape=None,
    spatial_dims=None,
    data_format=None
)
","[['Performs ', 'op', ' on the space-to-batch representation of ', 'input', '.']]",[]
"tf.compat.v1.nn.xw_plus_b(
    x, weights, biases, name=None
)
",[],[]
"tf.math.zero_fraction(
    value, name=None
)
","[['Returns the fraction of zeros in ', 'value', '.']]","    z = tf.nn.relu(...)
    summ = tf.compat.v1.summary.scalar('sparsity', tf.nn.zero_fraction(z))
"
"tf.no_gradient(
    op_type
)
","[['Specifies that ops of type ', 'op_type', ' is not differentiable.']]","tf.no_gradient(""Size"")
"
"tf.no_op(
    name=None
)
",[],[]
"tf.compat.v1.no_regularizer(
    _
)
",[],[]
"tf.nondifferentiable_batch_function(
    num_batch_threads,
    max_batch_size,
    batch_timeout_micros,
    allowed_batch_sizes=None,
    max_enqueued_batches=10,
    autograph=True,
    enable_large_batch_splitting=True
)
",[],"@batch_function(1, 2, 3)
def layer(a):
  return tf.matmul(a, a)

b = layer(w)
"
"tf.compat.v1.norm(
    tensor,
    ord='euclidean',
    axis=None,
    keepdims=None,
    name=None,
    keep_dims=None
)
",[],[]
"tf.math.not_equal(
    x, y, name=None
)
",[],"x = tf.constant([2, 4])
y = tf.constant(2)
tf.math.not_equal(x, y)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.numpy_function(
    func, inp, Tout, stateful=True, name=None
)
",[],"def my_numpy_func(x):
  # x will be a numpy array with the contents of the input to the
  # tf.function
  return np.sinh(x)
@tf.function(input_signature=[tf.TensorSpec(None, tf.float32)])
def tf_function(input):
  y = tf.numpy_function(my_numpy_func, [input], tf.float32)
  return y * y
tf_function(tf.constant(1.))
<tf.Tensor: shape=(), dtype=float32, numpy=1.3810978>"
"tf.one_hot(
    indices,
    depth,
    on_value=None,
    off_value=None,
    axis=None,
    dtype=None,
    name=None
)
",[],"  features x depth if axis == -1
  depth x features if axis == 0
"
"tf.ones(
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
",[],"tf.ones([3, 4], tf.int32)
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
array([[1, 1, 1, 1],
       [1, 1, 1, 1],
       [1, 1, 1, 1]], dtype=int32)>"
"tf.compat.v1.keras.initializers.Ones(
    dtype=tf.dtypes.float32
)
",[],">>> initializer = tf.compat.v1.keras.initializers.ones()
>>> initializer((1, 1))
<tf.Tensor: shape=(1, 1), dtype=float32, numpy=array([[1.]], dtype=float32)>
"
"tf.compat.v1.ones_like(
    tensor, dtype=None, name=None, optimize=True
)
",[],"tensor = tf.constant([[1, 2, 3], [4, 5, 6]])
tf.ones_like(tensor)  # [[1, 1, 1], [1, 1, 1]]
"
"tf.compat.v1.keras.initializers.Orthogonal(
    gain=1.0,
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"@classmethod
from_config(
    config
)
"
"tf.compat.v1.pad(
    tensor, paddings, mode='CONSTANT', name=None, constant_values=0
)
",[],"t = tf.constant([[1, 2, 3], [4, 5, 6]])
paddings = tf.constant([[1, 1,], [2, 2]])
# 'constant_values' is 0.
# rank of 't' is 2.
tf.pad(t, paddings, ""CONSTANT"")  # [[0, 0, 0, 0, 0, 0, 0],
                                 #  [0, 0, 1, 2, 3, 0, 0],
                                 #  [0, 0, 4, 5, 6, 0, 0],
                                 #  [0, 0, 0, 0, 0, 0, 0]]

tf.pad(t, paddings, ""REFLECT"")  # [[6, 5, 4, 5, 6, 5, 4],
                                #  [3, 2, 1, 2, 3, 2, 1],
                                #  [6, 5, 4, 5, 6, 5, 4],
                                #  [3, 2, 1, 2, 3, 2, 1]]

tf.pad(t, paddings, ""SYMMETRIC"")  # [[2, 1, 1, 2, 3, 3, 2],
                                  #  [2, 1, 1, 2, 3, 3, 2],
                                  #  [5, 4, 4, 5, 6, 6, 5],
                                  #  [5, 4, 4, 5, 6, 6, 5]]
"
"tf.parallel_stack(
    values, name='parallel_stack'
)
","[['Stacks a list of rank-', 'R', ' tensors into one rank-', '(R+1)', ' tensor in parallel.']]","x = tf.constant([1, 4])
y = tf.constant([2, 5])
z = tf.constant([3, 6])
tf.parallel_stack([x, y, z])  # [[1, 4], [2, 5], [3, 6]]
"
"tf.compat.v1.parse_example(
    serialized, features, name=None, example_names=None
)
","[['Parses ', 'Example', ' protos into a ', 'dict', ' of tensors.']]","serialized = [
  features
    { feature { key: ""ft"" value { float_list { value: [1.0, 2.0] } } } },
  features
    { feature []},
  features
    { feature { key: ""ft"" value { float_list { value: [3.0] } } }
]
"
"tf.compat.v1.parse_single_example(
    serialized, features, name=None, example_names=None
)
","[['Parses a single ', 'Example', ' proto.']]",[]
"tf.io.parse_single_sequence_example(
    serialized,
    context_features=None,
    sequence_features=None,
    example_name=None,
    name=None
)
","[['Parses a single ', 'SequenceExample', ' proto.']]",[]
"tf.io.parse_tensor(
    serialized, out_type, name=None
)
",[],[]
"tf.compat.v1.placeholder(
    dtype, shape=None, name=None
)
",[],[]
"tf.compat.v1.placeholder_with_default(
    input, shape, name=None
)
","[['A placeholder op that passes through ', 'input', ' when its output is not fed.']]","@tf.function
def f():
  x = tf.compat.v1.placeholder_with_default(
      tf.constant([[1., 2., 3.], [4., 5., 6.]]), [None, 3])
  y = tf.constant([[1.],[2.], [3.]])
  z = tf.matmul(x, y)
  assert z.shape[0] == None
  assert z.shape[1] == 1"
"tf.math.polygamma(
    a, x, name=None
)
","[[None, '\n']]",[]
"tf.math.pow(
    x, y, name=None
)
","[[None, '\n']]","x = tf.constant([[2, 2], [3, 3]])
y = tf.constant([[8, 16], [2, 3]])
tf.pow(x, y)  # [[256, 65536], [9, 27]]
"
"tf.print(
    *inputs, **kwargs
)
",[],"tensor = tf.range(10)
tf.print(tensor, output_stream=sys.stderr)
"
"tf.compat.v1.profiler.ProfileOptionBuilder(
    options=None
)
",[],"# Users can use pre-built options:
opts = (
    tf.profiler.ProfileOptionBuilder.trainable_variables_parameter())

# Or, build your own options:
opts = (tf.compat.v1.profiler.ProfileOptionBuilder()
    .with_max_depth(10)
    .with_min_micros(1000)
    .select(['accelerator_micros'])
    .with_stdout_output()
    .build()

# Or customize the pre-built options:
opts = (tf.compat.v1.profiler.ProfileOptionBuilder(
    tf.profiler.ProfileOptionBuilder.time_and_memory())
    .with_displaying_options(show_name_regexes=['.*rnn.*'])
    .build())

# Finally, profiling with the options:
_ = tf.compat.v1.profiler.profile(tf.compat.v1.get_default_graph(),
                        run_meta=run_meta,
                        cmd='scope',
                        options=opts)
"
"tf.compat.v1.profiler.Profiler(
    graph=None, op_log=None
)
",[],"Typical use case:
  # Currently we are only allowed to create 1 profiler per process.
  profiler = Profiler(sess.graph)

  for i in range(total_steps):
    if i % 10000 == 0:
      run_meta = tf.compat.v1.RunMetadata()
      _ = sess.run(...,
                   options=tf.compat.v1.RunOptions(
                       trace_level=tf.RunOptions.FULL_TRACE),
                   run_metadata=run_meta)
      profiler.add_step(i, run_meta)

      # Profile the parameters of your model.
      profiler.profile_name_scope(options=(option_builder.ProfileOptionBuilder
          .trainable_variables_parameter()))

      # Or profile the timing of your model operations.
      opts = option_builder.ProfileOptionBuilder.time_and_memory()
      profiler.profile_operations(options=opts)

      # Or you can generate a timeline:
      opts = (option_builder.ProfileOptionBuilder(
              option_builder.ProfileOptionBuilder.time_and_memory())
              .with_step(i)
              .with_timeline_output(filename).build())
      profiler.profile_graph(options=opts)
    else:
      _ = sess.run(...)
  # Auto detect problems and generate advice.
  profiler.advise()
"
"tf.compat.v1.profiler.advise(
    graph=None, run_meta=None, options=_DEFAULT_ADVISE_OPTIONS
)
",[],[]
"tf.compat.v1.profiler.profile(
    graph=None,
    run_meta=None,
    op_log=None,
    cmd='scope',
    options=_DEFAULT_PROFILE_OPTIONS
)
",[],[]
"tf.compat.v1.profiler.write_op_log(
    graph, log_dir, op_log=None, run_meta=None, add_trace=True
)
",[],[]
"tf.compat.v1.py_func(
    func, inp, Tout, stateful=True, name=None
)
",[],"def fn_using_numpy(x):
  x[0] = 0.
  return x
tf.compat.v1.py_func(fn_using_numpy, inp=[tf.constant([1., 2.])],
    Tout=tf.float32, stateful=False)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([0., 2.], dtype=float32)>"
"tf.py_function(
    func, inp, Tout, name=None
)
",[],"def log_huber(x, m):
  if tf.abs(x) <= m:
    return x**2
  else:
    return m**2 * (1 - 2 * tf.math.log(m) + tf.math.log(x**2))

x = tf.constant(1.0)
m = tf.constant(2.0)

with tf.GradientTape() as t:
  t.watch([x, m])
  y = tf.py_function(func=log_huber, inp=[x, m], Tout=tf.float32)

dy_dx = t.gradient(y, x)
assert dy_dx.numpy() == 2.0
"
"tf.io.TFRecordOptions(
    compression_type=None,
    flush_mode=None,
    input_buffer_size=None,
    output_buffer_size=None,
    window_bits=None,
    compression_level=None,
    compression_method=None,
    mem_level=None,
    compression_strategy=None
)
",[],"@classmethod
get_compression_type_string(
    options
)
"
"tf.io.TFRecordWriter(
    path, options=None
)
",[],"import tempfile
example_path = os.path.join(tempfile.gettempdir(), ""example.tfrecords"")
np.random.seed(0)"
"tf.compat.v1.io.tf_record_iterator(
    path, options=None
)
",[],[]
"tf.linalg.qr(
    input, full_matrices=False, name=None
)
",[],"# a is a tensor.
# q is a tensor of orthonormal matrices.
# r is a tensor of upper triangular matrices.
q, r = qr(a)
q_full, r_full = qr(a, full_matrices=True)
"
"tf.quantization.dequantize(
    input,
    min_range,
    max_range,
    mode='MIN_COMBINED',
    name=None,
    axis=None,
    narrow_range=False,
    dtype=tf.dtypes.float32
)
",[],"if T == qint8: in[i] += (range(T) + 1)/ 2.0
out[i] = min_range + (in[i]* (max_range - min_range) / range(T))
"
"tf.quantization.fake_quant_with_min_max_args(
    inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_args_gradient(
    gradients, inputs, min=-6, max=6, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_vars(
    inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_vars_gradient(
    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_vars_per_channel(
    inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.fake_quant_with_min_max_vars_per_channel_gradient(
    gradients, inputs, min, max, num_bits=8, narrow_range=False, name=None
)
",[],[]
"tf.quantization.quantize(
    input,
    min_range,
    max_range,
    T,
    mode='MIN_COMBINED',
    round_mode='HALF_AWAY_FROM_ZERO',
    name=None,
    narrow_range=False,
    axis=None,
    ensure_minimum_range=0.01
)
",[],"out[i] = (in[i] - min_range) * range(T) / (max_range - min_range)
if T == qint8: out[i] -= (range(T) + 1) / 2.0
"
"tf.quantization.quantize_and_dequantize(
    input,
    input_min,
    input_max,
    signed_input=True,
    num_bits=8,
    range_given=False,
    round_mode='HALF_TO_EVEN',
    name=None,
    narrow_range=False,
    axis=None
)
",[],[]
"tf.quantization.quantize_and_dequantize_v2(
    input,
    input_min,
    input_max,
    signed_input=True,
    num_bits=8,
    range_given=False,
    round_mode='HALF_TO_EVEN',
    name=None,
    narrow_range=False,
    axis=None
)
",[],"def getQuantizeOp(input):
    input_tensor = tf.placeholder(tf.float32, shape=[4, 4])
    net = tf.quantization.quantize_and_dequantize(input,
                                                  input_min=min_threshold,
                                                  input_max=max_threshold,
                                                  range_given=True)

To simulate v1 behavior:

def testDecomposeQuantizeDequantize(self):
    def f(input_tensor):
      return tf.quantization.quantize_and_dequantize_v2(input_tensor,
                                                        input_min = 5.0,
                                                        input_max= -10.0,
                                                        range_given=True)
    input_tensor = tf.placeholder(tf.float32, shape=[4, 4])
    net = tf.grad_pass_through(f)(input_tensor)
"
"tf.quantization.quantized_concat(
    concat_dim, values, input_mins, input_maxes, name=None
)
",[],[]
"tf.quantization.quantize(
    input,
    min_range,
    max_range,
    T,
    mode='MIN_COMBINED',
    round_mode='HALF_AWAY_FROM_ZERO',
    name=None,
    narrow_range=False,
    axis=None,
    ensure_minimum_range=0.01
)
",[],"out[i] = (in[i] - min_range) * range(T) / (max_range - min_range)
if T == qint8: out[i] -= (range(T) + 1) / 2.0
"
"tf.compat.v1.quantize_v2(
    input,
    min_range,
    max_range,
    T,
    mode='MIN_COMBINED',
    name=None,
    round_mode='HALF_AWAY_FROM_ZERO',
    narrow_range=False,
    axis=None,
    ensure_minimum_range=0.01
)
","[['Please use ', 'tf.quantization.quantize', ' instead.']]",[]
"tf.quantization.quantized_concat(
    concat_dim, values, input_mins, input_maxes, name=None
)
",[],[]
"tf.queue.FIFOQueue(
    capacity,
    dtypes,
    shapes=None,
    names=None,
    shared_name=None,
    name='fifo_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.queue.PaddingFIFOQueue(
    capacity,
    dtypes,
    shapes,
    names=None,
    shared_name=None,
    name='padding_fifo_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.queue.PriorityQueue(
    capacity,
    types,
    shapes=None,
    names=None,
    shared_name=None,
    name='priority_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.queue.QueueBase(
    dtypes, shapes, names, queue_ref
)
",[],"close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.queue.RandomShuffleQueue(
    capacity,
    min_after_dequeue,
    dtypes,
    shapes=None,
    names=None,
    seed=None,
    shared_name=None,
    name='random_shuffle_queue'
)
","[['Inherits From: ', 'QueueBase']]","close(
    cancel_pending_enqueues=False, name=None
)
"
"tf.compat.v1.ragged.RaggedTensorValue(
    values, row_splits
)
","[['Represents the value of a ', 'RaggedTensor', '.']]","to_list()
"
"tf.ragged.boolean_mask(
    data, mask, name=None
)
","[['Applies a boolean mask to ', 'data', ' without flattening the mask dimensions.']]","# Aliases for True & False so data and mask line up.
T, F = (True, False)"
"tf.ragged.constant(
    pylist,
    dtype=None,
    ragged_rank=None,
    inner_shape=None,
    name=None,
    row_splits_dtype=tf.dtypes.int64
)
",[],"tf.ragged.constant([[1, 2], [3], [4, 5, 6]])
<tf.RaggedTensor [[1, 2], [3], [4, 5, 6]]>"
"tf.compat.v1.ragged.constant_value(
    pylist,
    dtype=None,
    ragged_rank=None,
    inner_shape=None,
    row_splits_dtype='int64'
)
",[],"tf.compat.v1.ragged.constant_value([[1, 2], [3], [4, 5, 6]])
tf.RaggedTensorValue(values=array([1, 2, 3, 4, 5, 6]),
                     row_splits=array([0, 2, 3, 6]))"
"tf.ragged.cross(
    inputs, name=None
)
",[],"tf.ragged.cross([tf.ragged.constant([['a'], ['b', 'c']]),
                 tf.ragged.constant([['d'], ['e']]),
                 tf.ragged.constant([['f'], ['g']])])
<tf.RaggedTensor [[b'a_X_d_X_f'], [b'b_X_e_X_g', b'c_X_e_X_g']]>"
"tf.ragged.cross_hashed(
    inputs, num_buckets=0, hash_key=None, name=None
)
",[],"tf.ragged.cross_hashed([tf.ragged.constant([['a'], ['b', 'c']]),
                        tf.ragged.constant([['d'], ['e']]),
                        tf.ragged.constant([['f'], ['g']])],
                       num_buckets=100)
<tf.RaggedTensor [[78], [66, 74]]>"
"tf.ragged.map_flat_values(
    op, *args, **kwargs
)
","[['Applies ', 'op', ' to the ', 'flat_values', ' of one or more RaggedTensors.']]","rt = tf.ragged.constant([[1, 2, 3], [], [4, 5], [6]])
tf.ragged.map_flat_values(tf.ones_like, rt)
<tf.RaggedTensor [[1, 1, 1], [], [1, 1], [1]]>
tf.ragged.map_flat_values(tf.multiply, rt, rt)
<tf.RaggedTensor [[1, 4, 9], [], [16, 25], [36]]>
tf.ragged.map_flat_values(tf.add, rt, 5)
<tf.RaggedTensor [[6, 7, 8], [], [9, 10], [11]]>"
"tf.compat.v1.ragged.placeholder(
    dtype, ragged_rank, value_shape=None, name=None
)
","[['Creates a placeholder for a ', 'tf.RaggedTensor', ' that will always be fed.']]",[]
"tf.ragged.range(
    starts,
    limits=None,
    deltas=1,
    dtype=None,
    name=None,
    row_splits_dtype=tf.dtypes.int64
)
","[['Returns a ', 'RaggedTensor', ' containing the specified sequences of numbers.']]","ragged.range(starts, limits, deltas)[i] ==
    tf.range(starts[i], limits[i], deltas[i])
"
"tf.ragged.row_splits_to_segment_ids(
    splits, name=None, out_type=None
)
","[['Generates the segmentation corresponding to a RaggedTensor ', 'row_splits', '.']]","print(tf.ragged.row_splits_to_segment_ids([0, 3, 3, 5, 6, 9]))
 tf.Tensor([0 0 0 2 2 3 4 4 4], shape=(9,), dtype=int64)"
"tf.ragged.segment_ids_to_row_splits(
    segment_ids, num_segments=None, out_type=None, name=None
)
","[['Generates the RaggedTensor ', 'row_splits', ' corresponding to a segmentation.']]","print(tf.ragged.segment_ids_to_row_splits([0, 0, 0, 2, 2, 3, 4, 4, 4]))
tf.Tensor([0 3 3 5 6 9], shape=(6,), dtype=int64)"
"tf.ragged.stack(
    values: typing.List[ragged_tensor.RaggedOrDense], axis=0, name=None
)
","[['Stacks a list of rank-', 'R', ' tensors into one rank-', '(R+1)', ' ', 'RaggedTensor', '.']]","# Stacking two ragged tensors.
t1 = tf.ragged.constant([[1, 2], [3, 4, 5]])
t2 = tf.ragged.constant([[6], [7, 8, 9]])
tf.ragged.stack([t1, t2], axis=0)
<tf.RaggedTensor [[[1, 2], [3, 4, 5]], [[6], [7, 8, 9]]]>
tf.ragged.stack([t1, t2], axis=1)
<tf.RaggedTensor [[[1, 2], [6]], [[3, 4, 5], [7, 8, 9]]]>"
"tf.ragged.stack_dynamic_partitions(
    data, partitions, num_partitions, name=None
)
",[],"data           = ['a', 'b', 'c', 'd', 'e']
partitions     = [  3,   0,   2,   2,   3]
num_partitions = 5
tf.ragged.stack_dynamic_partitions(data, partitions, num_partitions)
<tf.RaggedTensor [[b'b'], [], [b'c', b'd'], [b'a', b'e'], []]>"
"tf.random.Generator(
    copy_from=None, state=None, alg=None
)
",[],"g = tf.random.Generator.from_seed(1234)
g.normal(shape=(2, 3))
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.9356609 ,  1.0854305 , -0.93788373],
       [-0.5061547 ,  1.3169702 ,  0.7137579 ]], dtype=float32)>"
"tf.random.all_candidate_sampler(
    true_classes, num_true, num_sampled, unique, seed=None, name=None
)
",[],[]
"tf.random.categorical(
    logits, num_samples, dtype=None, seed=None, name=None
)
",[],"# samples has shape [1, 5], where each value is either 0 or 1 with equal
# probability.
samples = tf.random.categorical(tf.math.log([[0.5, 0.5]]), 5)
"
"tf.random.create_rng_state(
    seed, alg
)
",[],"tf.random.create_rng_state(
    1234, ""philox"")
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1234,    0,    0])>
tf.random.create_rng_state(
    [12, 34], ""threefry"")
<tf.Tensor: shape=(2,), dtype=int64, numpy=array([12, 34])>"
"tf.random.Generator(
    copy_from=None, state=None, alg=None
)
",[],"g = tf.random.Generator.from_seed(1234)
g.normal(shape=(2, 3))
<tf.Tensor: shape=(2, 3), dtype=float32, numpy=
array([[ 0.9356609 ,  1.0854305 , -0.93788373],
       [-0.5061547 ,  1.3169702 ,  0.7137579 ]], dtype=float32)>"
"tf.random.create_rng_state(
    seed, alg
)
",[],"tf.random.create_rng_state(
    1234, ""philox"")
<tf.Tensor: shape=(3,), dtype=int64, numpy=array([1234,    0,    0])>
tf.random.create_rng_state(
    [12, 34], ""threefry"")
<tf.Tensor: shape=(2,), dtype=int64, numpy=array([12, 34])>"
"tf.random.experimental.index_shuffle(
    index, seed, max_index
)
","[['Outputs the position of ', 'index', ' in a permutation of [0, ..., max_index].']]","vector = tf.constant(['e0', 'e1', 'e2', 'e3'])
indices = tf.random.experimental.index_shuffle(tf.range(4), [5, 9], 3)
shuffled_vector = tf.gather(vector, indices)
print(shuffled_vector)
tf.Tensor([b'e2' b'e0' b'e1' b'e3'], shape=(4,), dtype=string)"
"tf.random.set_global_generator(
    generator
)
","[['Replaces the global generator with another ', 'Generator', ' object.']]","rng = tf.random.get_global_generator()
rng.reset_from_seed(30)"
"tf.random.experimental.stateless_fold_in(
    seed, data, alg='auto_select'
)
",[],"master_seed = [1, 2]
replica_id = 3
replica_seed = tf.random.experimental.stateless_fold_in(
  master_seed, replica_id)
print(replica_seed)
tf.Tensor([1105988140          3], shape=(2,), dtype=int32)
tf.random.stateless_normal(shape=[3], seed=replica_seed)
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([0.03197195, 0.8979765 ,
0.13253039], dtype=float32)>"
"tf.random.experimental.stateless_shuffle(
    value, seed, alg='auto_select', name=None
)
",[],"[[1, 2],       [[5, 6],
 [3, 4],  ==>   [1, 2],
 [5, 6]]        [3, 4]]
"
"tf.random.experimental.stateless_split(
    seed, num=2, alg='auto_select'
)
","[['Splits an RNG seed into ', 'num', ' new seeds by adding a leading axis.']]","seed = [1, 2]
new_seeds = tf.random.experimental.stateless_split(seed, num=3)
print(new_seeds)
tf.Tensor(
[[1105988140 1738052849]
 [-335576002  370444179]
 [  10670227 -246211131]], shape=(3, 2), dtype=int32)
tf.random.stateless_normal(shape=[3], seed=new_seeds[0, :])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([-0.59835213, -0.9578608 ,
0.9002807 ], dtype=float32)>"
"tf.random.fixed_unigram_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    vocab_file='',
    distortion=1.0,
    num_reserved_ids=0,
    num_shards=1,
    shard=0,
    unigrams=(),
    seed=None,
    name=None
)
",[],[]
"tf.random.gamma(
    shape,
    alpha,
    beta=None,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","[['Draws ', 'shape', ' samples from each of the given Gamma distribution(s).']]","samples = tf.random.gamma([10], [0.5, 1.5])
# samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents
# the samples drawn from each distribution

samples = tf.random.gamma([7, 5], [0.5, 1.5])
# samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1]
# represents the 7x5 samples drawn from each of the two distributions

alpha = tf.constant([[1.],[3.],[5.]])
beta = tf.constant([[3., 4.]])
samples = tf.random.gamma([30], alpha=alpha, beta=beta)
# samples has shape [30, 3, 2], with 30 samples each of 3x2 distributions.

loss = tf.reduce_mean(tf.square(samples))
dloss_dalpha, dloss_dbeta = tf.gradients(loss, [alpha, beta])
# unbiased stochastic derivatives of the loss function
alpha.shape == dloss_dalpha.shape  # True
beta.shape == dloss_dbeta.shape  # True
"
"tf.compat.v1.get_seed(
    op_seed
)
",[],[]
"tf.random.learned_unigram_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=None,
    name=None
)
",[],[]
"tf.random.log_uniform_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=None,
    name=None
)
",[],[]
"tf.compat.v1.multinomial(
    logits, num_samples, seed=None, name=None, output_dtype=None
)
",[],"# samples has shape [1, 5], where each value is either 0 or 1 with equal
# probability.
samples = tf.random.categorical(tf.math.log([[0.5, 0.5]]), 5)
"
"tf.random.normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
",[],"tf.random.set_seed(5);
tf.random.normal([4], 0, 1, tf.float32)
<tf.Tensor: shape=(4,), dtype=float32, numpy=..., dtype=float32)>"
"tf.compat.v1.random_poisson(
    lam,
    shape,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","[['Draws ', 'shape', ' samples from each of the given Poisson distribution(s).']]","samples = tf.random.poisson([0.5, 1.5], [10])
# samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents
# the samples drawn from each distribution

samples = tf.random.poisson([12.2, 3.3], [7, 5])
# samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1]
# represents the 7x5 samples drawn from each of the two distributions
"
"tf.random.set_global_generator(
    generator
)
","[['Replaces the global generator with another ', 'Generator', ' object.']]","rng = tf.random.get_global_generator()
rng.reset_from_seed(30)"
"tf.compat.v1.set_random_seed(
    seed
)
",[],[]
"tf.random.shuffle(
    value, seed=None, name=None
)
",[],"[[1, 2],       [[5, 6],
 [3, 4],  ==>   [1, 2],
 [5, 6]]        [3, 4]]
"
"tf.random.stateless_binomial(
    shape,
    seed,
    counts,
    probs,
    output_dtype=tf.dtypes.int32,
    name=None
)
",[],"counts = [10., 20.]
# Probability of success.
probs = [0.8]

binomial_samples = tf.random.stateless_binomial(
    shape=[2], seed=[123, 456], counts=counts, probs=probs)

counts = ... # Shape [3, 1, 2]
probs = ...  # Shape [1, 4, 2]
shape = [3, 4, 3, 4, 2]
# Sample shape will be [3, 4, 3, 4, 2]
binomial_samples = tf.random.stateless_binomial(
    shape=shape, seed=[123, 456], counts=counts, probs=probs)
"
"tf.random.stateless_categorical(
    logits,
    num_samples,
    seed,
    dtype=tf.dtypes.int64,
    name=None
)
",[],"# samples has shape [1, 5], where each value is either 0 or 1 with equal
# probability.
samples = tf.random.stateless_categorical(
    tf.math.log([[0.5, 0.5]]), 5, seed=[7, 17])
"
"tf.random.stateless_gamma(
    shape,
    seed,
    alpha,
    beta=None,
    dtype=tf.dtypes.float32,
    name=None
)
",[],"samples = tf.random.stateless_gamma([10, 2], seed=[12, 34], alpha=[0.5, 1.5])
# samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents
# the samples drawn from each distribution

samples = tf.random.stateless_gamma([7, 5, 2], seed=[12, 34], alpha=[.5, 1.5])
# samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1]
# represents the 7x5 samples drawn from each of the two distributions

alpha = tf.constant([[1.], [3.], [5.]])
beta = tf.constant([[3., 4.]])
samples = tf.random.stateless_gamma(
    [30, 3, 2], seed=[12, 34], alpha=alpha, beta=beta)
# samples has shape [30, 3, 2], with 30 samples each of 3x2 distributions.

with tf.GradientTape() as tape:
  tape.watch([alpha, beta])
  loss = tf.reduce_mean(tf.square(tf.random.stateless_gamma(
      [30, 3, 2], seed=[12, 34], alpha=alpha, beta=beta)))
dloss_dalpha, dloss_dbeta = tape.gradient(loss, [alpha, beta])
# unbiased stochastic derivatives of the loss function
alpha.shape == dloss_dalpha.shape  # True
beta.shape == dloss_dbeta.shape  # True
"
"tf.compat.v1.random.stateless_multinomial(
    logits,
    num_samples,
    seed,
    output_dtype=tf.dtypes.int64,
    name=None
)
",[],"# samples has shape [1, 5], where each value is either 0 or 1 with equal
# probability.
samples = tf.random.stateless_categorical(
    tf.math.log([[0.5, 0.5]]), 5, seed=[7, 17])
"
"tf.random.stateless_normal(
    shape,
    seed,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    name=None,
    alg='auto_select'
)
",[],[]
"tf.random.stateless_parameterized_truncated_normal(
    shape, seed, means=0.0, stddevs=1.0, minvals=-2.0, maxvals=2.0, name=None
)
",[],"means = 0.
stddevs = tf.math.exp(tf.random.uniform(shape=[2, 3]))
minvals = [-1., -2., -1000.]
maxvals = [[10000.], [1.]]
y = tf.random.stateless_parameterized_truncated_normal(
  shape=[10, 2, 3], seed=[7, 17],
  means=means, stddevs=stddevs, minvals=minvals, maxvals=maxvals)
y.shape
TensorShape([10, 2, 3])"
"tf.random.stateless_poisson(
    shape,
    seed,
    lam,
    dtype=tf.dtypes.int32,
    name=None
)
",[],"samples = tf.random.stateless_poisson([10, 2], seed=[12, 34], lam=[5, 15])
# samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents
# the samples drawn from each distribution

samples = tf.random.stateless_poisson([7, 5, 2], seed=[12, 34], lam=[5, 15])
# samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1]
# represents the 7x5 samples drawn from each of the two distributions

rate = tf.constant([[1.], [3.], [5.]])
samples = tf.random.stateless_poisson([30, 3, 1], seed=[12, 34], lam=rate)
# samples has shape [30, 3, 1], with 30 samples each of 3x1 distributions.
"
"tf.random.stateless_truncated_normal(
    shape,
    seed,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    name=None,
    alg='auto_select'
)
",[],[]
"tf.random.stateless_uniform(
    shape,
    seed,
    minval=0,
    maxval=None,
    dtype=tf.dtypes.float32,
    name=None,
    alg='auto_select'
)
",[],"ints = tf.random.stateless_uniform(
    [10], seed=(2, 3), minval=None, maxval=None, dtype=tf.int32)
"
"tf.random.truncated_normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
",[],"tf.random.truncated_normal(shape=[2])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>"
"tf.random.uniform(
    shape,
    minval=0,
    maxval=None,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
",[],"tf.random.uniform(shape=[2])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>
tf.random.uniform(shape=[], minval=-1., maxval=0.)
<tf.Tensor: shape=(), dtype=float32, numpy=-...>
tf.random.uniform(shape=[], minval=5, maxval=10, dtype=tf.int64)
<tf.Tensor: shape=(), dtype=int64, numpy=...>"
"tf.random.uniform_candidate_sampler(
    true_classes,
    num_true,
    num_sampled,
    unique,
    range_max,
    seed=None,
    name=None
)
",[],[]
"tf.image.random_crop(
    value, size, seed=None, name=None
)
",[],"image = [[1, 2, 3], [4, 5, 6]]
result = tf.image.random_crop(value=image, size=(1, 3))
result.shape.as_list()
[1, 3]"
"tf.random.gamma(
    shape,
    alpha,
    beta=None,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","[['Draws ', 'shape', ' samples from each of the given Gamma distribution(s).']]","samples = tf.random.gamma([10], [0.5, 1.5])
# samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents
# the samples drawn from each distribution

samples = tf.random.gamma([7, 5], [0.5, 1.5])
# samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1]
# represents the 7x5 samples drawn from each of the two distributions

alpha = tf.constant([[1.],[3.],[5.]])
beta = tf.constant([[3., 4.]])
samples = tf.random.gamma([30], alpha=alpha, beta=beta)
# samples has shape [30, 3, 2], with 30 samples each of 3x2 distributions.

loss = tf.reduce_mean(tf.square(samples))
dloss_dalpha, dloss_dbeta = tf.gradients(loss, [alpha, beta])
# unbiased stochastic derivatives of the loss function
alpha.shape == dloss_dalpha.shape  # True
beta.shape == dloss_dbeta.shape  # True
"
"tf.random_index_shuffle(
    index, seed, max_index, rounds=4, name=None
)
","[['Outputs the position of ', 'value', ' in a permutation of [0, ..., max_index].']]",[]
"tf.random.normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
",[],"tf.random.set_seed(5);
tf.random.normal([4], 0, 1, tf.float32)
<tf.Tensor: shape=(4,), dtype=float32, numpy=..., dtype=float32)>"
"tf.compat.v1.random_normal_initializer(
    mean=0.0,
    stddev=1.0,
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"initializer = tf.compat.v1.random_normal_initializer(
  mean=mean,
  stddev=stddev,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.compat.v1.random_poisson(
    lam,
    shape,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
","[['Draws ', 'shape', ' samples from each of the given Poisson distribution(s).']]","samples = tf.random.poisson([0.5, 1.5], [10])
# samples has shape [10, 2], where each slice [:, 0] and [:, 1] represents
# the samples drawn from each distribution

samples = tf.random.poisson([12.2, 3.3], [7, 5])
# samples has shape [7, 5, 2], where each slice [:, :, 0] and [:, :, 1]
# represents the 7x5 samples drawn from each of the two distributions
"
"tf.random.shuffle(
    value, seed=None, name=None
)
",[],"[[1, 2],       [[5, 6],
 [3, 4],  ==>   [1, 2],
 [5, 6]]        [3, 4]]
"
"tf.random.uniform(
    shape,
    minval=0,
    maxval=None,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
",[],"tf.random.uniform(shape=[2])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>
tf.random.uniform(shape=[], minval=-1., maxval=0.)
<tf.Tensor: shape=(), dtype=float32, numpy=-...>
tf.random.uniform(shape=[], minval=5, maxval=10, dtype=tf.int64)
<tf.Tensor: shape=(), dtype=int64, numpy=...>"
"tf.compat.v1.random_uniform_initializer(
    minval=0.0,
    maxval=None,
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"initializer = tf.compat.v1.random_uniform_initializer(
  minval=minval,
  maxval=maxval,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.rank(
    input, name=None
)
",[],"# shape of tensor 't' is [2, 2, 3]
t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])
tf.rank(t)  # 3
"
"tf.io.read_file(
    filename, name=None
)
",[],"with open(""/tmp/file.txt"", ""w"") as f:
  f.write(""asdf"")
4
tf.io.read_file(""/tmp/file.txt"")
<tf.Tensor: shape=(), dtype=string, numpy=b'asdf'>"
"tf.math.real(
    input, name=None
)
",[],"x = tf.constant([-2.25 + 4.75j, 3.25 + 5.75j])
tf.math.real(x)  # [-2.25, 3.25]
"
"tf.realdiv(
    x, y, name=None
)
",[],[]
"tf.math.reciprocal(
    x, name=None
)
","[[None, '\n']]",[]
"tf.recompute_grad(
    f
)
",[],y = tf.Variable(1.0)
"tf.compat.v1.reduce_all(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
","[['Computes ', 'tf.math.logical_and', ' of elements across dimensions of a tensor. (deprecated arguments)']]",">>> x = tf.constant([[True,  True], [False, False]])
>>> tf.math.reduce_all(x)
<tf.Tensor: shape=(), dtype=bool, numpy=False>
>>> tf.math.reduce_all(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False, False])>
>>> tf.math.reduce_all(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.compat.v1.reduce_any(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
","[['Computes ', 'tf.math.logical_or', ' of elements across dimensions of a tensor. (deprecated arguments)']]",">>> x = tf.constant([[True,  True], [False, False]])
>>> tf.reduce_any(x)
<tf.Tensor: shape=(), dtype=bool, numpy=True>
>>> tf.reduce_any(x, 0)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True,  True])>
>>> tf.reduce_any(x, 1)
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
"
"tf.compat.v1.reduce_join(
    inputs,
    axis=None,
    keep_dims=None,
    separator='',
    name=None,
    reduction_indices=None,
    keepdims=None
)
",[],"tf.strings.reduce_join([['abc','123'],
                        ['def','456']]).numpy()
b'abc123def456'
tf.strings.reduce_join([['abc','123'],
                        ['def','456']], axis=-1).numpy()
array([b'abc123', b'def456'], dtype=object)
tf.strings.reduce_join([['abc','123'],
                        ['def','456']],
                       axis=-1,
                       separator="" "").numpy()
array([b'abc 123', b'def 456'], dtype=object)"
"tf.compat.v1.reduce_logsumexp(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
",[],"x = tf.constant([[0., 0., 0.], [0., 0., 0.]])
tf.reduce_logsumexp(x)  # log(6)
tf.reduce_logsumexp(x, 0)  # [log(2), log(2), log(2)]
tf.reduce_logsumexp(x, 1)  # [log(3), log(3)]
tf.reduce_logsumexp(x, 1, keepdims=True)  # [[log(3)], [log(3)]]
tf.reduce_logsumexp(x, [0, 1])  # log(6)
"
"tf.compat.v1.reduce_max(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
","[['Computes ', 'tf.math.maximum', ' of elements across dimensions of a tensor. (deprecated arguments)']]",">>> x = tf.constant([5, 1, 2, 4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=5>
>>> x = tf.constant([-5, -1, -2, -4])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=-1>
>>> x = tf.constant([4, float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('nan'), float('nan')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('-inf'), float('inf')])
>>> tf.reduce_max(x)
<tf.Tensor: shape=(), dtype=float32, numpy=inf>
"
"tf.compat.v1.reduce_mean(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
",[],"x = tf.constant([[1., 1.], [2., 2.]])
tf.reduce_mean(x)
<tf.Tensor: shape=(), dtype=float32, numpy=1.5>
tf.reduce_mean(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1.5, 1.5], dtype=float32)>
tf.reduce_mean(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([1., 2.], dtype=float32)>"
"tf.compat.v1.reduce_min(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
","[['Computes the ', 'tf.math.minimum', ' of elements across dimensions of a tensor. (deprecated arguments)']]",">>> x = tf.constant([5, 1, 2, 4])
>>> tf.reduce_min(x)
<tf.Tensor: shape=(), dtype=int32, numpy=1>
>>> x = tf.constant([-5, -1, -2, -4])
>>> tf.reduce_min(x)
<tf.Tensor: shape=(), dtype=int32, numpy=-5>
>>> x = tf.constant([4, float('nan')])
>>> tf.reduce_min(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('nan'), float('nan')])
>>> tf.reduce_min(x)
<tf.Tensor: shape=(), dtype=float32, numpy=nan>
>>> x = tf.constant([float('-inf'), float('inf')])
>>> tf.reduce_min(x)
<tf.Tensor: shape=(), dtype=float32, numpy=-inf>
"
"tf.compat.v1.reduce_prod(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
","[['Computes ', 'tf.math.multiply', ' of elements across dimensions of a tensor. (deprecated arguments)']]",">>> x = tf.constant([[1., 2.], [3., 4.]])
>>> tf.math.reduce_prod(x)
<tf.Tensor: shape=(), dtype=float32, numpy=24.>
>>> tf.math.reduce_prod(x, 0)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([3., 8.], dtype=float32)>
>>> tf.math.reduce_prod(x, 1)
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([2., 12.],
dtype=float32)>
"
"tf.compat.v1.reduce_sum(
    input_tensor,
    axis=None,
    keepdims=None,
    name=None,
    reduction_indices=None,
    keep_dims=None
)
",[],">>> # x has a shape of (2, 3) (two rows and three columns):
>>> x = tf.constant([[1, 1, 1], [1, 1, 1]])
>>> x.numpy()
array([[1, 1, 1],
       [1, 1, 1]], dtype=int32)
>>> # sum all the elements
>>> # 1 + 1 + 1 + 1 + 1+ 1 = 6
>>> tf.reduce_sum(x).numpy()
6
>>> # reduce along the first dimension
>>> # the result is [1, 1, 1] + [1, 1, 1] = [2, 2, 2]
>>> tf.reduce_sum(x, 0).numpy()
array([2, 2, 2], dtype=int32)
>>> # reduce along the second dimension
>>> # the result is [1, 1] + [1, 1] + [1, 1] = [3, 3]
>>> tf.reduce_sum(x, 1).numpy()
array([3, 3], dtype=int32)
>>> # keep the original dimensions
>>> tf.reduce_sum(x, 1, keepdims=True).numpy()
array([[3],
       [3]], dtype=int32)
>>> # reduce along both dimensions
>>> # the result is 1 + 1 + 1 + 1 + 1 + 1 = 6
>>> # or, equivalently, reduce along rows, then reduce the resultant array
>>> # [1, 1, 1] + [1, 1, 1] = [2, 2, 2]
>>> # 2 + 2 + 2 = 6
>>> tf.reduce_sum(x, [0, 1]).numpy()
6
"
"tf.strings.regex_replace(
    input, pattern, rewrite, replace_global=True, name=None
)
","[['Replace elements of ', 'input', ' matching regex ', 'pattern', ' with ', 'rewrite', '.']]","tf.strings.regex_replace(""Text with tags.<br /><b>contains html</b>"",
                         ""<[^>]+>"", "" "")
<tf.Tensor: shape=(), dtype=string, numpy=b'Text with tags.  contains html '>"
"tf.register_tensor_conversion_function(
    base_type, conversion_func, priority=100
)
","[['Registers a function for converting objects of ', 'base_type', ' to ', 'Tensor', '.']]","    def conversion_func(value, dtype=None, name=None, as_ref=False):
      # ...
"
"tf.repeat(
    input, repeats, axis=None, name=None
)
","[['Repeat elements of ', 'input', '.']]","repeat(['a', 'b', 'c'], repeats=[3, 0, 2], axis=0)
<tf.Tensor: shape=(5,), dtype=string,
numpy=array([b'a', b'a', b'a', b'c', b'c'], dtype=object)>"
"tf.compat.v1.report_uninitialized_variables(
    var_list=None, name='report_uninitialized_variables'
)
",[],[]
"tf.required_space_to_batch_paddings(
    input_shape, block_shape, base_paddings=None, name=None
)
",[],[]
"tf.reshape(
    tensor, shape, name=None
)
",[],"t1 = [[1, 2, 3],
      [4, 5, 6]]
print(tf.shape(t1).numpy())
[2 3]
t2 = tf.reshape(t1, [6])
t2
<tf.Tensor: shape=(6,), dtype=int32,
  numpy=array([1, 2, 3, 4, 5, 6], dtype=int32)>
tf.reshape(t2, [3, 2])
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
  array([[1, 2],
         [3, 4],
         [5, 6]], dtype=int32)>"
"tf.compat.v1.resource_loader.get_path_to_datafile(
    path
)
",[],[]
"tf.compat.v1.resource_loader.load_resource(
    path
)
",[],[]
"tf.compat.v1.resource_loader.readahead_file_path(
    path, readahead='128M'
)
",[],[]
"tf.reverse(
    tensor, axis, name=None
)
",[],"# tensor 't' is [[[[ 0,  1,  2,  3],
#                  [ 4,  5,  6,  7],
#                  [ 8,  9, 10, 11]],
#                 [[12, 13, 14, 15],
#                  [16, 17, 18, 19],
#                  [20, 21, 22, 23]]]]
# tensor 't' shape is [1, 2, 3, 4]

# 'dims' is [3] or 'dims' is [-1]
reverse(t, dims) ==> [[[[ 3,  2,  1,  0],
                        [ 7,  6,  5,  4],
                        [ 11, 10, 9, 8]],
                       [[15, 14, 13, 12],
                        [19, 18, 17, 16],
                        [23, 22, 21, 20]]]]

# 'dims' is '[1]' (or 'dims' is '[-3]')
reverse(t, dims) ==> [[[[12, 13, 14, 15],
                        [16, 17, 18, 19],
                        [20, 21, 22, 23]
                       [[ 0,  1,  2,  3],
                        [ 4,  5,  6,  7],
                        [ 8,  9, 10, 11]]]]

# 'dims' is '[2]' (or 'dims' is '[-2]')
reverse(t, dims) ==> [[[[8, 9, 10, 11],
                        [4, 5, 6, 7],
                        [0, 1, 2, 3]]
                       [[20, 21, 22, 23],
                        [16, 17, 18, 19],
                        [12, 13, 14, 15]]]]
"
"tf.compat.v1.reverse_sequence(
    input,
    seq_lengths,
    seq_axis=None,
    batch_axis=None,
    name=None,
    seq_dim=None,
    batch_dim=None
)
",[],"seq_lengths = [7, 2, 3, 5]
input = [[1, 2, 3, 4, 5, 0, 0, 0], [1, 2, 0, 0, 0, 0, 0, 0],
         [1, 2, 3, 4, 0, 0, 0, 0], [1, 2, 3, 4, 5, 6, 7, 8]]
output = tf.reverse_sequence(input, seq_lengths, seq_axis=1, batch_axis=0)
output
<tf.Tensor: shape=(4, 8), dtype=int32, numpy=
array([[0, 0, 5, 4, 3, 2, 1, 0],
       [2, 1, 0, 0, 0, 0, 0, 0],
       [3, 2, 1, 4, 0, 0, 0, 0],
       [5, 4, 3, 2, 1, 6, 7, 8]], dtype=int32)>"
"tf.reverse(
    tensor, axis, name=None
)
",[],"# tensor 't' is [[[[ 0,  1,  2,  3],
#                  [ 4,  5,  6,  7],
#                  [ 8,  9, 10, 11]],
#                 [[12, 13, 14, 15],
#                  [16, 17, 18, 19],
#                  [20, 21, 22, 23]]]]
# tensor 't' shape is [1, 2, 3, 4]

# 'dims' is [3] or 'dims' is [-1]
reverse(t, dims) ==> [[[[ 3,  2,  1,  0],
                        [ 7,  6,  5,  4],
                        [ 11, 10, 9, 8]],
                       [[15, 14, 13, 12],
                        [19, 18, 17, 16],
                        [23, 22, 21, 20]]]]

# 'dims' is '[1]' (or 'dims' is '[-3]')
reverse(t, dims) ==> [[[[12, 13, 14, 15],
                        [16, 17, 18, 19],
                        [20, 21, 22, 23]
                       [[ 0,  1,  2,  3],
                        [ 4,  5,  6,  7],
                        [ 8,  9, 10, 11]]]]

# 'dims' is '[2]' (or 'dims' is '[-2]')
reverse(t, dims) ==> [[[[8, 9, 10, 11],
                        [4, 5, 6, 7],
                        [0, 1, 2, 3]]
                       [[20, 21, 22, 23],
                        [16, 17, 18, 19],
                        [12, 13, 14, 15]]]]
"
"tf.math.rint(
    x, name=None
)
",[],"rint(-1.5) ==> -2.0
rint(0.5000001) ==> 1.0
rint([-1.7, -1.5, -0.2, 0.2, 1.5, 1.7, 2.0]) ==> [-2., -2., -0., 0., 2., 2., 2.]
"
"tf.roll(
    input, shift, axis, name=None
)
",[],"# 't' is [0, 1, 2, 3, 4]
roll(t, shift=2, axis=0) ==> [3, 4, 0, 1, 2]

# shifting along multiple dimensions
# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
roll(t, shift=[1, -2], axis=[0, 1]) ==> [[7, 8, 9, 5, 6], [2, 3, 4, 0, 1]]

# shifting along the same axis multiple times
# 't' is [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
roll(t, shift=[2, -3], axis=[1, 1]) ==> [[1, 2, 3, 4, 0], [6, 7, 8, 9, 5]]
"
"tf.math.round(
    x, name=None
)
",[],"x = tf.constant([0.9, 2.5, 2.3, 1.5, -4.5])
tf.round(x)  # [ 1.0, 2.0, 2.0, 2.0, -4.0 ]
"
"tf.math.rsqrt(
    x, name=None
)
",[],"x = tf.constant([2., 0., -2.])
tf.math.rsqrt(x)
<tf.Tensor: shape=(3,), dtype=float32,
numpy=array([0.707, inf, nan], dtype=float32)>"
"tf.dtypes.saturate_cast(
    value, dtype, name=None
)
","[['Performs a safe saturating cast of ', 'value', ' to ', 'dtype', '.']]",[]
"tf.saved_model.Asset(
    path
)
",[],"filename = tf.saved_model.Asset(""file.txt"")

@tf.function(input_signature=[])
def func():
  return tf.io.read_file(filename)

trackable_obj = tf.train.Checkpoint()
trackable_obj.func = func
trackable_obj.filename = filename
tf.saved_model.save(trackable_obj, ""/tmp/saved_model"")

# The created SavedModel is hermetic, it does not depend on
# the original file and can be moved to another path.
tf.io.gfile.remove(""file.txt"")
tf.io.gfile.rename(""/tmp/saved_model"", ""/tmp/new_location"")

reloaded_obj = tf.saved_model.load(""/tmp/new_location"")
print(reloaded_obj.func())
"
"tf.compat.v1.saved_model.Builder(
    export_dir
)
","[['Builds the ', 'SavedModel', ' protocol buffer and saves variables and assets.']]","...
builder = tf.compat.v1.saved_model.Builder(export_dir)

with tf.compat.v1.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph_and_variables(sess,
                                  [""foo-tag""],
                                  signature_def_map=foo_signatures,
                                  assets_collection=foo_assets)
...

with tf.compat.v1.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph([""bar-tag"", ""baz-tag""])
...

builder.save()
"
"tf.saved_model.SaveOptions(
    namespace_whitelist=None,
    save_debug_info=False,
    function_aliases=None,
    experimental_io_device=None,
    experimental_variable_policy=None,
    experimental_custom_gradients=True
)
",[],"class Adder(tf.Module):
  @tf.function
  def double(self, x):
    return x + x"
"tf.compat.v1.saved_model.build_signature_def(
    inputs=None, outputs=None, method_name=None
)
",[],[]
"tf.compat.v1.saved_model.build_tensor_info(
    tensor
)
",[],[]
"tf.compat.v1.saved_model.Builder(
    export_dir
)
","[['Builds the ', 'SavedModel', ' protocol buffer and saves variables and assets.']]","...
builder = tf.compat.v1.saved_model.Builder(export_dir)

with tf.compat.v1.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph_and_variables(sess,
                                  [""foo-tag""],
                                  signature_def_map=foo_signatures,
                                  assets_collection=foo_assets)
...

with tf.compat.v1.Session(graph=tf.Graph()) as sess:
  ...
  builder.add_meta_graph([""bar-tag"", ""baz-tag""])
...

builder.save()
"
"tf.compat.v1.saved_model.classification_signature_def(
    examples, classes, scores
)
",[],[]
"tf.compat.v1.saved_model.contains_saved_model(
    export_dir
)
",[],[]
"tf.saved_model.experimental.TrackableResource(
    device=''
)
",[],"class DemoResource(tf.saved_model.experimental.TrackableResource):
  def __init__(self):
    super().__init__()
    self._initialize()
  def _create_resource(self):
    return tf.raw_ops.VarHandleOp(dtype=tf.float32, shape=[2])
  def _initialize(self):
    tf.raw_ops.AssignVariableOp(
        resource=self.resource_handle, value=tf.ones([2]))
  def _destroy_resource(self):
    tf.raw_ops.DestroyResourceOp(resource=self.resource_handle)
class DemoModule(tf.Module):
  def __init__(self):
    self.resource = DemoResource()
  def increment(self, tensor):
    return tensor + tf.raw_ops.ReadVariableOp(
        resource=self.resource.resource_handle, dtype=tf.float32)
demo = DemoModule()
demo.increment([5, 1])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([6., 2.], dtype=float32)>"
"tf.saved_model.save(
    obj, export_dir, signatures=None, options=None
)
","[['Exports a ', 'tf.Module', ' (and subclasses) ', 'obj', ' to ', 'SavedModel format', '.']]","class Adder(tf.Module):
  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32)])
  def add(self, x):
    return x + x"
"tf.compat.v1.saved_model.get_tensor_from_tensor_info(
    tensor_info, graph=None, import_scope=None
)
",[],[]
"tf.compat.v1.saved_model.is_valid_signature(
    signature_def
)
",[],[]
"tf.compat.v1.saved_model.load(
    sess, tags, export_dir, import_scope=None, **saver_kwargs
)
",[],"with tf.compat.v1.Session(graph=tf.Graph()) as sess:
  tf.compat.v1.saved_model.loader.load(sess, [""foo-tag""], export_dir)
"
"tf.saved_model.load(
    export_dir, tags=None, options=None
)
","[['Load a SavedModel from ', 'export_dir', '.']]","imported = tf.saved_model.load(path)
f = imported.signatures[""serving_default""]
print(f(x=tf.constant([[1.]])))
"
"tf.compat.v1.saved_model.load(
    sess, tags, export_dir, import_scope=None, **saver_kwargs
)
",[],"with tf.compat.v1.Session(graph=tf.Graph()) as sess:
  tf.compat.v1.saved_model.loader.load(sess, [""foo-tag""], export_dir)
"
"tf.compat.v1.saved_model.contains_saved_model(
    export_dir
)
",[],[]
"tf.compat.v1.saved_model.main_op_with_restore(
    restore_op_name
)
",[],[]
"tf.compat.v1.saved_model.main_op_with_restore(
    restore_op_name
)
",[],[]
"tf.compat.v1.saved_model.contains_saved_model(
    export_dir
)
",[],[]
"tf.compat.v1.saved_model.predict_signature_def(
    inputs, outputs
)
",[],[]
"tf.compat.v1.saved_model.regression_signature_def(
    examples, predictions
)
",[],[]
"tf.saved_model.save(
    obj, export_dir, signatures=None, options=None
)
","[['Exports a ', 'tf.Module', ' (and subclasses) ', 'obj', ' to ', 'SavedModel format', '.']]","class Adder(tf.Module):
  @tf.function(input_signature=[tf.TensorSpec(shape=[], dtype=tf.float32)])
  def add(self, x):
    return x + x"
"tf.compat.v1.saved_model.signature_def_utils.MethodNameUpdater(
    export_dir
)
",[],"...
updater = tf.compat.v1.saved_model.signature_def_utils.MethodNameUpdater(
    export_dir)
# Update all signature_defs with key ""foo"" in all meta graph defs.
updater.replace_method_name(signature_key=""foo"", method_name=""regress"")
# Update a single signature_def with key ""bar"" in the meta graph def with
# tags [""serve""]
updater.replace_method_name(signature_key=""bar"", method_name=""classify"",
                            tags=""serve"")
updater.save(new_export_dir)
"
"tf.compat.v1.saved_model.build_signature_def(
    inputs=None, outputs=None, method_name=None
)
",[],[]
"tf.compat.v1.saved_model.classification_signature_def(
    examples, classes, scores
)
",[],[]
"tf.compat.v1.saved_model.is_valid_signature(
    signature_def
)
",[],[]
"tf.compat.v1.saved_model.predict_signature_def(
    inputs, outputs
)
",[],[]
"tf.compat.v1.saved_model.regression_signature_def(
    examples, predictions
)
",[],[]
"tf.compat.v1.saved_model.simple_save(
    session, export_dir, inputs, outputs, legacy_init_op=None
)
",[],"simple_save(session,
            export_dir,
            inputs={""x"": x, ""y"": y},
            outputs={""z"": z})
"
"tf.compat.v1.saved_model.build_tensor_info(
    tensor
)
",[],[]
"tf.compat.v1.saved_model.get_tensor_from_tensor_info(
    tensor_info, graph=None, import_scope=None
)
",[],[]
"tf.compat.v1.scalar_mul(
    scalar, x, name=None
)
","[['Multiplies a scalar times a ', 'Tensor', ' or ', 'IndexedSlices', ' object.']]","x = tf.reshape(tf.range(30, dtype=tf.float32), [10, 3])
with tf.GradientTape() as g:
  g.watch(x)
  y = tf.gather(x, [1, 2])  # IndexedSlices
  z = tf.math.scalar_mul(10.0, y)"
"tf.compat.v1.scan(
    fn,
    elems,
    initializer=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    infer_shape=True,
    reverse=False,
    name=None
)
","[['scan on the list of tensors unpacked from ', 'elems', ' on dimension 0.']]","elems = np.array([1, 2, 3, 4, 5, 6])
sum = scan(lambda a, x: a + x, elems)
# sum == [1, 3, 6, 10, 15, 21]
sum = scan(lambda a, x: a + x, elems, reverse=True)
# sum == [21, 20, 18, 15, 11, 6]
"
"tf.compat.v1.scatter_add(
    ref, indices, updates, use_locking=False, name=None
)
","[['Adds sparse updates to the variable referenced by ', 'resource', '.']]","    # Scalar indices
    ref[indices, ...] += updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] += updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] += updates[i, ..., j, ...]
"
"tf.compat.v1.scatter_div(
    ref, indices, updates, use_locking=False, name=None
)
",[],"    # Scalar indices
    ref[indices, ...] /= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] /= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] /= updates[i, ..., j, ...]
"
"tf.compat.v1.scatter_max(
    ref, indices, updates, use_locking=False, name=None
)
","[['Reduces sparse updates into a variable reference using the ', 'max', ' operation.']]","# Scalar indices
ref[indices, ...] = max(ref[indices, ...], updates[...])

# Vector indices (for each i)
ref[indices[i], ...] = max(ref[indices[i], ...], updates[i, ...])

# High rank indices (for each i, ..., j)
ref[indices[i, ..., j], ...] = max(ref[indices[i, ..., j], ...],
updates[i, ..., j, ...])
"
"tf.compat.v1.scatter_min(
    ref, indices, updates, use_locking=False, name=None
)
","[['Reduces sparse updates into a variable reference using the ', 'min', ' operation.']]","# Scalar indices
ref[indices, ...] = min(ref[indices, ...], updates[...])

# Vector indices (for each i)
ref[indices[i], ...] = min(ref[indices[i], ...], updates[i, ...])

# High rank indices (for each i, ..., j)
ref[indices[i, ..., j], ...] = min(ref[indices[i, ..., j], ...],
updates[i, ..., j, ...])
"
"tf.compat.v1.scatter_mul(
    ref, indices, updates, use_locking=False, name=None
)
",[],"    # Scalar indices
    ref[indices, ...] *= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] *= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] *= updates[i, ..., j, ...]
"
"tf.scatter_nd(
    indices, updates, shape, name=None
)
","[['Scatters ', 'updates', ' into a tensor of shape ', 'shape', ' according to ', 'indices', '.']]","indices.shape[-1] <= shape.rank
"
"tf.compat.v1.scatter_nd_add(
    ref, indices, updates, use_locking=False, name=None
)
",[],"[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]]
"
"tf.compat.v1.scatter_nd_sub(
    ref, indices, updates, use_locking=False, name=None
)
",[],"[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]]
"
"tf.compat.v1.scatter_nd_update(
    ref, indices, updates, use_locking=True, name=None
)
","[['Applies sparse ', 'updates', ' to individual values or slices in a Variable.']]","[d_0, ..., d_{Q-2}, ref.shape[K], ..., ref.shape[P-1]].
"
"tf.compat.v1.scatter_sub(
    ref, indices, updates, use_locking=False, name=None
)
",[],"    # Scalar indices
    ref[indices, ...] -= updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] -= updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] -= updates[i, ..., j, ...]
"
"tf.compat.v1.scatter_update(
    ref, indices, updates, use_locking=True, name=None
)
",[],"    # Scalar indices
    ref[indices, ...] = updates[...]

    # Vector indices (for each i)
    ref[indices[i], ...] = updates[i, ...]

    # High rank indices (for each i, ..., j)
    ref[indices[i, ..., j], ...] = updates[i, ..., j, ...]
"
"tf.searchsorted(
    sorted_sequence,
    values,
    side='left',
    out_type=tf.dtypes.int32,
    name=None
)
",[],"edges = [-1, 3.3, 9.1, 10.0]
values = [0.0, 4.1, 12.0]
tf.searchsorted(edges, values).numpy()
array([1, 2, 4], dtype=int32)"
"tf.math.segment_max(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_max(c, tf.constant([0, 0, 1])).numpy()
array([[4, 3, 3, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_mean(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1.0,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_mean(c, tf.constant([0, 0, 1])).numpy()
array([[2.5, 2.5, 2.5, 2.5],
       [5., 6., 7., 8.]], dtype=float32)"
"tf.math.segment_min(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_min(c, tf.constant([0, 0, 1])).numpy()
array([[1, 2, 2, 1],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_prod(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_prod(c, tf.constant([0, 0, 1])).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.segment_sum(
    data, segment_ids, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [4, 3, 2, 1], [5,6,7,8]])
tf.math.segment_sum(c, tf.constant([0, 0, 1])).numpy()
array([[5, 5, 5, 5],
       [5, 6, 7, 8]], dtype=int32)"
"tf.linalg.eigh(
    tensor, name=None
)
",[],[]
"tf.linalg.eigvalsh(
    tensor, name=None
)
",[],[]
"tf.sequence_mask(
    lengths,
    maxlen=None,
    dtype=tf.dtypes.bool,
    name=None
)
",[],"mask[i_1, i_2, ..., i_n, j] = (j < lengths[i_1, i_2, ..., i_n])
"
"tf.compat.v1.serialize_many_sparse(
    sp_input,
    name=None,
    out_type=tf.dtypes.string
)
","[['Serialize ', 'N', '-minibatch ', 'SparseTensor', ' into an ', '[N, 3]', ' ', 'Tensor', '.']]",[]
"tf.compat.v1.serialize_sparse(
    sp_input,
    name=None,
    out_type=tf.dtypes.string
)
","[['Serialize a ', 'SparseTensor', ' into a 3-vector (1-D ', 'Tensor', ') object.']]",[]
"tf.io.serialize_tensor(
    tensor, name=None
)
",[],"t = tf.constant(1)
tf.io.serialize_tensor(t)
<tf.Tensor: shape=(), dtype=string, numpy=b'\x08...\x00'>"
"tf.compat.v1.set_random_seed(
    seed
)
",[],[]
"tf.compat.v1.setdiff1d(
    x,
    y,
    index_dtype=tf.dtypes.int32,
    name=None
)
",[],"x = [1, 2, 3, 4, 5, 6]
y = [1, 3, 5]
"
"tf.sets.difference(
    a, b, aminusb=True, validate_indices=True
)
","[['Compute set difference of elements in last dimension of ', 'a', ' and ', 'b', '.']]","  import tensorflow as tf
  import collections

  # Represent the following array of sets as a sparse tensor:
  # a = np.array([[{1, 2}, {3}], [{4}, {5, 6}]])
  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2, 2, 2])

  # np.array([[{1, 3}, {2}], [{4, 5}, {5, 6, 7, 8}]])
  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 3),
      ((0, 1, 0), 2),
      ((1, 0, 0), 4),
      ((1, 0, 1), 5),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  # `set_difference` is applied to each aligned pair of sets.
  tf.sets.difference(a, b)

  # The result will be equivalent to either of:
  #
  # np.array([[{2}, {3}], [{}, {}]])
  #
  # collections.OrderedDict([
  #     ((0, 0, 0), 2),
  #     ((0, 1, 0), 3),
  # ])
"
"tf.sets.intersection(
    a, b, validate_indices=True
)
","[['Compute set intersection of elements in last dimension of ', 'a', ' and ', 'b', '.']]","  import tensorflow as tf
  import collections

  # Represent the following array of sets as a sparse tensor:
  # a = np.array([[{1, 2}, {3}], [{4}, {5, 6}]])
  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2,2,2])

  # b = np.array([[{1}, {}], [{4}, {5, 6, 7, 8}]])
  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  # `tf.sets.intersection` is applied to each aligned pair of sets.
  tf.sets.intersection(a, b)

  # The result will be equivalent to either of:
  #
  # np.array([[{1}, {}], [{4}, {5, 6}]])
  #
  # collections.OrderedDict([
  #     ((0, 0, 0), 1),
  #     ((1, 0, 0), 4),
  #     ((1, 1, 0), 5),
  #     ((1, 1, 1), 6),
  # ])
"
"tf.sets.difference(
    a, b, aminusb=True, validate_indices=True
)
","[['Compute set difference of elements in last dimension of ', 'a', ' and ', 'b', '.']]","  import tensorflow as tf
  import collections

  # Represent the following array of sets as a sparse tensor:
  # a = np.array([[{1, 2}, {3}], [{4}, {5, 6}]])
  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2, 2, 2])

  # np.array([[{1, 3}, {2}], [{4, 5}, {5, 6, 7, 8}]])
  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 3),
      ((0, 1, 0), 2),
      ((1, 0, 0), 4),
      ((1, 0, 1), 5),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  # `set_difference` is applied to each aligned pair of sets.
  tf.sets.difference(a, b)

  # The result will be equivalent to either of:
  #
  # np.array([[{2}, {3}], [{}, {}]])
  #
  # collections.OrderedDict([
  #     ((0, 0, 0), 2),
  #     ((0, 1, 0), 3),
  # ])
"
"tf.sets.intersection(
    a, b, validate_indices=True
)
","[['Compute set intersection of elements in last dimension of ', 'a', ' and ', 'b', '.']]","  import tensorflow as tf
  import collections

  # Represent the following array of sets as a sparse tensor:
  # a = np.array([[{1, 2}, {3}], [{4}, {5, 6}]])
  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2,2,2])

  # b = np.array([[{1}, {}], [{4}, {5, 6, 7, 8}]])
  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  # `tf.sets.intersection` is applied to each aligned pair of sets.
  tf.sets.intersection(a, b)

  # The result will be equivalent to either of:
  #
  # np.array([[{1}, {}], [{4}, {5, 6}]])
  #
  # collections.OrderedDict([
  #     ((0, 0, 0), 1),
  #     ((1, 0, 0), 4),
  #     ((1, 1, 0), 5),
  #     ((1, 1, 1), 6),
  # ])
"
"tf.sets.size(
    a, validate_indices=True
)
","[['Compute number of unique elements along last dimension of ', 'a', '.']]",[]
"tf.sets.union(
    a, b, validate_indices=True
)
","[['Compute set union of elements in last dimension of ', 'a', ' and ', 'b', '.']]","  import tensorflow as tf
  import collections

  # [[{1, 2}, {3}], [{4}, {5, 6}]]
  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2, 2, 2])

  # [[{1, 3}, {2}], [{4, 5}, {5, 6, 7, 8}]]
  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 3),
      ((0, 1, 0), 2),
      ((1, 0, 0), 4),
      ((1, 0, 1), 5),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  # `set_union` is applied to each aligned pair of sets.
  tf.sets.union(a, b)

  # The result will be a equivalent to either of:
  #
  # np.array([[{1, 2, 3}, {2, 3}], [{4, 5}, {5, 6, 7, 8}]])
  #
  # collections.OrderedDict([
  #     ((0, 0, 0), 1),
  #     ((0, 0, 1), 2),
  #     ((0, 0, 2), 3),
  #     ((0, 1, 0), 2),
  #     ((0, 1, 1), 3),
  #     ((1, 0, 0), 4),
  #     ((1, 0, 1), 5),
  #     ((1, 1, 0), 5),
  #     ((1, 1, 1), 6),
  #     ((1, 1, 2), 7),
  #     ((1, 1, 3), 8),
  # ])
"
"tf.sets.size(
    a, validate_indices=True
)
","[['Compute number of unique elements along last dimension of ', 'a', '.']]",[]
"tf.sets.union(
    a, b, validate_indices=True
)
","[['Compute set union of elements in last dimension of ', 'a', ' and ', 'b', '.']]","  import tensorflow as tf
  import collections

  # [[{1, 2}, {3}], [{4}, {5, 6}]]
  a = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 2),
      ((0, 1, 0), 3),
      ((1, 0, 0), 4),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
  ])
  a = tf.sparse.SparseTensor(list(a.keys()), list(a.values()),
                             dense_shape=[2, 2, 2])

  # [[{1, 3}, {2}], [{4, 5}, {5, 6, 7, 8}]]
  b = collections.OrderedDict([
      ((0, 0, 0), 1),
      ((0, 0, 1), 3),
      ((0, 1, 0), 2),
      ((1, 0, 0), 4),
      ((1, 0, 1), 5),
      ((1, 1, 0), 5),
      ((1, 1, 1), 6),
      ((1, 1, 2), 7),
      ((1, 1, 3), 8),
  ])
  b = tf.sparse.SparseTensor(list(b.keys()), list(b.values()),
                             dense_shape=[2, 2, 4])

  # `set_union` is applied to each aligned pair of sets.
  tf.sets.union(a, b)

  # The result will be a equivalent to either of:
  #
  # np.array([[{1, 2, 3}, {2, 3}], [{4, 5}, {5, 6, 7, 8}]])
  #
  # collections.OrderedDict([
  #     ((0, 0, 0), 1),
  #     ((0, 0, 1), 2),
  #     ((0, 0, 2), 3),
  #     ((0, 1, 0), 2),
  #     ((0, 1, 1), 3),
  #     ((1, 0, 0), 4),
  #     ((1, 0, 1), 5),
  #     ((1, 1, 0), 5),
  #     ((1, 1, 1), 6),
  #     ((1, 1, 2), 7),
  #     ((1, 1, 3), 8),
  # ])
"
"tf.compat.v1.shape(
    input,
    name=None,
    out_type=tf.dtypes.int32
)
",[],"t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])
tf.shape(t)  # [2, 2, 3]
"
"tf.shape_n(
    input,
    out_type=tf.dtypes.int32,
    name=None
)
",[],[]
"tf.math.sigmoid(
    x, name=None
)
","[[None, '\n'], ['Computes sigmoid of ', 'x', ' element-wise.']]","x = tf.constant([0.0, 1.0, 50.0, 100.0])
tf.math.sigmoid(x)
<tf.Tensor: shape=(4,), dtype=float32,
numpy=array([0.5, 0.7310586, 1.0, 1.0], dtype=float32)>"
"tf.math.sign(
    x, name=None
)
",[],"# real number
tf.math.sign([0., 2., -3.])
<tf.Tensor: shape=(3,), dtype=float32,
numpy=array([ 0.,  1., -1.], dtype=float32)>"
"tf.signal.dct(
    input, type=2, n=None, axis=-1, norm=None, name=None
)
","[['Computes the 1D ', 'Discrete Cosine Transform (DCT)', ' of ', 'input', '.']]",[]
"tf.signal.fft(
    input, name=None
)
",[],[]
"tf.signal.fft2d(
    input, name=None
)
",[],[]
"tf.signal.fft3d(
    input, name=None
)
",[],[]
"tf.signal.fftshift(
    x, axes=None, name=None
)
",[],"x = tf.signal.fftshift([ 0.,  1.,  2.,  3.,  4., -5., -4., -3., -2., -1.])
x.numpy() # array([-5., -4., -3., -2., -1.,  0.,  1.,  2.,  3.,  4.])
"
"tf.signal.frame(
    signal,
    frame_length,
    frame_step,
    pad_end=False,
    pad_value=0,
    axis=-1,
    name=None
)
","[['Expands ', 'signal', ""'s "", 'axis', ' dimension into frames of ', 'frame_length', '.']]","# A batch size 3 tensor of 9152 audio samples.
audio = tf.random.normal([3, 9152])
# Compute overlapping frames of length 512 with a step of 180 (frames overlap
# by 332 samples). By default, only 49 frames are generated since a frame
# with start position j*180 for j > 48 would overhang the end.
frames = tf.signal.frame(audio, 512, 180)
frames.shape.assert_is_compatible_with([3, 49, 512])
# When pad_end is enabled, the final two frames are kept (padded with zeros).
frames = tf.signal.frame(audio, 512, 180, pad_end=True)
frames.shape.assert_is_compatible_with([3, 51, 512])"
"tf.signal.hamming_window(
    window_length,
    periodic=True,
    dtype=tf.dtypes.float32,
    name=None
)
","[['Generate a ', 'Hamming', ' window.']]",[]
"tf.signal.hann_window(
    window_length,
    periodic=True,
    dtype=tf.dtypes.float32,
    name=None
)
","[['Generate a ', 'Hann window', '.']]",[]
"tf.signal.idct(
    input, type=2, n=None, axis=-1, norm=None, name=None
)
","[['Computes the 1D ', 'Inverse Discrete Cosine Transform (DCT)', ' of ', 'input', '.']]",[]
"tf.signal.ifft(
    input, name=None
)
",[],[]
"tf.signal.ifft2d(
    input, name=None
)
",[],[]
"tf.signal.ifft3d(
    input, name=None
)
",[],[]
"tf.signal.ifftshift(
    x, axes=None, name=None
)
",[],"x = tf.signal.ifftshift([[ 0.,  1.,  2.],[ 3.,  4., -4.],[-3., -2., -1.]])
x.numpy() # array([[ 4., -4.,  3.],[-2., -1., -3.],[ 1.,  2.,  0.]])
"
"tf.signal.inverse_mdct(
    mdcts,
    window_fn=tf.signal.vorbis_window,
    norm=None,
    name=None
)
","[['Computes the inverse modified DCT of ', 'mdcts', '.']]","@tf.function
def compare_round_trip():
  samples = 1000
  frame_length = 400
  halflen = frame_length // 2
  waveform = tf.random.normal(dtype=tf.float32, shape=[samples])
  waveform_pad = tf.pad(waveform, [[halflen, 0],])
  mdct = tf.signal.mdct(waveform_pad, frame_length, pad_end=True,
                        window_fn=tf.signal.vorbis_window)
  inverse_mdct = tf.signal.inverse_mdct(mdct,
                                        window_fn=tf.signal.vorbis_window)
  inverse_mdct = inverse_mdct[halflen: halflen + samples]
  return waveform, inverse_mdct
waveform, inverse_mdct = compare_round_trip()
np.allclose(waveform.numpy(), inverse_mdct.numpy(), rtol=1e-3, atol=1e-4)
True"
"tf.signal.inverse_stft(
    stfts,
    frame_length,
    frame_step,
    fft_length=None,
    window_fn=tf.signal.hann_window,
    name=None
)
","[['Computes the inverse ', 'Short-time Fourier Transform', ' of ', 'stfts', '.']]","frame_length = 400
frame_step = 160
waveform = tf.random.normal(dtype=tf.float32, shape=[1000])
stft = tf.signal.stft(waveform, frame_length, frame_step)
inverse_stft = tf.signal.inverse_stft(
    stft, frame_length, frame_step,
    window_fn=tf.signal.inverse_stft_window_fn(frame_step))
"
"tf.signal.inverse_stft_window_fn(
    frame_step,
    forward_window_fn=tf.signal.hann_window,
    name=None
)
","[['Generates a window function that can be used in ', 'inverse_stft', '.']]",[]
"tf.signal.irfft(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.irfft2d(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.irfft3d(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.kaiser_bessel_derived_window(
    window_length,
    beta=12.0,
    dtype=tf.dtypes.float32,
    name=None
)
","[['Generate a ', 'Kaiser Bessel derived window', '.']]",[]
"tf.signal.kaiser_window(
    window_length,
    beta=12.0,
    dtype=tf.dtypes.float32,
    name=None
)
","[['Generate a ', 'Kaiser window', '.']]",[]
"tf.signal.linear_to_mel_weight_matrix(
    num_mel_bins=20,
    num_spectrogram_bins=129,
    sample_rate=8000,
    lower_edge_hertz=125.0,
    upper_edge_hertz=3800.0,
    dtype=tf.dtypes.float32,
    name=None
)
","[[None, '\n'], ['Returns a matrix to warp linear scale spectrograms to the ', 'mel scale', '.']]","$$\textrm{mel}(f) = 2595 * \textrm{log}_{10}(1 + \frac{f}{700})$$
"
"tf.signal.mdct(
    signals,
    frame_length,
    window_fn=tf.signal.vorbis_window,
    pad_end=False,
    norm=None,
    name=None
)
","[['Computes the ', 'Modified Discrete Cosine Transform', ' of ', 'signals', '.']]",[]
"tf.signal.mfccs_from_log_mel_spectrograms(
    log_mel_spectrograms, name=None
)
","[['Computes ', 'MFCCs', ' of ', 'log_mel_spectrograms', '.']]","batch_size, num_samples, sample_rate = 32, 32000, 16000.0
# A Tensor of [batch_size, num_samples] mono PCM samples in the range [-1, 1].
pcm = tf.random.normal([batch_size, num_samples], dtype=tf.float32)

# A 1024-point STFT with frames of 64 ms and 75% overlap.
stfts = tf.signal.stft(pcm, frame_length=1024, frame_step=256,
                       fft_length=1024)
spectrograms = tf.abs(stfts)

# Warp the linear scale spectrograms into the mel-scale.
num_spectrogram_bins = stfts.shape[-1].value
lower_edge_hertz, upper_edge_hertz, num_mel_bins = 80.0, 7600.0, 80
linear_to_mel_weight_matrix = tf.signal.linear_to_mel_weight_matrix(
  num_mel_bins, num_spectrogram_bins, sample_rate, lower_edge_hertz,
  upper_edge_hertz)
mel_spectrograms = tf.tensordot(
  spectrograms, linear_to_mel_weight_matrix, 1)
mel_spectrograms.set_shape(spectrograms.shape[:-1].concatenate(
  linear_to_mel_weight_matrix.shape[-1:]))

# Compute a stabilized log to get log-magnitude mel-scale spectrograms.
log_mel_spectrograms = tf.math.log(mel_spectrograms + 1e-6)

# Compute MFCCs from log_mel_spectrograms and take the first 13.
mfccs = tf.signal.mfccs_from_log_mel_spectrograms(
  log_mel_spectrograms)[..., :13]
"
"tf.signal.overlap_and_add(
    signal, frame_step, name=None
)
",[],"output_size = (frames - 1) * frame_step + frame_length
"
"tf.signal.rfft(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.rfft2d(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.rfft3d(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.stft(
    signals,
    frame_length,
    frame_step,
    fft_length=None,
    window_fn=tf.signal.hann_window,
    pad_end=False,
    name=None
)
","[['Computes the ', 'Short-time Fourier Transform', ' of ', 'signals', '.']]",[]
"tf.signal.vorbis_window(
    window_length,
    dtype=tf.dtypes.float32,
    name=None
)
","[['Generate a ', 'Vorbis power complementary window', '.']]",[]
"tf.math.sin(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10, float(""inf"")])
  tf.math.sin(x) ==> [nan -0.4121185 -0.47942555 0.84147096 0.9320391 -0.87329733 -0.54402107 nan]
"
"tf.math.sinh(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 2, 10, float(""inf"")])
  tf.math.sinh(x) ==> [-inf -4.0515420e+03 -5.2109528e-01 1.1752012e+00 1.5094614e+00 3.6268604e+00 1.1013232e+04 inf]
"
"tf.compat.v1.size(
    input,
    name=None,
    out_type=tf.dtypes.int32
)
",[],"t = tf.constant([[[1, 1, 1], [2, 2, 2]], [[3, 3, 3], [4, 4, 4]]])
tf.size(t)  # 12
"
"tf.slice(
    input_, begin, size, name=None
)
",[],"t = tf.constant([[[1, 1, 1], [2, 2, 2]],
                 [[3, 3, 3], [4, 4, 4]],
                 [[5, 5, 5], [6, 6, 6]]])
tf.slice(t, [1, 0, 0], [1, 1, 3])  # [[[3, 3, 3]]]
tf.slice(t, [1, 0, 0], [1, 2, 3])  # [[[3, 3, 3],
                                   #   [4, 4, 4]]]
tf.slice(t, [1, 0, 0], [2, 1, 3])  # [[[3, 3, 3]],
                                   #  [[5, 5, 5]]]
"
"tf.sort(
    values, axis=-1, direction='ASCENDING', name=None
)
",[],"a = [1, 10, 26.9, 2.8, 166.32, 62.3]
tf.sort(a).numpy()
array([  1.  ,   2.8 ,  10.  ,  26.9 ,  62.3 , 166.32], dtype=float32)"
"tf.compat.v1.space_to_batch(
    input, paddings, block_size=None, name=None, block_shape=None
)
",[],"[batch*block_size*block_size, height_pad/block_size, width_pad/block_size,
 depth]
"
"tf.space_to_batch_nd(
    input, block_shape, paddings, name=None
)
",[],"x = [[[[1], [2]], [[3], [4]]]]
"
"tf.compat.v1.space_to_depth(
    input, block_size, name=None, data_format='NHWC'
)
",[],"x = [[[[1], [2]],
      [[3], [4]]]]
"
"tf.compat.v1.SparseConditionalAccumulator(
    dtype,
    shape=None,
    shared_name=None,
    name='sparse_conditional_accumulator',
    reduction_type='MEAN'
)
","[['Inherits From: ', 'ConditionalAccumulatorBase']]","apply_grad(
    grad_indices, grad_values, grad_shape=None, local_step=0, name=None
)
"
"tf.sparse.SparseTensor(
    indices, values, dense_shape
)
",[],"dense.shape = dense_shape
dense[tuple(indices[i])] = values[i]
"
"tf.compat.v1.sparse_add(
    a, b, threshold=None, thresh=None
)
","[['Adds two tensors, at least one of each is a ', 'SparseTensor', '. (deprecated arguments)']]","[       2]
[.1     0]
[ 6   -.2]
"
"tf.sparse.bincount(
    values,
    weights=None,
    axis=0,
    minlength=None,
    maxlength=None,
    binary_output=False,
    name=None
)
",[],"data = np.array([[10, 20, 30, 20], [11, 101, 11, 10001]], dtype=np.int64)
output = tf.sparse.bincount(data, axis=-1)
print(output)
SparseTensor(indices=tf.Tensor(
[[    0    10]
 [    0    20]
 [    0    30]
 [    1    11]
 [    1   101]
 [    1 10001]], shape=(6, 2), dtype=int64),
 values=tf.Tensor([1 2 1 2 1 1], shape=(6,), dtype=int64),
 dense_shape=tf.Tensor([    2 10002], shape=(2,), dtype=int64))"
"tf.compat.v1.sparse_concat(
    axis,
    sp_inputs,
    name=None,
    expand_nonconcat_dim=False,
    concat_dim=None,
    expand_nonconcat_dims=None
)
","[['Concatenates a list of ', 'SparseTensor', ' along the specified dimension. (deprecated arguments)']]","sp_inputs[0]: shape = [2, 3]
[0, 2]: ""a""
[1, 0]: ""b""
[1, 1]: ""c""

sp_inputs[1]: shape = [2, 4]
[0, 1]: ""d""
[0, 2]: ""e""
"
"tf.sparse.cross(
    inputs, name=None, separator=None
)
",[],"* inputs[0]: SparseTensor with shape = [2, 2]
  [0, 0]: ""a""
  [1, 0]: ""b""
  [1, 1]: ""c""
* inputs[1]: SparseTensor with shape = [2, 1]
  [0, 0]: ""d""
  [1, 0]: ""e""
* inputs[2]: Tensor [[""f""], [""g""]]
"
"tf.sparse.cross_hashed(
    inputs, num_buckets=0, hash_key=None, name=None
)
",[],"* inputs[0]: SparseTensor with shape = [2, 2]
  [0, 0]: ""a""
  [1, 0]: ""b""
  [1, 1]: ""c""
* inputs[1]: SparseTensor with shape = [2, 1]
  [0, 0]: ""d""
  [1, 0]: ""e""
* inputs[2]: Tensor [[""f""], [""g""]]
"
"tf.sparse.expand_dims(
    sp_input, axis=None, name=None
)
","[['Returns a tensor with an length 1 axis inserted at index ', 'axis', '.']]","sp = tf.sparse.SparseTensor(indices=[[3,4,1]], values=[7,],
                            dense_shape=[10,10,3])"
"tf.sparse.eye(
    num_rows,
    num_columns=None,
    dtype=tf.dtypes.float32,
    name=None
)
",[],[]
"tf.sparse.fill_empty_rows(
    sp_input, default_value, name=None
)
","[['Fills empty rows in the input 2-D ', 'SparseTensor', ' with a default value.']]","[0, 1]: a
[0, 3]: b
[2, 0]: c
[3, 1]: d
"
"tf.sparse.from_dense(
    tensor, name=None
)
",[],"sp = tf.sparse.from_dense([0, 0, 3, 0, 1])
sp.shape.as_list()
[5]
sp.values.numpy()
array([3, 1], dtype=int32)
sp.indices.numpy()
array([[2],
       [4]])"
"tf.sparse.mask(
    a, mask_indices, name=None
)
","[['Masks elements of ', 'IndexedSlices', '.']]","# `a` contains slices at indices [12, 26, 37, 45] from a large tensor
# with shape [1000, 10]
a.indices  # [12, 26, 37, 45]
tf.shape(a.values)  # [4, 10]

# `b` will be the subset of `a` slices at its second and third indices, so
# we want to mask its first and last indices (which are at absolute
# indices 12, 45)
b = tf.sparse.mask(a, [12, 45])

b.indices  # [26, 37]
tf.shape(b.values)  # [2, 10]
"
"tf.sparse.sparse_dense_matmul(
    sp_a, b, adjoint_a=False, adjoint_b=False, name=None
)
",[],"[[  a      ]
 [b       c]
 [    d    ]]
"
"tf.sparse.maximum(
    sp_a, sp_b, name=None
)
",[],">>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])
>>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])
>>> res = tf.sparse.maximum(sp_zero, sp_one)
>>> res.indices
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
array([[0],
       [1]])>
>>> res.values
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>
>>> res.dense_shape
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
"
"tf.compat.v1.sparse_merge(
    sp_ids, sp_values, vocab_size, name=None, already_sorted=False
)
","[['Combines a batch of feature ids and values into a single ', 'SparseTensor', '. (deprecated)']]","  vector1 = [-3, 0, 0, 0, 0, 0]
  vector2 = [ 0, 1, 0, 4, 1, 0]
  vector3 = [ 5, 0, 0, 9, 0, 0]
"
"tf.sparse.minimum(
    sp_a, sp_b, name=None
)
",[],">>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])
>>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])
>>> res = tf.sparse.minimum(sp_zero, sp_one)
>>> res.indices
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
array([[0],
       [1]])>
>>> res.values
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>
>>> res.dense_shape
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
"
"tf.compat.v1.sparse_placeholder(
    dtype, shape=None, name=None
)
",[],[]
"tf.compat.v1.sparse_reduce_max(
    sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None
)
","[['Computes ', 'tf.sparse.maximum', ' of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments)']]","x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])
tf.sparse.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=3>
tf.sparse.reduce_max(x, 0)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>
tf.sparse.reduce_max(x, 1)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>
tf.sparse.reduce_max(x, 1, keepdims=True)
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
array([[2],
       [3]], dtype=int32)>
tf.sparse.reduce_max(x, [0, 1])
<tf.Tensor: shape=(), dtype=int32, numpy=3>"
"tf.compat.v1.sparse_reduce_max_sparse(
    sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None
)
",[],[]
"tf.compat.v1.sparse_reduce_sum(
    sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None
)
","[['Computes ', 'tf.sparse.add', ' of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments)']]","x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])
tf.sparse.reduce_sum(x)
<tf.Tensor: shape=(), dtype=int32, numpy=3>
tf.sparse.reduce_sum(x, 0)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>
tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>
tf.sparse.reduce_sum(x, 1, keepdims=True)
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
array([[2],
       [1]], dtype=int32)>
tf.sparse.reduce_sum(x, [0, 1])
<tf.Tensor: shape=(), dtype=int32, numpy=3>"
"tf.compat.v1.sparse_reduce_sum_sparse(
    sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None
)
",[],[]
"tf.sparse.reorder(
    sp_input, name=None
)
","[['Reorders a ', 'SparseTensor', ' into the canonical, row-major ordering.']]","[0, 3]: b
[0, 1]: a
[3, 1]: d
[2, 0]: c
"
"tf.sparse.reset_shape(
    sp_input, new_shape=None
)
","[['Resets the shape of a ', 'SparseTensor', ' with indices and values unchanged.']]",[]
"tf.sparse.reshape(
    sp_input, shape, name=None
)
","[['Reshapes a ', 'SparseTensor', ' to represent values in a new dense shape.']]","[0, 0, 0]: a
[0, 0, 1]: b
[0, 1, 0]: c
[1, 0, 0]: d
[1, 2, 3]: e
"
"tf.sparse.retain(
    sp_input, to_retain
)
","[['Retains specified non-empty values within a ', 'SparseTensor', '.']]","[0, 1]: a
[0, 3]: b
[2, 0]: c
[3, 1]: d
"
"tf.compat.v1.sparse_segment_mean(
    data, indices, segment_ids, name=None, num_segments=None
)
",[],[]
"tf.compat.v1.sparse_segment_sqrt_n(
    data, indices, segment_ids, name=None, num_segments=None
)
",[],[]
"tf.compat.v1.sparse_segment_sum(
    data, indices, segment_ids, name=None, num_segments=None
)
",[],"c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])

# Select two rows, one segment.
tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))
# => [[0 0 0 0]]

# Select two rows, two segment.
tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))
# => [[ 1  2  3  4]
#     [-1 -2 -3 -4]]

# With missing segment ids.
tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 2]),
                      num_segments=4)
# => [[ 1  2  3  4]
#     [ 0  0  0  0]
#     [-1 -2 -3 -4]
#     [ 0  0  0  0]]

# Select all rows, two segments.
tf.sparse.segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))
# => [[0 0 0 0]
#     [5 6 7 8]]

# Which is equivalent to:
tf.math.segment_sum(c, tf.constant([0, 0, 1]))
"
"tf.sparse.slice(
    sp_input, start, size, name=None
)
","[['Slice a ', 'SparseTensor', ' based on the ', 'start', ' and ', 'size', '.']]","input_tensor = shape = [2, 7]
[    a   d e  ]
[b c          ]
"
"tf.sparse.softmax(
    sp_input, name=None
)
","[['Applies softmax to a batched N-D ', 'SparseTensor', '.']]","# First batch:
# [?   e.]
# [1.  ? ]
# Second batch:
# [e   ? ]
# [e   e ]
shape = [2, 2, 2]  # 3-D SparseTensor
values = np.asarray([[[0., np.e], [1., 0.]], [[np.e, 0.], [np.e, np.e]]])
indices = np.vstack(np.where(values)).astype(np.int64).T

result = tf.sparse.softmax(tf.sparse.SparseTensor(indices, values, shape))
# ...returning a 3-D SparseTensor, equivalent to:
# [?   1.]     [1    ?]
# [1.  ? ] and [.5  .5]
# where ? means implicitly zero.
"
"tf.sparse.sparse_dense_matmul(
    sp_a, b, adjoint_a=False, adjoint_b=False, name=None
)
",[],"[[  a      ]
 [b       c]
 [    d    ]]
"
"tf.compat.v1.sparse_split(
    keyword_required=KeywordRequired(),
    sp_input=None,
    num_split=None,
    axis=None,
    name=None,
    split_dim=None
)
","[['Split a ', 'SparseTensor', ' into ', 'num_split', ' tensors along ', 'axis', '. (deprecated arguments)']]","input_tensor = shape = [2, 7]
[    a   d e  ]
[b c          ]
"
"tf.sparse.to_dense(
    sp_input, default_value=None, validate_indices=True, name=None
)
","[['Converts a ', 'SparseTensor', ' into a dense tensor.']]","sp_input = tf.sparse.SparseTensor(
  dense_shape=[3, 5],
  values=[7, 8, 9],
  indices =[[0, 1],
            [0, 3],
            [2, 0]])"
"tf.sparse.to_indicator(
    sp_input, vocab_size, name=None
)
","[['Converts a ', 'SparseTensor', ' of ids into a dense bool indicator tensor.']]","output[d_0, d_1, ..., d_n, sp_input[d_0, d_1, ..., d_n, k]] = True
"
"tf.sparse.transpose(
    sp_input, perm=None, name=None
)
","[['Transposes a ', 'SparseTensor']]","[0, 3]: b
[0, 1]: a
[3, 1]: d
[2, 0]: c
"
"tf.compat.v1.sparse_add(
    a, b, threshold=None, thresh=None
)
","[['Adds two tensors, at least one of each is a ', 'SparseTensor', '. (deprecated arguments)']]","[       2]
[.1     0]
[ 6   -.2]
"
"tf.compat.v1.sparse_concat(
    axis,
    sp_inputs,
    name=None,
    expand_nonconcat_dim=False,
    concat_dim=None,
    expand_nonconcat_dims=None
)
","[['Concatenates a list of ', 'SparseTensor', ' along the specified dimension. (deprecated arguments)']]","sp_inputs[0]: shape = [2, 3]
[0, 2]: ""a""
[1, 0]: ""b""
[1, 1]: ""c""

sp_inputs[1]: shape = [2, 4]
[0, 1]: ""d""
[0, 2]: ""e""
"
"tf.sparse.fill_empty_rows(
    sp_input, default_value, name=None
)
","[['Fills empty rows in the input 2-D ', 'SparseTensor', ' with a default value.']]","[0, 1]: a
[0, 3]: b
[2, 0]: c
[3, 1]: d
"
"tf.sparse.mask(
    a, mask_indices, name=None
)
","[['Masks elements of ', 'IndexedSlices', '.']]","# `a` contains slices at indices [12, 26, 37, 45] from a large tensor
# with shape [1000, 10]
a.indices  # [12, 26, 37, 45]
tf.shape(a.values)  # [4, 10]

# `b` will be the subset of `a` slices at its second and third indices, so
# we want to mask its first and last indices (which are at absolute
# indices 12, 45)
b = tf.sparse.mask(a, [12, 45])

b.indices  # [26, 37]
tf.shape(b.values)  # [2, 10]
"
"tf.compat.v1.sparse_matmul(
    a,
    b,
    transpose_a=False,
    transpose_b=False,
    a_is_sparse=False,
    b_is_sparse=False,
    name=None
)
",[],[]
"tf.sparse.maximum(
    sp_a, sp_b, name=None
)
",[],">>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])
>>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])
>>> res = tf.sparse.maximum(sp_zero, sp_one)
>>> res.indices
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
array([[0],
       [1]])>
>>> res.values
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 1], dtype=int32)>
>>> res.dense_shape
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
"
"tf.compat.v1.sparse_merge(
    sp_ids, sp_values, vocab_size, name=None, already_sorted=False
)
","[['Combines a batch of feature ids and values into a single ', 'SparseTensor', '. (deprecated)']]","  vector1 = [-3, 0, 0, 0, 0, 0]
  vector2 = [ 0, 1, 0, 4, 1, 0]
  vector3 = [ 5, 0, 0, 9, 0, 0]
"
"tf.sparse.minimum(
    sp_a, sp_b, name=None
)
",[],">>> sp_zero = tf.sparse.SparseTensor([[0]], [0], [7])
>>> sp_one = tf.sparse.SparseTensor([[1]], [1], [7])
>>> res = tf.sparse.minimum(sp_zero, sp_one)
>>> res.indices
<tf.Tensor: shape=(2, 1), dtype=int64, numpy=
array([[0],
       [1]])>
>>> res.values
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([0, 0], dtype=int32)>
>>> res.dense_shape
<tf.Tensor: shape=(1,), dtype=int64, numpy=array([7])>
"
"tf.compat.v1.sparse_placeholder(
    dtype, shape=None, name=None
)
",[],[]
"tf.compat.v1.sparse_reduce_max(
    sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None
)
","[['Computes ', 'tf.sparse.maximum', ' of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments)']]","x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 2, 3], [2, 3])
tf.sparse.reduce_max(x)
<tf.Tensor: shape=(), dtype=int32, numpy=3>
tf.sparse.reduce_max(x, 0)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 3, 2], dtype=int32)>
tf.sparse.reduce_max(x, 1)
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 3], dtype=int32)>
tf.sparse.reduce_max(x, 1, keepdims=True)
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
array([[2],
       [3]], dtype=int32)>
tf.sparse.reduce_max(x, [0, 1])
<tf.Tensor: shape=(), dtype=int32, numpy=3>"
"tf.compat.v1.sparse_reduce_max_sparse(
    sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None
)
",[],[]
"tf.compat.v1.sparse_reduce_sum(
    sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None
)
","[['Computes ', 'tf.sparse.add', ' of elements across dimensions of a SparseTensor. (deprecated arguments) (deprecated arguments)']]","x = tf.sparse.SparseTensor([[0, 0], [0, 2], [1, 1]], [1, 1, 1], [2, 3])
tf.sparse.reduce_sum(x)
<tf.Tensor: shape=(), dtype=int32, numpy=3>
tf.sparse.reduce_sum(x, 0)
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([1, 1, 1], dtype=int32)>
tf.sparse.reduce_sum(x, 1)  # Can also use -1 as the axis
<tf.Tensor: shape=(2,), dtype=int32, numpy=array([2, 1], dtype=int32)>
tf.sparse.reduce_sum(x, 1, keepdims=True)
<tf.Tensor: shape=(2, 1), dtype=int32, numpy=
array([[2],
       [1]], dtype=int32)>
tf.sparse.reduce_sum(x, [0, 1])
<tf.Tensor: shape=(), dtype=int32, numpy=3>"
"tf.compat.v1.sparse_reduce_sum_sparse(
    sp_input, axis=None, keepdims=None, reduction_axes=None, keep_dims=None
)
",[],[]
"tf.sparse.reorder(
    sp_input, name=None
)
","[['Reorders a ', 'SparseTensor', ' into the canonical, row-major ordering.']]","[0, 3]: b
[0, 1]: a
[3, 1]: d
[2, 0]: c
"
"tf.sparse.reset_shape(
    sp_input, new_shape=None
)
","[['Resets the shape of a ', 'SparseTensor', ' with indices and values unchanged.']]",[]
"tf.sparse.reshape(
    sp_input, shape, name=None
)
","[['Reshapes a ', 'SparseTensor', ' to represent values in a new dense shape.']]","[0, 0, 0]: a
[0, 0, 1]: b
[0, 1, 0]: c
[1, 0, 0]: d
[1, 2, 3]: e
"
"tf.sparse.retain(
    sp_input, to_retain
)
","[['Retains specified non-empty values within a ', 'SparseTensor', '.']]","[0, 1]: a
[0, 3]: b
[2, 0]: c
[3, 1]: d
"
"tf.compat.v1.sparse_segment_mean(
    data, indices, segment_ids, name=None, num_segments=None
)
",[],[]
"tf.compat.v1.sparse_segment_sqrt_n(
    data, indices, segment_ids, name=None, num_segments=None
)
",[],[]
"tf.compat.v1.sparse_segment_sum(
    data, indices, segment_ids, name=None, num_segments=None
)
",[],"c = tf.constant([[1,2,3,4], [-1,-2,-3,-4], [5,6,7,8]])

# Select two rows, one segment.
tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 0]))
# => [[0 0 0 0]]

# Select two rows, two segment.
tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 1]))
# => [[ 1  2  3  4]
#     [-1 -2 -3 -4]]

# With missing segment ids.
tf.sparse.segment_sum(c, tf.constant([0, 1]), tf.constant([0, 2]),
                      num_segments=4)
# => [[ 1  2  3  4]
#     [ 0  0  0  0]
#     [-1 -2 -3 -4]
#     [ 0  0  0  0]]

# Select all rows, two segments.
tf.sparse.segment_sum(c, tf.constant([0, 1, 2]), tf.constant([0, 0, 1]))
# => [[0 0 0 0]
#     [5 6 7 8]]

# Which is equivalent to:
tf.math.segment_sum(c, tf.constant([0, 0, 1]))
"
"tf.sparse.slice(
    sp_input, start, size, name=None
)
","[['Slice a ', 'SparseTensor', ' based on the ', 'start', ' and ', 'size', '.']]","input_tensor = shape = [2, 7]
[    a   d e  ]
[b c          ]
"
"tf.sparse.softmax(
    sp_input, name=None
)
","[['Applies softmax to a batched N-D ', 'SparseTensor', '.']]","# First batch:
# [?   e.]
# [1.  ? ]
# Second batch:
# [e   ? ]
# [e   e ]
shape = [2, 2, 2]  # 3-D SparseTensor
values = np.asarray([[[0., np.e], [1., 0.]], [[np.e, 0.], [np.e, np.e]]])
indices = np.vstack(np.where(values)).astype(np.int64).T

result = tf.sparse.softmax(tf.sparse.SparseTensor(indices, values, shape))
# ...returning a 3-D SparseTensor, equivalent to:
# [?   1.]     [1    ?]
# [1.  ? ] and [.5  .5]
# where ? means implicitly zero.
"
"tf.compat.v1.sparse_split(
    keyword_required=KeywordRequired(),
    sp_input=None,
    num_split=None,
    axis=None,
    name=None,
    split_dim=None
)
","[['Split a ', 'SparseTensor', ' into ', 'num_split', ' tensors along ', 'axis', '. (deprecated arguments)']]","input_tensor = shape = [2, 7]
[    a   d e  ]
[b c          ]
"
"tf.sparse.sparse_dense_matmul(
    sp_a, b, adjoint_a=False, adjoint_b=False, name=None
)
",[],"[[  a      ]
 [b       c]
 [    d    ]]
"
"tf.sparse.to_dense(
    sp_input, default_value=None, validate_indices=True, name=None
)
","[['Converts a ', 'SparseTensor', ' into a dense tensor.']]","sp_input = tf.sparse.SparseTensor(
  dense_shape=[3, 5],
  values=[7, 8, 9],
  indices =[[0, 1],
            [0, 3],
            [2, 0]])"
"tf.compat.v1.sparse_to_dense(
    sparse_indices,
    output_shape,
    sparse_values,
    default_value=0,
    validate_indices=True,
    name=None
)
",[],"# If sparse_indices is scalar
dense[i] = (i == sparse_indices ? sparse_values : default_value)

# If sparse_indices is a vector, then for each i
dense[sparse_indices[i]] = sparse_values[i]

# If sparse_indices is an n by d matrix, then for each i in [0, n)
dense[sparse_indices[i][0], ..., sparse_indices[i][d-1]] = sparse_values[i]
"
"tf.sparse.to_indicator(
    sp_input, vocab_size, name=None
)
","[['Converts a ', 'SparseTensor', ' of ids into a dense bool indicator tensor.']]","output[d_0, d_1, ..., d_n, sp_input[d_0, d_1, ..., d_n, k]] = True
"
"tf.sparse.transpose(
    sp_input, perm=None, name=None
)
","[['Transposes a ', 'SparseTensor']]","[0, 3]: b
[0, 1]: a
[3, 1]: d
[2, 0]: c
"
"tf.signal.dct(
    input, type=2, n=None, axis=-1, norm=None, name=None
)
","[['Computes the 1D ', 'Discrete Cosine Transform (DCT)', ' of ', 'input', '.']]",[]
"tf.signal.fft(
    input, name=None
)
",[],[]
"tf.signal.fft2d(
    input, name=None
)
",[],[]
"tf.signal.fft3d(
    input, name=None
)
",[],[]
"tf.signal.idct(
    input, type=2, n=None, axis=-1, norm=None, name=None
)
","[['Computes the 1D ', 'Inverse Discrete Cosine Transform (DCT)', ' of ', 'input', '.']]",[]
"tf.signal.ifft(
    input, name=None
)
",[],[]
"tf.signal.ifft2d(
    input, name=None
)
",[],[]
"tf.signal.ifft3d(
    input, name=None
)
",[],[]
"tf.signal.irfft(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.irfft2d(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.irfft3d(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.rfft(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.rfft2d(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.signal.rfft3d(
    input_tensor, fft_length=None, name=None
)
",[],[]
"tf.split(
    value, num_or_size_splits, axis=0, num=None, name='split'
)
","[['Splits a tensor ', 'value', ' into a list of sub tensors.']]","x = tf.Variable(tf.random.uniform([5, 30], -1, 1))
# Split `x` into 3 tensors along dimension 1
s0, s1, s2 = tf.split(x, num_or_size_splits=3, axis=1)
tf.shape(s0).numpy()
array([ 5, 10], dtype=int32)
# Split `x` into 3 tensors with sizes [4, 15, 11] along dimension 1
split0, split1, split2 = tf.split(x, [4, 15, 11], 1)
tf.shape(split0).numpy()
array([5, 4], dtype=int32)
tf.shape(split1).numpy()
array([ 5, 15], dtype=int32)
tf.shape(split2).numpy()
array([ 5, 11], dtype=int32)"
"tf.math.sqrt(
    x, name=None
)
",[],"x = tf.constant([[4.0], [16.0]])
tf.sqrt(x)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[2.],
         [4.]], dtype=float32)>
y = tf.constant([[-4.0], [16.0]])
tf.sqrt(y)
<tf.Tensor: shape=(2, 1), dtype=float32, numpy=
  array([[nan],
         [ 4.]], dtype=float32)>
z = tf.constant([[-1.0], [16.0]], dtype=tf.complex128)
tf.sqrt(z)
<tf.Tensor: shape=(2, 1), dtype=complex128, numpy=
  array([[0.0+1.j],
         [4.0+0.j]])>"
"tf.math.square(
    x, name=None
)
","[[None, '\n']]","tf.math.square([-2., 0., 3.])
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([4., 0., 9.], dtype=float32)>"
"tf.math.squared_difference(
    x, y, name=None
)
",[],[]
"tf.compat.v1.squeeze(
    input, axis=None, name=None, squeeze_dims=None
)
",[],"# 't' is a tensor of shape [1, 2, 1, 3, 1, 1]
t = tf.ones([1, 2, 1, 3, 1, 1])
print(tf.shape(tf.squeeze(t)).numpy())
[2 3]"
"tf.stack(
    values, axis=0, name='stack'
)
","[['Stacks a list of rank-', 'R', ' tensors into one rank-', '(R+1)', ' tensor.']]","x = tf.constant([1, 4])
y = tf.constant([2, 5])
z = tf.constant([3, 6])
tf.stack([x, y, z])
<tf.Tensor: shape=(3, 2), dtype=int32, numpy=
array([[1, 4],
       [2, 5],
       [3, 6]], dtype=int32)>
tf.stack([x, y, z], axis=1)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6]], dtype=int32)>"
"tf.stop_gradient(
    input, name=None
)
",[],"
  def softmax(x):
    numerator = tf.exp(x)
    denominator = tf.reduce_sum(numerator)
    return numerator / denominator
"
"tf.strided_slice(
    input_,
    begin,
    end,
    strides=None,
    begin_mask=0,
    end_mask=0,
    ellipsis_mask=0,
    new_axis_mask=0,
    shrink_axis_mask=0,
    var=None,
    name=None
)
",[],"t = tf.constant([[[1, 1, 1], [2, 2, 2]],
                 [[3, 3, 3], [4, 4, 4]],
                 [[5, 5, 5], [6, 6, 6]]])
tf.strided_slice(t, [1, 0, 0], [2, 1, 3], [1, 1, 1])  # [[[3, 3, 3]]]
tf.strided_slice(t, [1, 0, 0], [2, 2, 3], [1, 1, 1])  # [[[3, 3, 3],
                                                      #   [4, 4, 4]]]
tf.strided_slice(t, [1, -1, 0], [2, -3, 3], [1, -1, 1])  # [[[4, 4, 4],
                                                         #   [3, 3, 3]]]
"
"tf.strings.join(
    inputs, separator='', name=None
)
",[],"tf.strings.join(['abc','def']).numpy()
b'abcdef'
tf.strings.join([['abc','123'],
                 ['def','456'],
                 ['ghi','789']]).numpy()
array([b'abcdefghi', b'123456789'], dtype=object)
tf.strings.join([['abc','123'],
                 ['def','456']],
                 separator="" "").numpy()
array([b'abc def', b'123 456'], dtype=object)"
"tf.compat.v1.string_split(
    source,
    sep=None,
    skip_empty=True,
    delimiter=None,
    result_type='SparseTensor',
    name=None
)
","[['Split elements of ', 'source', ' based on ', 'delimiter', '. (deprecated arguments)']]","print(tf.compat.v1.string_split(['hello world', 'a b c']))
SparseTensor(indices=tf.Tensor( [[0 0] [0 1] [1 0] [1 1] [1 2]], ...),
             values=tf.Tensor([b'hello' b'world' b'a' b'b' b'c'], ...),
             dense_shape=tf.Tensor([2 3], shape=(2,), dtype=int64))"
"tf.strings.strip(
    input, name=None
)
",[],"tf.strings.strip([""\nTensorFlow"", ""     The python library    ""]).numpy()
array([b'TensorFlow', b'The python library'], dtype=object)"
"tf.compat.v1.string_to_hash_bucket(
    string_tensor=None, num_buckets=None, name=None, input=None
)
",[],[]
"tf.strings.to_hash_bucket_fast(
    input, num_buckets, name=None
)
",[],"tf.strings.to_hash_bucket_fast([""Hello"", ""TensorFlow"", ""2.x""], 3).numpy()
array([0, 2, 2])"
"tf.strings.to_hash_bucket_strong(
    input, num_buckets, key, name=None
)
",[],"tf.strings.to_hash_bucket_strong([""Hello"", ""TF""], 3, [1, 2]).numpy()
array([2, 0])"
"tf.compat.v1.string_to_number(
    string_tensor=None,
    out_type=tf.dtypes.float32,
    name=None,
    input=None
)
",[],"strings = [""5.0"", ""3.0"", ""7.0""]
tf.strings.to_number(strings)
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([5., 3., 7.], dtype=float32)>"
"tf.strings.as_string(
    input,
    precision=-1,
    scientific=False,
    shortest=False,
    width=-1,
    fill='',
    name=None
)
",[],"tf.strings.as_string([3, 2])
<tf.Tensor: shape=(2,), dtype=string, numpy=array([b'3', b'2'], dtype=object)>
tf.strings.as_string([3.1415926, 2.71828], precision=2).numpy()
array([b'3.14', b'2.72'], dtype=object)"
"tf.strings.bytes_split(
    input, name=None
)
","[['Split string elements of ', 'input', ' into bytes.']]","tf.strings.bytes_split('hello').numpy()
array([b'h', b'e', b'l', b'l', b'o'], dtype=object)
tf.strings.bytes_split(['hello', '123'])
<tf.RaggedTensor [[b'h', b'e', b'l', b'l', b'o'], [b'1', b'2', b'3']]>"
"tf.strings.format(
    template, inputs, placeholder='{}', summarize=3, name=None
)
",[],"tensor = tf.range(5)
tf.strings.format(""tensor: {}, suffix"", tensor)
<tf.Tensor: shape=(), dtype=string, numpy=b'tensor: [0 1 2 3 4], suffix'>"
"tf.strings.join(
    inputs, separator='', name=None
)
",[],"tf.strings.join(['abc','def']).numpy()
b'abcdef'
tf.strings.join([['abc','123'],
                 ['def','456'],
                 ['ghi','789']]).numpy()
array([b'abcdefghi', b'123456789'], dtype=object)
tf.strings.join([['abc','123'],
                 ['def','456']],
                 separator="" "").numpy()
array([b'abc def', b'123 456'], dtype=object)"
"tf.compat.v1.strings.length(
    input, name=None, unit='BYTE'
)
",[],"strings = tf.constant(['Hello','TensorFlow', ''])
tf.strings.length(strings).numpy() # default counts bytes
array([ 5, 10, 4], dtype=int32)
tf.strings.length(strings, unit=""UTF8_CHAR"").numpy()
array([ 5, 10, 1], dtype=int32)"
"tf.strings.lower(
    input, encoding='', name=None
)
",[],"tf.strings.lower(""CamelCase string and ALL CAPS"")
<tf.Tensor: shape=(), dtype=string, numpy=b'camelcase string and all caps'>"
"tf.strings.ngrams(
    data,
    ngram_width,
    separator=' ',
    pad_values=None,
    padding_width=None,
    preserve_short_sequences=False,
    name=None
)
","[['Create a tensor of n-grams based on ', 'data', '.']]","tf.strings.ngrams([""A"", ""B"", ""C"", ""D""], 2).numpy()
array([b'A B', b'B C', b'C D'], dtype=object)
tf.strings.ngrams([""TF"", ""and"", ""keras""], 1).numpy()
array([b'TF', b'and', b'keras'], dtype=object)"
"tf.compat.v1.reduce_join(
    inputs,
    axis=None,
    keep_dims=None,
    separator='',
    name=None,
    reduction_indices=None,
    keepdims=None
)
",[],"tf.strings.reduce_join([['abc','123'],
                        ['def','456']]).numpy()
b'abc123def456'
tf.strings.reduce_join([['abc','123'],
                        ['def','456']], axis=-1).numpy()
array([b'abc123', b'def456'], dtype=object)
tf.strings.reduce_join([['abc','123'],
                        ['def','456']],
                       axis=-1,
                       separator="" "").numpy()
array([b'abc 123', b'def 456'], dtype=object)"
"tf.strings.regex_full_match(
    input, pattern, name=None
)
","[[None, '\n']]","tf.strings.regex_full_match([""TF lib"", ""lib TF""], "".*lib$"")
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([ True, False])>
tf.strings.regex_full_match([""TF lib"", ""lib TF""], "".*TF$"")
<tf.Tensor: shape=(2,), dtype=bool, numpy=array([False,  True])>"
"tf.strings.regex_replace(
    input, pattern, rewrite, replace_global=True, name=None
)
","[['Replace elements of ', 'input', ' matching regex ', 'pattern', ' with ', 'rewrite', '.']]","tf.strings.regex_replace(""Text with tags.<br /><b>contains html</b>"",
                         ""<[^>]+>"", "" "")
<tf.Tensor: shape=(), dtype=string, numpy=b'Text with tags.  contains html '>"
"tf.compat.v1.strings.split(
    input=None,
    sep=None,
    maxsplit=-1,
    result_type='SparseTensor',
    source=None,
    name=None
)
","[['Split elements of ', 'input', ' based on ', 'sep', '.']]","print(tf.compat.v1.strings.split(['hello world', 'a b c']))
SparseTensor(indices=tf.Tensor( [[0 0] [0 1] [1 0] [1 1] [1 2]], ...),
             values=tf.Tensor([b'hello' b'world' b'a' b'b' b'c'], ...),
             dense_shape=tf.Tensor([2 3], shape=(2,), dtype=int64))"
"tf.strings.strip(
    input, name=None
)
",[],"tf.strings.strip([""\nTensorFlow"", ""     The python library    ""]).numpy()
array([b'TensorFlow', b'The python library'], dtype=object)"
"tf.compat.v1.strings.substr(
    input, pos, len, name=None, unit='BYTE'
)
","[['Return substrings from ', 'Tensor', ' of strings.']]","input = [b'Hello', b'World']
position = 1
length = 3

output = [b'ell', b'orl']
"
"tf.compat.v1.string_to_hash_bucket(
    string_tensor=None, num_buckets=None, name=None, input=None
)
",[],[]
"tf.strings.to_hash_bucket_fast(
    input, num_buckets, name=None
)
",[],"tf.strings.to_hash_bucket_fast([""Hello"", ""TensorFlow"", ""2.x""], 3).numpy()
array([0, 2, 2])"
"tf.strings.to_hash_bucket_strong(
    input, num_buckets, key, name=None
)
",[],"tf.strings.to_hash_bucket_strong([""Hello"", ""TF""], 3, [1, 2]).numpy()
array([2, 0])"
"tf.compat.v1.string_to_number(
    string_tensor=None,
    out_type=tf.dtypes.float32,
    name=None,
    input=None
)
",[],"strings = [""5.0"", ""3.0"", ""7.0""]
tf.strings.to_number(strings)
<tf.Tensor: shape=(3,), dtype=float32, numpy=array([5., 3., 7.], dtype=float32)>"
"tf.strings.unicode_decode(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    name=None
)
","[['Decodes each string in ', 'input', ' into a sequence of Unicode code points.']]","input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
tf.strings.unicode_decode(input, 'UTF-8').to_list()
[[71, 246, 246, 100, 110, 105, 103, 104, 116], [128522]]"
"tf.strings.unicode_decode_with_offsets(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    name=None
)
",[],"input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
result = tf.strings.unicode_decode_with_offsets(input, 'UTF-8')
result[0].to_list()  # codepoints
[[71, 246, 246, 100, 110, 105, 103, 104, 116], [128522]]
result[1].to_list()  # offsets
[[0, 1, 3, 5, 6, 7, 8, 9, 10], [0]]"
"tf.strings.unicode_encode(
    input,
    output_encoding,
    errors='replace',
    replacement_char=65533,
    name=None
)
","[['Encodes each sequence of Unicode code points in ', 'input', ' into a string.']]","  * `'replace'`: Replace invalid codepoint with the
    `replacement_char`. (default)
  * `'ignore'`: Skip invalid codepoints.
  * `'strict'`: Raise an exception for any invalid codepoint.
"
"tf.strings.unicode_script(
    input, name=None
)
",[],"tf.strings.unicode_script([1, 31, 38])
<tf.Tensor: shape=(3,), dtype=int32, numpy=array([0, 0, 0], dtype=int32)>"
"tf.strings.unicode_split(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    name=None
)
","[['Splits each string in ', 'input', ' into a sequence of Unicode code points.']]","input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
tf.strings.unicode_split(input, 'UTF-8').to_list()
[[b'G', b'\xc3\xb6', b'\xc3\xb6', b'd', b'n', b'i', b'g', b'h', b't'],
 [b'\xf0\x9f\x98\x8a']]"
"tf.strings.unicode_split_with_offsets(
    input,
    input_encoding,
    errors='replace',
    replacement_char=65533,
    name=None
)
",[],"input = [s.encode('utf8') for s in (u'G\xf6\xf6dnight', u'\U0001f60a')]
result = tf.strings.unicode_split_with_offsets(input, 'UTF-8')
result[0].to_list()  # character substrings
[[b'G', b'\xc3\xb6', b'\xc3\xb6', b'd', b'n', b'i', b'g', b'h', b't'],
 [b'\xf0\x9f\x98\x8a']]
result[1].to_list()  # offsets
[[0, 1, 3, 5, 6, 7, 8, 9, 10], [0]]"
"tf.strings.unicode_transcode(
    input,
    input_encoding,
    output_encoding,
    errors='replace',
    replacement_char=65533,
    replace_control_characters=False,
    name=None
)
",[],"tf.strings.unicode_transcode([""Hello"", ""TensorFlow"", ""2.x""], ""UTF-8"", ""UTF-16-BE"")
<tf.Tensor: shape=(3,), dtype=string, numpy=
array([b'\x00H\x00e\x00l\x00l\x00o',
       b'\x00T\x00e\x00n\x00s\x00o\x00r\x00F\x00l\x00o\x00w',
       b'\x002\x00.\x00x'], dtype=object)>
tf.strings.unicode_transcode([""A"", ""B"", ""C""], ""US ASCII"", ""UTF-8"").numpy()
array([b'A', b'B', b'C'], dtype=object)"
"tf.strings.unsorted_segment_join(
    inputs, segment_ids, num_segments, separator='', name=None
)
","[['Joins the elements of ', 'inputs', ' based on ', 'segment_ids', '.']]","output[i, k1...kM] = strings.join([data[j1...jN, k1...kM])
"
"tf.strings.upper(
    input, encoding='', name=None
)
",[],"tf.strings.upper(""CamelCase string and ALL CAPS"")
<tf.Tensor: shape=(), dtype=string, numpy=b'CAMELCASE STRING AND ALL CAPS'>"
"tf.compat.v1.substr(
    input, pos, len, name=None, unit='BYTE'
)
","[['Return substrings from ', 'Tensor', ' of strings.']]","input = [b'Hello', b'World']
position = 1
length = 3

output = [b'ell', b'orl']
"
"tf.math.subtract(
    x, y, name=None
)
",[],"x = [1, 2, 3, 4, 5]
y = 1
tf.subtract(x, y)
<tf.Tensor: shape=(5,), dtype=int32, numpy=array([0, 1, 2, 3, 4], dtype=int32)>
tf.subtract(y, x)
<tf.Tensor: shape=(5,), dtype=int32,
numpy=array([ 0, -1, -2, -3, -4], dtype=int32)>"
"tf.compat.v1.summary.FileWriter(
    logdir,
    graph=None,
    max_queue=10,
    flush_secs=120,
    graph_def=None,
    filename_suffix=None,
    session=None
)
","[['Writes ', 'Summary', ' protocol buffers to event files.']]","dist = tf.compat.v1.placeholder(tf.float32, [100])
tf.compat.v1.summary.histogram(name=""distribution"", values=dist)
writer = tf.compat.v1.summary.FileWriter(""/tmp/tf1_summary_example"")
summaries = tf.compat.v1.summary.merge_all()

sess = tf.compat.v1.Session()
for step in range(100):
  mean_moving_normal = np.random.normal(loc=step, scale=1, size=[100])
  summ = sess.run(summaries, feed_dict={dist: mean_moving_normal})
  writer.add_summary(summ, global_step=step)
"
"tf.compat.v1.summary.audio(
    name, tensor, sample_rate, max_outputs=3, collections=None, family=None
)
","[['Outputs a ', 'Summary', ' protocol buffer with audio.']]",[]
"tf.compat.v1.summary.get_summary_description(
    node_def
)
",[],[]
"tf.compat.v1.summary.histogram(
    name, values, collections=None, family=None
)
","[['Outputs a ', 'Summary', ' protocol buffer with a histogram.']]",[]
"tf.compat.v1.summary.image(
    name, tensor, max_outputs=3, collections=None, family=None
)
","[['Outputs a ', 'Summary', ' protocol buffer with images.']]",[]
"tf.compat.v1.summary.initialize(
    graph=None, session=None
)
",[],[]
"tf.compat.v1.summary.merge(
    inputs, collections=None, name=None
)
",[],"dist = tf.compat.v1.placeholder(tf.float32, [100])
tf.compat.v1.summary.histogram(name=""distribution"", values=dist)
writer = tf.compat.v1.summary.FileWriter(""/tmp/tf1_summary_example"")
summaries = tf.compat.v1.summary.merge_all()

sess = tf.compat.v1.Session()
for step in range(100):
  mean_moving_normal = np.random.normal(loc=step, scale=1, size=[100])
  summ = sess.run(summaries, feed_dict={dist: mean_moving_normal})
  writer.add_summary(summ, global_step=step)
"
"tf.compat.v1.summary.merge_all(
    key=_ops.GraphKeys.SUMMARIES, scope=None, name=None
)
",[],"dist = tf.compat.v1.placeholder(tf.float32, [100])
tf.compat.v1.summary.histogram(name=""distribution"", values=dist)
writer = tf.compat.v1.summary.FileWriter(""/tmp/tf1_summary_example"")
summaries = tf.compat.v1.summary.merge_all()

sess = tf.compat.v1.Session()
for step in range(100):
  mean_moving_normal = np.random.normal(loc=step, scale=1, size=[100])
  summ = sess.run(summaries, feed_dict={dist: mean_moving_normal})
  writer.add_summary(summ, global_step=step)
"
"tf.compat.v1.summary.scalar(
    name, tensor, collections=None, family=None
)
","[['Outputs a ', 'Summary', ' protocol buffer containing a single scalar value.']]",[]
"tf.compat.v1.summary.tensor_summary(
    name,
    tensor,
    summary_description=None,
    collections=None,
    summary_metadata=None,
    family=None,
    display_name=None
)
","[['Outputs a ', 'Summary', ' protocol buffer with a serialized tensor.proto.']]",[]
"tf.compat.v1.summary.text(
    name, tensor, collections=None
)
",[],[]
"tf.linalg.svd(
    tensor, full_matrices=False, compute_uv=True, name=None
)
","[[None, '\n']]","# a is a tensor.
# s is a tensor of singular values.
# u is a tensor of left singular vectors.
# v is a tensor of right singular vectors.
s, u, v = svd(a)
s = svd(a, compute_uv=False)
"
"tf.switch_case(
    branch_index, branch_fns, default=None, name='switch_case'
)
",[],"switch (branch_index) {  // c-style switch
  case 0: return 17;
  case 1: return 31;
  default: return -1;
}
"
"tf.compat.v1.tables_initializer(
    name='init_all_tables'
)
",[],"with tf.compat.v1.Session():
  init = tf.compat.v1.lookup.KeyValueTensorInitializer(['a', 'b'], [1, 2])
  table = tf.compat.v1.lookup.StaticHashTable(init, default_value=-1)
  tf.compat.v1.tables_initializer().run()
  result = table.lookup(tf.constant(['a', 'c'])).eval()
result
array([ 1, -1], dtype=int32)"
"tf.math.tan(
    x, name=None
)
",[],"  x = tf.constant([-float(""inf""), -9, -0.5, 1, 1.2, 200, 10000, float(""inf"")])
  tf.math.tan(x) ==> [nan 0.45231566 -0.5463025 1.5574077 2.572152 -1.7925274 0.32097113 nan]
"
"tf.math.tanh(
    x, name=None
)
","[['Computes hyperbolic tangent of ', 'x', ' element-wise.']]",[]
"tf.tensor_scatter_nd_add(
    tensor, indices, updates, name=None
)
","[['Adds sparse ', 'updates', ' to an existing tensor according to ', 'indices', '.']]","indices.shape[-1] <= tensor.shape.rank
"
"tf.tensor_scatter_nd_add(
    tensor, indices, updates, name=None
)
","[['Adds sparse ', 'updates', ' to an existing tensor according to ', 'indices', '.']]","indices.shape[-1] <= tensor.shape.rank
"
"tf.tensor_scatter_nd_max(
    tensor, indices, updates, name=None
)
",[],"tensor = [0, 0, 0, 0, 0, 0, 0, 0]
indices = [[1], [4], [5]]
updates = [1, -1, 1]
tf.tensor_scatter_nd_max(tensor, indices, updates).numpy()
array([0, 1, 0, 0, 0, 1, 0, 0], dtype=int32)"
"tf.tensor_scatter_nd_min(
    tensor, indices, updates, name=None
)
",[],[]
"tf.tensor_scatter_nd_sub(
    tensor, indices, updates, name=None
)
","[['Subtracts sparse ', 'updates', ' from an existing tensor according to ', 'indices', '.']]","indices.shape[-1] <= shape.rank
"
"tf.tensor_scatter_nd_update(
    tensor, indices, updates, name=None
)
","[['Scatter ', 'updates', ' into an existing tensor according to ', 'indices', '.']]","# Not implemented: tensors cannot be updated inplace.
tensor[indices] = updates
"
"tf.tensor_scatter_nd_sub(
    tensor, indices, updates, name=None
)
","[['Subtracts sparse ', 'updates', ' from an existing tensor according to ', 'indices', '.']]","indices.shape[-1] <= shape.rank
"
"tf.tensor_scatter_nd_update(
    tensor, indices, updates, name=None
)
","[['Scatter ', 'updates', ' into an existing tensor according to ', 'indices', '.']]","# Not implemented: tensors cannot be updated inplace.
tensor[indices] = updates
"
"tf.tensordot(
    a, b, axes, name=None
)
","[[None, '\n']]",[]
"tf.test.TestCase(
    methodName='runTest'
)
","[[None, '\n']]","@classmethod
addClassCleanup(
    function, *args, /, **kwargs
)
"
"tf.test.TestCase.failureException(
    *args, **kwargs
)
",[],[]
"tf.compat.v1.test.assert_equal_graph_def(
    actual, expected, checkpoint_v2=False, hash_table_shared_name=False
)
","[['Asserts that two ', 'GraphDef', 's are (mostly) the same.']]",[]
"tf.compat.v1.test.compute_gradient(
    x,
    x_shape,
    y,
    y_shape,
    x_init_value=None,
    delta=0.001,
    init_targets=None,
    extra_feed_dict=None
)
",[],"J[:m, :n] = d(Re y)/d(Re x)
J[:m, n:] = d(Im y)/d(Re x)
J[m:, :n] = d(Re y)/d(Im x)
J[m:, n:] = d(Im y)/d(Im x)
"
"tf.compat.v1.test.compute_gradient_error(
    x,
    x_shape,
    y,
    y_shape,
    x_init_value=None,
    delta=0.001,
    init_targets=None,
    extra_feed_dict=None
)
",[],[]
"tf.test.create_local_cluster(
    num_workers,
    num_ps,
    protocol='grpc',
    worker_config=None,
    ps_config=None
)
","[['Create and start local servers and return the associated ', 'Server', ' objects.']]","workers, _ = tf.test.create_local_cluster(num_workers=2, num_ps=2)

worker_sessions = [tf.compat.v1.Session(w.target) for w in workers]

with tf.device(""/job:ps/task:0""):
  ...
with tf.device(""/job:ps/task:1""):
  ...
with tf.device(""/job:worker/task:0""):
  ...
with tf.device(""/job:worker/task:1""):
  ...

worker_sessions[0].run(...)
"
"tf.test.disable_with_predicate(
    pred, skip_message
)
",[],[]
"tf.test.is_gpu_available(
    cuda_only=False, min_cuda_compute_capability=None
)
",[],">>> gpu_available = tf.test.is_gpu_available()
>>> is_cuda_gpu_available = tf.test.is_gpu_available(cuda_only=True)
>>> is_cuda_gpu_min_3 = tf.test.is_gpu_available(True, (3,0))
"
"tf.test.main(
    argv=None
)
",[],[]
"tf.compat.v1.test.test_src_dir_path(
    relative_path
)
",[],[]
"tf.test.with_eager_op_as_function(
    cls=None, only_as_function=False
)
",[],[]
"tf.tile(
    input, multiples, name=None
)
",[],"a = tf.constant([[1,2,3],[4,5,6]], tf.int32)
b = tf.constant([1,2], tf.int32)
tf.tile(a, b)
<tf.Tensor: shape=(2, 6), dtype=int32, numpy=
array([[1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6]], dtype=int32)>
c = tf.constant([2,1], tf.int32)
tf.tile(a, c)
<tf.Tensor: shape=(4, 3), dtype=int32, numpy=
array([[1, 2, 3],
       [4, 5, 6],
       [1, 2, 3],
       [4, 5, 6]], dtype=int32)>
d = tf.constant([2,2], tf.int32)
tf.tile(a, d)
<tf.Tensor: shape=(4, 6), dtype=int32, numpy=
array([[1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6],
       [1, 2, 3, 1, 2, 3],
       [4, 5, 6, 4, 5, 6]], dtype=int32)>"
"tf.timestamp(
    name=None
)
",[],[]
"tf.compat.v1.to_bfloat16(
    x, name='ToBFloat16'
)
","[['Casts a tensor to type ', 'bfloat16', '. (deprecated)']]","tf.compat.v1.to_bfloat16(tf.constant(3.14, dtype=tf.float32))
<tf.Tensor: shape=(), dtype=bfloat16, numpy=3.14>"
"tf.compat.v1.to_complex128(
    x, name='ToComplex128'
)
","[['Casts a tensor to type ', 'complex128', '. (deprecated)']]","tf.compat.v1.to_complex128(tf.constant(1. + 2.j, dtype=tf.complex64))
<tf.Tensor: shape=(), dtype=complex128, numpy=(1+2j)>"
"tf.compat.v1.to_complex64(
    x, name='ToComplex64'
)
","[['Casts a tensor to type ', 'complex64', '. (deprecated)']]","tf.compat.v1.to_complex64(tf.constant(1. + 2.j, dtype=tf.complex128))
<tf.Tensor: shape=(), dtype=complex64, numpy=(1+2j)>"
"tf.compat.v1.to_double(
    x, name='ToDouble'
)
","[['Casts a tensor to type ', 'float64', '. (deprecated)']]","tf.compat.v1.to_double(tf.constant(3.14, dtype=tf.float32))
<tf.Tensor: shape=(), dtype=float64, numpy=3.14>"
"tf.compat.v1.to_float(
    x, name='ToFloat'
)
","[['Casts a tensor to type ', 'float32', '. (deprecated)']]","tf.compat.v1.to_float(tf.constant(3.14, dtype=tf.double))
<tf.Tensor: shape=(), dtype=float32, numpy=3.14>"
"tf.compat.v1.to_int32(
    x, name='ToInt32'
)
","[['Casts a tensor to type ', 'int32', '. (deprecated)']]","tf.compat.v1.to_int32(tf.constant(1, dtype=tf.int64))
<tf.Tensor: shape=(), dtype=int32, numpy=1>"
"tf.compat.v1.to_int64(
    x, name='ToInt64'
)
","[['Casts a tensor to type ', 'int64', '. (deprecated)']]","tf.compat.v1.to_int64(tf.constant(1, dtype=tf.int32))
<tf.Tensor: shape=(), dtype=int64, numpy=1>"
"tf.compat.v1.tpu.CrossShardOptimizer(
    opt,
    reduction=losses.Reduction.MEAN,
    name='CrossShardOptimizer',
    group_assignment=None
)
","[['Inherits From: ', 'Optimizer']]","apply_gradients(
    grads_and_vars, global_step=None, name=None
)
"
"tf.tpu.XLAOptions(
    use_spmd_for_xla_partitioning=True, enable_xla_dynamic_padder=True
)
",[],[]
"tf.compat.v1.tpu.batch_parallel(
    computation: Callable[..., Any],
    inputs: Optional[List[List[Optional[core_types.Tensor]]]] = None,
    num_shards: int = 1,
    infeed_queue: Optional[tpu_feed.InfeedQueue] = None,
    device_assignment: Optional[tf.tpu.experimental.DeviceAssignment] = None,
    name: Optional[Text] = None,
    xla_options: Optional[tf.tpu.XLAOptions] = None
)
","[['Shards ', 'computation', ' along the batch dimension for parallel execution.']]",[]
"tf.compat.v1.tpu.core(
    num: int
) -> Text
",[],[]
"tf.compat.v1.tpu.cross_replica_sum(
    x, group_assignment=None, name=None
)
",[],[]
"tf.compat.v1.tpu.experimental.AdagradParameters(
    learning_rate: float,
    initial_accumulator: float = 0.1,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: Optional[bool] = None,
    clip_gradient_min: Optional[float] = None,
    clip_gradient_max: Optional[float] = None
)
",[],"estimator = tf.estimator.tpu.TPUEstimator(
    ...
    embedding_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(
        ...
        optimization_parameters=tf.tpu.experimental.AdagradParameters(0.1),
        ...))
"
"tf.compat.v1.tpu.experimental.AdamParameters(
    learning_rate: float,
    beta1: float = 0.9,
    beta2: float = 0.999,
    epsilon: float = 1e-08,
    lazy_adam: bool = True,
    sum_inside_sqrt: bool = True,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: Optional[bool] = None,
    clip_gradient_min: Optional[float] = None,
    clip_gradient_max: Optional[float] = None
)
",[],"estimator = tf.estimator.tpu.TPUEstimator(
    ...
    embedding_config_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(
        ...
        optimization_parameters=tf.tpu.experimental.AdamParameters(0.1),
        ...))
"
"tf.tpu.experimental.DeviceAssignment(
    topology: tf.tpu.experimental.Topology,
    core_assignment: np.ndarray
)
",[],"@staticmethod
build(
    topology: "
"tf.compat.v1.tpu.experimental.FtrlParameters(
    learning_rate: float,
    learning_rate_power: float = -0.5,
    initial_accumulator_value: float = 0.1,
    l1_regularization_strength: float = 0.0,
    l2_regularization_strength: float = 0.0,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: Optional[bool] = None,
    multiply_linear_by_learning_rate: bool = False,
    beta: float = 0,
    allow_zero_accumulator: bool = False,
    clip_gradient_min: Optional[float] = None,
    clip_gradient_max: Optional[float] = None
)
",[],"estimator = tf.estimator.tpu.TPUEstimator(
    ...
    embedding_config_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(
        ...
        optimization_parameters=tf.tpu.experimental.FtrlParameters(0.1),
        ...))
"
"tf.tpu.experimental.HardwareFeature(
    tpu_hardware_feature_proto
)
",[],[]
"tf.compat.v1.tpu.experimental.StochasticGradientDescentParameters(
    learning_rate: float,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: Optional[bool] = None,
    clip_gradient_min: Optional[float] = None,
    clip_gradient_max: Optional[float] = None
)
",[],"estimator = tf.estimator.tpu.TPUEstimator(
    ...
    embedding_config_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(
        ...
        optimization_parameters=(
            tf.tpu.experimental.StochasticGradientDescentParameters(0.1))))
"
"tf.tpu.experimental.TPUSystemMetadata(
    num_cores, num_hosts, num_of_cores_per_host, topology, devices
)
",[],[]
"tf.tpu.experimental.Topology(
    serialized=None, mesh_shape=None, device_coordinates=None
)
",[],"cpu_device_name_at_coordinates(
    device_coordinates, job=None
)
"
"tf.tpu.experimental.embedding.Adagrad(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    initial_accumulator_value: float = 0.1,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None
)
",[],"embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.Adagrad(0.1))
"
"tf.tpu.experimental.embedding.AdagradMomentum(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    momentum: float = 0.0,
    use_nesterov: bool = False,
    exponent: float = 2,
    beta2: float = 1,
    epsilon: float = 1e-10,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None
)
",[],"embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.AdagradMomentum(0.1))
"
"tf.tpu.experimental.embedding.Adam(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    beta_1: float = 0.9,
    beta_2: float = 0.999,
    epsilon: float = 1e-07,
    lazy_adam: bool = True,
    sum_inside_sqrt: bool = True,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None
)
",[],"embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.Adam(0.1))
"
"tf.tpu.experimental.embedding.FTRL(
    learning_rate: Union[float, Callable[[], float]] = 0.001,
    learning_rate_power: float = -0.5,
    l1_regularization_strength: float = 0.0,
    l2_regularization_strength: float = 0.0,
    beta: float = 0.0,
    initial_accumulator_value: float = 0.1,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    slot_variable_creation_fn: Optional[SlotVarCreationFnType] = None,
    clipvalue: Optional[ClipValueType] = None,
    multiply_linear_by_learning_rate: bool = False,
    allow_zero_accumulator: bool = False
)
",[],"embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.FTRL(0.1))
"
"tf.tpu.experimental.embedding.FeatureConfig(
    table: tf.tpu.experimental.embedding.TableConfig,
    max_sequence_length: int = 0,
    validate_weights_and_indices: bool = True,
    output_shape: Optional[Union[List[int], tf.TensorShape]] = None,
    name: Optional[Text] = None
)
",[],"table_config_one = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
table_config_two = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
feature_config = {
    'feature_one': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_two': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_three': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_two)}
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config=feature_config,
    batch_size=...
    optimizer=tf.tpu.experimental.embedding.Adam(0.1))
"
"tf.tpu.experimental.embedding.QuantizationConfig(
    num_buckets: int, lower: float, upper: float
)
",[],"if input < lower
  input = lower
if input > upper
  input = upper
quantum = (upper - lower) / (num_buckets - 1)
input = math.floor((input - lower) / quantum + 0.5) * quantium + lower
"
"tf.tpu.experimental.embedding.SGD(
    learning_rate: Union[float, Callable[[], float]] = 0.01,
    use_gradient_accumulation: bool = True,
    clip_weight_min: Optional[float] = None,
    clip_weight_max: Optional[float] = None,
    weight_decay_factor: Optional[float] = None,
    multiply_weight_decay_factor_by_learning_rate: bool = None,
    clipvalue: Optional[ClipValueType] = None
)
",[],"embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    ...
    optimizer=tf.tpu.experimental.embedding.SGD(0.1))
"
"tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],
    optimizer: Optional[tpu_embedding_v2_utils._Optimizer],
    pipeline_execution_with_tensor_core: bool = False
)
",[],"table_config_one = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
table_config_two = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
feature_config = {
    'feature_one': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_two': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_three': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_two)}
"
"tf.tpu.experimental.embedding.TPUEmbeddingForServing(
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],
    optimizer: Optional[tpu_embedding_v2_utils._Optimizer]
)
",[],"model = model_fn(...)
strategy = tf.distribute.TPUStrategy(...)
with strategy.scope():
  embedding = tf.tpu.experimental.embedding.TPUEmbedding(
      feature_config=feature_config,
      optimizer=tf.tpu.experimental.embedding.SGD(0.1))

# Your custom training code.

checkpoint = tf.train.Checkpoint(model=model, embedding=embedding)
checkpoint.save(...)

"
"tf.tpu.experimental.embedding.TPUEmbeddingV0(
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable],
    optimizer: Optional[tpu_embedding_v2_utils._Optimizer]
)
",[],"strategy = tf.distribute.TPUStrategy(...)
with strategy.scope():
  embedding = tf.tpu.experimental.embedding.TPUEmbeddingV0(
      feature_config=feature_config,
      optimizer=tf.tpu.experimental.embedding.SGD(0.1))
"
"tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size: int,
    dim: int,
    initializer: Optional[Callable[[Any], None]] = None,
    optimizer: Optional[_Optimizer] = None,
    combiner: Text = 'mean',
    name: Optional[Text] = None,
    quantization_config: tf.tpu.experimental.embedding.QuantizationConfig = None
)
",[],"table_config_one = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
table_config_two = tf.tpu.experimental.embedding.TableConfig(
    vocabulary_size=...,
    dim=...)
feature_config = {
    'feature_one': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_two': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_one),
    'feature_three': tf.tpu.experimental.embedding.FeatureConfig(
        table=table_config_two)}
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config=feature_config,
    batch_size=...
    optimizer=tf.tpu.experimental.embedding.Adam(0.1))
"
"tf.tpu.experimental.embedding.serving_embedding_lookup(
    inputs: Any,
    weights: Optional[Any],
    tables: Dict[tf.tpu.experimental.embedding.TableConfig, tf.Variable],
    feature_config: Union[tf.tpu.experimental.embedding.FeatureConfig, Iterable]
) -> Any
","[['Apply standard lookup ops with ', 'tf.tpu.experimental.embedding', ' configs.']]","model = model_fn(...)
embedding = tf.tpu.experimental.embedding.TPUEmbedding(
    feature_config=feature_config,
    batch_size=1024,
    optimizer=tf.tpu.experimental.embedding.SGD(0.1))
checkpoint = tf.train.Checkpoint(model=model, embedding=embedding)
checkpoint.restore(...)

@tf.function(input_signature=[{'feature_one': tf.TensorSpec(...),
                               'feature_two': tf.TensorSpec(...),
                               'feature_three': tf.TensorSpec(...)}])
def serve_tensors(embedding_features):
  embedded_features = tf.tpu.experimental.embedding.serving_embedding_lookup(
      embedding_features, None, embedding.embedding_tables,
      feature_config)
  return model(embedded_features)

model.embedding_api = embedding
tf.saved_model.save(model,
                    export_dir=...,
                    signatures={'serving_default': serve_tensors})

"
"tf.compat.v1.tpu.experimental.embedding_column(
    categorical_column,
    dimension,
    combiner='mean',
    initializer=None,
    max_sequence_length=0,
    learning_rate_fn=None,
    embedding_lookup_device=None,
    tensor_core_shape=None,
    use_safe_embedding_lookup=True
)
","[['TPU version of ', 'tf.compat.v1.feature_column.embedding_column', '.']]","column = tf.feature_column.categorical_column_with_identity(...)
tpu_column = tf.tpu.experimental.embedding_column(column, 10)
...
def model_fn(features):
  dense_feature = tf.keras.layers.DenseFeature(tpu_column)
  embedded_feature = dense_feature(features)
  ...

estimator = tf.estimator.tpu.TPUEstimator(
    model_fn=model_fn,
    ...
    embedding_config_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(
      column=[tpu_column],
      ...))
"
"tf.tpu.experimental.initialize_tpu_system(
    cluster_resolver=None
)
",[],[]
"tf.compat.v1.tpu.experimental.shared_embedding_columns(
    categorical_columns,
    dimension,
    combiner='mean',
    initializer=None,
    shared_embedding_collection_name=None,
    max_sequence_lengths=None,
    learning_rate_fn=None,
    embedding_lookup_device=None,
    tensor_core_shape=None,
    use_safe_embedding_lookup=True
)
","[['TPU version of ', 'tf.compat.v1.feature_column.shared_embedding_columns', '.']]","column_a = tf.feature_column.categorical_column_with_identity(...)
column_b = tf.feature_column.categorical_column_with_identity(...)
tpu_columns = tf.tpu.experimental.shared_embedding_columns(
    [column_a, column_b], 10)
...
def model_fn(features):
  dense_feature = tf.keras.layers.DenseFeature(tpu_columns)
  embedded_feature = dense_feature(features)
  ...

estimator = tf.estimator.tpu.TPUEstimator(
    model_fn=model_fn,
    ...
    embedding_config_spec=tf.estimator.tpu.experimental.EmbeddingConfigSpec(
        column=tpu_columns,
        ...))
"
"tf.tpu.experimental.shutdown_tpu_system(
    cluster_resolver=None
)
",[],[]
"tf.compat.v1.tpu.initialize_system(
    embedding_config: Optional[embedding_pb2.TPUEmbeddingConfiguration] = None,
    job: Optional[Text] = None,
    compilation_failure_closes_chips: bool = True,
    tpu_cancellation_closes_chips: Optional[bool] = None
) -> core_types.Tensor
",[],[]
"tf.compat.v1.tpu.outside_compilation(
    computation: Callable[..., Any], *args, **kwargs
) -> Any
",[],"def computation_with_string_ops(x):
  # strings types are not supported on TPU's and below ops must
  # run on CPU instead.
  output = tf.strings.format('1{}', x)
  return tf.strings.to_number(output)

def tpu_computation():
  # Expected output is 11.
  output = tf.tpu.outside_compilation(computation_with_string_ops, 1)
"
"tf.compat.v1.tpu.replicate(
    computation: Callable[..., Any],
    inputs: Optional[List[List[core_types.Tensor]]] = None,
    infeed_queue: Optional[tpu_feed.InfeedQueue] = None,
    device_assignment: Optional[tf.tpu.experimental.DeviceAssignment] = None,
    name: Optional[Text] = None,
    maximum_shapes: Optional[Any] = None,
    padding_spec: Optional[tf.compat.v1.tpu.PaddingSpec] = None,
    xla_options: Optional[tf.tpu.XLAOptions] = None
) -> List[Any]
",[],"
def computation(x):
  x = x + 1
  return tf.math.reduce_mean(x)

x = tf.convert_to_tensor([1., 2., 3.])
y = tf.convert_to_tensor([4., 5., 6.])
tf.compat.v1.tpu.replicate(computation, inputs=[[x], [y]])
"
"tf.compat.v1.tpu.rewrite(
    computation: Callable[..., Any],
    inputs: Optional[List[List[Optional[core_types.Tensor]]]] = None,
    infeed_queue: Optional[tpu_feed.InfeedQueue] = None,
    device_assignment: Optional[tf.tpu.experimental.DeviceAssignment] = None,
    name: Optional[Text] = None,
    xla_options: Optional[tf.tpu.XLAOptions] = None
) -> Any
","[['Rewrites ', 'computation', ' for execution on a TPU system.']]",[]
"tf.compat.v1.tpu.shard(
    computation: Callable[..., Any],
    inputs: Optional[List[core_types.Tensor]] = None,
    num_shards: int = 1,
    input_shard_axes: Optional[List[int]] = None,
    outputs_from_all_shards: Union[bool, List[bool]] = True,
    output_shard_axes: Optional[List[int]] = None,
    infeed_queue: Optional[tpu_feed.InfeedQueue] = None,
    device_assignment: Optional[tf.tpu.experimental.DeviceAssignment] = None,
    name: Optional[Text] = None,
    xla_options: Optional[tf.tpu.XLAOptions] = None
) -> List[core_types.Tensor]
","[['Shards ', 'computation', ' for parallel execution.']]",[]
"tf.compat.v1.tpu.shutdown_system(
    job: Optional[Text] = None
) -> tf.Operation
",[],[]
"tf.linalg.trace(
    x, name=None
)
","[['Compute the trace of a tensor ', 'x', '.']]","x = tf.constant([[1, 2], [3, 4]])
tf.linalg.trace(x)  # 5

x = tf.constant([[1, 2, 3],
                 [4, 5, 6],
                 [7, 8, 9]])
tf.linalg.trace(x)  # 15

x = tf.constant([[[1, 2, 3],
                  [4, 5, 6],
                  [7, 8, 9]],
                 [[-1, -2, -3],
                  [-4, -5, -6],
                  [-7, -8, -9]]])
tf.linalg.trace(x)  # [15, -15]
"
"tf.compat.v1.train.AdadeltaOptimizer(
    learning_rate=0.001,
    rho=0.95,
    epsilon=1e-08,
    use_locking=False,
    name='Adadelta'
)
","[['Inherits From: ', 'Optimizer']]","optimizer = tf.compat.v1.train.AdadeltaOptimizer(
  learning_rate=learning_rate,
  rho=rho,
  epsilon=epsilon)
"
"tf.compat.v1.train.AdagradDAOptimizer(
    learning_rate,
    global_step,
    initial_gradient_squared_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    use_locking=False,
    name='AdagradDA'
)
","[['Inherits From: ', 'Optimizer']]","apply_gradients(
    grads_and_vars, global_step=None, name=None
)
"
"tf.compat.v1.train.AdagradOptimizer(
    learning_rate,
    initial_accumulator_value=0.1,
    use_locking=False,
    name='Adagrad'
)
","[['Inherits From: ', 'Optimizer']]","optimizer = tf.compat.v1.train.AdagradOptimizer(
  learning_rate=learning_rate,
  initial_accumulator_value=initial_accumulator_value)
"
"tf.compat.v1.train.AdamOptimizer(
    learning_rate=0.001,
    beta1=0.9,
    beta2=0.999,
    epsilon=1e-08,
    use_locking=False,
    name='Adam'
)
","[['Inherits From: ', 'Optimizer']]","optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate=0.001)
"
tf.io.parse_example(,"[['Used in ', 'tf.train.Example', ' protos. Holds a list of byte-strings.'], ['An ', 'Example', ' proto is a representation of the following python type:'], ['This proto implements the ', 'List[bytes]', ' portion.'], ['\n', 'from google.protobuf import text_format', '\n', ""example = text_format.Parse('''"", '\n', '  features {', '\n', '    feature {key: ""my_feature""', '\n', ""             value {bytes_list {value: ['abc', '12345' ]} } }"", '\n', ""  }''',"", '\n', '  tf.train.Example())', '\n', None, '\n', ""example.features.feature['my_feature'].bytes_list.value"", '\n', '[""abc"", ""12345""]', '\n'], ['Use ', 'tf.io.parse_example', ' to extract tensors from a serialized ', 'Example', ' proto:']]","from google.protobuf import text_format
example = text_format.Parse('''
  features {
    feature {key: ""my_feature""
             value {bytes_list {value: ['abc', '12345' ]} } }
  }''',
  tf.train.Example())
example.features.feature['my_feature'].bytes_list.value
[""abc"", ""12345""]"
"tf.compat.v1.train.Checkpoint(
    **kwargs
)
",[],"import tensorflow as tf
import os

checkpoint_directory = ""/tmp/training_checkpoints""
checkpoint_prefix = os.path.join(checkpoint_directory, ""ckpt"")

checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
status = checkpoint.restore(tf.train.latest_checkpoint(checkpoint_directory))
train_op = optimizer.minimize( ... )
status.assert_consumed()  # Optional sanity checks.
with tf.compat.v1.Session() as session:
  # Use the Session to restore variables, or initialize them if
  # tf.train.latest_checkpoint returned None.
  status.initialize_or_restore(session)
  for _ in range(num_training_steps):
    session.run(train_op)
  checkpoint.save(file_prefix=checkpoint_prefix)
"
"tf.train.CheckpointManager(
    checkpoint,
    directory,
    max_to_keep,
    keep_checkpoint_every_n_hours=None,
    checkpoint_name='ckpt',
    step_counter=None,
    checkpoint_interval=None,
    init_fn=None
)
",[],"import tensorflow as tf
checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=model)
manager = tf.train.CheckpointManager(
    checkpoint, directory=""/tmp/model"", max_to_keep=5)
status = checkpoint.restore(manager.latest_checkpoint)
while True:
  # train
  manager.save()
"
"tf.train.CheckpointOptions(
    experimental_io_device=None, experimental_enable_async_checkpoint=False
)
",[],"step = tf.Variable(0, name=""step"")
checkpoint = tf.train.Checkpoint(step=step)
options = tf.train.CheckpointOptions(experimental_io_device=""/job:localhost"")
checkpoint.save(""/tmp/ckpt"", options=options)
"
"tf.estimator.CheckpointSaverHook(
    checkpoint_dir,
    save_secs=None,
    save_steps=None,
    saver=None,
    checkpoint_basename='model.ckpt',
    scaffold=None,
    listeners=None,
    save_graph_def=True
)
","[['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.compat.v1.train.ChiefSessionCreator(
    scaffold=None,
    master='',
    config=None,
    checkpoint_dir=None,
    checkpoint_filename_with_path=None
)
","[['Inherits From: ', 'SessionCreator']]","create_session()
"
"tf.train.ClusterSpec(
    cluster
)
",[],"cluster = tf.train.ClusterSpec({""worker"": [""worker0.example.com:2222"",
                                           ""worker1.example.com:2222"",
                                           ""worker2.example.com:2222""],
                                ""ps"": [""ps0.example.com:2222"",
                                       ""ps1.example.com:2222""]})
"
"tf.train.Coordinator(
    clean_stop_exception_types=None
)
",[],"# Create a coordinator.
coord = Coordinator()
# Start a number of threads, passing the coordinator to each of them.
...start thread 1...(coord, ...)
...start thread N...(coord, ...)
# Wait for all the threads to terminate.
coord.join(threads)
"
tf.io.parse_example(,"[['An ', 'Example', ' is a standard proto storing data for training and inference.'], ['An ', 'Example', ' proto is a representation of the following python type:'], ['It contains a key-value store ', 'Example.features', ' where each key (string) maps\nto a ', 'tf.train.Feature', ' message which contains a fixed-type list. This flexible\nand compact format allows the storage of large amounts of typed data, but\nrequires that the data shape and use be determined by the configuration files\nand parsers that are used to read and write this format (refer to\n', 'tf.io.parse_example', ' for details).'], ['\n', 'from google.protobuf import text_format', '\n', ""example = text_format.Parse('''"", '\n', '  features {', '\n', '    feature {key: ""my_feature""', '\n', '             value {int64_list {value: [1, 2, 3, 4]} } }', '\n', ""  }''',"", '\n', '  tf.train.Example())', '\n'], ['Use ', 'tf.io.parse_example', ' to extract tensors from a serialized ', 'Example', ' proto:']]","from google.protobuf import text_format
example = text_format.Parse('''
  features {
    feature {key: ""my_feature""
             value {int64_list {value: [1, 2, 3, 4]} } }
  }''',
  tf.train.Example())"
"tf.train.ExponentialMovingAverage(
    decay,
    num_updates=None,
    zero_debias=False,
    name='ExponentialMovingAverage'
)
",[],"# Create variables.
var0 = tf.Variable(...)
var1 = tf.Variable(...)
# ... use the variables to build a training model...

# Create an ExponentialMovingAverage object
ema = tf.train.ExponentialMovingAverage(decay=0.9999)

# The first `apply` creates the shadow variables that hold the moving averages
ema.apply([var0, var1])

# grab the moving averages for checkpointing purposes or to be able to
# load the moving averages into the model weights
averages = [ema.average(var0), ema.average(var1)]

...
def train_step(...):
...
  # Apply the optimizer.
  opt.minimize(my_loss, [var0, var1])

  # Update the moving averages
  # of var0 and var1 with additional calls to `apply`
  ema.apply([var0, var1])

...train the model by running train_step multiple times...
"
tf.io.parse_example(,"[['Used in ', 'tf.train.Example', ' protos. Contains a list of values.'], ['An ', 'Example', ' proto is a representation of the following python type:'], ['This proto implements the ', 'Union', '.'], ['\n', 'tf.train.BytesList', '\n', 'tf.train.FloatList', '\n', 'tf.train.Int64List', '\n'], ['\n', 'int_feature = tf.train.Feature(', '\n', '    int64_list=tf.train.Int64List(value=[1, 2, 3, 4]))', '\n', 'float_feature = tf.train.Feature(', '\n', '    float_list=tf.train.FloatList(value=[1., 2., 3., 4.]))', '\n', 'bytes_feature = tf.train.Feature(', '\n', '    bytes_list=tf.train.BytesList(value=[b""abc"", b""1234""]))', '\n', None, '\n', 'example = tf.train.Example(', '\n', '    features=tf.train.Features(feature={', '\n', ""        'my_ints': int_feature,"", '\n', ""        'my_floats': float_feature,"", '\n', ""        'my_bytes': bytes_feature,"", '\n', '    }))', '\n'], ['Use ', 'tf.io.parse_example', ' to extract tensors from a serialized ', 'Example', ' proto:']]","int_feature = tf.train.Feature(
    int64_list=tf.train.Int64List(value=[1, 2, 3, 4]))
float_feature = tf.train.Feature(
    float_list=tf.train.FloatList(value=[1., 2., 3., 4.]))
bytes_feature = tf.train.Feature(
    bytes_list=tf.train.BytesList(value=[b""abc"", b""1234""]))
example = tf.train.Example(
    features=tf.train.Features(feature={
        'my_ints': int_feature,
        'my_floats': float_feature,
        'my_bytes': bytes_feature,
    }))"
tf.io.parse_example(,"[['Used in ', 'tf.train.Example', ' protos. Contains the mapping from keys to ', 'Feature', '.'], ['An ', 'Example', ' proto is a representation of the following python type:'], ['This proto implements the ', 'Dict', '.'], ['\n', 'int_feature = tf.train.Feature(', '\n', '    int64_list=tf.train.Int64List(value=[1, 2, 3, 4]))', '\n', 'float_feature = tf.train.Feature(', '\n', '    float_list=tf.train.FloatList(value=[1., 2., 3., 4.]))', '\n', 'bytes_feature = tf.train.Feature(', '\n', '    bytes_list=tf.train.BytesList(value=[b""abc"", b""1234""]))', '\n', None, '\n', 'example = tf.train.Example(', '\n', '    features=tf.train.Features(feature={', '\n', ""        'my_ints': int_feature,"", '\n', ""        'my_floats': float_feature,"", '\n', ""        'my_bytes': bytes_feature,"", '\n', '    }))', '\n'], ['Use ', 'tf.io.parse_example', ' to extract tensors from a serialized ', 'Example', ' proto:']]","int_feature = tf.train.Feature(
    int64_list=tf.train.Int64List(value=[1, 2, 3, 4]))
float_feature = tf.train.Feature(
    float_list=tf.train.FloatList(value=[1., 2., 3., 4.]))
bytes_feature = tf.train.Feature(
    bytes_list=tf.train.BytesList(value=[b""abc"", b""1234""]))
example = tf.train.Example(
    features=tf.train.Features(feature={
        'my_ints': int_feature,
        'my_floats': float_feature,
        'my_bytes': bytes_feature,
    }))"
"tf.estimator.FeedFnHook(
    feed_fn
)
","[['Runs ', 'feed_fn', ' and sets the ', 'feed_dict', ' accordingly.'], ['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.estimator.FinalOpsHook(
    final_ops, final_ops_feed_dict=None
)
","[['A hook which evaluates ', 'Tensors', ' at the end of a session.'], ['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
tf.io.parse_example(,"[['Used in ', 'tf.train.Example', ' protos. Holds a list of floats.'], ['An ', 'Example', ' proto is a representation of the following python type:'], ['This proto implements the ', 'List[float]', ' portion.'], ['\n', 'from google.protobuf import text_format', '\n', ""example = text_format.Parse('''"", '\n', '  features {', '\n', '    feature {key: ""my_feature""', '\n', '             value {float_list {value: [1., 2., 3., 4. ]} } }', '\n', ""  }''',"", '\n', '  tf.train.Example())', '\n', None, '\n', ""example.features.feature['my_feature'].float_list.value"", '\n', '[1.0, 2.0, 3.0, 4.0]', '\n'], ['Use ', 'tf.io.parse_example', ' to extract tensors from a serialized ', 'Example', ' proto:']]","from google.protobuf import text_format
example = text_format.Parse('''
  features {
    feature {key: ""my_feature""
             value {float_list {value: [1., 2., 3., 4. ]} } }
  }''',
  tf.train.Example())
example.features.feature['my_feature'].float_list.value
[1.0, 2.0, 3.0, 4.0]"
"tf.compat.v1.train.FtrlOptimizer(
    learning_rate,
    learning_rate_power=-0.5,
    initial_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    use_locking=False,
    name='Ftrl',
    accum_name=None,
    linear_name=None,
    l2_shrinkage_regularization_strength=0.0,
    beta=None
)
","[['Inherits From: ', 'Optimizer']]","apply_gradients(
    grads_and_vars, global_step=None, name=None
)
"
"tf.estimator.GlobalStepWaiterHook(
    wait_until_step
)
","[['Delays execution until global step reaches ', 'wait_until_step', '.'], ['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.compat.v1.train.GradientDescentOptimizer(
    learning_rate, use_locking=False, name='GradientDescent'
)
","[['Inherits From: ', 'Optimizer']]","apply_gradients(
    grads_and_vars, global_step=None, name=None
)
"
tf.io.parse_example(,"[['Used in ', 'tf.train.Example', ' protos. Holds a list of Int64s.'], ['An ', 'Example', ' proto is a representation of the following python type:'], ['This proto implements the ', 'List[int64]', ' portion.'], ['\n', 'from google.protobuf import text_format', '\n', ""example = text_format.Parse('''"", '\n', '  features {', '\n', '    feature {key: ""my_feature""', '\n', '             value {int64_list {value: [1, 2, 3, 4]} } }', '\n', ""  }''',"", '\n', '  tf.train.Example())', '\n', None, '\n', ""example.features.feature['my_feature'].int64_list.value"", '\n', '[1, 2, 3, 4]', '\n'], ['Use ', 'tf.io.parse_example', ' to extract tensors from a serialized ', 'Example', ' proto:']]","from google.protobuf import text_format
example = text_format.Parse('''
  features {
    feature {key: ""my_feature""
             value {int64_list {value: [1, 2, 3, 4]} } }
  }''',
  tf.train.Example())
example.features.feature['my_feature'].int64_list.value
[1, 2, 3, 4]"
"tf.estimator.LoggingTensorHook(
    tensors, every_n_iter=None, every_n_secs=None, at_end=False, formatter=None
)
","[['Inherits From: ', 'SessionRunHook']]",[]
"tf.compat.v1.train.LooperThread(
    coord, timer_interval_secs, target=None, args=None, kwargs=None
)
",[],"getName()
"
"tf.compat.v1.train.MomentumOptimizer(
    learning_rate,
    momentum,
    use_locking=False,
    name='Momentum',
    use_nesterov=False
)
","[['Inherits From: ', 'Optimizer']]","optimizer = tf.compat.v1.train.MomentumOptimizer(
  learning_rate=learning_rate,
  momentum=momentum,
  use_nesterov=use_nesterov)
"
"tf.compat.v1.train.MonitoredSession(
    session_creator=None, hooks=None, stop_grace_period_secs=120
)
",[],[]
"tf.compat.v1.train.MonitoredSession.StepContext(
    session, run_with_hooks_fn
)
","[['Control flow instrument for the ', 'step_fn', ' from ', 'run_step_fn()', '.']]","request_stop()
"
"tf.compat.v1.train.MonitoredTrainingSession(
    master='',
    is_chief=True,
    checkpoint_dir=None,
    scaffold=None,
    hooks=None,
    chief_only_hooks=None,
    save_checkpoint_secs=USE_DEFAULT,
    save_summaries_steps=USE_DEFAULT,
    save_summaries_secs=USE_DEFAULT,
    config=None,
    stop_grace_period_secs=120,
    log_step_count_steps=100,
    max_wait_secs=7200,
    save_checkpoint_steps=USE_DEFAULT,
    summary_dir=None,
    save_graph_def=True
)
","[['Creates a ', 'MonitoredSession', ' for training.']]",[]
"tf.estimator.NanLossDuringTrainingError(
    *args, **kwargs
)
",[],[]
"tf.estimator.NanTensorHook(
    loss_tensor, fail_on_nan_loss=True
)
","[['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.compat.v1.train.NewCheckpointReader(
    filepattern
)
",[],[]
"tf.compat.v1.train.Optimizer(
    use_locking, name
)
",[],"sgd_op = tf.compat.v1.train.GradientDescentOptimizer(3.0)
opt_op = sgd_op.minimize(cost, global_step, [var0, var1])
opt_op.run(session=session)
"
"tf.estimator.ProfilerHook(
    save_steps=None,
    save_secs=None,
    output_dir='',
    show_dataflow=True,
    show_memory=False
)
","[['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.compat.v1.train.ProximalAdagradOptimizer(
    learning_rate,
    initial_accumulator_value=0.1,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    use_locking=False,
    name='ProximalAdagrad'
)
","[['Inherits From: ', 'Optimizer']]","apply_gradients(
    grads_and_vars, global_step=None, name=None
)
"
"tf.compat.v1.train.ProximalGradientDescentOptimizer(
    learning_rate,
    l1_regularization_strength=0.0,
    l2_regularization_strength=0.0,
    use_locking=False,
    name='ProximalGradientDescent'
)
","[['Inherits From: ', 'Optimizer']]","apply_gradients(
    grads_and_vars, global_step=None, name=None
)
"
"tf.compat.v1.train.QueueRunner(
    queue=None,
    enqueue_ops=None,
    close_op=None,
    cancel_op=None,
    queue_closed_exception_types=None,
    queue_runner_def=None,
    import_scope=None
)
",[],[]
"tf.compat.v1.train.RMSPropOptimizer(
    learning_rate,
    decay=0.9,
    momentum=0.0,
    epsilon=1e-10,
    use_locking=False,
    centered=False,
    name='RMSProp'
)
","[['Inherits From: ', 'Optimizer']]","optimizer = tf.compat.v1.train.RMSPropOptimizer(
  learning_rate=learning_rate,
  decay=decay,
  momentum=momentum,
  epsilon=epsilon)
"
"tf.compat.v1.train.Saver(
    var_list=None,
    reshape=False,
    sharded=False,
    max_to_keep=5,
    keep_checkpoint_every_n_hours=10000.0,
    name=None,
    restore_sequentially=False,
    saver_def=None,
    builder=None,
    defer_build=False,
    allow_empty=False,
    write_version=saver_pb2.SaverDef.V2,
    pad_step_number=False,
    save_relative_paths=False,
    filename=None
)
",[],"{
    'sequential/dense/bias': model.variables[0],
    'sequential/dense/kernel': model.variables[1]
}
"
"tf.compat.v1.train.Scaffold(
    init_op=None,
    init_feed_dict=None,
    init_fn=None,
    ready_op=None,
    ready_for_local_init_op=None,
    local_init_op=None,
    summary_op=None,
    saver=None,
    copy_from_scaffold=None,
    local_init_feed_dict=None
)
",[],"@staticmethod
default_local_init_op()
"
"tf.estimator.SecondOrStepTimer(
    every_secs=None, every_steps=None
)
",[],"last_triggered_step()
"
"tf.distribute.Server(
    server_or_cluster_def,
    job_name=None,
    task_index=None,
    protocol=None,
    config=None,
    start=True
)
",[],"server = tf.distribute.Server(...)
with tf.compat.v1.Session(server.target):
  # ...
"
"tf.compat.v1.train.SessionManager(
    local_init_op=None,
    ready_op=None,
    ready_for_local_init_op=None,
    graph=None,
    recovery_wait_secs=30,
    local_init_run_options=None,
    local_init_feed_dict=None
)
",[],"with tf.Graph().as_default():
   ...add operations to the graph...
  # Create a SessionManager that will checkpoint the model in '/tmp/mydir'.
  sm = SessionManager()
  sess = sm.prepare_session(master, init_op, saver, checkpoint_dir)
  # Use the session to train the graph.
  while True:
    sess.run(<my_train_op>)
"
"tf.estimator.SessionRunArgs(
    fetches, feed_dict=None, options=None
)
","[['Represents arguments to be added to a ', 'Session.run()', ' call.']]",[]
"tf.estimator.SessionRunContext(
    original_args, session
)
","[['Provides information about the ', 'session.run()', ' call being made.']]","request_stop()
"
"tf.estimator.SessionRunValues(
    results, options, run_metadata
)
","[['Contains the results of ', 'Session.run()', '.']]",[]
"tf.compat.v1.train.SingularMonitoredSession(
    hooks=None,
    scaffold=None,
    master='',
    config=None,
    checkpoint_dir=None,
    stop_grace_period_secs=120,
    checkpoint_filename_with_path=None
)
",[],[]
"tf.compat.v1.train.MonitoredSession.StepContext(
    session, run_with_hooks_fn
)
","[['Control flow instrument for the ', 'step_fn', ' from ', 'run_step_fn()', '.']]","request_stop()
"
"tf.estimator.StepCounterHook(
    every_n_steps=100, every_n_secs=None, output_dir=None, summary_writer=None
)
","[['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.estimator.StopAtStepHook(
    num_steps=None, last_step=None
)
","[['Inherits From: ', 'SessionRunHook']]",[]
"tf.estimator.SummarySaverHook(
    save_steps=None,
    save_secs=None,
    output_dir=None,
    summary_writer=None,
    scaffold=None,
    summary_op=None
)
","[['Inherits From: ', 'SessionRunHook']]","after_create_session(
    session, coord
)
"
"tf.compat.v1.train.Supervisor(
    graph=None,
    ready_op=USE_DEFAULT,
    ready_for_local_init_op=USE_DEFAULT,
    is_chief=True,
    init_op=USE_DEFAULT,
    init_feed_dict=None,
    local_init_op=USE_DEFAULT,
    logdir=None,
    summary_op=USE_DEFAULT,
    saver=USE_DEFAULT,
    global_step=USE_DEFAULT,
    save_summaries_secs=120,
    save_model_secs=600,
    recovery_wait_secs=30,
    stop_grace_secs=120,
    checkpoint_basename='model.ckpt',
    session_manager=None,
    summary_writer=USE_DEFAULT,
    init_fn=None,
    local_init_run_options=None
)
",[],"with tf.Graph().as_default():
  ...add operations to the graph...
  # Create a Supervisor that will checkpoint the model in '/tmp/mydir'.
  sv = Supervisor(logdir='/tmp/mydir')
  # Get a TensorFlow session managed by the supervisor.
  with sv.managed_session(FLAGS.master) as sess:
    # Use the session to train the graph.
    while not sv.should_stop():
      sess.run(<my_train_op>)
"
"tf.compat.v1.train.SyncReplicasOptimizer(
    opt,
    replicas_to_aggregate,
    total_num_replicas=None,
    variable_averages=None,
    variables_to_average=None,
    use_locking=False,
    name='sync_replicas'
)
","[['Inherits From: ', 'Optimizer']]","# Create any optimizer to update the variables, say a simple SGD:
opt = GradientDescentOptimizer(learning_rate=0.1)

# Wrap the optimizer with sync_replicas_optimizer with 50 replicas: at each
# step the optimizer collects 50 gradients before applying to variables.
# Note that if you want to have 2 backup replicas, you can change
# total_num_replicas=52 and make sure this number matches how many physical
# replicas you started in your job.
opt = tf.compat.v1.train.SyncReplicasOptimizer(opt, replicas_to_aggregate=50,
                               total_num_replicas=50)

# Some models have startup_delays to help stabilize the model but when using
# sync_replicas training, set it to 0.

# Now you can call `minimize()` or `compute_gradients()` and
# `apply_gradients()` normally
training_op = opt.minimize(total_loss, global_step=self.global_step)


# You can create the hook which handles initialization and queues.
sync_replicas_hook = opt.make_session_run_hook(is_chief)
"
"tf.compat.v1.train.WorkerSessionCreator(
    scaffold=None, master='', config=None, max_wait_secs=(30 * 60)
)
","[['Inherits From: ', 'SessionCreator']]","create_session()
"
"tf.compat.v1.train.add_queue_runner(
    qr, collection=ops.GraphKeys.QUEUE_RUNNERS
)
","[['Adds a ', 'QueueRunner', ' to a collection in the graph. (deprecated)']]",[]
"tf.compat.v1.train.assert_global_step(
    global_step_tensor
)
","[['Asserts ', 'global_step_tensor', ' is a scalar int ', 'Variable', ' or ', 'Tensor', '.']]",[]
"tf.compat.v1.train.basic_train_loop(
    supervisor, train_step_fn, args=None, kwargs=None, master=''
)
",[],"train_step_fn(session, *args, **kwargs)
"
"tf.compat.v1.train.batch(
    tensors,
    batch_size,
    num_threads=1,
    capacity=32,
    enqueue_many=False,
    shapes=None,
    dynamic_pad=False,
    allow_smaller_final_batch=False,
    shared_name=None,
    name=None
)
","[['Creates batches of tensors in ', 'tensors', '. (deprecated)']]",[]
"tf.compat.v1.train.batch_join(
    tensors_list,
    batch_size,
    capacity=32,
    enqueue_many=False,
    shapes=None,
    dynamic_pad=False,
    allow_smaller_final_batch=False,
    shared_name=None,
    name=None
)
",[],[]
"tf.compat.v1.train.checkpoint_exists(
    checkpoint_prefix
)
",[],[]
"tf.train.checkpoints_iterator(
    checkpoint_dir, min_interval_secs=0, timeout=None, timeout_fn=None
)
",[],[]
"tf.compat.v1.train.cosine_decay(
    learning_rate, global_step, decay_steps, alpha=0.0, name=None
)
",[],"global_step = min(global_step, decay_steps)
cosine_decay = 0.5 * (1 + cos(pi * global_step / decay_steps))
decayed = (1 - alpha) * cosine_decay + alpha
decayed_learning_rate = learning_rate * decayed
"
"tf.compat.v1.train.cosine_decay_restarts(
    learning_rate,
    global_step,
    first_decay_steps,
    t_mul=2.0,
    m_mul=1.0,
    alpha=0.0,
    name=None
)
",[],"first_decay_steps = 1000
lr_decayed = cosine_decay_restarts(learning_rate, global_step,
                                   first_decay_steps)
"
"tf.compat.v1.train.create_global_step(
    graph=None
)
",[],"def compute_loss(x):
  v = tf.Variable(3.0)
  y = x * v
  loss = x * 5 - x * v
  return loss, [v]"
"tf.compat.v1.train.do_quantize_training_on_graphdef(
    input_graph, num_bits
)
","[['A general quantization scheme is being developed in ', 'tf.contrib.quantize', '. (deprecated)']]",[]
"tf.compat.v1.mixed_precision.DynamicLossScale(
    initial_loss_scale=(2 ** 15), increment_period=2000, multiplier=2.0
)
","[['Inherits From: ', 'LossScale']]","@classmethod
from_config(
    config
)
"
"tf.compat.v1.mixed_precision.FixedLossScale(
    loss_scale_value
)
","[['Inherits From: ', 'LossScale']]","@classmethod
from_config(
    config
)
"
"tf.compat.v1.mixed_precision.MixedPrecisionLossScaleOptimizer(
    opt, loss_scale
)
","[['Inherits From: ', 'Optimizer']]","loss = ...
loss *= loss_scale
grads = gradients(loss, vars)
grads /= loss_scale
"
"tf.compat.v1.mixed_precision.enable_mixed_precision_graph_rewrite(
    opt, loss_scale='dynamic'
)
",[],"model = tf.keras.models.Sequential([
    tf.keras.layers.Dense(64, activation='relu'),
    tf.keras.layers.Dense(64, activation='softmax'),
])

opt = tf.keras.optimizers.SGD()
opt = tf.train.experimental.enable_mixed_precision_graph_rewrite(opt)
model.compile(loss=""mse"", optimizer=opt)

x_train = np.random.random((1024, 64))
y_train = np.random.random((1024, 64))
model.fit(x_train, y_train)
"
"tf.compat.v1.train.exponential_decay(
    learning_rate,
    global_step,
    decay_steps,
    decay_rate,
    staircase=False,
    name=None
)
",[],"decayed_learning_rate = learning_rate *
                        decay_rate ^ (global_step / decay_steps)
"
"tf.compat.v1.train.export_meta_graph(
    filename=None,
    meta_info_def=None,
    graph_def=None,
    saver_def=None,
    collection_list=None,
    as_text=False,
    graph=None,
    export_scope=None,
    clear_devices=False,
    clear_extraneous_savers=False,
    strip_default_attrs=False,
    save_debug_info=False,
    **kwargs
)
","[['Returns ', 'MetaGraphDef', ' proto.']]",[]
"tf.compat.v1.train.generate_checkpoint_state_proto(
    save_dir,
    model_checkpoint_path,
    all_model_checkpoint_paths=None,
    all_model_checkpoint_timestamps=None,
    last_preserved_timestamp=None
)
",[],[]
"tf.compat.v1.train.get_checkpoint_mtimes(
    checkpoint_prefixes
)
",[],[]
"tf.train.get_checkpoint_state(
    checkpoint_dir, latest_filename=None
)
",[],[]
"tf.compat.v1.train.get_global_step(
    graph=None
)
",[],"def compute_loss(x):
  v = tf.Variable(3.0)
  y = x * v
  loss = x * 5 - x * v
  return loss, [v]"
"tf.compat.v1.train.get_or_create_global_step(
    graph=None
)
",[],"def compute_loss(x):
  v = tf.Variable(3.0)
  y = x * v
  loss = x * 5 - x * v
  return loss, [v]"
"tf.compat.v1.train.global_step(
    sess, global_step_tensor
)
",[],"# Create a variable to hold the global_step.
global_step_tensor = tf.Variable(10, trainable=False, name='global_step')
# Create a session.
sess = tf.compat.v1.Session()
# Initialize the variable
sess.run(global_step_tensor.initializer)
# Get the variable value.
print('global_step: %s' % tf.compat.v1.train.global_step(sess,
global_step_tensor))

global_step: 10
"
"tf.compat.v1.train.import_meta_graph(
    meta_graph_or_file, clear_devices=False, import_scope=None, **kwargs
)
","[['Recreates a Graph saved in a ', 'MetaGraphDef', ' proto.']]","...
# Create a saver.
saver = tf.compat.v1.train.Saver(...variables...)
# Remember the training_op we want to run by adding it to a collection.
tf.compat.v1.add_to_collection('train_op', train_op)
sess = tf.compat.v1.Session()
for step in range(1000000):
    sess.run(train_op)
    if step % 1000 == 0:
        # Saves checkpoint, which by default also exports a meta_graph
        # named 'my-model-global_step.meta'.
        saver.save(sess, 'my-model', global_step=step)
"
"tf.compat.v1.train.init_from_checkpoint(
    ckpt_dir_or_file, assignment_map
)
","[['Replaces ', 'tf.Variable', ' initializers so they load from a checkpoint file.']]","{
    'sequential/dense/bias': model.variables[0],
    'sequential/dense/kernel': model.variables[1]
}
"
"tf.compat.v1.train.input_producer(
    input_tensor,
    element_shape=None,
    num_epochs=None,
    shuffle=True,
    seed=None,
    capacity=32,
    shared_name=None,
    summary_name=None,
    name=None,
    cancel_op=None
)
","[['Output the rows of ', 'input_tensor', ' to a queue for an input pipeline. (deprecated)']]",[]
"tf.compat.v1.train.inverse_time_decay(
    learning_rate,
    global_step,
    decay_steps,
    decay_rate,
    staircase=False,
    name=None
)
",[],"decayed_learning_rate = learning_rate / (1 + decay_rate * global_step /
decay_step)
"
"tf.train.latest_checkpoint(
    checkpoint_dir, latest_filename=None
)
",[],[]
"tf.compat.v1.train.limit_epochs(
    tensor, num_epochs=None, name=None
)
","[['Returns tensor ', 'num_epochs', ' times and then raises an ', 'OutOfRange', ' error. (deprecated)']]",[]
"tf.compat.v1.train.linear_cosine_decay(
    learning_rate,
    global_step,
    decay_steps,
    num_periods=0.5,
    alpha=0.0,
    beta=0.001,
    name=None
)
",[],"global_step = min(global_step, decay_steps)
linear_decay = (decay_steps - global_step) / decay_steps)
cosine_decay = 0.5 * (
    1 + cos(pi * 2 * num_periods * global_step / decay_steps))
decayed = (alpha + linear_decay) * cosine_decay + beta
decayed_learning_rate = learning_rate * decayed
"
"tf.train.list_variables(
    ckpt_dir_or_file
)
",[],"</td>
</tr>

</table>


import tensorflow as tf
import os
ckpt_directory = ""/tmp/training_checkpoints/ckpt""
ckpt = tf.train.Checkpoint(optimizer=optimizer, model=model)
manager = tf.train.CheckpointManager(ckpt, ckpt_directory, max_to_keep=3)
train_and_checkpoint(model, manager)
tf.train.list_variables(manager.latest_checkpoint)
"
"tf.train.load_checkpoint(
    ckpt_dir_or_file
)
","[['Returns ', 'CheckpointReader', ' for checkpoint found in ', 'ckpt_dir_or_file', '.']]",[]
"tf.train.load_variable(
    ckpt_dir_or_file, name
)
",[],[]
"tf.io.match_filenames_once(
    pattern, name=None
)
",[],[]
"tf.compat.v1.train.maybe_batch(
    tensors,
    keep_input,
    batch_size,
    num_threads=1,
    capacity=32,
    enqueue_many=False,
    shapes=None,
    dynamic_pad=False,
    allow_smaller_final_batch=False,
    shared_name=None,
    name=None
)
","[['Conditionally creates batches of tensors based on ', 'keep_input', '. (deprecated)']]",[]
"tf.compat.v1.train.maybe_batch_join(
    tensors_list,
    keep_input,
    batch_size,
    capacity=32,
    enqueue_many=False,
    shapes=None,
    dynamic_pad=False,
    allow_smaller_final_batch=False,
    shared_name=None,
    name=None
)
",[],[]
"tf.compat.v1.train.maybe_shuffle_batch(
    tensors,
    batch_size,
    capacity,
    min_after_dequeue,
    keep_input,
    num_threads=1,
    seed=None,
    enqueue_many=False,
    shapes=None,
    allow_smaller_final_batch=False,
    shared_name=None,
    name=None
)
",[],[]
"tf.compat.v1.train.maybe_shuffle_batch_join(
    tensors_list,
    batch_size,
    capacity,
    min_after_dequeue,
    keep_input,
    seed=None,
    enqueue_many=False,
    shapes=None,
    allow_smaller_final_batch=False,
    shared_name=None,
    name=None
)
",[],[]
"tf.compat.v1.train.natural_exp_decay(
    learning_rate,
    global_step,
    decay_steps,
    decay_rate,
    staircase=False,
    name=None
)
",[],"decayed_learning_rate = learning_rate * exp(-decay_rate * global_step /
decay_step)
"
"tf.compat.v1.train.noisy_linear_cosine_decay(
    learning_rate,
    global_step,
    decay_steps,
    initial_variance=1.0,
    variance_decay=0.55,
    num_periods=0.5,
    alpha=0.0,
    beta=0.001,
    name=None
)
",[],"global_step = min(global_step, decay_steps)
linear_decay = (decay_steps - global_step) / decay_steps)
cosine_decay = 0.5 * (
    1 + cos(pi * 2 * num_periods * global_step / decay_steps))
decayed = (alpha + linear_decay + eps_t) * cosine_decay + beta
decayed_learning_rate = learning_rate * decayed
"
"tf.compat.v1.train.piecewise_constant(
    x, boundaries, values, name=None
)
",[],"global_step = tf.Variable(0, trainable=False)
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate = tf.compat.v1.train.piecewise_constant(global_step, boundaries,
values)

# Later, whenever we perform an optimization step, we increment global_step.
"
"tf.compat.v1.train.piecewise_constant(
    x, boundaries, values, name=None
)
",[],"global_step = tf.Variable(0, trainable=False)
boundaries = [100000, 110000]
values = [1.0, 0.5, 0.1]
learning_rate = tf.compat.v1.train.piecewise_constant(global_step, boundaries,
values)

# Later, whenever we perform an optimization step, we increment global_step.
"
"tf.compat.v1.train.polynomial_decay(
    learning_rate,
    global_step,
    decay_steps,
    end_learning_rate=0.0001,
    power=1.0,
    cycle=False,
    name=None
)
",[],"global_step = min(global_step, decay_steps)
decayed_learning_rate = (learning_rate - end_learning_rate) *
                        (1 - global_step / decay_steps) ^ (power) +
                        end_learning_rate

"
"tf.compat.v1.train.QueueRunner(
    queue=None,
    enqueue_ops=None,
    close_op=None,
    cancel_op=None,
    queue_closed_exception_types=None,
    queue_runner_def=None,
    import_scope=None
)
",[],[]
"tf.compat.v1.train.add_queue_runner(
    qr, collection=ops.GraphKeys.QUEUE_RUNNERS
)
","[['Adds a ', 'QueueRunner', ' to a collection in the graph. (deprecated)']]",[]
"tf.compat.v1.train.start_queue_runners(
    sess=None,
    coord=None,
    daemon=True,
    start=True,
    collection=ops.GraphKeys.QUEUE_RUNNERS
)
",[],[]
"tf.compat.v1.train.range_input_producer(
    limit,
    num_epochs=None,
    shuffle=True,
    seed=None,
    capacity=32,
    shared_name=None,
    name=None
)
",[],[]
"tf.compat.v1.train.remove_checkpoint(
    checkpoint_prefix,
    checkpoint_format_version=saver_pb2.SaverDef.V2,
    meta_graph_suffix='meta'
)
","[['Removes a checkpoint given by ', 'checkpoint_prefix', '. (deprecated)']]",[]
"tf.compat.v1.train.replica_device_setter(
    ps_tasks=0,
    ps_device='/job:ps',
    worker_device='/job:worker',
    merge_devices=True,
    cluster=None,
    ps_ops=None,
    ps_strategy=None
)
","[['Return a ', 'device function', ' to use when building a Graph for replicas.']]","# To build a cluster with two ps jobs on hosts ps0 and ps1, and 3 worker
# jobs on hosts worker0, worker1 and worker2.
cluster_spec = {
    ""ps"": [""ps0:2222"", ""ps1:2222""],
    ""worker"": [""worker0:2222"", ""worker1:2222"", ""worker2:2222""]}
with
tf.compat.v1.device(tf.compat.v1.train.replica_device_setter(cluster=cluster_spec)):
  # Build your graph
  v1 = tf.Variable(...)  # assigned to /job:ps/task:0
  v2 = tf.Variable(...)  # assigned to /job:ps/task:1
  v3 = tf.Variable(...)  # assigned to /job:ps/task:0
# Run compute
"
"tf.compat.v1.train.sdca_fprint(
    input, name=None
)
",[],[]
"tf.compat.v1.train.sdca_optimizer(
    sparse_example_indices,
    sparse_feature_indices,
    sparse_feature_values,
    dense_features,
    example_weights,
    example_labels,
    sparse_indices,
    sparse_weights,
    dense_weights,
    example_state_data,
    loss_type,
    l1,
    l2,
    num_loss_partitions,
    num_inner_iterations,
    adaptative=True,
    name=None
)
","[[None, '\n']]",[]
"tf.compat.v1.train.sdca_shrink_l1(
    weights, l1, l2, name=None
)
",[],[]
"tf.compat.v1.train.shuffle_batch(
    tensors,
    batch_size,
    capacity,
    min_after_dequeue,
    num_threads=1,
    seed=None,
    enqueue_many=False,
    shapes=None,
    allow_smaller_final_batch=False,
    shared_name=None,
    name=None
)
",[],"# Creates batches of 32 images and 32 labels.
image_batch, label_batch = tf.compat.v1.train.shuffle_batch(
      [single_image, single_label],
      batch_size=32,
      num_threads=4,
      capacity=50000,
      min_after_dequeue=10000)
"
"tf.compat.v1.train.shuffle_batch_join(
    tensors_list,
    batch_size,
    capacity,
    min_after_dequeue,
    seed=None,
    enqueue_many=False,
    shapes=None,
    allow_smaller_final_batch=False,
    shared_name=None,
    name=None
)
",[],[]
"tf.compat.v1.train.slice_input_producer(
    tensor_list,
    num_epochs=None,
    shuffle=True,
    seed=None,
    capacity=32,
    shared_name=None,
    name=None
)
","[['Produces a slice of each ', 'Tensor', ' in ', 'tensor_list', '. (deprecated)']]",[]
"tf.compat.v1.train.start_queue_runners(
    sess=None,
    coord=None,
    daemon=True,
    start=True,
    collection=ops.GraphKeys.QUEUE_RUNNERS
)
",[],[]
"tf.compat.v1.train.string_input_producer(
    string_tensor,
    num_epochs=None,
    shuffle=True,
    seed=None,
    capacity=32,
    shared_name=None,
    name=None,
    cancel_op=None
)
",[],[]
"tf.compat.v1.train.summary_iterator(
    path
)
","[['Returns a iterator for reading ', 'Event', ' protocol buffers from an event file.']]","for e in tf.compat.v1.train.summary_iterator(path to events file):
    print(e)
"
"tf.compat.v1.train.update_checkpoint_state(
    save_dir,
    model_checkpoint_path,
    all_model_checkpoint_paths=None,
    latest_filename=None,
    all_model_checkpoint_timestamps=None,
    last_preserved_timestamp=None
)
",[],[]
"tf.compat.v1.train.warm_start(
    ckpt_to_initialize_from,
    vars_to_warm_start='.*',
    var_name_to_vocab_info=None,
    var_name_to_prev_var_name=None
)
",[],[]
"tf.io.write_graph(
    graph_or_graph_def, logdir, name, as_text=True
)
",[],"v = tf.Variable(0, name='my_variable')
sess = tf.compat.v1.Session()
tf.io.write_graph(sess.graph_def, '/tmp/my-model', 'train.pbtxt')
"
"tf.compat.v1.trainable_variables(
    scope=None
)
","[['Returns all variables created with ', 'trainable=True', '.']]",[]
"tf.compat.v1.transpose(
    a, perm=None, name='transpose', conjugate=False
)
","[['Transposes ', 'a', '.']]","x = tf.constant([[1, 2, 3], [4, 5, 6]])
tf.transpose(x)  # [[1, 4]
                 #  [2, 5]
                 #  [3, 6]]

# Equivalently
tf.transpose(x, perm=[1, 0])  # [[1, 4]
                              #  [2, 5]
                              #  [3, 6]]

# If x is complex, setting conjugate=True gives the conjugate transpose
x = tf.constant([[1 + 1j, 2 + 2j, 3 + 3j],
                 [4 + 4j, 5 + 5j, 6 + 6j]])
tf.transpose(x, conjugate=True)  # [[1 - 1j, 4 - 4j],
                                 #  [2 - 2j, 5 - 5j],
                                 #  [3 - 3j, 6 - 6j]]

# 'perm' is more useful for n-dimensional tensors, for n > 2
x = tf.constant([[[ 1,  2,  3],
                  [ 4,  5,  6]],
                 [[ 7,  8,  9],
                  [10, 11, 12]]])

# Take the transpose of the matrices in dimension-0
# (this common operation has a shorthand `linalg.matrix_transpose`)
tf.transpose(x, perm=[0, 2, 1])  # [[[1,  4],
                                 #   [2,  5],
                                 #   [3,  6]],
                                 #  [[7, 10],
                                 #   [8, 11],
                                 #   [9, 12]]]
"
"tf.math.truediv(
    x, y, name=None
)
",[],[]
"tf.random.truncated_normal(
    shape,
    mean=0.0,
    stddev=1.0,
    dtype=tf.dtypes.float32,
    seed=None,
    name=None
)
",[],"tf.random.truncated_normal(shape=[2])
<tf.Tensor: shape=(2,), dtype=float32, numpy=array([..., ...], dtype=float32)>"
"tf.compat.v1.truncated_normal_initializer(
    mean=0.0,
    stddev=1.0,
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"initializer = tf.compat.v1.truncated_normal_initializer(
  mean=mean,
  stddev=stddev,
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.truncatediv(
    x, y, name=None
)
",[],[]
"tf.truncatemod(
    x, y, name=None
)
",[],[]
"tf.compat.v1.tuple(
    tensors, name=None, control_inputs=None
)
",[],[]
"tf.type_spec_from_value(
    value
) -> tf.TypeSpec
","[['Returns a ', 'tf.TypeSpec', ' that represents the given ', 'value', '.']]",">>> tf.type_spec_from_value(tf.constant([1, 2, 3]))
TensorSpec(shape=(3,), dtype=tf.int32, name=None)
>>> tf.type_spec_from_value(np.array([4.0, 5.0], np.float64))
TensorSpec(shape=(2,), dtype=tf.float64, name=None)
>>> tf.type_spec_from_value(tf.ragged.constant([[1, 2], [3, 4, 5]]))
RaggedTensorSpec(TensorShape([2, None]), tf.int32, 1, tf.int64)
"
"tf.compat.v1.uniform_unit_scaling_initializer(
    factor=1.0,
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"[-sqrt(3) / sqrt(dim), sqrt(3) / sqrt(dim)]
"
"tf.unique(
    x,
    out_idx=tf.dtypes.int32,
    name=None
)
",[],"# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]
y, idx = unique(x)
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
"
"tf.unique_with_counts(
    x,
    out_idx=tf.dtypes.int32,
    name=None
)
",[],"# tensor 'x' is [1, 1, 2, 4, 4, 4, 7, 8, 8]
y, idx, count = unique_with_counts(x)
y ==> [1, 2, 4, 7, 8]
idx ==> [0, 0, 1, 2, 2, 2, 3, 4, 4]
count ==> [2, 1, 3, 1, 2]
"
"tf.unravel_index(
    indices, dims, name=None
)
",[],"y = tf.unravel_index(indices=[2, 5, 7], dims=[3, 3])
# 'dims' represent a hypothetical (3, 3) tensor of indices:
# [[0, 1, *2*],
#  [3, 4, *5*],
#  [6, *7*, 8]]
# For each entry from 'indices', this operation returns
# its coordinates (marked with '*'), such as
# 2 ==> (0, 2)
# 5 ==> (1, 2)
# 7 ==> (2, 1)
y ==> [[0, 1, 2], [2, 2, 1]]
"
"tf.math.unsorted_segment_max(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_max(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[4, 3, 3, 4],
       [5,  6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_mean(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]",[]
"tf.math.unsorted_segment_min(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_min(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[1, 2, 2, 1],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_prod(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = tf.constant([[1,2,3,4], [5,6,7,8], [4,3,2,1]])
tf.math.unsorted_segment_prod(c, tf.constant([0, 1, 0]), num_segments=2).numpy()
array([[4, 6, 6, 4],
       [5, 6, 7, 8]], dtype=int32)"
"tf.math.unsorted_segment_sqrt_n(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]",[]
"tf.math.unsorted_segment_sum(
    data, segment_ids, num_segments, name=None
)
","[[None, '\n']]","c = [[1,2,3,4], [5,6,7,8], [4,3,2,1]]
tf.math.unsorted_segment_sum(c, [0, 1, 0], num_segments=2).numpy()
array([[5, 5, 5, 5],
       [5, 6, 7, 8]], dtype=int32)"
"tf.unstack(
    value, num=None, axis=0, name='unstack'
)
","[['Unpacks the given dimension of a rank-', 'R', ' tensor into rank-', '(R-1)', ' tensors.']]","x = tf.reshape(tf.range(12), (3,4))
p, q, r = tf.unstack(x)
p.shape.as_list()
[4]"
"tf.compat.v1.variable_axis_size_partitioner(
    max_shard_bytes, axis=0, bytes_per_string_element=16, max_shards=None
)
","[['Get a partitioner for VariableScope to keep shards below ', 'max_shard_bytes', '.']]",[]
"tf.compat.v1.variable_scope(
    name_or_scope,
    default_name=None,
    values=None,
    initializer=None,
    regularizer=None,
    caching_device=None,
    partitioner=None,
    custom_getter=None,
    reuse=None,
    dtype=None,
    use_resource=None,
    constraint=None,
    auxiliary_name_scope=True
)
",[],[]
"tf.compat.v1.variables_initializer(
    var_list, name='init'
)
",[],[]
"tf.compat.v1.keras.initializers.VarianceScaling(
    scale=1.0,
    mode='fan_in',
    distribution='truncated_normal',
    seed=None,
    dtype=tf.dtypes.float32
)
",[],"initializer = tf.compat.v1.variance_scaling_initializer(
  scale=scale,
  mode=mode,
  distribution=distribution
  seed=seed,
  dtype=dtype)

weight_one = tf.Variable(initializer(shape_one))
weight_two = tf.Variable(initializer(shape_two))
"
"tf.vectorized_map(
    fn, elems, fallback_to_while_loop=True, warn=True
)
","[['Parallel map on the list of tensors unpacked from ', 'elems', ' on dimension 0.']]","def outer_product(a):
  return tf.tensordot(a, a, 0)

batch_size = 100
a = tf.ones((batch_size, 32, 32))
c = tf.vectorized_map(outer_product, a)
assert c.shape == (batch_size, 32, 32, 32, 32)
"
"tf.compat.v1.verify_tensor_all_finite(
    t=None, msg=None, name=None, x=None, message=None
)
",[],[]
"tf.compat.v1.where(
    condition, x=None, y=None, name=None
)
","[['Return the elements, either from ', 'x', ' or ', 'y', ', depending on the ', 'condition', '.']]","tf.where([True, False, False, True], [1,2,3,4], [100])
<tf.Tensor: shape=(4,), dtype=int32, numpy=array([  1, 100, 100,   4],
dtype=int32)>"
"tf.where(
    condition, x=None, y=None, name=None
)
","[['Returns the indices of non-zero elements, or multiplexes ', 'x', ' and ', 'y', '.']]","tf.where([True, False, False, True]).numpy()
array([[0],
       [3]])"
"tf.compat.v1.while_loop(
    cond,
    body,
    loop_vars,
    shape_invariants=None,
    parallel_iterations=10,
    back_prop=True,
    swap_memory=False,
    name=None,
    maximum_iterations=None,
    return_same_structure=False
)
","[['Repeat ', 'body', ' while the condition ', 'cond', ' is true.']]","i = tf.constant(0)
c = lambda i: tf.less(i, 10)
b = lambda i: tf.add(i, 1)
r = tf.while_loop(c, b, [i])
"
"tf.compat.v1.wrap_function(
    fn, signature, name=None
)
",[],"def f(x, do_add):
  v = tf.Variable(5.0)
  if do_add:
    op = v.assign_add(x)
  else:
    op = v.assign_sub(x)
  with tf.control_dependencies([op]):
    return v.read_value()

f_add = tf.compat.v1.wrap_function(f, [tf.TensorSpec((), tf.float32), True])

assert float(f_add(1.0)) == 6.0
assert float(f_add(1.0)) == 7.0

# Can call tf.compat.v1.wrap_function again to get a new trace, a new set
# of variables, and possibly different non-template arguments.
f_sub= tf.compat.v1.wrap_function(f, [tf.TensorSpec((), tf.float32), False])

assert float(f_sub(1.0)) == 4.0
assert float(f_sub(1.0)) == 3.0
"
"tf.io.write_file(
    filename, contents, name=None
)
","[['Writes ', 'contents', ' to the file at input ', 'filename', '.']]",[]
"tf.zeros(
    shape,
    dtype=tf.dtypes.float32,
    name=None
)
",[],"tf.zeros([3, 4], tf.int32)
<tf.Tensor: shape=(3, 4), dtype=int32, numpy=
array([[0, 0, 0, 0],
       [0, 0, 0, 0],
       [0, 0, 0, 0]], dtype=int32)>"
"tf.compat.v1.keras.initializers.Zeros(
    dtype=tf.dtypes.float32
)
",[],"initializer = tf.compat.v1.zeros_initializer(dtype=tf.float32)
variable = tf.Variable(initializer(shape=[3, 3]))
"
"tf.compat.v1.zeros_like(
    tensor, dtype=None, name=None, optimize=True
)
",[],">>> tensor = tf.constant([[1, 2, 3], [4, 5, 6]])
>>> tf.zeros_like(tensor)
<tf.Tensor: shape=(2, 3), dtype=int32, numpy=
array([[0, 0, 0],
       [0, 0, 0]], dtype=int32)>
"
"tf.math.zeta(
    x, q, name=None
)
","[[None, '\n']]",[]
